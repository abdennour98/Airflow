<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3AAI%26id_list%3D%26start%3D0%26max_results%3D1000" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:AI&amp;id_list=&amp;start=0&amp;max_results=1000</title>
  <id>http://arxiv.org/api/XC+Gl34RKtnhH/RFyQno9/ZNfGs</id>
  <updated>2023-03-01T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">12599</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1000</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2211.05075v1</id>
    <updated>2022-11-09T18:07:10Z</updated>
    <published>2022-11-09T18:07:10Z</published>
    <title>Supporting AI/ML Security Workers through an Adversarial Techniques,
  Tools, and Common Knowledge (AI/ML ATT&amp;CK) Framework</title>
    <summary>  This paper focuses on supporting AI/ML Security Workers -- professionals
involved in the development and deployment of secure AI-enabled software
systems. It presents AI/ML Adversarial Techniques, Tools, and Common Knowledge
(AI/ML ATT&amp;CK) framework to enable AI/ML Security Workers intuitively to
explore offensive and defensive tactics.
</summary>
    <author>
      <name>Mohamad Fazelnia</name>
    </author>
    <author>
      <name>Ahmet Okutan</name>
    </author>
    <author>
      <name>Mehdi Mirakhorli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AI/ML ATT&amp;CK</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.05075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.05075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.15294v1</id>
    <updated>2021-03-29T02:57:48Z</updated>
    <published>2021-03-29T02:57:48Z</published>
    <title>"Weak AI" is Likely to Never Become "Strong AI", So What is its Greatest
  Value for us?</title>
    <summary>  AI has surpassed humans across a variety of tasks such as image
classification, playing games (e.g., go, "Starcraft" and poker), and protein
structure prediction. However, at the same time, AI is also bearing serious
controversies. Many researchers argue that little substantial progress has been
made for AI in recent decades. In this paper, the author (1) explains why
controversies about AI exist; (2) discriminates two paradigms of AI research,
termed "weak AI" and "strong AI" (a.k.a. artificial general intelligence); (3)
clarifies how to judge which paradigm a research work should be classified
into; (4) discusses what is the greatest value of "weak AI" if it has no chance
to develop into "strong AI".
</summary>
    <author>
      <name>Bin Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.15294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.00335v2</id>
    <updated>2023-01-31T10:32:13Z</updated>
    <published>2022-06-01T08:55:27Z</published>
    <title>Putting AI Ethics into Practice: The Hourglass Model of Organizational
  AI Governance</title>
    <summary>  The organizational use of artificial intelligence (AI) has rapidly spread
across various sectors. Alongside the awareness of the benefits brought by AI,
there is a growing consensus on the necessity of tackling the risks and
potential harms, such as bias and discrimination, brought about by advanced AI
technologies. A multitude of AI ethics principles have been proposed to tackle
these risks, but the outlines of organizational processes and practices for
ensuring socially responsible AI development are in a nascent state. To address
the paucity of comprehensive governance models, we present an AI governance
framework, the hourglass model of organizational AI governance, which targets
organizations that develop and use AI systems. The framework is designed to
help organizations deploying AI systems translate ethical AI principles into
practice and align their AI systems and processes with the forthcoming European
AI Act. The hourglass framework includes governance requirements at the
environmental, organizational, and AI system levels. At the AI system level, we
connect governance requirements to AI system life cycles to ensure governance
throughout the system's life span. The governance model highlights the systemic
nature of AI governance and opens new research avenues into its practical
implementation, the mechanisms that connect different AI governance layers, and
the dynamics between the AI governance actors. The model also offers a starting
point for organizational decision-makers to consider the governance components
needed to ensure social acceptability, mitigate risks, and realize the
potential of AI.
</summary>
    <author>
      <name>Matti Mäntymäki</name>
    </author>
    <author>
      <name>Matti Minkkinen</name>
    </author>
    <author>
      <name>Teemu Birkstedt</name>
    </author>
    <author>
      <name>Mika Viljanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.00335v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.00335v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.10418v1</id>
    <updated>2019-06-25T09:46:02Z</updated>
    <published>2019-06-25T09:46:02Z</published>
    <title>Towards Enterprise-Ready AI Deployments Minimizing the Risk of Consuming
  AI Models in Business Applications</title>
    <summary>  The stochastic nature of artificial intelligence (AI) models introduces risk
to business applications that use AI models without careful consideration. This
paper offers an approach to use AI techniques to gain insights on the usage of
the AI models and control how they are deployed to a production application.
  Keywords: artificial intelligence (AI), machine learning, microservices,
business process
</summary>
    <author>
      <name>Aleksander Slominski</name>
    </author>
    <author>
      <name>Vinod Muthusamy</name>
    </author>
    <author>
      <name>Vatche Ishakian</name>
    </author>
    <link href="http://arxiv.org/abs/1906.10418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.16168v1</id>
    <updated>2021-03-30T08:49:45Z</updated>
    <published>2021-03-30T08:49:45Z</published>
    <title>Understanding Mental Models of AI through Player-AI Interaction</title>
    <summary>  Designing human-centered AI-driven applications require deep understandings
of how people develop mental models of AI. Currently, we have little knowledge
of this process and limited tools to study it. This paper presents the position
that AI-based games, particularly the player-AI interaction component, offer an
ideal domain to study the process in which mental models evolve. We present a
case study to illustrate the benefits of our approach for explainable AI.
</summary>
    <author>
      <name>Jennifer Villareale</name>
    </author>
    <author>
      <name>Jichen Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2103.16168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.16168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.04073v1</id>
    <updated>2020-07-18T15:31:29Z</updated>
    <published>2020-07-18T15:31:29Z</published>
    <title>AI Failures: A Review of Underlying Issues</title>
    <summary>  Instances of Artificial Intelligence (AI) systems failing to deliver
consistent, satisfactory performance are legion. We investigate why AI failures
occur. We address only a narrow subset of the broader field of AI Safety. We
focus on AI failures on account of flaws in conceptualization, design and
deployment. Other AI Safety issues like trade-offs between privacy and security
or convenience, bad actors hacking into AI systems to create mayhem or bad
actors deploying AI for purposes harmful to humanity and are out of scope of
our discussion. We find that AI systems fail on account of omission and
commission errors in the design of the AI system, as well as upon failure to
develop an appropriate interpretation of input information. Moreover, even when
there is no significant flaw in the AI software, an AI system may fail because
the hardware is incapable of robust performance across environments. Finally an
AI system is quite likely to fail in situations where, in effect, it is called
upon to deliver moral judgments -- a capability AI does not possess. We observe
certain trade-offs in measures to mitigate a subset of AI failures and provide
some recommendations.
</summary>
    <author>
      <name>Debarag Narayan Banerjee</name>
    </author>
    <author>
      <name>Sasanka Sekhar Chanda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.04073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.04073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.08460v4</id>
    <updated>2023-02-18T07:10:04Z</updated>
    <published>2021-11-12T06:11:30Z</published>
    <title>Enabling human-centered AI: A new junction and shared journey between AI
  and HCI communities</title>
    <summary>  Artificial intelligence (AI) has brought benefits, but it may also cause harm
if it is not appropriately developed. Current development is mainly driven by a
"technology-centered" approach, causing many failures. For example, the AI
Incident Database has documented over a thousand AI-related accidents. To
address these challenges, a human-centered AI (HCAI) approach has been promoted
and has received a growing level of acceptance over the last few years. HCAI
calls for combining AI with user experience (UX) design will enable the
development of AI systems (e.g., autonomous vehicles, intelligent user
interfaces, or intelligent decision-making systems) to achieve its design goals
such as usable/explainable AI, human-controlled AI, and ethical AI. While HCAI
promotion continues, it has not specifically addressed the collaboration
between AI and human-computer interaction (HCI) communities, resulting in
uncertainty about what action should be taken by both sides to apply HCAI in
developing AI systems. This Viewpoint focuses on the collaboration between the
AI and HCI communities, which leads to nine recommendations for effective
collaboration to enable HCAI in developing AI systems.
</summary>
    <author>
      <name>Wei Xu</name>
    </author>
    <author>
      <name>Marvin Dainoff</name>
    </author>
    <link href="http://arxiv.org/abs/2111.08460v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.08460v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.00866v2</id>
    <updated>2021-04-27T09:08:18Z</updated>
    <published>2020-02-24T23:38:07Z</published>
    <title>Enabling AI in Future Wireless Networks: A Data Life Cycle Perspective</title>
    <summary>  Recent years have seen rapid deployment of mobile computing and Internet of
Things (IoT) networks, which can be mostly attributed to the increasing
communication and sensing capabilities of wireless systems. Big data analysis,
pervasive computing, and eventually artificial intelligence (AI) are envisaged
to be deployed on top of the IoT and create a new world featured by data-driven
AI. In this context, a novel paradigm of merging AI and wireless
communications, called Wireless AI that pushes AI frontiers to the network
edge, is widely regarded as a key enabler for future intelligent network
evolution. To this end, we present a comprehensive survey of the latest studies
in wireless AI from the data-driven perspective. Specifically, we first propose
a novel Wireless AI architecture that covers five key data-driven AI themes in
wireless networks, including Sensing AI, Network Device AI, Access AI, User
Device AI and Data-provenance AI. Then, for each data-driven AI theme, we
present an overview on the use of AI approaches to solve the emerging
data-related problems and show how AI can empower wireless network
functionalities. Particularly, compared to the other related survey papers, we
provide an in-depth discussion on the Wireless AI applications in various
data-driven domains wherein AI proves extremely useful for wireless network
design and optimization. Finally, research challenges and future visions are
also discussed to spur further research in this promising area.
</summary>
    <author>
      <name>Dinh C. Nguyen</name>
    </author>
    <author>
      <name>Peng Cheng</name>
    </author>
    <author>
      <name>Ming Ding</name>
    </author>
    <author>
      <name>David Lopez-Perez</name>
    </author>
    <author>
      <name>Pubudu N. Pathirana</name>
    </author>
    <author>
      <name>Jun Li</name>
    </author>
    <author>
      <name>Aruna Seneviratne</name>
    </author>
    <author>
      <name>Yonghui Li</name>
    </author>
    <author>
      <name>H. Vincent Poor</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/COMST.2020.3024783</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/COMST.2020.3024783" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the IEEE Communications Surveys &amp; Tutorials, 42 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.00866v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.00866v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.08867v1</id>
    <updated>2021-05-19T00:55:40Z</updated>
    <published>2021-05-19T00:55:40Z</published>
    <title>AI and Ethics -- Operationalising Responsible AI</title>
    <summary>  In the last few years, AI continues demonstrating its positive impact on
society while sometimes with ethically questionable consequences. Building and
maintaining public trust in AI has been identified as the key to successful and
sustainable innovation. This chapter discusses the challenges related to
operationalizing ethical AI principles and presents an integrated view that
covers high-level ethical AI principles, the general notion of
trust/trustworthiness, and product/process support in the context of
responsible AI, which helps improve both trust and trustworthiness of AI for a
wider set of stakeholders.
</summary>
    <author>
      <name>Liming Zhu</name>
    </author>
    <author>
      <name>Xiwei Xu</name>
    </author>
    <author>
      <name>Qinghua Lu</name>
    </author>
    <author>
      <name>Guido Governatori</name>
    </author>
    <author>
      <name>Jon Whittle</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Humanity Driven AI: Productivity, Wellbeing, Sustainability and
  Partnership, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2105.08867v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.08867v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09534v1</id>
    <updated>2017-09-25T19:14:21Z</updated>
    <published>2017-09-25T19:14:21Z</published>
    <title>Tweeting AI: Perceptions of Lay vs Expert Twitterati</title>
    <summary>  With the recent advancements in Artificial Intelligence (AI), various
organizations and individuals are debating about the progress of AI as a
blessing or a curse for the future of the society. This paper conducts an
investigation on how the public perceives the progress of AI by utilizing the
data shared on Twitter. Specifically, this paper performs a comparative
analysis on the understanding of users belonging to two categories -- general
AI-Tweeters (AIT) and expert AI-Tweeters (EAIT) who share posts about AI on
Twitter. Our analysis revealed that users from both the categories express
distinct emotions and interests towards AI. Users from both the categories
regard AI as positive and are optimistic about the progress of AI but the
experts are more negative than the general AI-Tweeters. Expert AI-Tweeters
share relatively large percentage of tweets about their personal news compared
to technical aspects of AI. However, the effects of automation on the future
are of primary concern to AIT than to EAIT. When the expert category is
sub-categorized, the emotion analysis revealed that students and industry
professionals have more insights in their tweets about AI than academicians.
</summary>
    <author>
      <name>Lydia Manikonda</name>
    </author>
    <author>
      <name>Subbarao Kambhampati</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1704.08389</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.09534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.10672v2</id>
    <updated>2022-06-26T20:52:22Z</updated>
    <published>2020-11-20T22:31:37Z</published>
    <title>AI Governance for Businesses</title>
    <summary>  Artificial Intelligence (AI) governance regulates the exercise of authority
and control over the management of AI. It aims at leveraging AI through
effective use of data and minimization of AI-related cost and risk. While
topics such as AI governance and AI ethics are thoroughly discussed on a
theoretical, philosophical, societal and regulatory level, there is limited
work on AI governance targeted to companies and corporations. This work views
AI products as systems, where key functionality is delivered by machine
learning (ML) models leveraging (training) data. We derive a conceptual
framework by synthesizing literature on AI and related fields such as ML. Our
framework decomposes AI governance into governance of data, (ML) models and
(AI) systems along four dimensions. It relates to existing IT and data
governance frameworks and practices. It can be adopted by practitioners and
academics alike. For practitioners the synthesis of mainly research papers, but
also practitioner publications and publications of regulatory bodies provides a
valuable starting point to implement AI governance, while for academics the
paper highlights a number of areas of AI governance that deserve more
attention.
</summary>
    <author>
      <name>Johannes Schneider</name>
    </author>
    <author>
      <name>Rene Abraham</name>
    </author>
    <author>
      <name>Christian Meske</name>
    </author>
    <author>
      <name>Jan vom Brocke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/10580530.2022.2085825</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/10580530.2022.2085825" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Systems Management, 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2011.10672v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.10672v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08389v2</id>
    <updated>2017-09-28T02:06:08Z</updated>
    <published>2017-04-27T00:37:05Z</published>
    <title>Tweeting AI: Perceptions of AI-Tweeters (AIT) vs Expert AI-Tweeters
  (EAIT)</title>
    <summary>  With the recent advancements in Artificial Intelligence (AI), various
organizations and individuals started debating about the progress of AI as a
blessing or a curse for the future of the society. This paper conducts an
investigation on how the public perceives the progress of AI by utilizing the
data shared on Twitter. Specifically, this paper performs a comparative
analysis on the understanding of users from two categories -- general
AI-Tweeters (AIT) and the expert AI-Tweeters (EAIT) who share posts about AI on
Twitter. Our analysis revealed that users from both the categories express
distinct emotions and interests towards AI. Users from both the categories
regard AI as positive and are optimistic about the progress of AI but the
experts are more negative than the general AI-Tweeters. Characterization of
users manifested that `London' is the popular location of users from where they
tweet about AI. Tweets posted by AIT are highly retweeted than posts made by
EAIT that reveals greater diffusion of information from AIT.
</summary>
    <author>
      <name>Lydia Manikonda</name>
    </author>
    <author>
      <name>Cameron Dudley</name>
    </author>
    <author>
      <name>Subbarao Kambhampati</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">New results at arXiv:1709.09534</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08389v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08389v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.06220v2</id>
    <updated>2021-01-18T10:25:19Z</updated>
    <published>2021-01-15T17:07:03Z</published>
    <title>Player-AI Interaction: What Neural Network Games Reveal About AI as Play</title>
    <summary>  The advent of artificial intelligence (AI) and machine learning (ML) bring
human-AI interaction to the forefront of HCI research. This paper argues that
games are an ideal domain for studying and experimenting with how humans
interact with AI. Through a systematic survey of neural network games (n = 38),
we identified the dominant interaction metaphors and AI interaction patterns in
these games. In addition, we applied existing human-AI interaction guidelines
to further shed light on player-AI interaction in the context of AI-infused
systems. Our core finding is that AI as play can expand current notions of
human-AI interaction, which are predominantly productivity-based. In
particular, our work suggests that game and UX designers should consider flow
to structure the learning curve of human-AI interaction, incorporate
discovery-based learning to play around with the AI and observe the
consequences, and offer users an invitation to play to explore new forms of
human-AI interaction.
</summary>
    <author>
      <name>Jichen Zhu</name>
    </author>
    <author>
      <name>Jennifer Villareale</name>
    </author>
    <author>
      <name>Nithesh Javvaji</name>
    </author>
    <author>
      <name>Sebastian Risi</name>
    </author>
    <author>
      <name>Mathias Löwe</name>
    </author>
    <author>
      <name>Rush Weigelt</name>
    </author>
    <author>
      <name>Casper Harteveld</name>
    </author>
    <link href="http://arxiv.org/abs/2101.06220v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.06220v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.08984v1</id>
    <updated>2022-09-28T22:23:10Z</updated>
    <published>2022-09-28T22:23:10Z</published>
    <title>AI Governance and Ethics Framework for Sustainable AI and Sustainability</title>
    <summary>  AI is transforming the existing technology landscape at a rapid phase
enabling data-informed decision making and autonomous decision making. Unlike
any other technology, because of the decision-making ability of AI, ethics and
governance became a key concern. There are many emerging AI risks for humanity,
such as autonomous weapons, automation-spurred job loss, socio-economic
inequality, bias caused by data and algorithms, privacy violations and
deepfakes. Social diversity, equity and inclusion are considered key success
factors of AI to mitigate risks, create values and drive social justice.
Sustainability became a broad and complex topic entangled with AI. Many
organizations (government, corporate, not-for-profits, charities and NGOs) have
diversified strategies driving AI for business optimization and
social-and-environmental justice. Partnerships and collaborations become
important more than ever for equity and inclusion of diversified and
distributed people, data and capabilities. Therefore, in our journey towards an
AI-enabled sustainable future, we need to address AI ethics and governance as a
priority. These AI ethics and governance should be underpinned by human ethics.
</summary>
    <author>
      <name>Mahendra Samarawickrama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Attribution: M. Samarawickrama, AI Governance and Ethics Framework
  for Sustainable AI and Sustainability, Submission in response to the
  Department of the Prime Minister and Cabinet issues paper Positioning
  Australia as a leader in digital economy regulation - Automated Decision
  Making and AI Regulation, Apr. 2022, ISBN: 978-0-6454693-0-1</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.08984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.08984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.05957v1</id>
    <updated>2022-02-12T02:26:46Z</updated>
    <published>2022-02-12T02:26:46Z</published>
    <title>Confident AI</title>
    <summary>  In this paper, we propose "Confident AI" as a means to designing Artificial
Intelligence (AI) and Machine Learning (ML) systems with both algorithm and
user confidence in model predictions and reported results. The 4 basic tenets
of Confident AI are Repeatability, Believability, Sufficiency, and
Adaptability. Each of the tenets is used to explore fundamental issues in
current AI/ML systems and together provide an overall approach to Confident AI.
</summary>
    <author>
      <name>Jim Davis</name>
    </author>
    <link href="http://arxiv.org/abs/2202.05957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.05957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05122v1</id>
    <updated>2017-08-17T03:27:53Z</updated>
    <published>2017-08-17T03:27:53Z</published>
    <title>Evaluating Visual Conversational Agents via Cooperative Human-AI Games</title>
    <summary>  As AI continues to advance, human-AI teams are inevitable. However, progress
in AI is routinely measured in isolation, without a human in the loop. It is
crucial to benchmark progress in AI, not just in isolation, but also in terms
of how it translates to helping humans perform certain tasks, i.e., the
performance of human-AI teams.
  In this work, we design a cooperative game - GuessWhich - to measure human-AI
team performance in the specific context of the AI being a visual
conversational agent. GuessWhich involves live interaction between the human
and the AI. The AI, which we call ALICE, is provided an image which is unseen
by the human. Following a brief description of the image, the human questions
ALICE about this secret image to identify it from a fixed pool of images.
  We measure performance of the human-ALICE team by the number of guesses it
takes the human to correctly identify the secret image after a fixed number of
dialog rounds with ALICE. We compare performance of the human-ALICE teams for
two versions of ALICE. Our human studies suggest a counterintuitive trend -
that while AI literature shows that one version outperforms the other when
paired with an AI questioner bot, we find that this improvement in AI-AI
performance does not translate to improved human-AI performance. This suggests
a mismatch between benchmarking of AI in isolation and in the context of
human-AI teams.
</summary>
    <author>
      <name>Prithvijit Chattopadhyay</name>
    </author>
    <author>
      <name>Deshraj Yadav</name>
    </author>
    <author>
      <name>Viraj Prabhu</name>
    </author>
    <author>
      <name>Arjun Chandrasekaran</name>
    </author>
    <author>
      <name>Abhishek Das</name>
    </author>
    <author>
      <name>Stefan Lee</name>
    </author>
    <author>
      <name>Dhruv Batra</name>
    </author>
    <author>
      <name>Devi Parikh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">HCOMP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.05122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.09319v1</id>
    <updated>2023-02-18T12:35:55Z</updated>
    <published>2023-02-18T12:35:55Z</published>
    <title>MAILS -- Meta AI Literacy Scale: Development and Testing of an AI
  Literacy Questionnaire Based on Well-Founded Competency Models and
  Psychological Change- and Meta-Competencies</title>
    <summary>  The goal of the present paper is to develop and validate a questionnaire to
assess AI literacy. In particular, the questionnaire should be deeply grounded
in the existing literature on AI literacy, should be modular (i.e., including
different facets that can be used independently of each other) to be flexibly
applicable in professional life depending on the goals and use cases, and
should meet psychological requirements and thus includes further psychological
competencies in addition to the typical facets of AIL. We derived 60 items to
represent different facets of AI Literacy according to Ng and colleagues
conceptualisation of AI literacy and additional 12 items to represent
psychological competencies such as problem solving, learning, and emotion
regulation in regard to AI. For this purpose, data were collected online from
300 German-speaking adults. The items were tested for factorial structure in
confirmatory factor analyses. The result is a measurement instrument that
measures AI literacy with the facets Use &amp; apply AI, Understand AI, Detect AI,
and AI Ethics and the ability to Create AI as a separate construct, and AI
Self-efficacy in learning and problem solving and AI Self-management. This
study contributes to the research on AI literacy by providing a measurement
instrument relying on profound competency models. In addition, higher-order
psychological competencies are included that are particularly important in the
context of pervasive change through AI systems.
</summary>
    <author>
      <name>Astrid Carolus</name>
    </author>
    <author>
      <name>Martin Koch</name>
    </author>
    <author>
      <name>Samantha Straka</name>
    </author>
    <author>
      <name>Marc Erich Latoschik</name>
    </author>
    <author>
      <name>Carolin Wienrich</name>
    </author>
    <link href="http://arxiv.org/abs/2302.09319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.09319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.11117v1</id>
    <updated>2022-01-26T18:53:09Z</updated>
    <published>2022-01-26T18:53:09Z</published>
    <title>Cybertrust: From Explainable to Actionable and Interpretable AI (AI2)</title>
    <summary>  To benefit from AI advances, users and operators of AI systems must have
reason to trust it. Trust arises from multiple interactions, where predictable
and desirable behavior is reinforced over time. Providing the system's users
with some understanding of AI operations can support predictability, but
forcing AI to explain itself risks constraining AI capabilities to only those
reconcilable with human cognition. We argue that AI systems should be designed
with features that build trust by bringing decision-analytic perspectives and
formal tools into AI. Instead of trying to achieve explainable AI, we should
develop interpretable and actionable AI. Actionable and Interpretable AI (AI2)
will incorporate explicit quantifications and visualizations of user confidence
in AI recommendations. In doing so, it will allow examining and testing of AI
system predictions to establish a basis for trust in the systems' decision
making and ensure broad benefits from deploying and advancing its computational
capabilities.
</summary>
    <author>
      <name>Stephanie Galaitsi</name>
    </author>
    <author>
      <name>Benjamin D. Trump</name>
    </author>
    <author>
      <name>Jeffrey M. Keisler</name>
    </author>
    <author>
      <name>Igor Linkov</name>
    </author>
    <author>
      <name>Alexander Kott</name>
    </author>
    <link href="http://arxiv.org/abs/2201.11117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.11117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.01635v1</id>
    <updated>2022-12-03T15:31:34Z</updated>
    <published>2022-12-03T15:31:34Z</published>
    <title>AI-driven Mobile Apps: an Explorative Study</title>
    <summary>  Recent years have witnessed an astonishing explosion in the evolution of
mobile applications powered by AI technologies. The rapid growth of AI
frameworks enables the transition of AI technologies to mobile devices,
significantly prompting the adoption of AI apps (i.e., apps that integrate AI
into their functions) among smartphone devices. In this paper, we conduct the
most extensive empirical study on 56,682 published AI apps from three
perspectives: dataset characteristics, development issues, and user feedback
and privacy. To this end, we build an automated AI app identification tool, AI
Discriminator, that detects eligible AI apps from 7,259,232 mobile apps. First,
we carry out a dataset analysis, where we explore the AndroZoo large repository
to identify AI apps and their core characteristics. Subsequently, we pinpoint
key issues in AI app development (e.g., model protection). Finally, we focus on
user reviews and user privacy protection. Our paper provides several notable
findings. Some essential ones involve revealing the issue of insufficient model
protection by presenting the lack of model encryption, and demonstrating the
risk of user privacy data being leaked. We published our large-scale AI app
datasets to inspire more future research.
</summary>
    <author>
      <name>Yinghua Li</name>
    </author>
    <author>
      <name>Xueqi Dang</name>
    </author>
    <author>
      <name>Haoye Tian</name>
    </author>
    <author>
      <name>Tiezhu Sun</name>
    </author>
    <author>
      <name>Zhijie Wang</name>
    </author>
    <author>
      <name>Lei Ma</name>
    </author>
    <author>
      <name>Jacques Klein</name>
    </author>
    <author>
      <name>Tegawende F. Bissyande</name>
    </author>
    <link href="http://arxiv.org/abs/2212.01635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.01635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.06352v1</id>
    <updated>2022-12-13T03:10:31Z</updated>
    <published>2022-12-13T03:10:31Z</published>
    <title>Towards Seamless Management of AI Models in High-Performance Computing</title>
    <summary>  With the increasing prevalence of artificial intelligence (AI) in diverse
science/engineering communities, AI models emerge on an unprecedented scale
among various domains. However, given the complexity and diversity of the
software and hardware environments, reusing AI artifacts (models and datasets)
is extremely challenging, especially with AI-driven science applications.
Building an ecosystem to run and reuse AI applications/datasets at scale
efficiently becomes increasingly essential for diverse science and engineering
and high-performance computing (HPC) communities. In this paper, we innovate
over an HPC-AI ecosystem -- HPCFair, which enables the Findable, Accessible,
Interoperable, and Reproducible (FAIR) principles. HPCFair enables the
collection of AI models/datasets allowing users to download/upload AI artifacts
with authentications. Most importantly, our proposed framework provides
user-friendly APIs for users to easily run inference jobs and customize AI
artifacts to their tasks as needed. Our results show that, with HPCFair API,
users irrespective of technical expertise in AI, can easily leverage AI
artifacts to their tasks with minimal effort.
</summary>
    <author>
      <name>Sixing Yu</name>
    </author>
    <author>
      <name>Murali Emani</name>
    </author>
    <author>
      <name>Chunhua Liao</name>
    </author>
    <author>
      <name>Pei-Hung Lin</name>
    </author>
    <author>
      <name>Tristan Vanderbruggen</name>
    </author>
    <author>
      <name>Xipeng Shen</name>
    </author>
    <author>
      <name>Ali Jannesari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 2nd Annual AAAI Workshop on AI to Accelerate Science
  and Engineering (AI2ASE)</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.06352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.06352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.04283v1</id>
    <updated>2021-01-12T03:57:16Z</updated>
    <published>2021-01-12T03:57:16Z</published>
    <title>A Brief Survey of Associations Between Meta-Learning and General AI</title>
    <summary>  This paper briefly reviews the history of meta-learning and describes its
contribution to general AI. Meta-learning improves model generalization
capacity and devises general algorithms applicable to both in-distribution and
out-of-distribution tasks potentially. General AI replaces task-specific models
with general algorithmic systems introducing higher level of automation in
solving diverse tasks using AI. We summarize main contributions of
meta-learning to the developments in general AI, including memory module,
meta-learner, coevolution, curiosity, forgetting and AI-generating algorithm.
We present connections between meta-learning and general AI and discuss how
meta-learning can be used to formulate general AI algorithms.
</summary>
    <author>
      <name>Huimin Peng</name>
    </author>
    <link href="http://arxiv.org/abs/2101.04283v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.04283v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.08830v1</id>
    <updated>2021-11-16T23:24:31Z</updated>
    <published>2021-11-16T23:24:31Z</published>
    <title>How Mock Model Training Enhances User Perceptions of AI Systems</title>
    <summary>  Artificial Intelligence (AI) is an integral part of our daily technology use
and will likely be a critical component of emerging technologies. However,
negative user preconceptions may hinder adoption of AI-based decision making.
Prior work has highlighted the potential of factors such as transparency and
explainability in improving user perceptions of AI. We further contribute to
work on improving user perceptions of AI by demonstrating that bringing the
user in the loop through mock model training can improve their perceptions of
an AI agent's capability and their comfort with the possibility of using
technology employing the AI agent.
</summary>
    <author>
      <name>Amama Mahmood</name>
    </author>
    <author>
      <name>Gopika Ajaykumar</name>
    </author>
    <author>
      <name>Chien-Ming Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Human Centered AI (HCAI) workshop at NeurIPS (2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.08830v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.08830v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.01659v1</id>
    <updated>2022-01-05T15:33:33Z</updated>
    <published>2022-01-05T15:33:33Z</published>
    <title>From the Ground Truth Up: Doing AI Ethics from Practice to Principles</title>
    <summary>  Recent AI ethics has focused on applying abstract principles downward to
practice. This paper moves in the other direction. Ethical insights are
generated from the lived experiences of AI-designers working on tangible human
problems, and then cycled upward to influence theoretical debates surrounding
these questions: 1) Should AI as trustworthy be sought through explainability,
or accurate performance? 2) Should AI be considered trustworthy at all, or is
reliability a preferable aim? 3) Should AI ethics be oriented toward
establishing protections for users, or toward catalyzing innovation? Specific
answers are less significant than the larger demonstration that AI ethics is
currently unbalanced toward theoretical principles, and will benefit from
increased exposure to grounded practices and dilemmas.
</summary>
    <author>
      <name>James Brusseau</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00146-021-01336-4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00146-021-01336-4" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AI &amp; Soc (2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2201.01659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.01659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.04221v1</id>
    <updated>2021-01-22T22:01:30Z</updated>
    <published>2021-01-22T22:01:30Z</published>
    <title>The Sanction of Authority: Promoting Public Trust in AI</title>
    <summary>  Trusted AI literature to date has focused on the trust needs of users who
knowingly interact with discrete AIs. Conspicuously absent from the literature
is a rigorous treatment of public trust in AI. We argue that public distrust of
AI originates from the under-development of a regulatory ecosystem that would
guarantee the trustworthiness of the AIs that pervade society. Drawing from
structuration theory and literature on institutional trust, we offer a model of
public trust in AI that differs starkly from models driving Trusted AI efforts.
This model provides a theoretical scaffolding for Trusted AI research which
underscores the need to develop nothing less than a comprehensive and visibly
functioning regulatory ecosystem. We elaborate the pivotal role of externally
auditable AI documentation within this model and the work to be done to ensure
it is effective, and outline a number of actions that would promote public
trust in AI. We discuss how existing efforts to develop AI documentation within
organizations -- both to inform potential adopters of AI components and support
the deliberations of risk and ethics review boards -- is necessary but
insufficient assurance of the trustworthiness of AI. We argue that being
accountable to the public in ways that earn their trust, through elaborating
rules for AI and developing resources for enforcing these rules, is what will
ultimately make AI trustworthy enough to be woven into the fabric of our
society.
</summary>
    <author>
      <name>Bran Knowles</name>
    </author>
    <author>
      <name>John T. Richards</name>
    </author>
    <link href="http://arxiv.org/abs/2102.04221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.04221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.12387v1</id>
    <updated>2021-12-23T07:13:56Z</updated>
    <published>2021-12-23T07:13:56Z</published>
    <title>Human-AI Collaboration for UX Evaluation: Effects of Explanation and
  Synchronization</title>
    <summary>  Analyzing usability test videos is arduous. Although recent research showed
the promise of AI in assisting with such tasks, it remains largely unknown how
AI should be designed to facilitate effective collaboration between user
experience (UX) evaluators and AI. Inspired by the concepts of agency and work
context in human and AI collaboration literature, we studied two corresponding
design factors for AI-assisted UX evaluation: explanations and synchronization.
Explanations allow AI to further inform humans how it identifies UX problems
from a usability test session; synchronization refers to the two ways humans
and AI collaborate: synchronously and asynchronously. We iteratively designed a
tool, AI Assistant, with four versions of UIs corresponding to the two levels
of explanations (with/without) and synchronization (sync/async). By adopting a
hybrid wizard-of-oz approach to simulating an AI with reasonable performance,
we conducted a mixed-method study with 24 UX evaluators identifying UX problems
from usability test videos using AI Assistant. Our quantitative and qualitative
results show that AI with explanations, regardless of being presented
synchronously or asynchronously, provided better support for UX evaluators'
analysis and was perceived more positively; when without explanations,
synchronous AI better improved UX evaluators' performance and engagement
compared to the asynchronous AI. Lastly, we present the design implications for
AI-assisted UX evaluation and facilitating more effective human-AI
collaboration.
</summary>
    <author>
      <name>Mingming Fan</name>
    </author>
    <author>
      <name>Xianyou Yang</name>
    </author>
    <author>
      <name>Tsz Tung Yu</name>
    </author>
    <author>
      <name>Vera Q. Liao</name>
    </author>
    <author>
      <name>Jian Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the ACM on Human-Computer Interaction (PACM HCI),
  CSCW, 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.12387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.12387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.05159v2</id>
    <updated>2022-04-11T16:38:22Z</updated>
    <published>2022-01-13T19:30:16Z</published>
    <title>Structured access: an emerging paradigm for safe AI deployment</title>
    <summary>  Structured access is an emerging paradigm for the safe deployment of
artificial intelligence (AI). Instead of openly disseminating AI systems,
developers facilitate controlled, arm's length interactions with their AI
systems. The aim is to prevent dangerous AI capabilities from being widely
accessible, whilst preserving access to AI capabilities that can be used
safely. The developer must both restrict how the AI system can be used, and
prevent the user from circumventing these restrictions through modification or
reverse engineering of the AI system. Structured access is most effective when
implemented through cloud-based AI services, rather than disseminating AI
software that runs locally on users' hardware. Cloud-based interfaces provide
the AI developer greater scope for controlling how the AI system is used, and
for protecting against unauthorized modifications to the system's design. This
chapter expands the discussion of "publication norms" in the AI community,
which to date has focused on the question of how the informational content of
AI research projects should be disseminated (e.g., code and models). Although
this is an important question, there are limits to what can be achieved through
the control of information flows. Structured access views AI software not only
as information that can be shared but also as a tool with which users can have
arm's length interactions. There are early examples of structured access being
practiced by AI developers, but there is much room for further development,
both in the functionality of cloud-based interfaces and in the wider
institutional framework.
</summary>
    <author>
      <name>Toby Shevlane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.05159v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.05159v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.11207v1</id>
    <updated>2022-12-21T17:28:07Z</updated>
    <published>2022-12-21T17:28:07Z</published>
    <title>A Seven-Layer Model for Standardising AI Fairness Assessment</title>
    <summary>  Problem statement: Standardisation of AI fairness rules and benchmarks is
challenging because AI fairness and other ethical requirements depend on
multiple factors such as context, use case, type of the AI system, and so on.
In this paper, we elaborate that the AI system is prone to biases at every
stage of its lifecycle, from inception to its usage, and that all stages
require due attention for mitigating AI bias. We need a standardised approach
to handle AI fairness at every stage. Gap analysis: While AI fairness is a hot
research topic, a holistic strategy for AI fairness is generally missing. Most
researchers focus only on a few facets of AI model-building. Peer review shows
excessive focus on biases in the datasets, fairness metrics, and algorithmic
bias. In the process, other aspects affecting AI fairness get ignored. The
solution proposed: We propose a comprehensive approach in the form of a novel
seven-layer model, inspired by the Open System Interconnection (OSI) model, to
standardise AI fairness handling. Despite the differences in the various
aspects, most AI systems have similar model-building stages. The proposed model
splits the AI system lifecycle into seven abstraction layers, each
corresponding to a well-defined AI model-building or usage stage. We also
provide checklists for each layer and deliberate on potential sources of bias
in each layer and their mitigation methodologies. This work will facilitate
layer-wise standardisation of AI fairness rules and benchmarking parameters.
</summary>
    <author>
      <name>Avinash Agarwal</name>
    </author>
    <author>
      <name>Harsh Agarwal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.11207v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.11207v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.07255v2</id>
    <updated>2023-01-27T12:57:45Z</updated>
    <published>2023-01-18T01:33:50Z</published>
    <title>Understanding the Role of Human Intuition on Reliance in Human-AI
  Decision-Making with Explanations</title>
    <summary>  AI explanations are often mentioned as a way to improve human-AI
decision-making. Yet, empirical studies have not found consistent evidence of
explanations' effectiveness and, on the contrary, suggest that they can
increase overreliance when the AI system is wrong. While many factors may
affect reliance on AI support, one important factor is how decision-makers
reconcile their own intuition -- which may be based on domain knowledge, prior
task experience, or pattern recognition -- with the information provided by the
AI system to determine when to override AI predictions. We conduct a
think-aloud, mixed-methods study with two explanation types (feature- and
example-based) for two prediction tasks to explore how decision-makers'
intuition affects their use of AI predictions and explanations, and ultimately
their choice of when to rely on AI. Our results identify three types of
intuition involved in reasoning about AI predictions and explanations:
intuition about the task outcome, features, and AI limitations. Building on
these, we summarize three observed pathways for decision-makers to apply their
own intuition and override AI predictions. We use these pathways to explain why
(1) the feature-based explanations we used did not improve participants'
decision outcomes and increased their overreliance on AI, and (2) the
example-based explanations we used improved decision-makers' performance over
feature-based explanations and helped achieve complementary human-AI
performance. Overall, our work identifies directions for further development of
AI decision-support systems and explanation methods that help decision-makers
effectively apply their intuition to achieve appropriate reliance on AI.
</summary>
    <author>
      <name>Valerie Chen</name>
    </author>
    <author>
      <name>Q. Vera Liao</name>
    </author>
    <author>
      <name>Jennifer Wortman Vaughan</name>
    </author>
    <author>
      <name>Gagan Bansal</name>
    </author>
    <link href="http://arxiv.org/abs/2301.07255v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.07255v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.02093v1</id>
    <updated>2020-03-04T14:18:18Z</updated>
    <published>2020-03-04T14:18:18Z</published>
    <title>AI-Mediated Exchange Theory</title>
    <summary>  As Artificial Intelligence (AI) plays an ever-expanding role in
sociotechnical systems, it is important to articulate the relationships between
humans and AI. However, the scholarly communities studying human-AI
relationships -- including but not limited to social computing, machine
learning, science and technology studies, and other social sciences -- are
divided by the perspectives that define them. These perspectives vary both by
their focus on humans or AI, and in the micro/macro lenses through which they
approach subjects. These differences inhibit the integration of findings, and
thus impede science and interdisciplinarity. In this position paper, we propose
the development of a framework AI-Mediated Exchange Theory (AI-MET) to bridge
these divides. As an extension to Social Exchange Theory (SET) in the social
sciences, AI-MET views AI as influencing human-to-human relationships via a
taxonomy of mediation mechanisms. We list initial ideas of these mechanisms,
and show how AI-MET can be used to help human-AI research communities speak to
one another.
</summary>
    <author>
      <name>Xiao Ma</name>
    </author>
    <author>
      <name>Taylor W. Brown</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">For workshop "Human-Centered Approaches to Fair and Responsible AI"</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.02093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.02093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00641v1</id>
    <updated>2019-12-05T21:58:18Z</updated>
    <published>2019-12-05T21:58:18Z</published>
    <title>AI and Medicine</title>
    <summary>  Which part of medicine, if any, can and should be entrusted to AI, now or at
some moment in the future? That both medicine and AI will continue to change
goes without saying.
</summary>
    <author>
      <name>Mihai Nadin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">53 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.00641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.13016v1</id>
    <updated>2020-12-23T22:58:51Z</updated>
    <published>2020-12-23T22:58:51Z</published>
    <title>Antitrust and Artificial Intelligence (AAI): Antitrust Vigilance
  Lifecycle and AI Legal Reasoning Autonomy</title>
    <summary>  There is an increasing interest in the entwining of the field of antitrust
with the field of Artificial Intelligence (AI), frequently referred to jointly
as Antitrust and AI (AAI) in the research literature. This study focuses on the
synergies entangling antitrust and AI, doing so to extend the literature by
proffering the primary ways that these two fields intersect, consisting of: (1)
the application of antitrust to AI, and (2) the application of AI to antitrust.
To date, most of the existing research on this intermixing has concentrated on
the former, namely the application of antitrust to AI, entailing how the
marketplace will be altered by the advent of AI and the potential for adverse
antitrust behaviors arising accordingly. Opting to explore more deeply the
other side of this coin, this research closely examines the application of AI
to antitrust and establishes an antitrust vigilance lifecycle to which AI is
predicted to be substantively infused for purposes of enabling and bolstering
antitrust detection, enforcement, and post-enforcement monitoring. Furthermore,
a gradual and incremental injection of AI into antitrust vigilance is
anticipated to occur as significant advances emerge amidst the Levels of
Autonomy (LoA) for AI Legal Reasoning (AILR).
</summary>
    <author>
      <name>Lance Eliot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 12 figures. arXiv admin note: text overlap with
  arXiv:2010.02726, arXiv:2009.14620</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.13016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.13016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; J.7.0; K.5.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.09586v1</id>
    <updated>2021-09-20T14:41:50Z</updated>
    <published>2021-09-20T14:41:50Z</published>
    <title>Some Critical and Ethical Perspectives on the Empirical Turn of AI
  Interpretability</title>
    <summary>  We consider two fundamental and related issues currently faced by Artificial
Intelligence (AI) development: the lack of ethics and interpretability of AI
decisions. Can interpretable AI decisions help to address ethics in AI? Using a
randomized study, we experimentally show that the empirical and liberal turn of
the production of explanations tends to select AI explanations with a low
denunciatory power. Under certain conditions, interpretability tools are
therefore not means but, paradoxically, obstacles to the production of ethical
AI since they can give the illusion of being sensitive to ethical incidents. We
also show that the denunciatory power of AI explanations is highly dependent on
the context in which the explanation takes place, such as the gender or
education level of the person to whom the explication is intended for. AI
ethics tools are therefore sometimes too flexible and self-regulation through
the liberal production of explanations do not seem to be enough to address
ethical issues. We then propose two scenarios for the future development of
ethical AI: more external regulation or more liberalization of AI explanations.
These two opposite paths will play a major role on the future development of
ethical AI.
</summary>
    <author>
      <name>Jean-Marie John-Mathews</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MMS, LITEM</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2109.09586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.09586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.01167v2</id>
    <updated>2022-05-26T14:20:48Z</updated>
    <published>2021-10-04T03:20:39Z</published>
    <title>Trustworthy AI: From Principles to Practices</title>
    <summary>  The rapid development of Artificial Intelligence (AI) technology has enabled
the deployment of various systems based on it. However, many current AI systems
are found vulnerable to imperceptible attacks, biased against underrepresented
groups, lacking in user privacy protection. These shortcomings degrade user
experience and erode people's trust in all AI systems. In this review, we
provide AI practitioners with a comprehensive guide for building trustworthy AI
systems. We first introduce the theoretical framework of important aspects of
AI trustworthiness, including robustness, generalization, explainability,
transparency, reproducibility, fairness, privacy preservation, and
accountability. To unify currently available but fragmented approaches toward
trustworthy AI, we organize them in a systematic approach that considers the
entire lifecycle of AI systems, ranging from data acquisition to model
development, to system development and deployment, finally to continuous
monitoring and governance. In this framework, we offer concrete action items
for practitioners and societal stakeholders (e.g., researchers, engineers, and
regulators) to improve AI trustworthiness. Finally, we identify key
opportunities and challenges for the future development of trustworthy AI
systems, where we identify the need for a paradigm shift toward comprehensively
trustworthy AI systems.
</summary>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>Peng Qi</name>
    </author>
    <author>
      <name>Bo Liu</name>
    </author>
    <author>
      <name>Shuai Di</name>
    </author>
    <author>
      <name>Jingen Liu</name>
    </author>
    <author>
      <name>Jiquan Pei</name>
    </author>
    <author>
      <name>Jinfeng Yi</name>
    </author>
    <author>
      <name>Bowen Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2110.01167v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.01167v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.01122v1</id>
    <updated>2021-11-01T17:57:04Z</updated>
    <published>2021-11-01T17:57:04Z</published>
    <title>Stakeholder Participation in AI: Beyond "Add Diverse Stakeholders and
  Stir"</title>
    <summary>  There is a growing consensus in HCI and AI research that the design of AI
systems needs to engage and empower stakeholders who will be affected by AI.
However, the manner in which stakeholders should participate in AI design is
unclear. This workshop paper aims to ground what we dub a 'participatory turn'
in AI design by synthesizing existing literature on participation and through
empirical analysis of its current practices via a survey of recent published
research and a dozen semi-structured interviews with AI researchers and
practitioners. Based on our literature synthesis and empirical research, this
paper presents a conceptual framework for analyzing participatory approaches to
AI design and articulates a set of empirical findings that in ensemble detail
out the contemporary landscape of participatory practice in AI design. These
findings can help bootstrap a more principled discussion on how PD of AI should
move forward across AI, HCI, and other research communities.
</summary>
    <author>
      <name>Fernando Delgado</name>
    </author>
    <author>
      <name>Stephen Yang</name>
    </author>
    <author>
      <name>Michael Madaio</name>
    </author>
    <author>
      <name>Qian Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-print of an accepted paper at the Human-Centered AI workshop at
  NeurIPS 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.01122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.01122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01016v1</id>
    <updated>2021-12-02T07:02:27Z</updated>
    <published>2021-12-02T07:02:27Z</published>
    <title>On Two XAI Cultures: A Case Study of Non-technical Explanations in
  Deployed AI System</title>
    <summary>  Explainable AI (XAI) research has been booming, but the question "$\textbf{To
whom}$ are we making AI explainable?" is yet to gain sufficient attention. Not
much of XAI is comprehensible to non-AI experts, who nonetheless, are the
primary audience and major stakeholders of deployed AI systems in practice. The
gap is glaring: what is considered "explained" to AI-experts versus non-experts
are very different in practical scenarios. Hence, this gap produced two
distinct cultures of expectations, goals, and forms of XAI in real-life AI
deployments.
  We advocate that it is critical to develop XAI methods for non-technical
audiences. We then present a real-life case study, where AI experts provided
non-technical explanations of AI decisions to non-technical stakeholders, and
completed a successful deployment in a highly regulated industry. We then
synthesize lessons learned from the case, and share a list of suggestions for
AI experts to consider when explaining AI decisions to non-technical
stakeholders.
</summary>
    <author>
      <name>Helen Jiang</name>
    </author>
    <author>
      <name>Erwen Senge</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the Human-centered AI (HCAI) workshop at NeurIPS 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.01016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.08594v1</id>
    <updated>2022-03-09T07:01:32Z</updated>
    <published>2022-03-09T07:01:32Z</published>
    <title>Towards a Roadmap on Software Engineering for Responsible AI</title>
    <summary>  Although AI is transforming the world, there are serious concerns about its
ability to behave and make decisions responsibly. Many ethical regulations,
principles, and frameworks for responsible AI have been issued recently.
However, they are high level and difficult to put into practice. On the other
hand, most AI researchers focus on algorithmic solutions, while the responsible
AI challenges actually crosscut the entire engineering lifecycle and components
of AI systems. To close the gap in operationalizing responsible AI, this paper
aims to develop a roadmap on software engineering for responsible AI. The
roadmap focuses on (i) establishing multi-level governance for responsible AI
systems, (ii) setting up the development processes incorporating
process-oriented practices for responsible AI systems, and (iii) building
responsible-AI-by-design into AI systems through system-level architectural
style, patterns and techniques.
</summary>
    <author>
      <name>Qinghua Lu</name>
    </author>
    <author>
      <name>Liming Zhu</name>
    </author>
    <author>
      <name>Xiwei Xu</name>
    </author>
    <author>
      <name>Jon Whittle</name>
    </author>
    <author>
      <name>Zhenchang Xing</name>
    </author>
    <link href="http://arxiv.org/abs/2203.08594v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.08594v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.07644v1</id>
    <updated>2022-04-15T20:41:54Z</updated>
    <published>2022-04-15T20:41:54Z</published>
    <title>Identifying Ethical Issues in AI Partners in Human-AI Co-Creation</title>
    <summary>  Human-AI co-creativity involves humans and AI collaborating on a shared
creative product as partners. In many existing co-creative systems, users
communicate with the AI using buttons or sliders. However, typically, the AI in
co-creative systems cannot communicate back to humans, limiting their potential
to be perceived as partners. This paper starts with an overview of a
comparative study with 38 participants to explore the impact of AI-to-human
communication on user perception and engagement in co-creative systems and the
results show improved collaborative experience and user engagement with the
system incorporating AI-to-human communication. The results also demonstrate
that users perceive co-creative AI as more reliable, personal and intelligent
when it can communicate with the users. The results indicate a need to identify
potential ethical issues from an engaging communicating co-creative AI. Later
in the paper, we present some potential ethical issues in human-AI co-creation
and propose to use participatory design fiction as the research methodology to
investigate the ethical issues associated with a co-creative AI that
communicates with users.
</summary>
    <author>
      <name>Jeba Rezwana</name>
    </author>
    <author>
      <name>Mary Lou Maher</name>
    </author>
    <link href="http://arxiv.org/abs/2204.07644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.07644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.03220v1</id>
    <updated>2022-05-09T10:10:47Z</updated>
    <published>2022-05-09T10:10:47Z</published>
    <title>A Transparency Index Framework for AI in Education</title>
    <summary>  Numerous AI ethics checklists and frameworks have been proposed focusing on
different dimensions of ethical AI such as fairness, explainability, and
safety. Yet, no such work has been done on developing transparent AI systems
for real-world educational scenarios. This paper presents a Transparency Index
framework that has been iteratively co-designed with different stakeholders of
AI in education, including educators, ed-tech experts, and AI practitioners. We
map the requirements of transparency for different categories of stakeholders
of AI in education and demonstrate that transparency considerations are
embedded in the entire AI development process from the data collection stage
until the AI system is deployed in the real world and iteratively improved. We
also demonstrate how transparency enables the implementation of other ethical
AI dimensions in Education like interpretability, accountability, and safety.
In conclusion, we discuss the directions for future research in this newly
emerging field. The main contribution of this study is that it highlights the
importance of transparency in developing AI-powered educational technologies
and proposes an index framework for its conceptualization for AI in education.
</summary>
    <author>
      <name>Muhammad Ali Chaudhry</name>
    </author>
    <author>
      <name>Mutlu Cukurova</name>
    </author>
    <author>
      <name>Rose Luckin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 Figures, 2 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.03220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.03220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.07635v2</id>
    <updated>2022-08-18T12:49:54Z</updated>
    <published>2022-06-15T16:25:57Z</published>
    <title>AI Ethics Issues in Real World: Evidence from AI Incident Database</title>
    <summary>  With the powerful performance of Artificial Intelligence (AI) also comes
prevalent ethical issues. Though governments and corporations have curated
multiple AI ethics guidelines to curb unethical behavior of AI, the effect has
been limited, probably due to the vagueness of the guidelines. In this paper,
we take a closer look at how AI ethics issues take place in real world, in
order to have a more in-depth and nuanced understanding of different ethical
issues as well as their social impact. With a content analysis of AI Incident
Database, which is an effort to prevent repeated real world AI failures by
cataloging incidents, we identified 13 application areas which often see
unethical use of AI, with intelligent service robots, language/vision models
and autonomous driving taking the lead. Ethical issues appear in 8 different
forms, from inappropriate use and racial discrimination, to physical safety and
unfair algorithm. With this taxonomy of AI ethics issues, we aim to provide AI
practitioners with a practical guideline when trying to deploy AI applications
ethically.
</summary>
    <author>
      <name>Mengyi Wei</name>
    </author>
    <author>
      <name>Zhixuan Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">56th Hawaii International Conference on System Sciences (HICSS)</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.07635v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.07635v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.00509v1</id>
    <updated>2022-09-01T14:54:15Z</updated>
    <published>2022-09-01T14:54:15Z</published>
    <title>Possibilities and Implications of the Multi-AI Competition</title>
    <summary>  The possibility of super-AIs taking over the world has been intensively
studied by numerous scholars. This paper focuses on the multi-AI competition
scenario under the premise of super-AIs in power. Firstly, the article points
out the defects of existing arguments supporting single-AI domination and
presents arguments in favour of multi-AI competition. Then the article
concludes that the multi-AI competition situation is a non-negligible
possibility. Attention then turns to whether multi-AI competition is better for
the overall good of humanity than a situation where a single AI is in power.
After analysing the best, worst, and intermediate scenarios, the article
concludes that multi-AI competition is better for humanity. Finally,
considering the factors related to the formation of the best-case scenario of
multiple AIs, the article gives some suggestions for current initiatives in AI
development.
</summary>
    <author>
      <name>Jialin Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2209.00509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.00509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.06937v1</id>
    <updated>2023-01-06T00:33:08Z</updated>
    <published>2023-01-06T00:33:08Z</published>
    <title>Improving Human-AI Collaboration With Descriptions of AI Behavior</title>
    <summary>  People work with AI systems to improve their decision making, but often
under- or over-rely on AI predictions and perform worse than they would have
unassisted. To help people appropriately rely on AI aids, we propose showing
them behavior descriptions, details of how AI systems perform on subgroups of
instances. We tested the efficacy of behavior descriptions through user studies
with 225 participants in three distinct domains: fake review detection,
satellite image classification, and bird classification. We found that behavior
descriptions can increase human-AI accuracy through two mechanisms: helping
people identify AI failures and increasing people's reliance on the AI when it
is more accurate. These findings highlight the importance of people's mental
models in human-AI collaboration and show that informing people of high-level
AI behaviors can significantly improve AI-assisted decision making.
</summary>
    <author>
      <name>Ángel Alexander Cabrera</name>
    </author>
    <author>
      <name>Adam Perer</name>
    </author>
    <author>
      <name>Jason I. Hong</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3579612</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3579612" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. ACM Hum.-Comput. Interact. 7, CSCW1, Article 136 (April
  2023)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2301.06937v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.06937v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.07631v2</id>
    <updated>2022-08-18T01:56:59Z</updated>
    <published>2021-11-15T09:35:53Z</published>
    <title>AI in Human-computer Gaming: Techniques, Challenges and Opportunities</title>
    <summary>  With breakthrough of the AlphaGo, human-computer gaming AI has ushered in a
big explosion, attracting more and more researchers all around the world. As a
recognized standard for testing artificial intelligence, various human-computer
gaming AI systems (AIs) have been developed such as the Libratus, OpenAI Five
and AlphaStar, beating professional human players. The rapid development of
human-computer gaming AIs indicate a big step of decision making intelligence,
and it seems that current techniques can handle very complex human-computer
games. So, one natural question raises: what are the possible challenges of
current techniques in human-computer gaming, and what are the future trends? To
answer the above question, in this paper, we survey recent successful game AIs,
covering board game AIs, card game AIs, first-person shooting game AIs and real
time strategy game AIs. Through this survey, we 1) compare the main
difficulties among different kinds of games and the corresponding techniques
utilized for achieving professional human level AIs; 2) summarize the
mainstream frameworks and techniques that can be properly relied on for
developing AIs for complex human-computer gaming; 3) raise the challenges or
drawbacks of current techniques in the successful AIs; and 4) try to point out
future trends in human-computer gaming AIs. Finally, we hope this brief review
can provide an introduction for beginners, and inspire insights for researchers
in the field of AI in human-computer gaming.
</summary>
    <author>
      <name>Qiyue Yin</name>
    </author>
    <author>
      <name>Jun Yang</name>
    </author>
    <author>
      <name>Kaiqi Huang</name>
    </author>
    <author>
      <name>Meijing Zhao</name>
    </author>
    <author>
      <name>Wancheng Ni</name>
    </author>
    <author>
      <name>Bin Liang</name>
    </author>
    <author>
      <name>Yan Huang</name>
    </author>
    <author>
      <name>Shu Wu</name>
    </author>
    <author>
      <name>Liang Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2111.07631v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.07631v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.05983v3</id>
    <updated>2022-10-28T01:43:51Z</updated>
    <published>2022-02-12T04:51:00Z</published>
    <title>Uncalibrated Models Can Improve Human-AI Collaboration</title>
    <summary>  In many practical applications of AI, an AI model is used as a decision aid
for human users. The AI provides advice that a human (sometimes) incorporates
into their decision-making process. The AI advice is often presented with some
measure of "confidence" that the human can use to calibrate how much they
depend on or trust the advice. In this paper, we present an initial exploration
that suggests showing AI models as more confident than they actually are, even
when the original AI is well-calibrated, can improve human-AI performance
(measured as the accuracy and confidence of the human's final prediction after
seeing the AI advice). We first train a model to predict human incorporation of
AI advice using data from thousands of human-AI interactions. This enables us
to explicitly estimate how to transform the AI's prediction confidence, making
the AI uncalibrated, in order to improve the final human prediction. We
empirically validate our results across four different tasks--dealing with
images, text and tabular data--involving hundreds of human participants. We
further support our findings with simulation analysis. Our findings suggest the
importance of jointly optimizing the human-AI system as opposed to the standard
paradigm of optimizing the AI model alone.
</summary>
    <author>
      <name>Kailas Vodrahalli</name>
    </author>
    <author>
      <name>Tobias Gerstenberg</name>
    </author>
    <author>
      <name>James Zou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 12 figures, NeurIPS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.05983v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.05983v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.09514v1</id>
    <updated>2022-06-20T00:28:51Z</updated>
    <published>2022-06-20T00:28:51Z</published>
    <title>Ethics in AI through the Developer's Prism: A Socio-Technical Grounded
  Theory Literature Review and Guidelines</title>
    <summary>  The term 'ethics' is widely used, explored, and debated in the context of
developing Artificial Intelligence (AI) systems. In recent years, there have
been numerous incidents that have raised the profile of ethical issues in AI
development and led to public concerns about the proliferation of AI technology
in our everyday lives. But what do we know about the views and experiences of
those who develop these systems - the AI developers? We conducted a
Socio-Technical Grounded Theory Literature Review (ST-GTLR) of 30 primary
empirical studies that included AI developers' views on ethics in AI to derive
five categories that discuss AI developers' views on AI ethics: developer's
awareness, perception, needs, challenges, and approach. These are underpinned
by multiple codes and concepts that we explain with evidence from the included
studies. Through the steps of advanced theory development, we also derived a
set of relationships between these categories and presented them as five
hypotheses, leading to the 'theory of ethics in AI through the developer's
prism' which explains that developers' awareness of AI ethics directly leads to
their perception about AI ethics and its implementation as well as to
identifying their needs, and indirectly leads to identifying their challenges
and coming up with approaches (applied and potential strategies) to overcome
them. The theory provides a landscape view of the key aspects that concern AI
developers when it comes to ethics in AI. We also share an agenda for future
research studies and recommendations for developers, managers, and
organisations to help in their efforts to better implement ethics in AI.
</summary>
    <author>
      <name>Aastha Pant</name>
    </author>
    <author>
      <name>Rashina Hoda</name>
    </author>
    <author>
      <name>Chakkrit Tantithamthavorn</name>
    </author>
    <author>
      <name>Burak Turhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 5 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.09514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.09514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.11981v1</id>
    <updated>2022-06-23T21:13:10Z</updated>
    <published>2022-06-23T21:13:10Z</published>
    <title>Never trust, always verify : a roadmap for Trustworthy AI?</title>
    <summary>  Artificial Intelligence (AI) is becoming the corner stone of many systems
used in our daily lives such as autonomous vehicles, healthcare systems, and
unmanned aircraft systems. Machine Learning is a field of AI that enables
systems to learn from data and make decisions on new data based on models to
achieve a given goal. The stochastic nature of AI models makes verification and
validation tasks challenging. Moreover, there are intrinsic biaises in AI
models such as reproductibility bias, selection bias (e.g., races, genders,
color), and reporting bias (i.e., results that do not reflect the reality).
Increasingly, there is also a particular attention to the ethical, legal, and
societal impacts of AI. AI systems are difficult to audit and certify because
of their black-box nature. They also appear to be vulnerable to threats; AI
systems can misbehave when untrusted data are given, making them insecure and
unsafe. Governments, national and international organizations have proposed
several principles to overcome these challenges but their applications in
practice are limited and there are different interpretations in the principles
that can bias implementations. In this paper, we examine trust in the context
of AI-based systems to understand what it means for an AI system to be
trustworthy and identify actions that need to be undertaken to ensure that AI
systems are trustworthy. To achieve this goal, we first review existing
approaches proposed for ensuring the trustworthiness of AI systems, in order to
identify potential conceptual gaps in understanding what trustworthy AI is.
Then, we suggest a trust (resp. zero-trust) model for AI and suggest a set of
properties that should be satisfied to ensure the trustworthiness of AI
systems.
</summary>
    <author>
      <name>Lionel Nganyewou Tidjon</name>
    </author>
    <author>
      <name>Foutse Khomh</name>
    </author>
    <link href="http://arxiv.org/abs/2206.11981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.11981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.11047v2</id>
    <updated>2023-01-31T12:47:06Z</updated>
    <published>2023-01-26T11:41:46Z</published>
    <title>A Systematic Review of Green AI</title>
    <summary>  With the ever-growing adoption of AI-based systems, the carbon footprint of
AI is no longer negligible. AI researchers and practitioners are therefore
urged to hold themselves accountable for the carbon emissions of the AI models
they design and use. This led in recent years to the appearance of researches
tackling AI environmental sustainability, a field referred to as Green AI.
Despite the rapid growth of interest in the topic, a comprehensive overview of
Green AI research is to date still missing. To address this gap, in this paper,
we present a systematic review of the Green AI literature. From the analysis of
98 primary studies, different patterns emerge. The topic experienced a
considerable growth from 2020 onward. Most studies consider monitoring AI model
footprint, tuning hyperparameters to improve model sustainability, or
benchmarking models. A mix of position papers, observational studies, and
solution papers are present. Most papers focus on the training phase, are
algorithm-agnostic or study neural networks, and use image data. Laboratory
experiments are the most common research strategy. Reported Green AI energy
savings go up to 115%, with savings over 50% being rather common. Industrial
parties are involved in Green AI studies, albeit most target academic readers.
Green AI tool provisioning is scarce. As a conclusion, the Green AI research
field results to have reached a considerable level of maturity. Therefore, from
this review emerges that the time is suitable to adopt other Green AI research
strategies, and port the numerous promising academic results to industrial
practice.
</summary>
    <author>
      <name>Roberto Verdecchia</name>
    </author>
    <author>
      <name>June Sallou</name>
    </author>
    <author>
      <name>Luís Cruz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review for Journal WIREs Data Mining and Knowledge Discovery</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.11047v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.11047v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.02787v1</id>
    <updated>2020-11-05T12:36:16Z</updated>
    <published>2020-11-05T12:36:16Z</published>
    <title>The State of AI Ethics Report (October 2020)</title>
    <summary>  The 2nd edition of the Montreal AI Ethics Institute's The State of AI Ethics
captures the most relevant developments in the field of AI Ethics since July
2020. This report aims to help anyone, from machine learning experts to human
rights activists and policymakers, quickly digest and understand the
ever-changing developments in the field. Through research and article
summaries, as well as expert commentary, this report distills the research and
reporting surrounding various domains related to the ethics of AI, including:
AI and society, bias and algorithmic justice, disinformation, humans and AI,
labor impacts, privacy, risk, and future of AI ethics.
  In addition, The State of AI Ethics includes exclusive content written by
world-class AI Ethics experts from universities, research institutes,
consulting firms, and governments. These experts include: Danit Gal (Tech
Advisor, United Nations), Amba Kak (Director of Global Policy and Programs,
NYU's AI Now Institute), Rumman Chowdhury (Global Lead for Responsible AI,
Accenture), Brent Barron (Director of Strategic Projects and Knowledge
Management, CIFAR), Adam Murray (U.S. Diplomat working on tech policy, Chair of
the OECD Network on AI), Thomas Kochan (Professor, MIT Sloan School of
Management), and Katya Klinova (AI and Economy Program Lead, Partnership on
AI).
  This report should be used not only as a point of reference and insight on
the latest thinking in the field of AI Ethics, but should also be used as a
tool for introspection as we aim to foster a more nuanced conversation
regarding the impacts of AI on the world.
</summary>
    <author>
      <name>Abhishek Gupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Microsoft</arxiv:affiliation>
    </author>
    <author>
      <name>Alexandrine Royer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Cambridge</arxiv:affiliation>
    </author>
    <author>
      <name>Victoria Heath</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Creative Commons</arxiv:affiliation>
    </author>
    <author>
      <name>Connor Wright</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Exeter</arxiv:affiliation>
    </author>
    <author>
      <name>Camylle Lanteigne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Concordia University</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Algora Lab</arxiv:affiliation>
    </author>
    <author>
      <name>Allison Cohen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">AI Global</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Mila</arxiv:affiliation>
    </author>
    <author>
      <name>Marianna Bergamaschi Ganapini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Union College</arxiv:affiliation>
    </author>
    <author>
      <name>Muriam Fancy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Toronto</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Ottawa</arxiv:affiliation>
    </author>
    <author>
      <name>Erick Galinkin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Rapid7</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Khurana</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Mo Akif</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Renjie Butalid</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Falaah Arif Khan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NYU Center for Responsible AI</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IIIT Hyderabad</arxiv:affiliation>
    </author>
    <author>
      <name>Masa Sweidan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">McGill University</arxiv:affiliation>
    </author>
    <author>
      <name>Audrey Balogh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">McGill University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">158 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.02787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.02787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.02724v1</id>
    <updated>2018-10-02T20:01:43Z</updated>
    <published>2018-10-02T20:01:43Z</published>
    <title>Human Indignity: From Legal AI Personhood to Selfish Memes</title>
    <summary>  It is possible to rely on current corporate law to grant legal personhood to
Artificially Intelligent (AI) agents. In this paper, after introducing pathways
to AI personhood, we analyze consequences of such AI empowerment on human
dignity, human safety and AI rights. We emphasize possibility of creating
selfish memes and legal system hacking in the context of artificial entities.
Finally, we consider some potential solutions for addressing described
problems.
</summary>
    <author>
      <name>Roman V. Yampolskiy</name>
    </author>
    <link href="http://arxiv.org/abs/1810.02724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.02724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.03215v1</id>
    <updated>2020-07-07T05:53:54Z</updated>
    <published>2020-07-07T05:53:54Z</published>
    <title>RCModel, a Risk Chain Model for Risk Reduction in AI Services</title>
    <summary>  With the increasing use of artificial intelligence (AI) services and products
in recent years, issues related to their trustworthiness have emerged and AI
service providers need to be prepared for various risks. In this policy
recommendation, we propose a risk chain model (RCModel) that supports AI
service providers in proper risk assessment and control. We hope that RCModel
will contribute to the realization of trustworthy AI services.
</summary>
    <author>
      <name>Takashi Matsumoto</name>
    </author>
    <author>
      <name>Arisa Ema</name>
    </author>
    <link href="http://arxiv.org/abs/2007.03215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.03215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.06773v1</id>
    <updated>2021-11-12T15:32:27Z</updated>
    <published>2021-11-12T15:32:27Z</published>
    <title>Explainability and the Fourth AI Revolution</title>
    <summary>  This chapter discusses AI from the prism of an automated process for the
organization of data, and exemplifies the role that explainability has to play
in moving from the current generation of AI systems to the next one, where the
role of humans is lifted from that of data annotators working for the AI
systems to that of collaborators working with the AI systems.
</summary>
    <author>
      <name>Loizos Michael</name>
    </author>
    <link href="http://arxiv.org/abs/2111.06773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.06773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.01297v1</id>
    <updated>2019-07-02T11:09:04Z</updated>
    <published>2019-07-02T11:09:04Z</published>
    <title>Neural Network Verification for the Masses (of AI graduates)</title>
    <summary>  Rapid development of AI applications has stimulated demand for, and has given
rise to, the rapidly growing number and diversity of AI MSc degrees. AI and
Robotics research communities, industries and students are becoming
increasingly aware of the problems caused by unsafe or insecure AI
applications. Among them, perhaps the most famous example is vulnerability of
deep neural networks to ``adversarial attacks''. Owing to wide-spread use of
neural networks in all areas of AI, this problem is seen as particularly acute
and pervasive.
  Despite of the growing number of research papers about safety and security
vulnerabilities of AI applications, there is a noticeable shortage of
accessible tools, methods and teaching materials for incorporating verification
into AI programs. LAIV -- the Lab for AI and Verification -- is a newly opened
research lab at Heriot-Watt university that engages AI and Robotics MSc
students in verification projects, as part of their MSc dissertation work. In
this paper, we will report on successes and unexpected difficulties LAIV faces,
many of which arise from limitations of existing programming languages used for
verification. We will discuss future directions for incorporating verification
into AI degrees.
</summary>
    <author>
      <name>Ekaterina Komendantskaya</name>
    </author>
    <author>
      <name>Rob Stewart</name>
    </author>
    <author>
      <name>Kirsy Duncan</name>
    </author>
    <author>
      <name>Daniel Kienitz</name>
    </author>
    <author>
      <name>Pierre Le Hen</name>
    </author>
    <author>
      <name>Pascal Bacchus</name>
    </author>
    <link href="http://arxiv.org/abs/1907.01297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.03466v1</id>
    <updated>2019-06-08T14:34:47Z</updated>
    <published>2019-06-08T14:34:47Z</published>
    <title>Strategies to architect AI Safety: Defense to guard AI from Adversaries</title>
    <summary>  The impact of designing for security of AI is critical for humanity in the AI
era. With humans increasingly becoming dependent upon AI, there is a need for
neural networks that work reliably, inspite of Adversarial attacks. The vision
for Safe and secure AI for popular use is achievable. To achieve safety of AI,
this paper explores strategies and a novel deep learning architecture. To guard
AI from adversaries, paper explores combination of 3 strategies:
  1. Introduce randomness at inference time to hide the representation learning
from adversaries.
  2. Detect presence of adversaries by analyzing the sequence of inferences.
  3. Exploit visual similarity.
  To realize these strategies, this paper designs a novel architecture, Dynamic
Neural Defense, DND. This defense has 3 deep learning architectural features:
  1. By hiding the way a neural network learns from exploratory attacks using a
random computation graph, DND evades attack.
  2. By analyzing input sequence to cloud AI inference engine with LSTM, DND
detects attack sequence.
  3. By inferring with visual similar inputs generated by VAE, any AI defended
by DND approach does not succumb to hackers.
  Thus, a roadmap to develop reliable, safe and secure AI is presented.
</summary>
    <author>
      <name>Rajagopal. A</name>
    </author>
    <author>
      <name>Nirmala. V</name>
    </author>
    <link href="http://arxiv.org/abs/1906.03466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.08964v1</id>
    <updated>2019-12-19T00:41:11Z</updated>
    <published>2019-12-19T00:41:11Z</published>
    <title>Exploring AI Futures Through Role Play</title>
    <summary>  We present an innovative methodology for studying and teaching the impacts of
AI through a role play game. The game serves two primary purposes: 1) training
AI developers and AI policy professionals to reflect on and prepare for future
social and ethical challenges related to AI and 2) exploring possible futures
involving AI technology development, deployment, social impacts, and
governance. While the game currently focuses on the inter relations between
short --, mid and long term impacts of AI, it has potential to be adapted for a
broad range of scenarios, exploring in greater depths issues of AI policy
research and affording training within organizations. The game presented here
has undergone two years of development and has been tested through over 30
events involving between 3 and 70 participants. The game is under active
development, but preliminary findings suggest that role play is a promising
methodology for both exploring AI futures and training individuals and
organizations in thinking about, and reflecting on, the impacts of AI and
strategic mistakes that can be avoided today.
</summary>
    <author>
      <name>Shahar Avin</name>
    </author>
    <author>
      <name>Ross Gruetzemacher</name>
    </author>
    <author>
      <name>James Fox</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3375627.3375817</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3375627.3375817" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to AIES</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.08964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.08964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.14779v3</id>
    <updated>2021-01-12T22:50:34Z</updated>
    <published>2020-06-26T03:34:04Z</published>
    <title>Does the Whole Exceed its Parts? The Effect of AI Explanations on
  Complementary Team Performance</title>
    <summary>  Many researchers motivate explainable AI with studies showing that human-AI
team performance on decision-making tasks improves when the AI explains its
recommendations. However, prior studies observed improvements from explanations
only when the AI, alone, outperformed both the human and the best team. Can
explanations help lead to complementary performance, where team accuracy is
higher than either the human or the AI working solo? We conduct mixed-method
user studies on three datasets, where an AI with accuracy comparable to humans
helps participants solve a task (explaining itself in some conditions). While
we observed complementary improvements from AI augmentation, they were not
increased by explanations. Rather, explanations increased the chance that
humans will accept the AI's recommendation, regardless of its correctness. Our
result poses new challenges for human-centered AI: Can we develop explanatory
approaches that encourage appropriate trust in AI, and therefore help generate
(or improve) complementary performance?
</summary>
    <author>
      <name>Gagan Bansal</name>
    </author>
    <author>
      <name>Tongshuang Wu</name>
    </author>
    <author>
      <name>Joyce Zhou</name>
    </author>
    <author>
      <name>Raymond Fok</name>
    </author>
    <author>
      <name>Besmira Nushi</name>
    </author>
    <author>
      <name>Ece Kamar</name>
    </author>
    <author>
      <name>Marco Tulio Ribeiro</name>
    </author>
    <author>
      <name>Daniel S. Weld</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CHI'21</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.14779v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.14779v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.04520v1</id>
    <updated>2020-08-11T05:31:10Z</updated>
    <published>2020-08-11T05:31:10Z</published>
    <title>Montreal AI Ethics Institute's (MAIEI) Submission to the World
  Intellectual Property Organization (WIPO) Conversation on Intellectual
  Property (IP) and Artificial Intelligence (AI) Second Session</title>
    <summary>  This document posits that, at best, a tenuous case can be made for providing
AI exclusive IP over their "inventions". Furthermore, IP protections for AI are
unlikely to confer the benefit of ensuring regulatory compliance. Rather, IP
protections for AI "inventors" present a host of negative externalities and
obscures the fact that the genuine inventor, deserving of IP, is the human
agent. This document will conclude by recommending strategies for WIPO to bring
IP law into the 21st century, enabling it to productively account for AI
"inventions".
  Theme: IP Protection for AI-Generated and AI-Assisted Works Based on insights
from the Montreal AI Ethics Institute (MAIEI) staff and supplemented by
workshop contributions from the AI Ethics community convened by MAIEI on July
5, 2020.
</summary>
    <author>
      <name>Allison Cohen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute and</arxiv:affiliation>
    </author>
    <author>
      <name>Abhishek Gupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute and</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Microsoft</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.04520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.04520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.15035v3</id>
    <updated>2021-02-01T01:33:29Z</updated>
    <published>2020-12-30T04:34:46Z</published>
    <title>Measuring Human Adaptation to AI in Decision Making: Application to
  Evaluate Changes after AlphaGo</title>
    <summary>  Across a growing number of domains, human experts are expected to learn from
and adapt to AI with superior decision making abilities. But how can we
quantify such human adaptation to AI? We develop a simple measure of human
adaptation to AI and test its usefulness in two case studies. In Study 1, we
analyze 1.3 million move decisions made by professional Go players and find
that a positive form of adaptation to AI (learning) occurred after the players
could observe the reasoning processes of AI, rather than mere actions of AI.
These findings based on our measure highlight the importance of explainability
for human learning from AI. In Study 2, we test whether our measure is
sufficiently sensitive to capture a negative form of adaptation to AI (cheating
aided by AI), which occurred in a match between professional Go players. We
discuss our measure's applications in domains other than Go, especially in
domains in which AI's decision making ability will likely surpass that of human
experts.
</summary>
    <author>
      <name>Minkyu Shin</name>
    </author>
    <author>
      <name>Jin Kim</name>
    </author>
    <author>
      <name>Minkyung Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2012.15035v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.15035v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.01524v2</id>
    <updated>2021-01-12T22:44:26Z</updated>
    <published>2021-01-04T05:32:48Z</published>
    <title>"Brilliant AI Doctor" in Rural China: Tensions and Challenges in
  AI-Powered CDSS Deployment</title>
    <summary>  Artificial intelligence (AI) technology has been increasingly used in the
implementation of advanced Clinical Decision Support Systems (CDSS). Research
demonstrated the potential usefulness of AI-powered CDSS (AI-CDSS) in clinical
decision making scenarios. However, post-adoption user perception and
experience remain understudied, especially in developing countries. Through
observations and interviews with 22 clinicians from 6 rural clinics in China,
this paper reports the various tensions between the design of an AI-CDSS system
("Brilliant Doctor") and the rural clinical context, such as the misalignment
with local context and workflow, the technical limitations and usability
barriers, as well as issues related to transparency and trustworthiness of
AI-CDSS. Despite these tensions, all participants expressed positive attitudes
toward the future of AI-CDSS, especially acting as "a doctor's AI assistant" to
realize a Human-AI Collaboration future in clinical settings. Finally we draw
on our findings to discuss implications for designing AI-CDSS interventions for
rural clinical contexts in developing countries.
</summary>
    <author>
      <name>Dakuo Wang</name>
    </author>
    <author>
      <name>Liuping Wang</name>
    </author>
    <author>
      <name>Zhan Zhang</name>
    </author>
    <author>
      <name>Ding Wang</name>
    </author>
    <author>
      <name>Haiyi Zhu</name>
    </author>
    <author>
      <name>Yvonne Gao</name>
    </author>
    <author>
      <name>Xiangmin Fan</name>
    </author>
    <author>
      <name>Feng Tian</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3411764.3445432</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3411764.3445432" rel="related"/>
    <link href="http://arxiv.org/abs/2101.01524v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.01524v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.09109v1</id>
    <updated>2021-02-18T01:38:11Z</updated>
    <published>2021-02-18T01:38:11Z</published>
    <title>Understanding and Creating Art with AI: Review and Outlook</title>
    <summary>  Technologies related to artificial intelligence (AI) have a strong impact on
the changes of research and creative practices in visual arts. The growing
number of research initiatives and creative applications that emerge in the
intersection of AI and art, motivates us to examine and discuss the creative
and explorative potentials of AI technologies in the context of art. This paper
provides an integrated review of two facets of AI and art: 1) AI is used for
art analysis and employed on digitized artwork collections; 2) AI is used for
creative purposes and generating novel artworks. In the context of AI-related
research for art understanding, we present a comprehensive overview of artwork
datasets and recent works that address a variety of tasks such as
classification, object detection, similarity retrieval, multimodal
representations, computational aesthetics, etc. In relation to the role of AI
in creating art, we address various practical and theoretical aspects of AI Art
and consolidate related works that deal with those topics in detail. Finally,
we provide a concise outlook on the future progression and potential impact of
AI technologies on our understanding and creation of art.
</summary>
    <author>
      <name>Eva Cetinic</name>
    </author>
    <author>
      <name>James She</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.09109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.09109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.07015v2</id>
    <updated>2022-06-01T22:26:39Z</updated>
    <published>2021-07-14T21:33:14Z</published>
    <title>Do Humans Trust Advice More if it Comes from AI? An Analysis of Human-AI
  Interactions</title>
    <summary>  In decision support applications of AI, the AI algorithm's output is framed
as a suggestion to a human user. The user may ignore this advice or take it
into consideration to modify their decision. With the increasing prevalence of
such human-AI interactions, it is important to understand how users react to AI
advice. In this paper, we recruited over 1100 crowdworkers to characterize how
humans use AI suggestions relative to equivalent suggestions from a group of
peer humans across several experimental settings. We find that participants'
beliefs about how human versus AI performance on a given task affects whether
they heed the advice. When participants do heed the advice, they use it
similarly for human and AI suggestions. Based on these results, we propose a
two-stage, "activation-integration" model for human behavior and use it to
characterize the factors that affect human-AI interactions.
</summary>
    <author>
      <name>Kailas Vodrahalli</name>
    </author>
    <author>
      <name>Roxana Daneshjou</name>
    </author>
    <author>
      <name>Tobias Gerstenberg</name>
    </author>
    <author>
      <name>James Zou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference on Artificial Intelligence, Ethics, and Society (AIES
  2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.07015v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.07015v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.09051v1</id>
    <updated>2021-07-20T01:39:10Z</updated>
    <published>2021-07-20T01:39:10Z</published>
    <title>AI in Finance: Challenges, Techniques and Opportunities</title>
    <summary>  AI in finance broadly refers to the applications of AI techniques in
financial businesses. This area has been lasting for decades with both classic
and modern AI techniques applied to increasingly broader areas of finance,
economy and society. In contrast to either discussing the problems, aspects and
opportunities of finance that have benefited from specific AI techniques and in
particular some new-generation AI and data science (AIDS) areas or reviewing
the progress of applying specific techniques to resolving certain financial
problems, this review offers a comprehensive and dense roadmap of the
overwhelming challenges, techniques and opportunities of AI research in finance
over the past decades. The landscapes and challenges of financial businesses
and data are firstly outlined, followed by a comprehensive categorization and a
dense overview of the decades of AI research in finance. We then structure and
illustrate the data-driven analytics and learning of financial businesses and
data. The comparison, criticism and discussion of classic vs. modern AI
techniques for finance are followed. Lastly, open issues and opportunities
address future AI-empowered finance and finance-motivated AI research.
</summary>
    <author>
      <name>Longbing Cao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper is in the revision for ACM Computing Surveys, 40 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.09051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.09051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.09290v2</id>
    <updated>2022-09-29T20:47:25Z</updated>
    <published>2021-10-14T14:19:30Z</published>
    <title>The AI Triplet: Computational, Conceptual, and Mathematical Knowledge in
  AI Education</title>
    <summary>  Efforts to enhance education and broaden participation in AI will benefit
from a systematic understanding of the competencies underlying AI expertise. In
this paper, we observe that AI expertise requires integrating computational,
conceptual, and mathematical knowledge and representations. We call this the
``AI triplet,'' similar in spirit to the ``chemistry triplet'' that has heavily
influenced the past four decades of chemistry education research. We describe a
theoretical foundation for this triplet and show how it maps onto two sample AI
topics: tree search and gradient descent. Finally, just as the chemistry
triplet has impacted chemistry education in concrete ways, we suggest two
initial hypotheses for how the AI triplet might impact AI education: 1) how we
can help AI students gain proficiency in moving between the corners of the
triplet; and 2) how all corners of the AI triplet highlight the need for
supporting students' spatial cognitive skills.
</summary>
    <author>
      <name>Maithilee Kunda</name>
    </author>
    <link href="http://arxiv.org/abs/2110.09290v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.09290v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.04263v1</id>
    <updated>2022-01-12T01:22:26Z</updated>
    <published>2022-01-12T01:22:26Z</published>
    <title>The Human Factor in AI Safety</title>
    <summary>  AI-based systems have been used widely across various industries for
different decisions ranging from operational decisions to tactical and
strategic ones in low- and high-stakes contexts. Gradually the weaknesses and
issues of these systems have been publicly reported including, ethical issues,
biased decisions, unsafe outcomes, and unfair decisions, to name a few.
Research has tended to optimize AI less has focused on its risk and unexpected
negative consequences. Acknowledging this serious potential risks and scarcity
of re-search I focus on unsafe outcomes of AI. Specifically, I explore this
issue from a Human-AI interaction lens during AI deployment. It will be
discussed how the interaction of individuals and AI during its deployment
brings new concerns, which need a solid and holistic mitigation plan. It will
be dis-cussed that only AI algorithms' safety is not enough to make its
operation safe. The AI-based systems' end-users and their decision-making
archetypes during collaboration with these systems should be considered during
the AI risk management. Using some real-world scenarios, it will be highlighted
that decision-making archetypes of users should be considered a design
principle in AI-based systems.
</summary>
    <author>
      <name>Morteza Saberi</name>
    </author>
    <link href="http://arxiv.org/abs/2201.04263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.04263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.13480v1</id>
    <updated>2022-04-28T13:13:48Z</updated>
    <published>2022-04-28T13:13:48Z</published>
    <title>The Value of Measuring Trust in AI - A Socio-Technical System
  Perspective</title>
    <summary>  Building trust in AI-based systems is deemed critical for their adoption and
appropriate use. Recent research has thus attempted to evaluate how various
attributes of these systems affect user trust. However, limitations regarding
the definition and measurement of trust in AI have hampered progress in the
field, leading to results that are inconsistent or difficult to compare. In
this work, we provide an overview of the main limitations in defining and
measuring trust in AI. We focus on the attempt of giving trust in AI a
numerical value and its utility in informing the design of real-world human-AI
interactions. Taking a socio-technical system perspective on AI, we explore two
distinct approaches to tackle these challenges. We provide actionable
recommendations on how these approaches can be implemented in practice and
inform the design of human-AI interactions. We thereby aim to provide a
starting point for researchers and designers to re-evaluate the current focus
on trust in AI, improving the alignment between what empirical research
paradigms may offer and the expectations of real-world human-AI interactions.
</summary>
    <author>
      <name>Michaela Benk</name>
    </author>
    <author>
      <name>Suzanne Tolmeijer</name>
    </author>
    <author>
      <name>Florian von Wangenheim</name>
    </author>
    <author>
      <name>Andrea Ferrario</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CHI 2022 - Workshop on Trust and Reliance in AI-Human
  Teams (TRAIT), 2022, New Orleans, LA</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.13480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.13480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.07271v3</id>
    <updated>2022-11-07T09:47:29Z</updated>
    <published>2022-06-15T03:18:56Z</published>
    <title>Human Heuristics for AI-Generated Language Are Flawed</title>
    <summary>  Human communication is increasingly intermixed with language generated by AI.
Across chat, email, and social media, AI systems produce smart replies,
autocompletes, and translations. AI-generated language is often not identified
as such but presented as language written by humans, raising concerns about
novel forms of deception and manipulation. Here, we study how humans discern
whether verbal self-presentations, one of the most personal and consequential
forms of language, were generated by AI. In six experiments, participants (N =
4,600) were unable to detect self-presentations generated by state-of-the-art
AI language models in professional, hospitality, and dating contexts. A
computational analysis of language features shows that human judgments of
AI-generated language are handicapped by intuitive but flawed heuristics such
as associating first-person pronouns, spontaneous wording, or family topics
with human-written language. We experimentally demonstrate that these
heuristics make human judgment of AI-generated language predictable and
manipulable, allowing AI systems to produce language perceived as more human
than human. We discuss solutions, such as AI accents, to reduce the deceptive
potential of language generated by AI, limiting the subversion of human
intuition.
</summary>
    <author>
      <name>Maurice Jakesch</name>
    </author>
    <author>
      <name>Jeffrey Hancock</name>
    </author>
    <author>
      <name>Mor Naaman</name>
    </author>
    <link href="http://arxiv.org/abs/2206.07271v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.07271v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.01369v1</id>
    <updated>2022-10-04T04:19:21Z</updated>
    <published>2022-10-04T04:19:21Z</published>
    <title>Understanding Older Adults' Perceptions and Challenges in Using
  AI-enabled Everyday Technologies</title>
    <summary>  Artificial intelligence (AI)-enabled everyday technologies could help address
age-related challenges like physical impairments and cognitive decline. While
recent research studied older adults' experiences with specific AI-enabled
products (e.g., conversational agents and assistive robots), it remains unknown
how older adults perceive and experience current AI-enabled everyday
technologies in general, which could impact their adoption of future AI-enabled
products. We conducted a survey study (N=41) and semi-structured interviews
(N=15) with older adults to understand their experiences and perceptions of AI.
We found that older adults were enthusiastic about learning and using
AI-enabled products, but they lacked learning avenues. Additionally, they
worried when AI-enabled products outwitted their expectations, intruded on
their privacy, or impacted their decision-making skills. Therefore, they held
mixed views towards AI-enabled products such as AI, an aid, or an adversary. We
conclude with design recommendations that make older adults feel inclusive,
secure, and in control of their interactions with AI-enabled products.
</summary>
    <author>
      <name>Esha Shandilya</name>
    </author>
    <author>
      <name>Mingming Fan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The Tenth International Symposium of Chinese CHI (Chinese CHI 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.01369v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.01369v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.03842v1</id>
    <updated>2022-10-07T22:46:04Z</updated>
    <published>2022-10-07T22:46:04Z</published>
    <title>Mutual Theory of Mind for Human-AI Communication</title>
    <summary>  From navigation systems to smart assistants, we communicate with various AI
on a daily basis. At the core of such human-AI communication, we convey our
understanding of the AI's capability to the AI through utterances with
different complexities, and the AI conveys its understanding of our needs and
goals to us through system outputs. However, this communication process is
prone to failures for two reasons: the AI might have the wrong understanding of
the user and the user might have the wrong understanding of the AI. To enhance
mutual understanding in human-AI communication, we posit the Mutual Theory of
Mind (MToM) framework, inspired by our basic human capability of "Theory of
Mind." In this paper, we discuss the motivation of the MToM framework and its
three key components that continuously shape the mutual understanding during
three stages of human-AI communication. We then describe a case study inspired
by the MToM framework to demonstrate the power of MToM framework to guide the
design and understanding of human-AI communication.
</summary>
    <author>
      <name>Qiaosi Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Georgia Institute of Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Ashok K. Goel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Georgia Institute of Technology</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.03842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.03842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.06495v1</id>
    <updated>2022-12-13T11:23:24Z</updated>
    <published>2022-12-13T11:23:24Z</published>
    <title>Inherent Limitations of AI Fairness</title>
    <summary>  As the real-world impact of Artificial Intelligence (AI) systems has been
steadily growing, so too have these systems come under increasing scrutiny. In
particular, the study of AI fairness has rapidly developed into a rich field of
research with links to computer science, social science, law, and philosophy.
Though many technical solutions for measuring and achieving AI fairness have
been proposed, their model of AI fairness has been widely criticized in recent
years for being misleading and unrealistic.
  In our paper, we survey these criticisms of AI fairness and identify key
limitations that are inherent to the prototypical paradigm of AI fairness. By
carefully outlining the extent to which technical solutions can realistically
help in achieving AI fairness, we aim to provide readers with the background
necessary to form a nuanced opinion on developments in the field of fair AI.
This delineation also provides research opportunities for non-AI solutions
peripheral to AI systems in supporting fair decision processes.
</summary>
    <author>
      <name>Maarten Buyl</name>
    </author>
    <author>
      <name>Tijl De Bie</name>
    </author>
    <link href="http://arxiv.org/abs/2212.06495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.06495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.03052v1</id>
    <updated>2023-01-08T15:02:38Z</updated>
    <published>2023-01-08T15:02:38Z</published>
    <title>AI Maintenance: A Robustness Perspective</title>
    <summary>  With the advancements in machine learning (ML) methods and compute resources,
artificial intelligence (AI) empowered systems are becoming a prevailing
technology. However, current AI technology such as deep learning is not
flawless. The significantly increased model complexity and data scale incur
intensified challenges when lacking trustworthiness and transparency, which
could create new risks and negative impacts. In this paper, we carve out AI
maintenance from the robustness perspective. We start by introducing some
highlighted robustness challenges in the AI lifecycle and motivating AI
maintenance by making analogies to car maintenance. We then propose an AI model
inspection framework to detect and mitigate robustness risks. We also draw
inspiration from vehicle autonomy to define the levels of AI robustness
automation. Our proposal for AI maintenance facilitates robustness assessment,
status tracking, risk scanning, model hardening, and regulation throughout the
AI lifecycle, which is an essential milestone toward building sustainable and
trustworthy AI ecosystems.
</summary>
    <author>
      <name>Pin-Yu Chen</name>
    </author>
    <author>
      <name>Payel Das</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE Computer Magazine. To be published in 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.03052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.03052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.05809v1</id>
    <updated>2023-01-14T02:51:01Z</updated>
    <published>2023-01-14T02:51:01Z</published>
    <title>Who Should I Trust: AI or Myself? Leveraging Human and AI Correctness
  Likelihood to Promote Appropriate Trust in AI-Assisted Decision-Making</title>
    <summary>  In AI-assisted decision-making, it is critical for human decision-makers to
know when to trust AI and when to trust themselves. However, prior studies
calibrated human trust only based on AI confidence indicating AI's correctness
likelihood (CL) but ignored humans' CL, hindering optimal team decision-making.
To mitigate this gap, we proposed to promote humans' appropriate trust based on
the CL of both sides at a task-instance level. We first modeled humans' CL by
approximating their decision-making models and computing their potential
performance in similar instances. We demonstrated the feasibility and
effectiveness of our model via two preliminary studies. Then, we proposed three
CL exploitation strategies to calibrate users' trust explicitly/implicitly in
the AI-assisted decision-making process. Results from a between-subjects
experiment (N=293) showed that our CL exploitation strategies promoted more
appropriate human trust in AI, compared with only using AI confidence. We
further provided practical implications for more human-compatible AI-assisted
decision-making.
</summary>
    <author>
      <name>Shuai Ma</name>
    </author>
    <author>
      <name>Ying Lei</name>
    </author>
    <author>
      <name>Xinru Wang</name>
    </author>
    <author>
      <name>Chengbo Zheng</name>
    </author>
    <author>
      <name>Chuhan Shi</name>
    </author>
    <author>
      <name>Ming Yin</name>
    </author>
    <author>
      <name>Xiaojuan Ma</name>
    </author>
    <link href="http://arxiv.org/abs/2301.05809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.05809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.08807v1</id>
    <updated>2023-02-17T11:07:17Z</updated>
    <published>2023-02-17T11:07:17Z</published>
    <title>Competent but Rigid: Identifying the Gap in Empowering AI to Participate
  Equally in Group Decision-Making</title>
    <summary>  Existing research on human-AI collaborative decision-making focuses mainly on
the interaction between AI and individual decision-makers. There is a limited
understanding of how AI may perform in group decision-making. This paper
presents a wizard-of-oz study in which two participants and an AI form a
committee to rank three English essays. One novelty of our study is that we
adopt a speculative design by endowing AI equal power to humans in group
decision-making.We enable the AI to discuss and vote equally with other human
members. We find that although the voice of AI is considered valuable, AI still
plays a secondary role in the group because it cannot fully follow the dynamics
of the discussion and make progressive contributions. Moreover, the divergent
opinions of our participants regarding an "equal AI" shed light on the possible
future of human-AI relations.
</summary>
    <author>
      <name>Chengbo Zheng</name>
    </author>
    <author>
      <name>Yuheng Wu</name>
    </author>
    <author>
      <name>Chuhan Shi</name>
    </author>
    <author>
      <name>Shuai Ma</name>
    </author>
    <author>
      <name>Jiehui Luo</name>
    </author>
    <author>
      <name>Xiaojuan Ma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3544548.3581131</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3544548.3581131" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in CHI'23</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.08807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.08807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.10720v1</id>
    <updated>2017-05-30T16:15:16Z</updated>
    <published>2017-05-30T16:15:16Z</published>
    <title>Low Impact Artificial Intelligences</title>
    <summary>  There are many goals for an AI that could become dangerous if the AI becomes
superintelligent or otherwise powerful. Much work on the AI control problem has
been focused on constructing AI goals that are safe even for such AIs. This
paper looks at an alternative approach: defining a general concept of `low
impact'. The aim is to ensure that a powerful AI which implements low impact
will not modify the world extensively, even if it is given a simple or
dangerous goal. The paper proposes various ways of defining and grounding low
impact, and discusses methods for ensuring that the AI can still be allowed to
have a (desired) impact despite the restriction. The end of the paper addresses
known issues with this approach and avenues for future research.
</summary>
    <author>
      <name>Stuart Armstrong</name>
    </author>
    <author>
      <name>Benjamin Levinstein</name>
    </author>
    <link href="http://arxiv.org/abs/1705.10720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.01170v2</id>
    <updated>2020-01-13T14:51:59Z</updated>
    <published>2019-10-02T19:09:43Z</published>
    <title>The tension between openness and prudence in AI research</title>
    <summary>  This paper explores the tension between openness and prudence in AI research,
evident in two core principles of the Montr\'eal Declaration for Responsible
AI. While the AI community has strong norms around open sharing of research,
concerns about the potential harms arising from misuse of research are growing,
prompting some to consider whether the field of AI needs to reconsider
publication norms. We discuss how different beliefs and values can lead to
differing perspectives on how the AI community should manage this tension, and
explore implications for what responsible publication norms in AI research
might look like in practice.
</summary>
    <author>
      <name>Jess Whittlestone</name>
    </author>
    <author>
      <name>Aviv Ovadya</name>
    </author>
    <link href="http://arxiv.org/abs/1910.01170v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.01170v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.06276v1</id>
    <updated>2020-02-14T22:45:36Z</updated>
    <published>2020-02-14T22:45:36Z</published>
    <title>Trustworthy AI</title>
    <summary>  The promise of AI is huge. AI systems have already achieved good enough
performance to be in our streets and in our homes. However, they can be brittle
and unfair. For society to reap the benefits of AI systems, society needs to be
able to trust them. Inspired by decades of progress in trustworthy computing,
we suggest what trustworthy properties would be desired of AI systems. By
enumerating a set of new research questions, we explore one approach--formal
verification--for ensuring trust in AI. Trustworthy AI ups the ante on both
trustworthy computing and formal methods.
</summary>
    <author>
      <name>Jeannette M. Wing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.06276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.06276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; D.3.1; D.4.6; F.3.1; G.3; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.03172v1</id>
    <updated>2022-02-04T15:51:59Z</updated>
    <published>2022-02-04T15:51:59Z</published>
    <title>The 6-Ds of Creating AI-Enabled Systems</title>
    <summary>  We are entering our tenth year of the current Artificial Intelligence (AI)
spring, and, as with previous AI hype cycles, the threat of an AI winter looms.
AI winters occurred because of ineffective approaches towards navigating the
technology valley of death. The 6-D framework provides an end-to-end framework
to successfully navigate this challenge. The 6-D framework starts with problem
decomposition to identify potential AI solutions, and ends with considerations
for deployment of AI-enabled systems. Each component of the 6-D framework and a
precision medicine use case is described in this paper.
</summary>
    <author>
      <name>John Piorkowski</name>
    </author>
    <link href="http://arxiv.org/abs/2202.03172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.03172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.12641v1</id>
    <updated>2022-04-27T00:22:13Z</updated>
    <published>2022-04-27T00:22:13Z</published>
    <title>Framework for disruptive AI/ML Innovation</title>
    <summary>  This framework enables C suite executive leaders to define a business plan
and manage technological dependencies for building AI/ML Solutions. The
business plan of this framework provides components and background information
to define strategy and analyze cost. Furthermore, the business plan represents
the fundamentals of AI/ML Innovation and AI/ML Solutions. Therefore, the
framework provides a menu for managing and investing in AI/ML. Finally, this
framework is constructed with an interdisciplinary and holistic view of AI/ML
Innovation and builds on advances in business strategy in harmony with
technological progress for AI/ML. This framework incorporates value chain,
supply chain, and ecosystem strategies.
</summary>
    <author>
      <name>Wim Verleyen</name>
    </author>
    <author>
      <name>William McGinnis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.12641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.12641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.00225v1</id>
    <updated>2022-06-01T04:21:39Z</updated>
    <published>2022-06-01T04:21:39Z</published>
    <title>Can Artificial Intelligence Transform DevOps?</title>
    <summary>  DevOps and Artificial Intelligence (AI) are interconnected with each other.
DevOps is a business-driven approach to providing quickly delivered quality
software, and AI is the technology that can be used in the system to enhance
its functionality. So, DevOps teams can use AI to test, code, release, monitor,
and improve the system. Through AI, the automation process delivered by DevOps
could be improved efficiently. This study aims to explore how AI can transform
DevOps. The research is useful in terms of facilitating software developers and
businesses to assess the importance of AI in DevOps. The study has practical
implications as it elaborates on how AI transforms DevOps and in what way it
can support businesses in their business.
</summary>
    <author>
      <name>Mamdouh Alenezi</name>
    </author>
    <author>
      <name>Mohammad Zarour</name>
    </author>
    <author>
      <name>Mohammad Akour</name>
    </author>
    <link href="http://arxiv.org/abs/2206.00225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.00225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.12346v1</id>
    <updated>2022-09-25T23:17:20Z</updated>
    <published>2022-09-25T23:17:20Z</published>
    <title>Political economy of superhuman AI</title>
    <summary>  In this note, I study the institutions and game theoretic assumptions that
would prevent the emergence of "superhuman-level" arfiticial general
intelligence, denoted by AI*. These assumptions are (i) the "Freedom of the
Mind," (ii) open source "access" to AI*, and (iii) rationality of the
representative human agent, who competes against AI*. I prove that under these
three assumptions it is impossible that an AI* exists. This result gives rise
to two immediate recommendations for public policy. First, "cloning" digitally
the human brain should be strictly regulated, and hypothetical AI*'s access to
brain should be prohibited. Second, AI* research should be made widely, if not
publicly, accessible.
</summary>
    <author>
      <name>Mehmet S. Ismail</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.12346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.12346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91A10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.05828v1</id>
    <updated>2022-11-04T01:01:31Z</updated>
    <published>2022-11-04T01:01:31Z</published>
    <title>Is Decentralized AI Safer?</title>
    <summary>  Artificial Intelligence (AI) has the potential to significantly benefit or
harm humanity. At present, a few for-profit companies largely control the
development and use of this technology, and therefore determine its outcomes.
In an effort to diversify and democratize work on AI, various groups are
building open AI systems, investigating their risks, and discussing their
ethics. In this paper, we demonstrate how blockchain technology can facilitate
and formalize these efforts. Concretely, we analyze multiple use-cases for
blockchain in AI research and development, including decentralized governance,
the creation of immutable audit trails, and access to more diverse and
representative datasets. We argue that decentralizing AI can help mitigate AI
risks and ethical concerns, while also introducing new issues that should be
considered in future work.
</summary>
    <author>
      <name>Casey Clifton</name>
    </author>
    <author>
      <name>Richard Blythman</name>
    </author>
    <author>
      <name>Kartika Tulusan</name>
    </author>
    <link href="http://arxiv.org/abs/2211.05828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.05828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.02114v1</id>
    <updated>2020-01-07T15:33:48Z</updated>
    <published>2020-01-07T15:33:48Z</published>
    <title>Effect of Confidence and Explanation on Accuracy and Trust Calibration
  in AI-Assisted Decision Making</title>
    <summary>  Today, AI is being increasingly used to help human experts make decisions in
high-stakes scenarios. In these scenarios, full automation is often
undesirable, not only due to the significance of the outcome, but also because
human experts can draw on their domain knowledge complementary to the model's
to ensure task success. We refer to these scenarios as AI-assisted decision
making, where the individual strengths of the human and the AI come together to
optimize the joint decision outcome. A key to their success is to appropriately
\textit{calibrate} human trust in the AI on a case-by-case basis; knowing when
to trust or distrust the AI allows the human expert to appropriately apply
their knowledge, improving decision outcomes in cases where the model is likely
to perform poorly. This research conducts a case study of AI-assisted decision
making in which humans and AI have comparable performance alone, and explores
whether features that reveal case-specific model information can calibrate
trust and improve the joint performance of the human and AI. Specifically, we
study the effect of showing confidence score and local explanation for a
particular prediction. Through two human experiments, we show that confidence
score can help calibrate people's trust in an AI model, but trust calibration
alone is not sufficient to improve AI-assisted decision making, which may also
depend on whether the human can bring in enough unique knowledge to complement
the AI's errors. We also highlight the problems in using local explanation for
AI-assisted decision making scenarios and invite the research community to
explore new approaches to explainability for calibrating human trust in AI.
</summary>
    <author>
      <name>Yunfeng Zhang</name>
    </author>
    <author>
      <name>Q. Vera Liao</name>
    </author>
    <author>
      <name>Rachel K. E. Bellamy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3351095.3372852</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3351095.3372852" rel="related"/>
    <link href="http://arxiv.org/abs/2001.02114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.02114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.13102v3</id>
    <updated>2021-02-19T20:22:20Z</updated>
    <published>2020-04-27T19:06:28Z</published>
    <title>Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork</title>
    <summary>  AI practitioners typically strive to develop the most accurate systems,
making an implicit assumption that the AI system will function autonomously.
However, in practice, AI systems often are used to provide advice to people in
domains ranging from criminal justice and finance to healthcare. In such
AI-advised decision making, humans and machines form a team, where the human is
responsible for making final decisions. But is the most accurate AI the best
teammate? We argue "No" -- predictable performance may be worth a slight
sacrifice in AI accuracy. Instead, we argue that AI systems should be trained
in a human-centered manner, directly optimized for team performance. We study
this proposal for a specific type of human-AI teaming, where the human overseer
chooses to either accept the AI recommendation or solve the task themselves. To
optimize the team performance for this setting we maximize the team's expected
utility, expressed in terms of the quality of the final decision, cost of
verifying, and individual accuracies of people and machines. Our experiments
with linear and non-linear models on real-world, high-stakes datasets show that
the most accuracy AI may not lead to highest team performance and show the
benefit of modeling teamwork during training through improvements in expected
team utility across datasets, considering parameters such as human skill and
the cost of mistakes. We discuss the shortcoming of current optimization
approaches beyond well-studied loss functions such as log-loss, and encourage
future work on AI optimization problems motivated by human-AI collaboration.
</summary>
    <author>
      <name>Gagan Bansal</name>
    </author>
    <author>
      <name>Besmira Nushi</name>
    </author>
    <author>
      <name>Ece Kamar</name>
    </author>
    <author>
      <name>Eric Horvitz</name>
    </author>
    <author>
      <name>Daniel S. Weld</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v2</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.13102v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.13102v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.05424v3</id>
    <updated>2022-01-08T02:37:41Z</updated>
    <published>2021-05-12T04:30:45Z</published>
    <title>Transitioning to human interaction with AI systems: New challenges and
  opportunities for HCI professionals to enable human-centered AI</title>
    <summary>  While AI has benefited humans, it may also harm humans if not appropriately
developed. The focus of HCI work is transiting from conventional human
interaction with non-AI computing systems to interaction with AI systems. We
conducted a high-level literature review and a holistic analysis of current
work in developing AI systems from an HCI perspective. Our review and analysis
highlight the new changes introduced by AI technology and the new challenges
that HCI professionals face when applying the human-centered AI (HCAI) approach
in the development of AI systems. We also identified seven main issues in human
interaction with AI systems, which HCI professionals did not encounter when
developing non-AI computing systems. To further enable the implementation of
the HCAI approach, we identified new HCI opportunities tied to specific
HCAI-driven design goals to guide HCI professionals in addressing these new
issues. Finally, our assessment of current HCI methods shows the limitations of
these methods in support of developing AI systems. We propose alternative
methods that can help overcome these limitations and effectively help HCI
professionals apply the HCAI approach to the development of AI systems. We also
offer strategic recommendations for HCI professionals to effectively influence
the development of AI systems with the HCAI approach, eventually developing
HCAI systems.
</summary>
    <author>
      <name>Wei Xu</name>
    </author>
    <author>
      <name>Marvin J. Dainoff</name>
    </author>
    <author>
      <name>Liezhong Ge</name>
    </author>
    <author>
      <name>Zaifeng Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">72 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.05424v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.05424v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.06641v3</id>
    <updated>2021-08-19T03:32:04Z</updated>
    <published>2021-07-12T14:21:46Z</published>
    <title>Trustworthy AI: A Computational Perspective</title>
    <summary>  In the past few decades, artificial intelligence (AI) technology has
experienced swift developments, changing everyone's daily life and profoundly
altering the course of human society. The intention of developing AI is to
benefit humans, by reducing human labor, bringing everyday convenience to human
lives, and promoting social good. However, recent research and AI applications
show that AI can cause unintentional harm to humans, such as making unreliable
decisions in safety-critical scenarios or undermining fairness by inadvertently
discriminating against one group. Thus, trustworthy AI has attracted immense
attention recently, which requires careful consideration to avoid the adverse
effects that AI may bring to humans, so that humans can fully trust and live in
harmony with AI technologies.
  Recent years have witnessed a tremendous amount of research on trustworthy
AI. In this survey, we present a comprehensive survey of trustworthy AI from a
computational perspective, to help readers understand the latest technologies
for achieving trustworthy AI. Trustworthy AI is a large and complex area,
involving various dimensions. In this work, we focus on six of the most crucial
dimensions in achieving trustworthy AI: (i) Safety &amp; Robustness, (ii)
Non-discrimination &amp; Fairness, (iii) Explainability, (iv) Privacy, (v)
Accountability &amp; Auditability, and (vi) Environmental Well-Being. For each
dimension, we review the recent related technologies according to a taxonomy
and summarize their applications in real-world systems. We also discuss the
accordant and conflicting interactions among different dimensions and discuss
potential aspects for trustworthy AI to investigate in the future.
</summary>
    <author>
      <name>Haochen Liu</name>
    </author>
    <author>
      <name>Yiqi Wang</name>
    </author>
    <author>
      <name>Wenqi Fan</name>
    </author>
    <author>
      <name>Xiaorui Liu</name>
    </author>
    <author>
      <name>Yaxin Li</name>
    </author>
    <author>
      <name>Shaili Jain</name>
    </author>
    <author>
      <name>Yunhao Liu</name>
    </author>
    <author>
      <name>Anil K. Jain</name>
    </author>
    <author>
      <name>Jiliang Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">55 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.06641v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.06641v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.06026v1</id>
    <updated>2022-01-16T11:49:33Z</updated>
    <published>2022-01-16T11:49:33Z</published>
    <title>Toward Among-Device AI from On-Device AI with Stream Pipelines</title>
    <summary>  Modern consumer electronic devices often provide intelligence services with
deep neural networks. We have started migrating the computing locations of
intelligence services from cloud servers (traditional AI systems) to the
corresponding devices (on-device AI systems). On-device AI systems generally
have the advantages of preserving privacy, removing network latency, and saving
cloud costs. With the emergent of on-device AI systems having relatively low
computing power, the inconsistent and varying hardware resources and
capabilities pose difficulties. Authors' affiliation has started applying a
stream pipeline framework, NNStreamer, for on-device AI systems, saving
developmental costs and hardware resources and improving performance. We want
to expand the types of devices and applications with on-device AI services
products of both the affiliation and second/third parties. We also want to make
each AI service atomic, re-deployable, and shared among connected devices of
arbitrary vendors; we now have yet another requirement introduced as it always
has been. The new requirement of "among-device AI" includes connectivity
between AI pipelines so that they may share computing resources and hardware
capabilities across a wide range of devices regardless of vendors and
manufacturers. We propose extensions of the stream pipeline framework,
NNStreamer, for on-device AI so that NNStreamer may provide among-device AI
capability. This work is a Linux Foundation (LF AI and Data) open source
project accepting contributions from the general public.
</summary>
    <author>
      <name>MyungJoo Ham</name>
    </author>
    <author>
      <name>Sangjung Woo</name>
    </author>
    <author>
      <name>Jaeyun Jung</name>
    </author>
    <author>
      <name>Wook Song</name>
    </author>
    <author>
      <name>Gichan Jang</name>
    </author>
    <author>
      <name>Yongjoo Ahn</name>
    </author>
    <author>
      <name>Hyoung Joo Ahn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in ICSE 2022 SEIP (preprint)</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.06026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.06026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.07435v1</id>
    <updated>2022-02-12T14:14:32Z</updated>
    <published>2022-02-12T14:14:32Z</published>
    <title>State of AI Ethics Report (Volume 6, February 2022)</title>
    <summary>  This report from the Montreal AI Ethics Institute (MAIEI) covers the most
salient progress in research and reporting over the second half of 2021 in the
field of AI ethics. Particular emphasis is placed on an "Analysis of the AI
Ecosystem", "Privacy", "Bias", "Social Media and Problematic Information", "AI
Design and Governance", "Laws and Regulations", "Trends", and other areas
covered in the "Outside the Boxes" section. The two AI spotlights feature
application pieces on "Constructing and Deconstructing Gender with AI-Generated
Art" as well as "Will an Artificial Intellichef be Cooking Your Next Meal at a
Michelin Star Restaurant?". Given MAIEI's mission to democratize AI,
submissions from external collaborators have featured, such as pieces on the
"Challenges of AI Development in Vietnam: Funding, Talent and Ethics" and using
"Representation and Imagination for Preventing AI Harms". The report is a
comprehensive overview of what the key issues in the field of AI ethics were in
2021, what trends are emergent, what gaps exist, and a peek into what to expect
from the field of AI ethics in 2022. It is a resource for researchers and
practitioners alike in the field to set their research and development agendas
to make contributions to the field of AI ethics.
</summary>
    <author>
      <name>Abhishek Gupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Microsoft</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Green Software Foundation</arxiv:affiliation>
    </author>
    <author>
      <name>Connor Wright</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Exeter</arxiv:affiliation>
    </author>
    <author>
      <name>Marianna Bergamaschi Ganapini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Union College</arxiv:affiliation>
    </author>
    <author>
      <name>Masa Sweidan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Renjie Butalid</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">295 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.07435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.07435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4; I.2; A.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.04963v3</id>
    <updated>2022-09-15T02:03:24Z</updated>
    <published>2022-09-12T00:09:08Z</published>
    <title>Responsible AI Pattern Catalogue: A Multivocal Literature Review</title>
    <summary>  Responsible AI has been widely considered as one of the greatest scientific
challenges of our time and the key to increase the adoption of AI. A number of
AI ethics principles frameworks have been published recently. However, without
further best practice guidance, practitioners are left with nothing much beyond
truisms. Also, significant efforts have been placed at algorithm-level rather
than system-level, mainly focusing on a subset of mathematics-amenable ethical
principles (such as fairness). Nevertheless, ethical issues can occur at any
step of the development lifecycle crosscutting many AI and non-AI components of
systems beyond AI algorithms and models. To operationalize responsible AI from
a system perspective, in this paper, we present a Responsible AI Pattern
Catalogue based on the results of a Multivocal Literature Review (MLR). Rather
than staying at the principle or algorithm level, we focus on patterns that AI
system stakeholders can undertake in practice to ensure that the developed AI
systems are responsible throughout the entire governance and engineering
lifecycle. The Responsible AI Pattern Catalogue classifies the patterns into
three groups: multi-level governance patterns, trustworthy process patterns,
and responsible-AI-by-design product patterns. These patterns provide a
systematic and actionable guidance for stakeholders to implement responsible
AI.
</summary>
    <author>
      <name>Qinghua Lu</name>
    </author>
    <author>
      <name>Liming Zhu</name>
    </author>
    <author>
      <name>Xiwei Xu</name>
    </author>
    <author>
      <name>Jon Whittle</name>
    </author>
    <author>
      <name>Didar Zowghi</name>
    </author>
    <author>
      <name>Aurelie Jacquet</name>
    </author>
    <link href="http://arxiv.org/abs/2209.04963v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.04963v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.01812v1</id>
    <updated>2020-09-03T17:26:04Z</updated>
    <published>2020-09-03T17:26:04Z</published>
    <title>The Pace of Artificial Intelligence Innovations: Speed, Talent, and
  Trial-and-Error</title>
    <summary>  Innovations in artificial intelligence (AI) are occurring at speeds faster
than ever witnessed before. However, few studies have managed to measure or
depict this increasing velocity of innovations in the field of AI. In this
paper, we combine data on AI from arXiv and Semantic Scholar to explore the
pace of AI innovations from three perspectives: AI publications, AI players,
and AI updates (trial and error). A research framework and three novel
indicators, Average Time Interval (ATI), Innovation Speed (IS) and Update Speed
(US), are proposed to measure the pace of innovations in the field of AI. The
results show that: (1) in 2019, more than 3 AI preprints were submitted to
arXiv per hour, over 148 times faster than in 1994. Furthermore, there was one
deep learning-related preprint submitted to arXiv every 0.87 hours in 2019,
over 1,064 times faster than in 1994. (2) For AI players, 5.26 new researchers
entered into the field of AI each hour in 2019, more than 175 times faster than
in the 1990s. (3) As for AI updates (trial and error), one updated AI preprint
was submitted to arXiv every 41 days, with around 33% of AI preprints having
been updated at least twice in 2019. In addition, as reported in 2019, it took,
on average, only around 0.2 year for AI preprints to receive their first
citations, which is 5 times faster than 2000-2007. This swift pace in AI
illustrates the increase in popularity of AI innovation. The systematic and
fine-grained analysis of the AI field enabled to portrait the pace of AI
innovation and demonstrated that the proposed approach can be adopted to
understand other fast-growing fields such as cancer research and nano science.
</summary>
    <author>
      <name>Xuli Tang</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Ying Ding</name>
    </author>
    <author>
      <name>Min Song</name>
    </author>
    <author>
      <name>Yi Bu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Informetrics 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.01812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.01812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.09696v1</id>
    <updated>2022-05-19T16:59:25Z</updated>
    <published>2022-05-19T16:59:25Z</published>
    <title>Who Goes First? Influences of Human-AI Workflow on Decision Making in
  Clinical Imaging</title>
    <summary>  Details of the designs and mechanisms in support of human-AI collaboration
must be considered in the real-world fielding of AI technologies. A critical
aspect of interaction design for AI-assisted human decision making are policies
about the display and sequencing of AI inferences within larger decision-making
workflows. We have a poor understanding of the influences of making AI
inferences available before versus after human review of a diagnostic task at
hand. We explore the effects of providing AI assistance at the start of a
diagnostic session in radiology versus after the radiologist has made a
provisional decision. We conducted a user study where 19 veterinary
radiologists identified radiographic findings present in patients' X-ray
images, with the aid of an AI tool. We employed two workflow configurations to
analyze (i) anchoring effects, (ii) human-AI team diagnostic performance and
agreement, (iii) time spent and confidence in decision making, and (iv)
perceived usefulness of the AI. We found that participants who are asked to
register provisional responses in advance of reviewing AI inferences are less
likely to agree with the AI regardless of whether the advice is accurate and,
in instances of disagreement with the AI, are less likely to seek the second
opinion of a colleague. These participants also reported the AI advice to be
less useful. Surprisingly, requiring provisional decisions on cases in advance
of the display of AI inferences did not lengthen the time participants spent on
the task. The study provides generalizable and actionable insights for the
deployment of clinical AI tools in human-in-the-loop systems and introduces a
methodology for studying alternative designs for human-AI collaboration. We
make our experimental platform available as open source to facilitate future
research on the influence of alternate designs on human-AI workflows.
</summary>
    <author>
      <name>Riccardo Fogliato</name>
    </author>
    <author>
      <name>Shreya Chappidi</name>
    </author>
    <author>
      <name>Matthew Lungren</name>
    </author>
    <author>
      <name>Michael Fitzke</name>
    </author>
    <author>
      <name>Mark Parkinson</name>
    </author>
    <author>
      <name>Diane Wilson</name>
    </author>
    <author>
      <name>Paul Fisher</name>
    </author>
    <author>
      <name>Eric Horvitz</name>
    </author>
    <author>
      <name>Kori Inkpen</name>
    </author>
    <author>
      <name>Besmira Nushi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACM Conference on Fairness, Accountability, and
  Transparency (FAccT), 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.09696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.09696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.08966v3</id>
    <updated>2023-02-23T18:54:08Z</updated>
    <published>2022-06-17T18:40:41Z</published>
    <title>Actionable Guidance for High-Consequence AI Risk Management: Towards
  Standards Addressing AI Catastrophic Risks</title>
    <summary>  Artificial intelligence (AI) systems can provide many beneficial capabilities
but also risks of adverse events. Some AI systems could present risks of events
with very high or catastrophic consequences at societal scale. The US National
Institute of Standards and Technology (NIST) has been developing the NIST
Artificial Intelligence Risk Management Framework (AI RMF) as voluntary
guidance on AI risk assessment and management for AI developers and others. For
addressing risks of events with catastrophic consequences, NIST indicated a
need to translate from high level principles to actionable risk management
guidance.
  In this document, we provide detailed actionable-guidance recommendations
focused on identifying and managing risks of events with very high or
catastrophic consequences, intended as a risk management practices resource for
NIST for AI RMF version 1.0 (released in January 2023), or for AI RMF users, or
for other AI risk management guidance and standards as appropriate. We also
provide our methodology for our recommendations.
  We provide actionable-guidance recommendations for AI RMF 1.0 on: identifying
risks from potential unintended uses and misuses of AI systems; including
catastrophic-risk factors within the scope of risk assessments and impact
assessments; identifying and mitigating human rights harms; and reporting
information on AI risk factors including catastrophic-risk factors.
  In addition, we provide recommendations on additional issues for a roadmap
for later versions of the AI RMF or supplementary publications. These include:
providing an AI RMF Profile with supplementary guidance for cutting-edge
increasingly multi-purpose or general-purpose AI.
  We aim for this work to be a concrete risk-management practices contribution,
and to stimulate constructive dialogue on how to address catastrophic risks and
associated issues in AI standards.
</summary>
    <author>
      <name>Anthony M. Barrett</name>
    </author>
    <author>
      <name>Dan Hendrycks</name>
    </author>
    <author>
      <name>Jessica Newman</name>
    </author>
    <author>
      <name>Brandie Nonnecke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">56 pages; updated throughout for general consistency with NIST AI RMF
  1.0</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.08966v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.08966v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.04734v1</id>
    <updated>2017-09-14T12:37:08Z</updated>
    <published>2017-09-14T12:37:08Z</published>
    <title>Perspectives for Evaluating Conversational AI</title>
    <summary>  Conversational AI systems are becoming famous in day to day lives. In this
paper, we are trying to address the following key question: To identify whether
design, as well as development efforts for search oriented conversational AI
are successful or not.It is tricky to define 'success' in the case of
conversational AI and equally tricky part is to use appropriate metrics for the
evaluation of conversational AI. We propose four different perspectives namely
user experience, information retrieval, linguistic and artificial intelligence
for the evaluation of conversational AI systems. Additionally, background
details of conversational AI systems are provided including desirable
characteristics of personal assistants, differences between chatbot and an AI
based personal assistant. An importance of personalization and how it can be
achieved is explained in detail. Current challenges in the development of an
ideal conversational AI (personal assistant) are also highlighted along with
guidelines for achieving personalized experience for users.
</summary>
    <author>
      <name>Mahipal Jadeja</name>
    </author>
    <author>
      <name>Neelanshi Varia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SCAI'17 - Search-Oriented Conversational AI (@ICTIR'17)</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.04734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.04734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.05849v1</id>
    <updated>2015-12-18T04:17:39Z</updated>
    <published>2015-12-18T04:17:39Z</published>
    <title>Modeling Progress in AI</title>
    <summary>  Participants in recent discussions of AI-related issues ranging from
intelligence explosion to technological unemployment have made diverse claims
about the nature, pace, and drivers of progress in AI. However, these theories
are rarely specified in enough detail to enable systematic evaluation of their
assumptions or to extrapolate progress quantitatively, as is often done with
some success in other technological domains. After reviewing relevant
literatures and justifying the need for more rigorous modeling of AI progress,
this paper contributes to that research program by suggesting ways to account
for the relationship between hardware speed increases and algorithmic
improvements in AI, the role of human inputs in enabling AI capabilities, and
the relationships between different sub-fields of AI. It then outlines ways of
tailoring AI progress models to generate insights on the specific issue of
technological unemployment, and outlines future directions for research on AI
progress.
</summary>
    <author>
      <name>Miles Brundage</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2016 Workshop on AI, Ethics, and Society</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.05849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.05849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.05673v1</id>
    <updated>2018-09-15T08:30:39Z</updated>
    <published>2018-09-15T08:30:39Z</published>
    <title>Distinguished Capabilities of Artificial Intelligence Wireless
  Communication Systems</title>
    <summary>  With the great success of artificial intelligence (AI) technologies in
pattern recognitions and signal processing, it is interesting to introduce AI
technologies into wireless communication systems. Currently, most of studies
are focused on applying AI technologies for solving old problems, e.g.,
wireless location accuracy and resource allocation optimization in wireless
communication systems. However, It is important to distinguish new capabilities
created by AI technologies and rethink wireless communication systems based on
AI running schemes. Compared with conventional capabilities of wireless
communication systems, three distinguished capabilities, i.e., the cognitive,
learning and proactive capabilities are proposed for future AI wireless
communication systems. Moreover, an intelligent vehicular communication system
is configured to validate the cognitive capability based on AI clustering
algorithm. Considering the revolutionary impact of AI technologies on the data,
transmission and protocol architecture of wireless communication systems, the
future challenges of AI wireless communication systems are analyzed. Driven by
new distinguished capabilities of AI wireless communication systems, the new
wireless communication theory and functions would indeed emerge in the next
round of the wireless communications revolution.
</summary>
    <author>
      <name>Xiaohu Ge</name>
    </author>
    <link href="http://arxiv.org/abs/1809.05673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.05673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.01148v1</id>
    <updated>2019-06-04T01:09:14Z</updated>
    <published>2019-06-04T01:09:14Z</published>
    <title>A Case for Backward Compatibility for Human-AI Teams</title>
    <summary>  AI systems are being deployed to support human decision making in high-stakes
domains. In many cases, the human and AI form a team, in which the human makes
decisions after reviewing the AI's inferences. A successful partnership
requires that the human develops insights into the performance of the AI
system, including its failures. We study the influence of updates to an AI
system in this setting. While updates can increase the AI's predictive
performance, they may also lead to changes that are at odds with the user's
prior experiences and confidence in the AI's inferences, hurting therefore the
overall team performance. We introduce the notion of the compatibility of an AI
update with prior user experience and present methods for studying the role of
compatibility in human-AI teams. Empirical results on three high-stakes domains
show that current machine learning algorithms do not produce compatible
updates. We propose a re-training objective to improve the compatibility of an
update by penalizing new errors. The objective offers full leverage of the
performance/compatibility tradeoff, enabling more compatible yet accurate
updates.
</summary>
    <author>
      <name>Gagan Bansal</name>
    </author>
    <author>
      <name>Besmira Nushi</name>
    </author>
    <author>
      <name>Ece Kamar</name>
    </author>
    <author>
      <name>Dan Weld</name>
    </author>
    <author>
      <name>Walter Lasecki</name>
    </author>
    <author>
      <name>Eric Horvitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">presented at 2019 ICML Workshop on Human in the Loop Learning (HILL
  2019), Long Beach, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.01148v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01148v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.03595v1</id>
    <updated>2019-06-09T08:44:23Z</updated>
    <published>2019-06-09T08:44:23Z</published>
    <title>Federated AI lets a team imagine together: Federated Learning of GANs</title>
    <summary>  Envisioning a new imaginative idea together is a popular human need.
Imagining together as a team can often lead to breakthrough ideas, but the
collaboration effort can also be challenging, especially when the team members
are separated by time and space. What if there is a AI that can assist the team
to collaboratively envision new ideas?. Is it possible to develop a working
model of such an AI? This paper aims to design such an intelligence. This paper
proposes a approach to design a creative and collaborative intelligence by
employing a form of distributed machine learning approach called Federated
Learning along with fusion on Generative Adversarial Networks, GAN. This
collaborative creative AI presents a new paradigm in AI, one that lets a team
of two or more to come together to imagine and envision ideas that synergies
well with interests of all members of the team. In short, this paper explores
the design of a novel type of AI paradigm, called Federated AI Imagination, one
that lets geographically distributed teams to collaboratively imagine.
</summary>
    <author>
      <name>Rajagopal. A</name>
    </author>
    <author>
      <name>Nirmala. V</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords. Artificial Intelligence, Distributed Machine Learning,
  Generative Deep Learning, Generative Adversarial Networks, Federated
  learning, Creative AI, AI based Collaboration, AI planning</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.03595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.07089v1</id>
    <updated>2019-10-15T22:34:50Z</updated>
    <published>2019-10-15T22:34:50Z</published>
    <title>Challenges of Human-Aware AI Systems</title>
    <summary>  From its inception, AI has had a rather ambivalent relationship to
humans---swinging between their augmentation and replacement. Now, as AI
technologies enter our everyday lives at an ever increasing pace, there is a
greater need for AI systems to work synergistically with humans. To do this
effectively, AI systems must pay more attention to aspects of intelligence that
helped humans work with each other---including social intelligence. I will
discuss the research challenges in designing such human-aware AI systems,
including modeling the mental states of humans in the loop, recognizing their
desires and intentions, providing proactive support, exhibiting explicable
behavior, giving cogent explanations on demand, and engendering trust. I will
survey the progress made so far on these challenges, and highlight some
promising directions. I will also touch on the additional ethical quandaries
that such systems pose. I will end by arguing that the quest for human-aware AI
systems broadens the scope of AI enterprise, necessitates and facilitates true
inter-disciplinary collaborations, and can go a long way towards increasing
public acceptance of AI technologies.
</summary>
    <author>
      <name>Subbarao Kambhampati</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in AI Magazine (Written version of AAAI 2018 Presidential
  Address. Video and slides at http://bit.ly/2tHyzAh )</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.07089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03216v1</id>
    <updated>2019-11-08T12:31:49Z</updated>
    <published>2019-11-08T12:31:49Z</published>
    <title>AI Ethics for Systemic Issues: A Structural Approach</title>
    <summary>  The debate on AI ethics largely focuses on technical improvements and
stronger regulation to prevent accidents or misuse of AI, with solutions
relying on holding individual actors accountable for responsible AI
development. While useful and necessary, we argue that this "agency" approach
disregards more indirect and complex risks resulting from AI's interaction with
the socio-economic and political context. This paper calls for a "structural"
approach to assessing AI's effects in order to understand and prevent such
systemic risks where no individual can be held accountable for the broader
negative impacts. This is particularly relevant for AI applied to systemic
issues such as climate change and food security which require political
solutions and global cooperation. To properly address the wide range of AI
risks and ensure 'AI for social good', agency-focused policies must be
complemented by policies informed by a structural approach.
</summary>
    <author>
      <name>Agnes Schim van der Loeff</name>
    </author>
    <author>
      <name>Iggy Bassi</name>
    </author>
    <author>
      <name>Sachin Kapila</name>
    </author>
    <author>
      <name>Jevgenij Gamper</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS AI for Social Good 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.03216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.06817v1</id>
    <updated>2019-12-14T09:54:36Z</updated>
    <published>2019-12-14T09:54:36Z</published>
    <title>Ten AI Stepping Stones for Cybersecurity</title>
    <summary>  With the turmoil in cybersecurity and the mind-blowing advances in AI, it is
only natural that cybersecurity practitioners consider further employing
learning techniques to help secure their organizations and improve the
efficiency of their security operation centers. But with great fears come great
opportunities for both the good and the evil, and a myriad of bad deals. This
paper discusses ten issues in cybersecurity that hopefully will make it easier
for practitioners to ask detailed questions about what they want from an AI
system in their cybersecurity operations. We draw on the state of the art to
provide factual arguments for a discussion on well-established AI in
cybersecurity issues, including the current scope of AI and its application to
cybersecurity, the impact of privacy concerns on the cybersecurity data that
can be collected and shared externally to the organization, how an AI decision
can be explained to the person running the operations center, and the
implications of the adversarial nature of cybersecurity in the learning
techniques. We then discuss the use of AI by attackers on a level playing field
including several issues in an AI battlefield, and an AI perspective on the old
cat-and-mouse game including how the adversary may assess your AI power.
</summary>
    <author>
      <name>Ricardo Morla</name>
    </author>
    <link href="http://arxiv.org/abs/1912.06817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.06817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.05375v1</id>
    <updated>2020-01-15T15:30:29Z</updated>
    <published>2020-01-15T15:30:29Z</published>
    <title>AAAI FSS-19: Human-Centered AI: Trustworthiness of AI Models and Data
  Proceedings</title>
    <summary>  To facilitate the widespread acceptance of AI systems guiding decision-making
in real-world applications, it is key that solutions comprise trustworthy,
integrated human-AI systems. Not only in safety-critical applications such as
autonomous driving or medicine, but also in dynamic open world systems in
industry and government it is crucial for predictive models to be
uncertainty-aware and yield trustworthy predictions. Another key requirement
for deployment of AI at enterprise scale is to realize the importance of
integrating human-centered design into AI systems such that humans are able to
use systems effectively, understand results and output, and explain findings to
oversight committees.
  While the focus of this symposium was on AI systems to improve data quality
and technical robustness and safety, we welcomed submissions from broadly
defined areas also discussing approaches addressing requirements such as
explainable models, human trust and ethical aspects of AI.
</summary>
    <author>
      <name>Florian Buettner</name>
    </author>
    <author>
      <name>John Piorkowski</name>
    </author>
    <author>
      <name>Ian McCulloh</name>
    </author>
    <author>
      <name>Ulli Waltinger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings for AAAI 2019 Fall Symposium Series - Human-centered AI:
  Trustworthiness of AI Models &amp; Data</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.05375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.14750v1</id>
    <updated>2020-06-26T01:32:53Z</updated>
    <published>2020-06-26T01:32:53Z</published>
    <title>Could regulating the creators deliver trustworthy AI?</title>
    <summary>  Is a new regulated profession, such as Artificial Intelligence (AI) Architect
who is responsible and accountable for AI outputs necessary to ensure
trustworthy AI? AI is becoming all pervasive and is often deployed in everyday
technologies, devices and services without our knowledge. There is heightened
awareness of AI in recent years which has brought with it fear. This fear is
compounded by the inability to point to a trustworthy source of AI, however
even the term "trustworthy AI" itself is troublesome. Some consider trustworthy
AI to be that which complies with relevant laws, while others point to the
requirement to comply with ethics and standards (whether in addition to or in
isolation of the law). This immediately raises questions of whose ethics and
which standards should be applied and whether these are sufficient to produce
trustworthy AI in any event.
</summary>
    <author>
      <name>Labhaoise Ni Fhaolain</name>
    </author>
    <author>
      <name>Andrew Hines</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in The Second Workshop on Implementing Machine
  Ethics, Dublin, Ireland, 30 June 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.14750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.14750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.12701v1</id>
    <updated>2021-01-29T17:37:44Z</updated>
    <published>2021-01-29T17:37:44Z</published>
    <title>Time for AI (Ethics) Maturity Model Is Now</title>
    <summary>  There appears to be a common agreement that ethical concerns are of high
importance when it comes to systems equipped with some sort of Artificial
Intelligence (AI). Demands for ethical AI are declared from all directions. As
a response, in recent years, public bodies, governments, and universities have
rushed in to provide a set of principles to be considered when AI based systems
are designed and used. We have learned, however, that high-level principles do
not turn easily into actionable advice for practitioners. Hence, also companies
are publishing their own ethical guidelines to guide their AI development. This
paper argues that AI software is still software and needs to be approached from
the software development perspective. The software engineering paradigm has
introduced maturity model thinking, which provides a roadmap for companies to
improve their performance from the selected viewpoints known as the key
capabilities. We want to voice out a call for action for the development of a
maturity model for AI software. We wish to discuss whether the focus should be
on AI ethics or, more broadly, the quality of an AI system, called a maturity
model for the development of AI systems.
</summary>
    <author>
      <name>Ville Vakkuri</name>
    </author>
    <author>
      <name>Marianna Jantunen</name>
    </author>
    <author>
      <name>Erika Halme</name>
    </author>
    <author>
      <name>Kai-Kristian Kemell</name>
    </author>
    <author>
      <name>Anh Nguyen-Duc</name>
    </author>
    <author>
      <name>Tommi Mikkonen</name>
    </author>
    <author>
      <name>Pekka Abrahamsson</name>
    </author>
    <link href="http://arxiv.org/abs/2101.12701v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.12701v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.07536v1</id>
    <updated>2021-02-15T13:15:12Z</updated>
    <published>2021-02-15T13:15:12Z</published>
    <title>The corruptive force of AI-generated advice</title>
    <summary>  Artificial Intelligence (AI) is increasingly becoming a trusted advisor in
people's lives. A new concern arises if AI persuades people to break ethical
rules for profit. Employing a large-scale behavioural experiment (N = 1,572),
we test whether AI-generated advice can corrupt people. We further test whether
transparency about AI presence, a commonly proposed policy, mitigates potential
harm of AI-generated advice. Using the Natural Language Processing algorithm,
GPT-2, we generated honesty-promoting and dishonesty-promoting advice.
Participants read one type of advice before engaging in a task in which they
could lie for profit. Testing human behaviour in interaction with actual AI
outputs, we provide first behavioural insights into the role of AI as an
advisor. Results reveal that AI-generated advice corrupts people, even when
they know the source of the advice. In fact, AI's corrupting force is as strong
as humans'.
</summary>
    <author>
      <name>Margarita Leib</name>
    </author>
    <author>
      <name>Nils C. Köbis</name>
    </author>
    <author>
      <name>Rainer Michael Rilke</name>
    </author>
    <author>
      <name>Marloes Hagens</name>
    </author>
    <author>
      <name>Bernd Irlenbusch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Leib &amp; K\"obis share first authorship</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.07536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.07536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.12848v1</id>
    <updated>2021-02-25T13:40:17Z</updated>
    <published>2021-02-25T13:40:17Z</published>
    <title>HPC AI500: Representative, Repeatable and Simple HPC AI Benchmarking</title>
    <summary>  Recent years witness a trend of applying large-scale distributed deep
learning algorithms (HPC AI) in both business and scientific computing areas,
whose goal is to speed up the training time to achieve a state-of-the-art
quality. The HPC AI benchmarks accelerate the process. Unfortunately,
benchmarking HPC AI systems at scale raises serious challenges. This paper
presents a representative, repeatable and simple HPC AI benchmarking
methodology. Among the seventeen AI workloads of AIBench Training -- by far the
most comprehensive AI Training benchmarks suite -- we choose two representative
and repeatable AI workloads. The selected HPC AI benchmarks include both
business and scientific computing: Image Classification and Extreme Weather
Analytics. To rank HPC AI systems, we present a new metric named Valid FLOPS,
emphasizing both throughput performance and a target quality. The
specification, source code, datasets, and HPC AI500 ranking numbers are
publicly available from \url{https://www.benchcouncil.org/HPCAI500/}.
</summary>
    <author>
      <name>Zihan Jiang</name>
    </author>
    <author>
      <name>Wanling Gao</name>
    </author>
    <author>
      <name>Fei Tang</name>
    </author>
    <author>
      <name>Xingwang Xiong</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Chuanxin Lan</name>
    </author>
    <author>
      <name>Chunjie Luo</name>
    </author>
    <author>
      <name>Hongxiao Li</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2007.00279</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.12848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.12848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.10248v1</id>
    <updated>2021-03-11T11:44:23Z</updated>
    <published>2021-03-11T11:44:23Z</published>
    <title>Systematic Mapping Study on the Machine Learning Lifecycle</title>
    <summary>  The development of artificial intelligence (AI) has made various industries
eager to explore the benefits of AI. There is an increasing amount of research
surrounding AI, most of which is centred on the development of new AI
algorithms and techniques. However, the advent of AI is bringing an increasing
set of practical problems related to AI model lifecycle management that need to
be investigated. We address this gap by conducting a systematic mapping study
on the lifecycle of AI model. Through quantitative research, we provide an
overview of the field, identify research opportunities, and provide suggestions
for future research. Our study yields 405 publications published from 2005 to
2020, mapped in 5 different main research topics, and 31 sub-topics. We observe
that only a minority of publications focus on data management and model
production problems, and that more studies should address the AI lifecycle from
a holistic perspective.
</summary>
    <author>
      <name>Yuanhao Xie</name>
    </author>
    <author>
      <name>Luís Cruz</name>
    </author>
    <author>
      <name>Petra Heck</name>
    </author>
    <author>
      <name>Jan S. Rellermeyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at WAIN21: 1st Workshop on AI Engineering - Software
  Engineering for AI</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.10248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.10248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01 (Primary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.9; I.2.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.03192v1</id>
    <updated>2021-05-07T12:01:31Z</updated>
    <published>2021-05-07T12:01:31Z</published>
    <title>An interdisciplinary conceptual study of Artificial Intelligence (AI)
  for helping benefit-risk assessment practices: Towards a comprehensive
  qualification matrix of AI programs and devices (pre-print 2020)</title>
    <summary>  This paper proposes a comprehensive analysis of existing concepts coming from
different disciplines tackling the notion of intelligence, namely psychology
and engineering, and from disciplines aiming to regulate AI innovations, namely
AI ethics and law. The aim is to identify shared notions or discrepancies to
consider for qualifying AI systems. Relevant concepts are integrated into a
matrix intended to help defining more precisely when and how computing tools
(programs or devices) may be qualified as AI while highlighting critical
features to serve a specific technical, ethical and legal assessment of
challenges in AI development. Some adaptations of existing notions of AI
characteristics are proposed. The matrix is a risk-based conceptual model
designed to allow an empirical, flexible and scalable qualification of AI
technologies in the perspective of benefit-risk assessment practices,
technological monitoring and regulatory compliance: it offers a structured
reflection tool for stakeholders in AI development that are engaged in
responsible research and innovation.Pre-print version (achieved on May 2020)
</summary>
    <author>
      <name>Gauthier Chassang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INSERM,PFGS</arxiv:affiliation>
    </author>
    <author>
      <name>Mogens Thomsen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INSERM</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Rumeau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Florence Sèdes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <author>
      <name>Alejandra Delfin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INSERM</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2105.03192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.03192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.08576v2</id>
    <updated>2021-11-05T16:39:20Z</updated>
    <published>2021-05-18T15:01:57Z</published>
    <title>AI-Native Network Slicing for 6G Networks</title>
    <summary>  With the global roll-out of the fifth generation (5G) networks, it is
necessary to look beyond 5G and envision the 6G networks. The 6G networks are
expected to have space-air-ground integrated networks, advanced network
virtualization, and ubiquitous intelligence. This article presents an
artificial intelligence (AI)-native network slicing architecture for 6G
networks to enable the synergy of AI and network slicing, thereby facilitating
intelligent network management and supporting emerging AI services. AI-based
solutions are first discussed across network slicing lifecycle to intelligently
manage network slices, i.e., AI for slicing. Then, network slicing solutions
are studied to support emerging AI services by constructing AI instances and
performing efficient resource management, i.e., slicing for AI. Finally, a case
study is presented, followed by a discussion of open research issues that are
essential for AI-native network slicing in 6G networks.
</summary>
    <author>
      <name>Wen Wu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name>Conghao Zhou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name>Mushu Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name>Huaqing Wu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name>Haibo Zhou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name>Ning Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name> Xuemin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name> Shen</name>
    </author>
    <author>
      <name>Weihua Zhuang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted by IEEE Wireless Communications Magazine</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.08576v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.08576v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.04684v1</id>
    <updated>2021-06-08T20:49:11Z</updated>
    <published>2021-06-08T20:49:11Z</published>
    <title>Explainable AI for medical imaging: Explaining pneumothorax diagnoses
  with Bayesian Teaching</title>
    <summary>  Limited expert time is a key bottleneck in medical imaging. Due to advances
in image classification, AI can now serve as decision-support for medical
experts, with the potential for great gains in radiologist productivity and, by
extension, public health. However, these gains are contingent on building and
maintaining experts' trust in the AI agents. Explainable AI may build such
trust by helping medical experts to understand the AI decision processes behind
diagnostic judgements. Here we introduce and evaluate explanations based on
Bayesian Teaching, a formal account of explanation rooted in the cognitive
science of human learning. We find that medical experts exposed to explanations
generated by Bayesian Teaching successfully predict the AI's diagnostic
decisions and are more likely to certify the AI for cases when the AI is
correct than when it is wrong, indicating appropriate trust. These results show
that Explainable AI can be used to support human-AI collaboration in medical
imaging.
</summary>
    <author>
      <name>Tomas Folke</name>
    </author>
    <author>
      <name>Scott Cheng-Hsin Yang</name>
    </author>
    <author>
      <name>Sean Anderson</name>
    </author>
    <author>
      <name>Patrick Shafto</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. SPIE 11746, Artificial Intelligence and Machine Learning for
  Multi-Domain Operations Applications III, 117462J (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.04684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.04684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.11913v2</id>
    <updated>2021-09-21T02:45:23Z</updated>
    <published>2021-07-26T00:26:12Z</published>
    <title>Measuring Ethics in AI with AI: A Methodology and Dataset Construction</title>
    <summary>  Recently, the use of sound measures and metrics in Artificial Intelligence
has become the subject of interest of academia, government, and industry.
Efforts towards measuring different phenomena have gained traction in the AI
community, as illustrated by the publication of several influential field
reports and policy documents. These metrics are designed to help decision
takers to inform themselves about the fast-moving and impacting influences of
key advances in Artificial Intelligence in general and Machine Learning in
particular. In this paper we propose to use such newfound capabilities of AI
technologies to augment our AI measuring capabilities. We do so by training a
model to classify publications related to ethical issues and concerns. In our
methodology we use an expert, manually curated dataset as the training set and
then evaluate a large set of research papers. Finally, we highlight the
implications of AI metrics, in particular their contribution towards developing
trustful and fair AI-based tools and technologies. Keywords: AI Ethics; AI
Fairness; AI Measurement. Ethics in Computer Science.
</summary>
    <author>
      <name>Pedro H. C. Avelar</name>
    </author>
    <author>
      <name>Rafael B. Audibert</name>
    </author>
    <author>
      <name>Anderson R. Tavares</name>
    </author>
    <author>
      <name>Luís C. Lamb</name>
    </author>
    <link href="http://arxiv.org/abs/2107.11913v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.11913v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; K.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.04454v1</id>
    <updated>2021-10-28T16:06:07Z</updated>
    <published>2021-10-28T16:06:07Z</published>
    <title>AI Federalism: Shaping AI Policy within States in Germany</title>
    <summary>  Recent AI governance research has focused heavily on the analysis of strategy
papers and ethics guidelines for AI published by national governments and
international bodies. Meanwhile, subnational institutions have also published
documents on Artificial Intelligence, yet these have been largely absent from
policy analyses. This is surprising because AI is connected to many policy
areas, such as economic or research policy, where the competences are already
distributed between the national and subnational level. To better understand
the current dynamics of AI governance, it is essential to consider the context
of policy making beyond the federal government. Although AI may be considered a
new policy field, it is created, contested and ultimately shaped within
existing political structures and dynamics. We therefore argue that more
attention should be dedicated to subnational efforts to shape AI and present
initial findings from our case study of Germany. Analyzing AI as a policy field
on different levels of government will contribute to a better understanding of
the developments and implementations of AI strategies in different national
contexts.
</summary>
    <author>
      <name>Anna Jobin</name>
    </author>
    <author>
      <name>Licinia Guettel</name>
    </author>
    <author>
      <name>Laura Liebig</name>
    </author>
    <author>
      <name>Christian Katzenbach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Working paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.04454v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.04454v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.06207v1</id>
    <updated>2021-11-11T13:54:31Z</updated>
    <published>2021-11-11T13:54:31Z</published>
    <title>Governance of Ethical and Trustworthy AI Systems: Research Gaps in the
  ECCOLA Method</title>
    <summary>  Advances in machine learning (ML) technologies have greatly improved
Artificial Intelligence (AI) systems. As a result, AI systems have become
ubiquitous, with their application prevalent in virtually all sectors. However,
AI systems have prompted ethical concerns, especially as their usage crosses
boundaries in sensitive areas such as healthcare, transportation, and security.
As a result, users are calling for better AI governance practices in ethical AI
systems. Therefore, AI development methods are encouraged to foster these
practices. This research analyzes the ECCOLA method for developing ethical and
trustworthy AI systems to determine if it enables AI governance in development
processes through ethical practices. The results demonstrate that while ECCOLA
fully facilitates AI governance in corporate governance practices in all its
processes, some of its practices do not fully foster data governance and
information governance practices. This indicates that the method can be further
improved.
</summary>
    <author>
      <name>Mamia Agbese</name>
    </author>
    <author>
      <name>Hanna-Kaisa Alanen</name>
    </author>
    <author>
      <name>Jani Antikainen</name>
    </author>
    <author>
      <name>Erika Halme</name>
    </author>
    <author>
      <name>Hannakaisa Isomäki</name>
    </author>
    <author>
      <name>Marianna Jantunen</name>
    </author>
    <author>
      <name>Kai-Kristian Kemell</name>
    </author>
    <author>
      <name>Rebekah Rousi</name>
    </author>
    <author>
      <name>Heidi Vainio-Pekka</name>
    </author>
    <author>
      <name>Ville Vakkuri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/REW53955.2021.00042</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/REW53955.2021.00042" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure, 2 tables, IEEE 29th International Requirements
  Engineering Conference Workshops (REW)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2021, pp 224-229</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2111.06207v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.06207v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.07642v1</id>
    <updated>2022-01-19T15:19:29Z</updated>
    <published>2022-01-19T15:19:29Z</published>
    <title>Problem examination for AI methods in product design</title>
    <summary>  Artificial Intelligence (AI) has significant potential for product design: AI
can check technical and non-technical constraints on products, it can support a
quick design of new product variants and new AI methods may also support
creativity. But currently product design and AI are separate communities
fostering different terms and theories. This makes a mapping of AI approaches
to product design needs difficult and prevents new solutions. As a solution,
this paper first clarifies important terms and concepts for the
interdisciplinary domain of AI methods in product design. A key contribution of
this paper is a new classification of design problems using the four
characteristics decomposability, inter-dependencies, innovation and creativity.
Definitions of these concepts are given where they are lacking. Early mappings
of these concepts to AI solutions are sketched and verified using design
examples. The importance of creativity in product design and a corresponding
gap in AI is pointed out for future research.
</summary>
    <author>
      <name>Philipp Rosenthal</name>
    </author>
    <author>
      <name>Oliver Niggemann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published at IJCAI 21 Workshop AI and Design</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.07642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.07642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.10848v1</id>
    <updated>2022-02-22T12:23:21Z</updated>
    <published>2022-02-22T12:23:21Z</published>
    <title>Speciesist bias in AI -- How AI applications perpetuate discrimination
  and unfair outcomes against animals</title>
    <summary>  Massive efforts are made to reduce biases in both data and algorithms in
order to render AI applications fair. These efforts are propelled by various
high-profile cases where biased algorithmic decision-making caused harm to
women, people of color, minorities, etc. However, the AI fairness field still
succumbs to a blind spot, namely its insensitivity to discrimination against
animals. This paper is the first to describe the 'speciesist bias' and
investigate it in several different AI systems. Speciesist biases are learned
and solidified by AI applications when they are trained on datasets in which
speciesist patterns prevail. These patterns can be found in image recognition
systems, large language models, and recommender systems. Therefore, AI
technologies currently play a significant role in perpetuating and normalizing
violence against animals. This can only be changed when AI fairness frameworks
widen their scope and include mitigation measures for speciesist biases. This
paper addresses the AI community in this regard and stresses the influence AI
systems can have on either increasing or reducing the violence that is
inflicted on animals, and especially on farmed animals.
</summary>
    <author>
      <name>Thilo Hagendorff</name>
    </author>
    <author>
      <name>Leonie Bossert</name>
    </author>
    <author>
      <name>Tse Yip Fai</name>
    </author>
    <author>
      <name>Peter Singer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s43681-022-00199-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s43681-022-00199-9" rel="related"/>
    <link href="http://arxiv.org/abs/2202.10848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.10848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.01467v1</id>
    <updated>2022-05-03T13:02:50Z</updated>
    <published>2022-05-03T13:02:50Z</published>
    <title>On the Effect of Information Asymmetry in Human-AI Teams</title>
    <summary>  Over the last years, the rising capabilities of artificial intelligence (AI)
have improved human decision-making in many application areas. Teaming between
AI and humans may even lead to complementary team performance (CTP), i.e., a
level of performance beyond the ones that can be reached by AI or humans
individually. Many researchers have proposed using explainable AI (XAI) to
enable humans to rely on AI advice appropriately and thereby reach CTP.
However, CTP is rarely demonstrated in previous work as often the focus is on
the design of explainability, while a fundamental prerequisite -- the presence
of complementarity potential between humans and AI -- is often neglected.
Therefore, we focus on the existence of this potential for effective human-AI
decision-making. Specifically, we identify information asymmetry as an
essential source of complementarity potential, as in many real-world
situations, humans have access to different contextual information. By
conducting an online experiment, we demonstrate that humans can use such
contextual information to adjust the AI's decision, finally resulting in CTP.
</summary>
    <author>
      <name>Patrick Hemmer</name>
    </author>
    <author>
      <name>Max Schemmer</name>
    </author>
    <author>
      <name>Niklas Kühl</name>
    </author>
    <author>
      <name>Michael Vössing</name>
    </author>
    <author>
      <name>Gerhard Satzger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CHI Conference on Human Factors in Computing Systems (CHI '22),
  Workshop on Human-Centered Explainable AI (HCXAI)</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.01467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.01467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.08347v1</id>
    <updated>2022-04-26T16:51:00Z</updated>
    <published>2022-04-26T16:51:00Z</published>
    <title>Landing AI on Networks: An equipment vendor viewpoint on Autonomous
  Driving Networks</title>
    <summary>  The tremendous achievements of Artificial Intelligence (AI) in computer
vision, natural language processing, games and robotics, has extended the reach
of the AI hype to other fields: in telecommunication networks, the long term
vision is to let AI fully manage, and autonomously drive, all aspects of
network operation. In this industry vision paper, we discuss challenges and
opportunities of Autonomous Driving Network (ADN) driven by AI technologies. To
understand how AI can be successfully landed in current and future networks, we
start by outlining challenges that are specific to the networking domain,
putting them in perspective with advances that AI has achieved in other fields.
We then present a system view, clarifying how AI can be fitted in the network
architecture. We finally discuss current achievements as well as future
promises of AI in networks, mentioning a roadmap to avoid bumps in the road
that leads to true large-scale deployment of AI technologies in networks.
</summary>
    <author>
      <name>Dario Rossi</name>
    </author>
    <author>
      <name>Liang Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNSM.2022.3169988</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNSM.2022.3169988" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Network and Service Management, 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2205.08347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.08347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.11832v1</id>
    <updated>2022-10-21T09:25:06Z</updated>
    <published>2022-10-21T09:25:06Z</published>
    <title>AI-HRI Brings New Dimensions to Human-Aware Design for Human-Aware AI</title>
    <summary>  Since the first AI-HRI held at the 2014 AAAI Fall Symposium Series, a lot of
the presented research and discussions have emphasized how artificial
intelligence (AI) developments can benefit human-robot interaction (HRI). This
portrays HRI as an application, a source of domain-specific problems to solve,
to the AI community. Likewise, this portrays AI as a tool, a source of
solutions available for relevant problems, to the HRI community. However,
members of the AI-HRI research community will point out that the relationship
has a deeper synergy than matchmaking problems and solutions -- there are
insights from each field that impact how the other one thinks about the world
and performs scientific research. There is no greater opportunity for sharing
perspectives at the moment than human-aware AI, which studies how to account
for the fact that people are more than a source of data or part of an
algorithm. We will explore how AI-HRI can change the way researchers think
about human-aware AI, from observation through validation, to make even the
algorithmic design process human-aware.
</summary>
    <author>
      <name>Richard G. Freedman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for presentation at the AAAI 2022 Fall Symposium Series, in
  the symposium for Artificial Intelligence for Human-Robot Interaction</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.11832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.11832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.07460v1</id>
    <updated>2022-11-14T15:42:27Z</updated>
    <published>2022-11-14T15:42:27Z</published>
    <title>An Analytics of Culture: Modeling Subjectivity, Scalability,
  Contextuality, and Temporality</title>
    <summary>  There is a bidirectional relationship between culture and AI; AI models are
increasingly used to analyse culture, thereby shaping our understanding of
culture. On the other hand, the models are trained on collections of cultural
artifacts thereby implicitly, and not always correctly, encoding expressions of
culture. This creates a tension that both limits the use of AI for analysing
culture and leads to problems in AI with respect to cultural complex issues
such as bias.
  One approach to overcome this tension is to more extensively take into
account the intricacies and complexities of culture. We structure our
discussion using four concepts that guide humanistic inquiry into culture:
subjectivity, scalability, contextuality, and temporality. We focus on these
concepts because they have not yet been sufficiently represented in AI
research. We believe that possible implementations of these aspects into AI
research leads to AI that better captures the complexities of culture. In what
follows, we briefly describe these four concepts and their absence in AI
research. For each concept, we define possible research challenges.
</summary>
    <author>
      <name>Nanne van Noord</name>
    </author>
    <author>
      <name>Melvin Wevers</name>
    </author>
    <author>
      <name>Tobias Blanke</name>
    </author>
    <author>
      <name>Julia Noordegraaf</name>
    </author>
    <author>
      <name>Marcel Worring</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at Cultures in AI/AI in Culture workshop at NeurIPS
  2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.07460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.07460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.05517v2</id>
    <updated>2023-02-27T06:07:17Z</updated>
    <published>2023-01-03T23:11:03Z</published>
    <title>Developing Responsible Chatbots for Financial Services: A
  Pattern-Oriented Responsible AI Engineering Approach</title>
    <summary>  The recent release of ChatGPT has gained huge attention and discussion
worldwide, with responsible AI being a key topic of discussion. How can we
ensure that AI systems, including ChatGPT, are developed and adopted in a
responsible way? To tackle the responsible AI challenges, various ethical
principles have been released by governments, organisations, and companies.
However, those principles are very abstract and not practical enough. Further,
significant efforts have been put on algorithm-level solutions that only
address a narrow set of principles, such as fairness and privacy. To fill the
gap, we adopt a pattern-oriented responsible AI engineering approach and build
a Responsible AI Pattern Catalogue to operationalise responsible AI from a
system perspective. In this article, we first summarise the major challenges in
operationalising responsible AI at scale and introduce how we use the
Responsible AI Pattern Catalogue to address those challenges. We then examine
the risks at each stage of the chatbot development process and recommend
pattern-driven mitigations to evaluate the the usefulness of the Responsible AI
Pattern Catalogue in a real-world setting.
</summary>
    <author>
      <name>Qinghua Lu</name>
    </author>
    <author>
      <name>Yuxiu Luo</name>
    </author>
    <author>
      <name>Liming Zhu</name>
    </author>
    <author>
      <name>Mingjian Tang</name>
    </author>
    <author>
      <name>Xiwei Xu</name>
    </author>
    <author>
      <name>Jon Whittle</name>
    </author>
    <link href="http://arxiv.org/abs/2301.05517v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.05517v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07233v1</id>
    <updated>2018-03-20T03:16:10Z</updated>
    <published>2018-03-20T03:16:10Z</published>
    <title>Closing the AI Knowledge Gap</title>
    <summary>  AI researchers employ not only the scientific method, but also methodology
from mathematics and engineering. However, the use of the scientific method -
specifically hypothesis testing - in AI is typically conducted in service of
engineering objectives. Growing interest in topics such as fairness and
algorithmic bias show that engineering-focused questions only comprise a subset
of the important questions about AI systems. This results in the AI Knowledge
Gap: the number of unique AI systems grows faster than the number of studies
that characterize these systems' behavior. To close this gap, we argue that the
study of AI could benefit from the greater inclusion of researchers who are
well positioned to formulate and test hypotheses about the behavior of AI
systems. We examine the barriers preventing social and behavioral scientists
from conducting such studies. Our diagnosis suggests that accelerating the
scientific study of AI systems requires new incentives for academia and
industry, mediated by new tools and institutions. To address these needs, we
propose a two-sided marketplace called TuringBox. On one side, AI contributors
upload existing and novel algorithms to be studied scientifically by others. On
the other side, AI examiners develop and post machine intelligence tasks
designed to evaluate and characterize algorithmic behavior. We discuss this
market's potential to democratize the scientific study of AI behavior, and thus
narrow the AI Knowledge Gap.
</summary>
    <author>
      <name>Ziv Epstein</name>
    </author>
    <author>
      <name>Blakeley H. Payne</name>
    </author>
    <author>
      <name>Judy Hanwen Shen</name>
    </author>
    <author>
      <name>Abhimanyu Dubey</name>
    </author>
    <author>
      <name>Bjarke Felbo</name>
    </author>
    <author>
      <name>Matthew Groh</name>
    </author>
    <author>
      <name>Nick Obradovich</name>
    </author>
    <author>
      <name>Manuel Cebrian</name>
    </author>
    <author>
      <name>Iyad Rahwan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, under review</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.07233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.09997v1</id>
    <updated>2018-04-26T11:37:03Z</updated>
    <published>2018-04-26T11:37:03Z</published>
    <title>PANDA: Facilitating Usable AI Development</title>
    <summary>  Recent advances in artificial intelligence (AI) and machine learning have
created a general perception that AI could be used to solve complex problems,
and in some situations over-hyped as a tool that can be so easily used.
Unfortunately, the barrier to realization of mass adoption of AI on various
business domains is too high because most domain experts have no background in
AI. Developing AI applications involves multiple phases, namely data
preparation, application modeling, and product deployment. The effort of AI
research has been spent mostly on new AI models (in the model training stage)
to improve the performance of benchmark tasks such as image recognition. Many
other factors such as usability, efficiency and security of AI have not been
well addressed, and therefore form a barrier to democratizing AI. Further, for
many real world applications such as healthcare and autonomous driving,
learning via huge amounts of possibility exploration is not feasible since
humans are involved. In many complex applications such as healthcare, subject
matter experts (e.g. Clinicians) are the ones who appreciate the importance of
features that affect health, and their knowledge together with existing
knowledge bases are critical to the end results. In this paper, we take a new
perspective on developing AI solutions, and present a solution for making AI
usable. We hope that this resolution will enable all subject matter experts
(eg. Clinicians) to exploit AI like data scientists.
</summary>
    <author>
      <name>Jinyang Gao</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Meihui Zhang</name>
    </author>
    <author>
      <name>Gang Chen</name>
    </author>
    <author>
      <name>H. V. Jagadish</name>
    </author>
    <author>
      <name>Guoliang Li</name>
    </author>
    <author>
      <name>Teck Khim Ng</name>
    </author>
    <author>
      <name>Beng Chin Ooi</name>
    </author>
    <author>
      <name>Sheng Wang</name>
    </author>
    <author>
      <name>Jingren Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1804.09997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.09997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.11194v1</id>
    <updated>2020-06-19T15:46:13Z</updated>
    <published>2020-06-19T15:46:13Z</published>
    <title>Does Explainable Artificial Intelligence Improve Human Decision-Making?</title>
    <summary>  Explainable AI provides insight into the "why" for model predictions,
offering potential for users to better understand and trust a model, and to
recognize and correct AI predictions that are incorrect. Prior research on
human and explainable AI interactions has focused on measures such as
interpretability, trust, and usability of the explanation. Whether explainable
AI can improve actual human decision-making and the ability to identify the
problems with the underlying model are open questions. Using real datasets, we
compare and evaluate objective human decision accuracy without AI (control),
with an AI prediction (no explanation), and AI prediction with explanation. We
find providing any kind of AI prediction tends to improve user decision
accuracy, but no conclusive evidence that explainable AI has a meaningful
impact. Moreover, we observed the strongest predictor for human decision
accuracy was AI accuracy and that users were somewhat able to detect when the
AI was correct versus incorrect, but this was not significantly affected by
including an explanation. Our results indicate that, at least in some
situations, the "why" information provided in explainable AI may not enhance
user decision-making, and further research may be needed to understand how to
integrate explainable AI into real systems.
</summary>
    <author>
      <name>Yasmeen Alufaisan</name>
    </author>
    <author>
      <name>Laura R. Marusich</name>
    </author>
    <author>
      <name>Jonathan Z. Bakdash</name>
    </author>
    <author>
      <name>Yan Zhou</name>
    </author>
    <author>
      <name>Murat Kantarcioglu</name>
    </author>
    <link href="http://arxiv.org/abs/2006.11194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.11194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.11820v3</id>
    <updated>2020-07-30T02:17:40Z</updated>
    <published>2020-07-23T06:40:46Z</published>
    <title>AI Data poisoning attack: Manipulating game AI of Go</title>
    <summary>  With the extensive use of AI in various fields, the issue of AI security has
become more significant. The AI data poisoning attacks will be the most
threatening approach against AI security after the adversarial examples. As the
continuous updating of AI applications online, the data pollution models can be
uploaded by attackers to achieve a certain malicious purpose. Recently, the
research on AI data poisoning attacks is mostly out of practice and use
self-built experimental environments so that it cannot be as close to reality
as adversarial example attacks. This article's first contribution is to provide
a solution and a breakthrough for the aforementioned issue with research
limitations, to aim at data poisoning attacks that target real businesses, in
this case: data poisoning attacks on real Go AI. We install a Trojan virus into
the real Go AI that manipulates the AI's behavior. It is the first time that we
succeed in manipulating complicated AI and provide a reliable approach to the
AI data poisoning attack verification method. The method of building Trojan in
this article can be expanded to more practical algorithms for other fields such
as content recommendation, text translation, and intelligent dialogue.
</summary>
    <author>
      <name>Junli Shen</name>
    </author>
    <author>
      <name>Maocai Xia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fixed some inappropriate information from previous versions</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.11820v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.11820v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.02032v5</id>
    <updated>2021-08-21T14:59:32Z</updated>
    <published>2021-01-01T17:34:42Z</published>
    <title>Socially Responsible AI Algorithms: Issues, Purposes, and Challenges</title>
    <summary>  In the current era, people and society have grown increasingly reliant on
artificial intelligence (AI) technologies. AI has the potential to drive us
towards a future in which all of humanity flourishes. It also comes with
substantial risks for oppression and calamity. Discussions about whether we
should (re)trust AI have repeatedly emerged in recent years and in many
quarters, including industry, academia, healthcare, services, and so on.
Technologists and AI researchers have a responsibility to develop trustworthy
AI systems. They have responded with great effort to design more responsible AI
algorithms. However, existing technical solutions are narrow in scope and have
been primarily directed towards algorithms for scoring or classification tasks,
with an emphasis on fairness and unwanted bias. To build long-lasting trust
between AI and human beings, we argue that the key is to think beyond
algorithmic fairness and connect major aspects of AI that potentially cause
AI's indifferent behavior. In this survey, we provide a systematic framework of
Socially Responsible AI Algorithms that aims to examine the subjects of AI
indifference and the need for socially responsible AI algorithms, define the
objectives, and introduce the means by which we may achieve these objectives.
We further discuss how to leverage this framework to improve societal
well-being through protection, information, and prevention/mitigation.
</summary>
    <author>
      <name>Lu Cheng</name>
    </author>
    <author>
      <name>Kush R. Varshney</name>
    </author>
    <author>
      <name>Huan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">45 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Artificial Intelligence Research 71 (2021) 1137-1181</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.02032v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.02032v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.09692v1</id>
    <updated>2021-02-19T00:38:53Z</updated>
    <published>2021-02-19T00:38:53Z</published>
    <title>To Trust or to Think: Cognitive Forcing Functions Can Reduce
  Overreliance on AI in AI-assisted Decision-making</title>
    <summary>  People supported by AI-powered decision support tools frequently overrely on
the AI: they accept an AI's suggestion even when that suggestion is wrong.
Adding explanations to the AI decisions does not appear to reduce the
overreliance and some studies suggest that it might even increase it. Informed
by the dual-process theory of cognition, we posit that people rarely engage
analytically with each individual AI recommendation and explanation, and
instead develop general heuristics about whether and when to follow the AI
suggestions. Building on prior research on medical decision-making, we designed
three cognitive forcing interventions to compel people to engage more
thoughtfully with the AI-generated explanations. We conducted an experiment
(N=199), in which we compared our three cognitive forcing designs to two simple
explainable AI approaches and to a no-AI baseline. The results demonstrate that
cognitive forcing significantly reduced overreliance compared to the simple
explainable AI approaches. However, there was a trade-off: people assigned the
least favorable subjective ratings to the designs that reduced the overreliance
the most. To audit our work for intervention-generated inequalities, we
investigated whether our interventions benefited equally people with different
levels of Need for Cognition (i.e., motivation to engage in effortful mental
activities). Our results show that, on average, cognitive forcing interventions
benefited participants higher in Need for Cognition more. Our research suggests
that human cognitive motivation moderates the effectiveness of explainable AI
solutions.
</summary>
    <author>
      <name>Zana Buçinca</name>
    </author>
    <author>
      <name>Maja Barbara Malaya</name>
    </author>
    <author>
      <name>Krzysztof Z. Gajos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3449287</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3449287" rel="related"/>
    <link href="http://arxiv.org/abs/2102.09692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.09692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.00965v1</id>
    <updated>2022-05-02T15:08:18Z</updated>
    <published>2022-05-02T15:08:18Z</published>
    <title>State-of-the-art in Open-domain Conversational AI: A Survey</title>
    <summary>  We survey SoTA open-domain conversational AI models with the purpose of
presenting the prevailing challenges that still exist to spur future research.
In addition, we provide statistics on the gender of conversational AI in order
to guide the ethics discussion surrounding the issue. Open-domain
conversational AI are known to have several challenges, including bland
responses and performance degradation when prompted with figurative language,
among others. First, we provide some background by discussing some topics of
interest in conversational AI. We then discuss the method applied to the two
investigations carried out that make up this study. The first investigation
involves a search for recent SoTA open-domain conversational AI models while
the second involves the search for 100 conversational AI to assess their
gender. Results of the survey show that progress has been made with recent SoTA
conversational AI, but there are still persistent challenges that need to be
solved, and the female gender is more common than the male for conversational
AI. One main take-away is that hybrid models of conversational AI offer more
advantages than any single architecture. The key contributions of this survey
are 1) the identification of prevailing challenges in SoTA open-domain
conversational AI, 2) the unusual discussion about open-domain conversational
AI for low-resource languages, and 3) the discussion about the ethics
surrounding the gender of conversational AI.
</summary>
    <author>
      <name>Tosin Adewumi</name>
    </author>
    <author>
      <name>Foteini Liwicki</name>
    </author>
    <author>
      <name>Marcus Liwicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.00965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.04132v1</id>
    <updated>2022-06-08T19:05:12Z</updated>
    <published>2022-06-08T19:05:12Z</published>
    <title>Forecasting AI Progress: Evidence from a Survey of Machine Learning
  Researchers</title>
    <summary>  Advances in artificial intelligence (AI) are shaping modern life, from
transportation, health care, science, finance, to national defense. Forecasts
of AI development could help improve policy- and decision-making. We report the
results from a large survey of AI and machine learning (ML) researchers on
their beliefs about progress in AI. The survey, fielded in late 2019, elicited
forecasts for near-term AI development milestones and high- or human-level
machine intelligence, defined as when machines are able to accomplish every or
almost every task humans are able to do currently. As part of this study, we
re-contacted respondents from a highly-cited study by Grace et al. (2018), in
which AI/ML researchers gave forecasts about high-level machine intelligence
and near-term milestones in AI development. Results from our 2019 survey show
that, in aggregate, AI/ML researchers surveyed placed a 50% likelihood of
human-level machine intelligence being achieved by 2060. The results show
researchers newly contacted in 2019 expressed similar beliefs about the
progress of advanced AI as respondents in the Grace et al. (2018) survey. For
the recontacted participants from the Grace et al. (2018) study, the aggregate
forecast for a 50% likelihood of high-level machine intelligence shifted from
2062 to 2076, although this change is not statistically significant, likely due
to the small size of our panel sample. Forecasts of several near-term AI
milestones have reduced in time, suggesting more optimism about AI progress.
Finally, AI/ML researchers also exhibited significant optimism about how
human-level machine intelligence will impact society.
</summary>
    <author>
      <name>Baobao Zhang</name>
    </author>
    <author>
      <name>Noemi Dreksler</name>
    </author>
    <author>
      <name>Markus Anderljung</name>
    </author>
    <author>
      <name>Lauren Kahn</name>
    </author>
    <author>
      <name>Charlie Giattino</name>
    </author>
    <author>
      <name>Allan Dafoe</name>
    </author>
    <author>
      <name>Michael C. Horowitz</name>
    </author>
    <link href="http://arxiv.org/abs/2206.04132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.04132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.08132v1</id>
    <updated>2022-10-14T21:55:57Z</updated>
    <published>2022-10-14T21:55:57Z</published>
    <title>VHetNets for AI and AI for VHetNets: An Anomaly Detection Case Study for
  Ubiquitous IoT</title>
    <summary>  Vertical heterogenous networks (VHetNets) and artificial intelligence (AI)
play critical roles in 6G and beyond networks. This article presents an
AI-native VHetNets architecture to enable the synergy of VHetNets and AI,
thereby supporting varieties of AI services while facilitating automatic and
intelligent network management. Anomaly detection in Internet of Things (IoT)
is a major AI service required by many fields, including intrusion detection,
state monitoring, device-activity analysis, security supervision and so on.
Conventional anomaly detection technologies mainly consider the anomaly
detection as a standalone service that is independent of any other network
management functionalities, which cannot be used directly in ubiquitous IoT due
to the resource constrained end nodes and decentralized data distribution. In
this article, we develop an AI-native VHetNets-enabled framework to provide the
anomaly detection service for ubiquitous IoT, whose implementation is assisted
by intelligent network management functionalities. We first discuss the
possibilities of VHetNets used for distributed AI model training to provide
anomaly detection service for ubiquitous IoT, i.e., VHetNets for AI. After
that, we study the application of AI approaches in helping provide automatic
and intelligent network management functionalities for VHetNets, i.e., AI for
VHetNets, whose aim is to facilitate the efficient implementation of anomaly
detection service. Finally, a case study is presented to demonstrate the
efficiency and effectiveness of the proposed AI-native VHetNets-enabled anomaly
detection framework.
</summary>
    <author>
      <name>Weili Wang</name>
    </author>
    <author>
      <name>Omid Abbasi</name>
    </author>
    <author>
      <name>Halim Yanikomeroglu</name>
    </author>
    <author>
      <name>Chengchao Liang</name>
    </author>
    <author>
      <name>Lun Tang</name>
    </author>
    <author>
      <name>Qianbin Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2210.08132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.08132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.03410v1</id>
    <updated>2022-12-07T02:42:29Z</updated>
    <published>2022-12-07T02:42:29Z</published>
    <title>SAIH: A Scalable Evaluation Methodology for Understanding AI Performance
  Trend on HPC Systems</title>
    <summary>  Novel artificial intelligence (AI) technology has expedited various
scientific research, e.g., cosmology, physics and bioinformatics, inevitably
becoming a significant category of workload on high performance computing (HPC)
systems. Existing AI benchmarks tend to customize well-recognized AI
applications, so as to evaluate the AI performance of HPC systems under
predefined problem size, in terms of datasets and AI models. Due to lack of
scalability on the problem size, static AI benchmarks might be under competent
to help understand the performance trend of evolving AI applications on HPC
systems, in particular, the scientific AI applications on large-scale systems.
  In this paper, we propose a scalable evaluation methodology (SAIH) for
analyzing the AI performance trend of HPC systems with scaling the problem
sizes of customized AI applications. To enable scalability, SAIH builds a set
of novel mechanisms for augmenting problem sizes. As the data and model
constantly scale, we can investigate the trend and range of AI performance on
HPC systems, and further diagnose system bottlenecks. To verify our
methodology, we augment a cosmological AI application to evaluate a real HPC
system equipped with GPUs as a case study of SAIH.
</summary>
    <author>
      <name>Jiangsu Du</name>
    </author>
    <author>
      <name>Dongsheng Li</name>
    </author>
    <author>
      <name>Yingpeng Wen</name>
    </author>
    <author>
      <name>Jiazhi Jiang</name>
    </author>
    <author>
      <name>Dan Huang</name>
    </author>
    <author>
      <name>Xiangke Liao</name>
    </author>
    <author>
      <name>Yutong Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.03410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.03410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.06576v1</id>
    <updated>2022-12-12T02:18:10Z</updated>
    <published>2022-12-12T02:18:10Z</published>
    <title>AI Model Utilization Measurements For Finding Class Encoding Patterns</title>
    <summary>  This work addresses the problems of (a) designing utilization measurements of
trained artificial intelligence (AI) models and (b) explaining how training
data are encoded in AI models based on those measurements. The problems are
motivated by the lack of explainability of AI models in security and safety
critical applications, such as the use of AI models for classification of
traffic signs in self-driving cars. We approach the problems by introducing
theoretical underpinnings of AI model utilization measurement and understanding
patterns in utilization-based class encodings of traffic signs at the level of
computation graphs (AI models), subgraphs, and graph nodes. Conceptually,
utilization is defined at each graph node (computation unit) of an AI model
based on the number and distribution of unique outputs in the space of all
possible outputs (tensor-states). In this work, utilization measurements are
extracted from AI models, which include poisoned and clean AI models. In
contrast to clean AI models, the poisoned AI models were trained with traffic
sign images containing systematic, physically realizable, traffic sign
modifications (i.e., triggers) to change a correct class label to another label
in a presence of such a trigger. We analyze class encodings of such clean and
poisoned AI models, and conclude with implications for trojan injection and
detection.
</summary>
    <author>
      <name>Peter Bajcsy</name>
    </author>
    <author>
      <name>Antonio Cardone</name>
    </author>
    <author>
      <name>Chenyi Ling</name>
    </author>
    <author>
      <name>Philippe Dessauw</name>
    </author>
    <author>
      <name>Michael Majurski</name>
    </author>
    <author>
      <name>Tim Blattner</name>
    </author>
    <author>
      <name>Derek Juba</name>
    </author>
    <author>
      <name>Walid Keyrouz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">45 pages, 29 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.06576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.06576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.03774v1</id>
    <updated>2023-02-07T22:06:24Z</updated>
    <published>2023-02-07T22:06:24Z</published>
    <title>AI and Core Electoral Processes: Mapping the Horizons</title>
    <summary>  Significant enthusiasm around AI uptake has been witnessed across societies
globally. The electoral process -- the time, place and manner of elections
within democratic nations -- has been among those very rare sectors in which AI
has not penetrated much. Electoral management bodies in many countries have
recently started exploring and deliberating over the use of AI in the electoral
process. In this paper, we consider five representative avenues within the core
electoral process which have potential for AI usage, and map the challenges
involved in using AI within them. These five avenues are: voter list
maintenance, determining polling booth locations, polling booth protection
processes, voter authentication and video monitoring of elections. Within each
of these avenues, we lay down the context, illustrate current or potential
usage of AI, and discuss extant or potential ramifications of AI usage, and
potential directions for mitigating risks while considering AI usage. We
believe that the scant current usage of AI within electoral processes provides
a very rare opportunity, that of being able to deliberate on the risks and
mitigation possibilities, prior to real and widespread AI deployment. This
paper is an attempt to map the horizons of risks and opportunities in using AI
within the electoral processes and to help shape the debate around the topic.
</summary>
    <author>
      <name>Deepak P</name>
    </author>
    <author>
      <name>Stanley Simoes</name>
    </author>
    <author>
      <name>Muiris MacCarthaigh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 7 figures, to be published in AI Magazine (Fall 2023)</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.03774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.03774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.15004v3</id>
    <updated>2021-04-05T16:10:19Z</updated>
    <published>2021-03-27T22:12:06Z</published>
    <title>eXtended Artificial Intelligence: New Prospects of Human-AI Interaction
  Research</title>
    <summary>  Artificial Intelligence (AI) covers a broad spectrum of computational
problems and use cases. Many of those implicate profound and sometimes
intricate questions of how humans interact or should interact with AIs.
Moreover, many users or future users do have abstract ideas of what AI is,
significantly depending on the specific embodiment of AI applications.
Human-centered-design approaches would suggest evaluating the impact of
different embodiments on human perception of and interaction with AI. An
approach that is difficult to realize due to the sheer complexity of
application fields and embodiments in reality. However, here XR opens new
possibilities to research human-AI interactions. The article's contribution is
twofold: First, it provides a theoretical treatment and model of human-AI
interaction based on an XR-AI continuum as a framework for and a perspective of
different approaches of XR-AI combinations. It motivates XR-AI combinations as
a method to learn about the effects of prospective human-AI interfaces and
shows why the combination of XR and AI fruitfully contributes to a valid and
systematic investigation of human-AI interactions and interfaces. Second, the
article provides two exemplary experiments investigating the aforementioned
approach for two distinct AI-systems. The first experiment reveals an
interesting gender effect in human-robot interaction, while the second
experiment reveals an Eliza effect of a recommender system. Here the article
introduces two paradigmatic implementations of the proposed XR testbed for
human-AI interactions and interfaces and shows how a valid and systematic
investigation can be conducted. In sum, the article opens new perspectives on
how XR benefits human-centered AI design and development.
</summary>
    <author>
      <name>Carolin Wienrich</name>
    </author>
    <author>
      <name>Marc Erich Latoschik</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/frvir.2021.686783</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/frvir.2021.686783" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Front. Virtual Real., 06 September 2021, Sec. Virtual Reality and
  Human Behaviour</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.15004v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15004v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.14086v1</id>
    <updated>2022-07-28T13:46:13Z</updated>
    <published>2022-07-28T13:46:13Z</published>
    <title>Ever heard of ethical AI? Investigating the salience of ethical AI
  issues among the German population</title>
    <summary>  Building and implementing ethical AI systems that benefit the whole society
is cost-intensive and a multi-faceted task fraught with potential problems.
While computer science focuses mostly on the technical questions to mitigate
social issues, social science addresses citizens' perceptions to elucidate
social and political demands that influence the societal implementation of AI
systems. Thus, in this study, we explore the salience of AI issues in the
public with an emphasis on ethical criteria to investigate whether it is likely
that ethical AI is actively requested by the population. Between May 2020 and
April 2021, we conducted 15 surveys asking the German population about the most
important AI-related issues (total of N=14,988 respondents). Our results show
that the majority of respondents were not concerned with AI at all. However, it
can be seen that general interest in AI and a higher educational level are
predictive of some engagement with AI. Among those, who reported having thought
about AI, specific applications (e.g., autonomous driving) were by far the most
mentioned topics. Ethical issues are voiced only by a small subset of citizens
with fairness, accountability, and transparency being the least mentioned ones.
These have been identified in several ethical guidelines (including the EU
Commission's proposal) as key elements for the development of ethical AI. The
salience of ethical issues affects the behavioral intentions of citizens in the
way that they 1) tend to avoid AI technology and 2) engage in public
discussions about AI. We conclude that the low level of ethical implications
may pose a serious problem for the actual implementation of ethical AI for the
Common Good and emphasize that those who are presumably most affected by
ethical issues of AI are especially unaware of ethical risks. Yet, once ethical
AI is top of the mind, there is some potential for activism.
</summary>
    <author>
      <name>Kimon Kieslich</name>
    </author>
    <author>
      <name>Marco Lünich</name>
    </author>
    <author>
      <name>Pero Došenović</name>
    </author>
    <link href="http://arxiv.org/abs/2207.14086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.14086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.13960v5</id>
    <updated>2023-01-23T11:57:41Z</updated>
    <published>2022-11-25T09:08:11Z</published>
    <title>The European AI Liability Directives -- Critique of a Half-Hearted
  Approach and Lessons for the Future</title>
    <summary>  As ChatGPT et al. conquer the world, the optimal liability framework for AI
systems remains an unsolved problem across the globe. In a much-anticipated
move, the European Commission advanced two proposals outlining the European
approach to AI liability in September 2022: a novel AI Liability Directive and
a revision of the Product Liability Directive. They constitute the final
cornerstone of EU AI regulation. Crucially, the liability proposals and the EU
AI Act are inherently intertwined: the latter does not contain any individual
rights of affected persons, and the former lack specific, substantive rules on
AI development and deployment. Taken together, these acts may well trigger a
Brussels Effect in AI regulation, with significant consequences for the US and
beyond.
  This paper makes three novel contributions. First, it examines in detail the
Commission proposals and shows that, while making steps in the right direction,
they ultimately represent a half-hearted approach: if enacted as foreseen, AI
liability in the EU will primarily rest on disclosure of evidence mechanisms
and a set of narrowly defined presumptions concerning fault, defectiveness and
causality. Hence, second, the article suggests amendments, which are collected
in an Annex at the end of the paper. Third, based on an analysis of the key
risks AI poses, the final part of the paper maps out a road for the future of
AI liability and regulation, in the EU and beyond. This includes: a
comprehensive framework for AI liability; provisions to support innovation; an
extension to non-discrimination/algorithmic fairness, as well as explainable
AI; and sustainability. I propose to jump-start sustainable AI regulation via
sustainability impact assessments in the AI Act and sustainable design defects
in the liability regime. In this way, the law may help spur not only fair AI
and XAI, but potentially also sustainable AI (SAI).
</summary>
    <author>
      <name>Philipp Hacker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">under peer-review; contains 3 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.13960v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.13960v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04232v2</id>
    <updated>2016-12-06T09:29:12Z</updated>
    <published>2016-05-11T17:49:24Z</published>
    <title>Review of state-of-the-arts in artificial intelligence with application
  to AI safety problem</title>
    <summary>  Here, I review current state-of-the-arts in many areas of AI to estimate when
it's reasonable to expect human level AI development. Predictions of prominent
AI researchers vary broadly from very pessimistic predictions of Andrew Ng to
much more moderate predictions of Geoffrey Hinton and optimistic predictions of
Shane Legg, DeepMind cofounder. Given huge rate of progress in recent years and
this broad range of predictions of AI experts, AI safety questions are also
discussed.
</summary>
    <author>
      <name>Vladimir Shakirov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">version 2 includes grant information</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04232v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04232v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.02388v2</id>
    <updated>2017-03-28T23:59:25Z</updated>
    <published>2017-01-09T23:25:43Z</published>
    <title>Stoic Ethics for Artificial Agents</title>
    <summary>  We present a position paper advocating the notion that Stoic philosophy and
ethics can inform the development of ethical A.I. systems. This is in sharp
contrast to most work on building ethical A.I., which has focused on
Utilitarian or Deontological ethical theories. We relate ethical A.I. to
several core Stoic notions, including the dichotomy of control, the four
cardinal virtues, the ideal Sage, Stoic practices, and Stoic perspectives on
emotion or affect. More generally, we put forward an ethical view of A.I. that
focuses more on internal states of the artificial agent rather than on external
actions of the agent. We provide examples relating to near-term A.I. systems as
well as hypothetical superintelligent agents.
</summary>
    <author>
      <name>Gabriel Murray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final accepted version submitted to Canadian A.I. 2017 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.02388v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.02388v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.07679v1</id>
    <updated>2020-03-02T14:24:59Z</updated>
    <published>2020-03-02T14:24:59Z</published>
    <title>Business (mis)Use Cases of Generative AI</title>
    <summary>  Generative AI is a class of machine learning technology that learns to
generate new data from training data. While deep fakes and media-and
art-related generative AI breakthroughs have recently caught people's attention
and imagination, the overall area is in its infancy for business use. Further,
little is known about generative AI's potential for malicious misuse at large
scale. Using co-creation design fictions with AI engineers, we explore the
plausibility and severity of business misuse cases.
</summary>
    <author>
      <name>Stephanie Houde</name>
    </author>
    <author>
      <name>Vera Liao</name>
    </author>
    <author>
      <name>Jacquelyn Martino</name>
    </author>
    <author>
      <name>Michael Muller</name>
    </author>
    <author>
      <name>David Piorkowski</name>
    </author>
    <author>
      <name>John Richards</name>
    </author>
    <author>
      <name>Justin Weisz</name>
    </author>
    <author>
      <name>Yunfeng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IUI 2020 Workshop on Human-AI Co-Creation with Generative Models</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.07679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.07679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.02279v2</id>
    <updated>2020-12-04T04:36:05Z</updated>
    <published>2020-10-31T18:49:39Z</published>
    <title>Ideal theory in AI ethics</title>
    <summary>  This paper addresses the ways AI ethics research operates on an ideology of
ideal theory, in the sense discussed by Mills (2005) and recently applied to AI
ethics by Fazelpour \&amp; Lipton (2020). I address the structural and
methodological conditions that attract AI ethics researchers to ideal
theorizing, and the consequences this approach has for the quality and future
of our research community. Finally, I discuss the possibilities for a nonideal
future in AI ethics.
</summary>
    <author>
      <name>Daniel Estrada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of the Navigating the Broader Impacts of AI Research Workshop at
  NeurIPS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.02279v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.02279v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05905v1</id>
    <updated>2017-11-16T03:30:29Z</updated>
    <published>2017-11-16T03:30:29Z</published>
    <title>Using experimental game theory to transit human values to ethical AI</title>
    <summary>  Knowing the reflection of game theory and ethics, we develop a mathematical
representation to bridge the gap between the concepts in moral philosophy
(e.g., Kantian and Utilitarian) and AI ethics industry technology standard
(e.g., IEEE P7000 standard series for Ethical AI). As an application, we
demonstrate how human value can be obtained from the experimental game theory
(e.g., trust game experiment) so as to build an ethical AI. Moreover, an
approach to test the ethics (rightness or wrongness) of a given AI algorithm by
using an iterated Prisoner's Dilemma Game experiment is discussed as an
example. Compared with existing mathematical frameworks and testing method on
AI ethics technology, the advantages of the proposed approach are analyzed.
</summary>
    <author>
      <name>Yijia Wang</name>
    </author>
    <author>
      <name>Yan Wan</name>
    </author>
    <author>
      <name>Zhijian Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.05905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.10036v1</id>
    <updated>2018-09-20T02:26:38Z</updated>
    <published>2018-09-20T02:26:38Z</published>
    <title>Federated AI for building AI Solutions across Multiple Agencies</title>
    <summary>  The different sets of regulations existing for differ-ent agencies within the
government make the task of creating AI enabled solutions in government
dif-ficult. Regulatory restrictions inhibit sharing of da-ta across different
agencies, which could be a significant impediment to training AI models. We
discuss the challenges that exist in environments where data cannot be freely
shared and assess tech-nologies which can be used to work around these
challenges. We present results on building AI models using the concept of
federated AI, which al-lows creation of models without moving the training data
around.
</summary>
    <author>
      <name>Dinesh Verma</name>
    </author>
    <author>
      <name>Simon Julier</name>
    </author>
    <author>
      <name>Greg Cirincione</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at AAAI FSS-18: Artificial Intelligence in Government and
  Public Sector, Arlington, Virginia, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.10036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.10036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.10239v2</id>
    <updated>2019-05-16T11:59:10Z</updated>
    <published>2019-04-23T10:41:11Z</published>
    <title>Ethics of Artificial Intelligence Demarcations</title>
    <summary>  In this paper we present a set of key demarcations, particularly important
when discussing ethical and societal issues of current AI research and
applications. Properly distinguishing issues and concerns related to Artificial
General Intelligence and weak AI, between symbolic and connectionist AI, AI
methods, data and applications are prerequisites for an informed debate. Such
demarcations would not only facilitate much-needed discussions on ethics on
current AI technologies and research. In addition sufficiently establishing
such demarcations would also enhance knowledge-sharing and support rigor in
interdisciplinary research between technical and social sciences.
</summary>
    <author>
      <name>Anders Braarud Hanssen</name>
    </author>
    <author>
      <name>Stefano Nichele</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Norwegian AI Symposium 2019 (NAIS 2019),
  Trondheim, Norway</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.10239v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.10239v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.10344v1</id>
    <updated>2019-12-14T15:09:43Z</updated>
    <published>2019-12-14T15:09:43Z</published>
    <title>XCloud: Design and Implementation of AI Cloud Platform with RESTful API
  Service</title>
    <summary>  In recent years, artificial intelligence (AI) has aroused much attention
among both industrial and academic areas. However, building and maintaining
efficient AI systems are quite difficult for many small business companies and
researchers if they are not familiar with machine learning and AI. In this
paper, we first evaluate the difficulties and challenges in building AI
systems. Then an cloud platform termed XCloud, which provides several common AI
services in form of RESTful APIs, is constructed. Technical details are
discussed in Section 2. This project is released as open-source software and
can be easily accessed for late research. Code is available at
https://github.com/lucasxlu/XCloud.git.
</summary>
    <author>
      <name>Lu Xu</name>
    </author>
    <author>
      <name>Yating Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1912.10344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.10344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.03024v1</id>
    <updated>2020-02-07T21:49:58Z</updated>
    <published>2020-02-07T21:49:58Z</published>
    <title>Cognitive Anthropomorphism of AI: How Humans and Computers Classify
  Images</title>
    <summary>  Modern AI image classifiers have made impressive advances in recent years,
but their performance often appears strange or violates expectations of users.
This suggests humans engage in cognitive anthropomorphism: expecting AI to have
the same nature as human intelligence. This mismatch presents an obstacle to
appropriate human-AI interaction. To delineate this mismatch, I examine known
properties of human classification, in comparison to image classifier systems.
Based on this examination, I offer three strategies for system design that can
address the mismatch between human and AI classification: explainable AI, novel
methods for training users, and new algorithms that match human cognition.
</summary>
    <author>
      <name>Shane T. Mueller</name>
    </author>
    <link href="http://arxiv.org/abs/2002.03024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.04644v1</id>
    <updated>2020-03-30T04:11:08Z</updated>
    <published>2020-03-30T04:11:08Z</published>
    <title>On the Ethics of Building AI in a Responsible Manner</title>
    <summary>  The AI-alignment problem arises when there is a discrepancy between the goals
that a human designer specifies to an AI learner and a potential catastrophic
outcome that does not reflect what the human designer really wants. We argue
that a formalism of AI alignment that does not distinguish between strategic
and agnostic misalignments is not useful, as it deems all technology as
un-safe. We propose a definition of a strategic-AI-alignment and prove that
most machine learning algorithms that are being used in practice today do not
suffer from the strategic-AI-alignment problem. However, without being careful,
today's technology might lead to strategic misalignment.
</summary>
    <author>
      <name>Shai Shalev-Shwartz</name>
    </author>
    <author>
      <name>Shaked Shammah</name>
    </author>
    <author>
      <name>Amnon Shashua</name>
    </author>
    <link href="http://arxiv.org/abs/2004.04644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.04644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.11434v1</id>
    <updated>2020-04-23T19:27:19Z</updated>
    <published>2020-04-23T19:27:19Z</published>
    <title>Responsible AI and Its Stakeholders</title>
    <summary>  Responsible Artificial Intelligence (AI) proposes a framework that holds all
stakeholders involved in the development of AI to be responsible for their
systems. It, however, fails to accommodate the possibility of holding AI
responsible per se, which could close some legal and moral gaps concerning the
deployment of autonomous and self-learning systems. We discuss three notions of
responsibility (i.e., blameworthiness, accountability, and liability) for all
stakeholders, including AI, and suggest the roles of jurisdiction and the
general public in this matter.
</summary>
    <author>
      <name>Gabriel Lima</name>
    </author>
    <author>
      <name>Meeyoung Cha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, accepted to the Fair &amp; Responsible AI Workshop at ACM CHI
  2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.11434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.11434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.02742v1</id>
    <updated>2020-07-06T13:27:00Z</updated>
    <published>2020-07-06T13:27:00Z</published>
    <title>Towards Game-Playing AI Benchmarks via Performance Reporting Standards</title>
    <summary>  While games have been used extensively as milestones to evaluate game-playing
AI, there exists no standardised framework for reporting the obtained
observations. As a result, it remains difficult to draw general conclusions
about the strengths and weaknesses of different game-playing AI algorithms. In
this paper, we propose reporting guidelines for AI game-playing performance
that, if followed, provide information suitable for unbiased comparisons
between different AI approaches. The vision we describe is to build benchmarks
and competitions based on such guidelines in order to be able to draw more
general conclusions about the behaviour of different AI algorithms, as well as
the types of challenges different games pose.
</summary>
    <author>
      <name>Vanessa Volz</name>
    </author>
    <author>
      <name>Boris Naujoks</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Conference on Games 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.02742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.02742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.05607v1</id>
    <updated>2020-08-03T17:22:53Z</updated>
    <published>2020-08-03T17:22:53Z</published>
    <title>A clarification of misconceptions, myths and desired status of
  artificial intelligence</title>
    <summary>  The field artificial intelligence (AI) has been founded over 65 years ago.
Starting with great hopes and ambitious goals the field progressed though
various stages of popularity and received recently a revival in the form of
deep neural networks. Some problems of AI are that so far neither
'intelligence' nor the goals of AI are formally defined causing confusion when
comparing AI to other fields. In this paper, we present a perspective on the
desired and current status of AI in relation to machine learning and statistics
and clarify common misconceptions and myths. Our discussion is intended to
uncurtain the veil of vagueness surrounding AI to see its true countenance.
</summary>
    <author>
      <name>Frank Emmert-Streib</name>
    </author>
    <author>
      <name>Olli Yli-Harja</name>
    </author>
    <author>
      <name>Matthias Dehmer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/frai.2020.524339</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/frai.2020.524339" rel="related"/>
    <link href="http://arxiv.org/abs/2008.05607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.05607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.07328v1</id>
    <updated>2020-08-04T16:12:30Z</updated>
    <published>2020-08-04T16:12:30Z</published>
    <title>An Ontological AI-and-Law Framework for the Autonomous Levels of AI
  Legal Reasoning</title>
    <summary>  A framework is proposed that seeks to identify and establish a set of robust
autonomous levels articulating the realm of Artificial Intelligence and Legal
Reasoning (AILR). Doing so provides a sound and parsimonious basis for being
able to assess progress in the application of AI to the law, and can be
utilized by scholars in academic pursuits of AI legal reasoning, along with
being used by law practitioners and legal professionals in gauging how advances
in AI are aiding the practice of law and the realization of aspirational versus
achieved results. A set of seven levels of autonomy for AI and Legal Reasoning
are meticulously proffered and mindfully discussed.
</summary>
    <author>
      <name>Lance Eliot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.07328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.07328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; J.7.0; K.5.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.03063v1</id>
    <updated>2021-01-08T15:30:09Z</updated>
    <published>2021-01-08T15:30:09Z</published>
    <title>Knowledge AI: New Medical AI Solution for Medical image Diagnosis</title>
    <summary>  The implementation of medical AI has always been a problem. The effect of
traditional perceptual AI algorithm in medical image processing needs to be
improved. Here we propose a method of knowledge AI, which is a combination of
perceptual AI and clinical knowledge and experience. Based on this method, the
geometric information mining of medical images can represent the experience and
information and evaluate the quality of medical images.
</summary>
    <author>
      <name>Yingni Wang</name>
    </author>
    <author>
      <name>Shuge Lei</name>
    </author>
    <author>
      <name>Jian Dai</name>
    </author>
    <author>
      <name>Kehong Yuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages,8 figures. arXiv admin note: text overlap with
  arXiv:2101.02639</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.03063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.03063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.14044v1</id>
    <updated>2021-07-14T15:16:51Z</updated>
    <published>2021-07-14T15:16:51Z</published>
    <title>Ethical AI for Social Good</title>
    <summary>  The concept of AI for Social Good(AI4SG) is gaining momentum in both
information societies and the AI community. Through all the advancement of
AI-based solutions, it can solve societal issues effectively. To date, however,
there is only a rudimentary grasp of what constitutes AI socially beneficial in
principle, what constitutes AI4SG in reality, and what are the policies and
regulations needed to ensure it. This paper fills the vacuum by addressing the
ethical aspects that are critical for future AI4SG efforts. Some of these
characteristics are new to AI, while others have greater importance due to its
usage.
</summary>
    <author>
      <name>Ramya Akula</name>
    </author>
    <author>
      <name>Ivan Garibay</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Human-Computer Interaction, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2107.14044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.14044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.09437v1</id>
    <updated>2021-11-17T22:43:13Z</updated>
    <published>2021-11-17T22:43:13Z</published>
    <title>Sustainable Artificial Intelligence through Continual Learning</title>
    <summary>  The increasing attention on Artificial Intelligence (AI) regulation has led
to the definition of a set of ethical principles grouped into the Sustainable
AI framework. In this article, we identify Continual Learning, an active area
of AI research, as a promising approach towards the design of systems compliant
with the Sustainable AI principles. While Sustainable AI outlines general
desiderata for ethical applications, Continual Learning provides means to put
such desiderata into practice.
</summary>
    <author>
      <name>Andrea Cossu</name>
    </author>
    <author>
      <name>Marta Ziosi</name>
    </author>
    <author>
      <name>Vincenzo Lomonaco</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 2021 International Conference on AI for People (CAIP)</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.09437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.09437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01281v1</id>
    <updated>2021-11-29T14:53:35Z</updated>
    <published>2021-11-29T14:53:35Z</published>
    <title>Expose Uncertainty, Instill Distrust, Avoid Explanations: Towards
  Ethical Guidelines for AI</title>
    <summary>  In this position paper, I argue that the best way to help and protect humans
using AI technology is to make them aware of the intrinsic limitations and
problems of AI algorithms. To accomplish this, I suggest three ethical
guidelines to be used in the presentation of results, mandating AI systems to
expose uncertainty, to instill distrust, and, contrary to traditional views, to
avoid explanations. The paper does a preliminary discussion of the guidelines
and provides some arguments for their adoption, aiming to start a debate in the
community about AI ethics in practice.
</summary>
    <author>
      <name>Claudio S. Pinhanez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in the NeurIPS 2021 workshop on Human-Centered AI. December
  13th 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.01281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.07067v1</id>
    <updated>2022-03-11T13:26:54Z</updated>
    <published>2022-03-11T13:26:54Z</published>
    <title>Toward Ethical AIED</title>
    <summary>  This paper presents the key conclusions to the forthcoming edited book on The
Ethics of Artificial Intelligence in Education: Practices, Challenges and
Debates (August 2022, Routlege). As well as highlighting the key contributions
to the book, it discusses the key questions and the grand challenges for the
field of AI in Education (AIED)in the context of ethics and ethical practices
within the field. The book itself presents diverse perspectives from outside
and from within the AIED as a way of achieving a broad perspective in the key
ethical issues for AIED and a deep understanding of work conducted to date by
the AIED community.
</summary>
    <author>
      <name>Kaska Porayska-Pomsta</name>
    </author>
    <author>
      <name>Wayne Holmes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages; concluding chapter from the forthcoming book to be printed
  by Routledge in August 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.07067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.07067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.04148v1</id>
    <updated>2022-07-23T22:37:22Z</updated>
    <published>2022-07-23T22:37:22Z</published>
    <title>A Historical Interaction between Artificial Intelligence and Philosophy</title>
    <summary>  This paper reviews the historical development of AI and representative
philosophical thinking from the perspective of the research paradigm.
Additionally, it considers the methodology and applications of AI from a
philosophical perspective and anticipates its continued advancement. In the
history of AI, Symbolism and connectionism are the two main paradigms in AI
research. Symbolism holds that the world can be explained by symbols and dealt
with through precise, logical processes, but connectionism believes this
process should be implemented through artificial neural networks. Regardless of
how intelligent machines or programs should achieve their smart goals, the
historical development of AI demonstrates the best answer at this time. Still,
it is not the final answer of AI research.
</summary>
    <author>
      <name>Youheng Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2208.04148v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.04148v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.08968v1</id>
    <updated>2022-08-18T17:17:53Z</updated>
    <published>2022-08-18T17:17:53Z</published>
    <title>"Melatonin": A Case Study on AI-induced Musical Style</title>
    <summary>  Although the use of AI tools in music composition and production is steadily
increasing, as witnessed by the newly founded AI song contest, analysis of
music produced using these tools is still relatively uncommon as a mean to gain
insight in the ways AI tools impact music production. In this paper we present
a case study of "Melatonin", a song produced by extensive use of BassNet, an AI
tool originally designed to generate bass lines. Through analysis of the
artists' work flow and song project, we identify style characteristics of the
song in relation to the affordances of the tool, highlighting manifestations of
style in terms of both idiom and sound.
</summary>
    <author>
      <name>Emmanuel Deruty</name>
    </author>
    <author>
      <name>Maarten Grachten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted paper at the 3rd Conference on AI Music Creativity
  (September 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.08968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.08968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.00987v3</id>
    <updated>2023-02-18T06:40:07Z</updated>
    <published>2023-01-03T07:41:29Z</published>
    <title>AI in HCI Design and User Experience</title>
    <summary>  In this chapter, we review and discuss the transformation of AI technology in
HCI/UX work and assess how AI technology will change how we do the work. We
first discuss how AI can be used to enhance the result of user research and
design evaluation. We then discuss how AI technology can be used to enhance
HCI/UX design. Finally, we discuss how AI-enabled capabilities can improve UX
when users interact with computing systems, applications, and services.
</summary>
    <author>
      <name>Wei Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.00987v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.00987v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.13396v1</id>
    <updated>2023-01-31T04:06:18Z</updated>
    <published>2023-01-31T04:06:18Z</published>
    <title>Study of Optical Networks, 5G, Artificial Intelligence and Their
  Applications</title>
    <summary>  This paper discusses the application of artificial intelligence (AI)
technology in optical communication networks and 5G. It primarily introduces
representative applications of AI technology and potential risks of AI
technology failure caused by the openness of optical communication networks,
and proposes some coping strategies, mainly including modeling AI systems
through modularization and miniaturization, combining with traditional
classical network modeling and planning methods, and improving the
effectiveness and interpretability of AI technology. At the same time, it
proposes response strategies based on network protection for the possible
failure and attack of AI technology.
</summary>
    <author>
      <name>Quanda Zhang</name>
    </author>
    <author>
      <name>Qi Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2301.13396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.13396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.04110v1</id>
    <updated>2023-02-08T15:04:57Z</updated>
    <published>2023-02-08T15:04:57Z</published>
    <title>Assessing the impact of regulations and standards on innovation in the
  field of AI</title>
    <summary>  Regulations and standards in the field of artificial intelligence (AI) are
necessary to minimise risks and maximise benefits, yet some argue that they
stifle innovation. This paper critically examines the idea that regulation
stifles innovation in the field of AI. Current trends in AI regulation,
particularly the proposed European AI Act and the standards supporting its
implementation, are discussed. Arguments in support of the idea that regulation
stifles innovation are analysed and criticised, and an alternative point of
view is offered, showing how regulation and standards can foster innovation in
the field of AI.
</summary>
    <author>
      <name>Alessio Tartaro</name>
    </author>
    <author>
      <name>Adam Leon Smith</name>
    </author>
    <author>
      <name>Patricia Shaw</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.04110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.04110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.3537v1</id>
    <updated>2008-11-21T09:29:54Z</updated>
    <published>2008-11-21T09:29:54Z</published>
    <title>Many Light Higgs Bosons in the NMSSM</title>
    <summary>  The next-to-minimal supersymmetric model with a light doublet-like
  CP-odd Higgs boson and small $\tan \beta$ can satisfy all experimental limits
on Higgs bosons even with light superpartners. In these scenarios, the two
lightest CP-even Higgs bosons, $\hi$ and $\hii$, and the charged Higgs boson,
$\hp$, can all be light enough to be produced at LEP and yet have decays that
have not been looked for or are poorly constrained by existing collider
experiments. The channel $\hi\to \ai\ai$ (where $\ai$ is the lightest CP-odd
boson and has mass below $2m_b$) with $\ai\to \tau^+\tau^-$ or $2j$ is still
awaiting LEP constraints for $\mhi&gt;86\gev$ or $82\gev$, respectively. LEP data
may also contain $\epem\to \hii\ai$ events where $\hii\to Z\ai$ is the dominant
decay, a channel that was never examined. Decays of the charged Higgs bosons
are often dominated by $H^\pm \to W^{\pm (\star)} \ai$ with $\ai \to gg,c \bar
c, \tau^+ \tau^-$. This is a channel that has so far been ignored in the search
for $t\to \hp b$ decays at the Tevatron. A specialized analysis might reveal a
signal. The light $\ai$ might be within the reach of $B$ factories via
$\Upsilon\to \gamma \ai$ decays. We study typical mass ranges and branching
ratios of Higgs bosons in this scenario and compare these scenarios where the
$\ai$ has a large doublet component to the more general scenarios with
arbitrary singlet component for the $\ai$.
</summary>
    <author>
      <name>Radovan Dermisek</name>
    </author>
    <author>
      <name>John F. Gunion</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevD.79.055014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevD.79.055014" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 47 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys.Rev.D79:055014,2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0811.3537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.3537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.03980v1</id>
    <updated>2018-12-10T18:58:05Z</updated>
    <published>2018-12-10T18:58:05Z</published>
    <title>Building Ethically Bounded AI</title>
    <summary>  The more AI agents are deployed in scenarios with possibly unexpected
situations, the more they need to be flexible, adaptive, and creative in
achieving the goal we have given them. Thus, a certain level of freedom to
choose the best path to the goal is inherent in making AI robust and flexible
enough. At the same time, however, the pervasive deployment of AI in our life,
whether AI is autonomous or collaborating with humans, raises several ethical
challenges. AI agents should be aware and follow appropriate ethical principles
and should thus exhibit properties such as fairness or other virtues. These
ethical principles should define the boundaries of AI's freedom and creativity.
However, it is still a challenge to understand how to specify and reason with
ethical boundaries in AI agents and how to combine them appropriately with
subjective preferences and goal specifications. Some initial attempts employ
either a data-driven example-based approach for both, or a symbolic rule-based
approach for both. We envision a modular approach where any AI technique can be
used for any of these essential ingredients in decision making or decision
support systems, paired with a contextual approach to define their combination
and relative weight. In a world where neither humans nor AI systems work in
isolation, but are tightly interconnected, e.g., the Internet of Things, we
also envision a compositional approach to building ethically bounded AI, where
the ethical properties of each component can be fruitfully exploited to derive
those of the overall system. In this paper we define and motivate the notion of
ethically-bounded AI, we describe two concrete examples, and we outline some
outstanding challenges.
</summary>
    <author>
      <name>Francesca Rossi</name>
    </author>
    <author>
      <name>Nicholas Mattei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at AAAI Blue Sky Track, winner of Blue Sky Award</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.03980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.03980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.08776v1</id>
    <updated>2019-05-21T17:46:16Z</updated>
    <published>2019-05-21T17:46:16Z</published>
    <title>Textured Neural Avatars</title>
    <summary>  We present a system for learning full-body neural avatars, i.e. deep networks
that produce full-body renderings of a person for varying body pose and camera
position. Our system takes the middle path between the classical graphics
pipeline and the recent deep learning approaches that generate images of humans
using image-to-image translation. In particular, our system estimates an
explicit two-dimensional texture map of the model surface. At the same time, it
abstains from explicit shape modeling in 3D. Instead, at test time, the system
uses a fully-convolutional network to directly map the configuration of body
feature points w.r.t. the camera to the 2D texture coordinates of individual
pixels in the image frame. We show that such a system is capable of learning to
generate realistic renderings while being trained on videos annotated with 3D
poses and foreground masks. We also demonstrate that maintaining an explicit
texture representation helps our system to achieve better generalization
compared to systems that use direct image-to-image translation.
</summary>
    <author>
      <name>Aliaksandra Shysheya</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Samsung AI Center, Skolkovo Institute of Science and Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Egor Zakharov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Samsung AI Center, Skolkovo Institute of Science and Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Kara-Ali Aliev</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Samsung AI Center</arxiv:affiliation>
    </author>
    <author>
      <name>Renat Bashirov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Samsung AI Center</arxiv:affiliation>
    </author>
    <author>
      <name>Egor Burkov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Samsung AI Center, Skolkovo Institute of Science and Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Karim Iskakov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Samsung AI Center</arxiv:affiliation>
    </author>
    <author>
      <name>Aleksei Ivakhnenko</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Samsung AI Center</arxiv:affiliation>
    </author>
    <author>
      <name>Yury Malkov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Samsung AI Center</arxiv:affiliation>
    </author>
    <author>
      <name>Igor Pasechnik</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Samsung AI Center</arxiv:affiliation>
    </author>
    <author>
      <name>Dmitry Ulyanov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Samsung AI Center, Skolkovo Institute of Science and Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Alexander Vakhitov</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Samsung AI Center, Skolkovo Institute of Science and Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Victor Lempitsky</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Samsung AI Center, Skolkovo Institute of Science and Technology</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1905.08776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.04477v13</id>
    <updated>2021-08-17T01:27:49Z</updated>
    <published>2020-07-08T23:50:28Z</published>
    <title>Good AI for the Present of Humanity Democratizing AI Governance</title>
    <summary>  What do Cyberpunk and AI Ethics have to do with each other? Cyberpunk is a
sub-genre of science fiction that explores the post-human relationships between
human experience and technology. One similarity between AI Ethics and Cyberpunk
literature is that both seek to explore future social and ethical problems that
our technological advances may bring upon society. In recent years, an
increasing number of ethical matters involving AI have been pointed and
debated, and several ethical principles and guides have been suggested as
governance policies for the tech industry. However, would this be the role of
AI Ethics? To serve as a soft and ambiguous version of the law? We would like
to advocate in this article for a more Cyberpunk way of doing AI Ethics, with a
more democratic way of governance. In this study, we will seek to expose some
of the deficits of the underlying power structures of the AI industry, and
suggest that AI governance be subject to public opinion, so that good AI can
become good AI for all.
</summary>
    <author>
      <name>Nicholas Kluge Corrêa</name>
    </author>
    <author>
      <name>Nythamar de Oliveira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.47289/AIEJ20210716-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.47289/AIEJ20210716-2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The AI Ethics Journal (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2007.04477v13" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.04477v13" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.10571v1</id>
    <updated>2020-07-21T02:42:26Z</updated>
    <published>2020-07-21T02:42:26Z</published>
    <title>AI Tax: The Hidden Cost of AI Data Center Applications</title>
    <summary>  Artificial intelligence and machine learning are experiencing widespread
adoption in industry and academia. This has been driven by rapid advances in
the applications and accuracy of AI through increasingly complex algorithms and
models; this, in turn, has spurred research into specialized hardware AI
accelerators. Given the rapid pace of advances, it is easy to forget that they
are often developed and evaluated in a vacuum without considering the full
application environment. This paper emphasizes the need for a holistic,
end-to-end analysis of AI workloads and reveals the "AI tax." We deploy and
characterize Face Recognition in an edge data center. The application is an
AI-centric edge video analytics application built using popular open source
infrastructure and ML tools. Despite using state-of-the-art AI and ML
algorithms, the application relies heavily on pre-and post-processing code. As
AI-centric applications benefit from the acceleration promised by accelerators,
we find they impose stresses on the hardware and software infrastructure:
storage and network bandwidth become major bottlenecks with increasing AI
acceleration. By specializing for AI applications, we show that a purpose-built
edge data center can be designed for the stresses of accelerated AI at 15%
lower TCO than one derived from homogeneous servers and infrastructure.
</summary>
    <author>
      <name>Daniel Richins</name>
    </author>
    <author>
      <name>Dharmisha Doshi</name>
    </author>
    <author>
      <name>Matthew Blackmore</name>
    </author>
    <author>
      <name>Aswathy Thulaseedharan Nair</name>
    </author>
    <author>
      <name>Neha Pathapati</name>
    </author>
    <author>
      <name>Ankit Patel</name>
    </author>
    <author>
      <name>Brainard Daguman</name>
    </author>
    <author>
      <name>Daniel Dobrijalowski</name>
    </author>
    <author>
      <name>Ramesh Illikkal</name>
    </author>
    <author>
      <name>Kevin Long</name>
    </author>
    <author>
      <name>David Zimmerman</name>
    </author>
    <author>
      <name>Vijay Janapa Reddi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages. 16 figures. Submitted to ACM "Transactions on Computer
  Systems."</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.10571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.10571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.02472v2</id>
    <updated>2020-10-10T01:24:19Z</updated>
    <published>2020-10-06T04:35:08Z</published>
    <title>What Makes a Popular Academic AI Repository?</title>
    <summary>  Many AI researchers are publishing code, data and other resources that
accompany their papers in GitHub repositories. In this paper, we refer to these
repositories as academic AI repositories. Our preliminary study shows that
highly cited papers are more likely to have popular academic AI repositories
(and vice versa). Hence, in this study, we perform an empirical study on
academic AI repositories to highlight good software engineering practices of
popular academic AI repositories for AI researchers.
  We collect 1,149 academic AI repositories, in which we label the top 20%
repositories that have the most number of stars as popular, and we label the
bottom 70% repositories as unpopular. The remaining 10% repositories are set as
a gap between popular and unpopular academic AI repositories. We propose 21
features to characterize the software engineering practices of academic AI
repositories. Our experimental results show that popular and unpopular academic
AI repositories are statistically significantly different in 11 of the studied
features---indicating that the two groups of repositories have significantly
different software engineering practices. Furthermore, we find that the number
of links to other GitHub repositories in the README file, the number of images
in the README file and the inclusion of a license are the most important
features for differentiating the two groups of academic AI repositories. Our
dataset and code are made publicly available to share with the community.
</summary>
    <author>
      <name>Yuanrui Fan</name>
    </author>
    <author>
      <name>Xin Xia</name>
    </author>
    <author>
      <name>David Lo</name>
    </author>
    <author>
      <name>Ahmed E. Hassan</name>
    </author>
    <author>
      <name>Shanping Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10664-020-09916-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10664-020-09916-6" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Empirical Software Engineering (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.02472v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.02472v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.07333v1</id>
    <updated>2021-02-15T04:16:27Z</updated>
    <published>2021-02-15T04:16:27Z</published>
    <title>AI Ethics Needs Good Data</title>
    <summary>  In this chapter we argue that discourses on AI must transcend the language of
'ethics' and engage with power and political economy in order to constitute
'Good Data'. In particular, we must move beyond the depoliticised language of
'ethics' currently deployed (Wagner 2018) in determining whether AI is 'good'
given the limitations of ethics as a frame through which AI issues can be
viewed. In order to circumvent these limits, we use instead the language and
conceptualisation of 'Good Data', as a more expansive term to elucidate the
values, rights and interests at stake when it comes to AI's development and
deployment, as well as that of other digital technologies. Good Data
considerations move beyond recurring themes of data protection/privacy and the
FAT (fairness, transparency and accountability) movement to include explicit
political economy critiques of power. Instead of yet more ethics principles
(that tend to say the same or similar things anyway), we offer four 'pillars'
on which Good Data AI can be built: community, rights, usability and politics.
Overall we view AI's 'goodness' as an explicly political (economy) question of
power and one which is always related to the degree which AI is created and
used to increase the wellbeing of society and especially to increase the power
of the most marginalized and disenfranchised. We offer recommendations and
remedies towards implementing 'better' approaches towards AI. Our strategies
enable a different (but complementary) kind of evaluation of AI as part of the
broader socio-technical systems in which AI is built and deployed.
</summary>
    <author>
      <name>Angela Daly</name>
    </author>
    <author>
      <name>S Kate Devitt</name>
    </author>
    <author>
      <name>Monique Mann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, under peer review in Pieter Verdegem (ed), AI for Everyone?
  Critical Perspectives. University of Westminster Press</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.07333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.07333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.1; K.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.13557v1</id>
    <updated>2021-02-26T15:51:03Z</updated>
    <published>2021-02-26T15:51:03Z</published>
    <title>A local characterization for the Cuntz semigroup of AI-algebras</title>
    <summary>  We give a local characterization for the Cuntz semigroup of AI-algebras
building upon Shen's characterization of dimension groups. Using this result,
we provide an abstract characterization for the Cuntz semigroup of AI-algebras.
</summary>
    <author>
      <name>Eduard Vilalta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.13557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.13557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 46L05, 46L85, 06F05, Secondary 06A06, 46L80, 19K99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.01984v2</id>
    <updated>2021-09-02T09:39:59Z</updated>
    <published>2021-05-05T11:22:08Z</published>
    <title>Software Engineering for AI-Based Systems: A Survey</title>
    <summary>  AI-based systems are software systems with functionalities enabled by at
least one AI component (e.g., for image- and speech-recognition, and autonomous
driving). AI-based systems are becoming pervasive in society due to advances in
AI. However, there is limited synthesized knowledge on Software Engineering
(SE) approaches for building, operating, and maintaining AI-based systems. To
collect and analyze state-of-the-art knowledge about SE for AI-based systems,
we conducted a systematic mapping study. We considered 248 studies published
between January 2010 and March 2020. SE for AI-based systems is an emerging
research area, where more than 2/3 of the studies have been published since
2018. The most studied properties of AI-based systems are dependability and
safety. We identified multiple SE approaches for AI-based systems, which we
classified according to the SWEBOK areas. Studies related to software testing
and software quality are very prevalent, while areas like software maintenance
seem neglected. Data-related issues are the most recurrent challenges. Our
results are valuable for: researchers, to quickly understand the state of the
art and learn which topics need more research; practitioners, to learn about
the approaches and challenges that SE entails for AI-based systems; and,
educators, to bridge the gap among SE and AI in their curricula.
</summary>
    <author>
      <name>Silverio Martínez-Fernández</name>
    </author>
    <author>
      <name>Justus Bogner</name>
    </author>
    <author>
      <name>Xavier Franch</name>
    </author>
    <author>
      <name>Marc Oriol</name>
    </author>
    <author>
      <name>Julien Siebert</name>
    </author>
    <author>
      <name>Adam Trendowicz</name>
    </author>
    <author>
      <name>Anna Maria Vollmer</name>
    </author>
    <author>
      <name>Stefan Wagner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3487043</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3487043" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in ACM Transactions on Software Engineering and Methodology
  (TOSEM). For its published version refer to the Journal of ACM TOSEM</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Trans. Softw. Eng. Methodol. 31, 2, Article 37e (March 2022),
  59 pages</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2105.01984v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.01984v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.03929v1</id>
    <updated>2021-08-09T10:47:14Z</updated>
    <published>2021-08-09T10:47:14Z</published>
    <title>The State of AI Ethics Report (Volume 5)</title>
    <summary>  This report from the Montreal AI Ethics Institute covers the most salient
progress in research and reporting over the second quarter of 2021 in the field
of AI ethics with a special emphasis on "Environment and AI", "Creativity and
AI", and "Geopolitics and AI." The report also features an exclusive piece
titled "Critical Race Quantum Computer" that applies ideas from quantum physics
to explain the complexities of human characteristics and how they can and
should shape our interactions with each other. The report also features special
contributions on the subject of pedagogy in AI ethics, sociology and AI ethics,
and organizational challenges to implementing AI ethics in practice. Given
MAIEI's mission to highlight scholars from around the world working on AI
ethics issues, the report also features two spotlights sharing the work of
scholars operating in Singapore and Mexico helping to shape policy measures as
they relate to the responsible use of technology. The report also has an
extensive section covering the gamut of issues when it comes to the societal
impacts of AI covering areas of bias, privacy, transparency, accountability,
fairness, interpretability, disinformation, policymaking, law, regulations, and
moral philosophy.
</summary>
    <author>
      <name>Abhishek Gupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Connor Wright</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Marianna Bergamaschi Ganapini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Masa Sweidan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Renjie Butalid</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">201 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.03929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.03929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4; I.2; A.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.07700v1</id>
    <updated>2021-08-05T13:18:48Z</updated>
    <published>2021-08-05T13:18:48Z</published>
    <title>In Oxford Handbook on AI Governance: The Role of Workers in AI Ethics
  and Governance</title>
    <summary>  While the role of states, corporations, and international organizations in AI
governance has been extensively theorized, the role of workers has received
comparatively little attention. This chapter looks at the role that workers
play in identifying and mitigating harms from AI technologies. Harms are the
causally assessed impacts of technologies. They arise despite technical
reliability and are not a result of technical negligence but rather of
normative uncertainty around questions of safety and fairness in complex social
systems. There is high consensus in the AI ethics community on the benefits of
reducing harms but less consensus on mechanisms for determining or addressing
harms. This lack of consensus has resulted in a number of collective actions by
workers protesting how harms are identified and addressed in their workplace.
We theorize the role of workers within AI governance and construct a model of
harm reporting processes in AI workplaces. The harm reporting process involves
three steps, identification, the governance decision, and the response. Workers
draw upon three types of claims to argue for jurisdiction over questions of AI
governance, subjection, control over the product of labor, and proximate
knowledge of systems. Examining the past decade of AI related worker activism
allows us to understand how different types of workers are positioned within a
workplace that produces AI systems, how their position informs their claims,
and the place of collective action in staking their claims. This chapter argues
that workers occupy a unique role in identifying and mitigating harms caused by
AI systems.
</summary>
    <author>
      <name>Nataliya Nedzhvetskaya</name>
    </author>
    <author>
      <name>JS Tan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In: Justin Bullock, Baobao Zhang, Yu-Che Chen, Johannes Himmelreich,
  Matthew Young, Antonin Korinek &amp; Valerie Hudson (eds.). Oxford Handbook on AI
  Governance (Oxford University Press, 2022 forthcoming)</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.07700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.07700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.06674v1</id>
    <updated>2021-10-13T12:18:09Z</updated>
    <published>2021-10-13T12:18:09Z</published>
    <title>Truthful AI: Developing and governing AI that does not lie</title>
    <summary>  In many contexts, lying -- the use of verbal falsehoods to deceive -- is
harmful. While lying has traditionally been a human affair, AI systems that
make sophisticated verbal statements are becoming increasingly prevalent. This
raises the question of how we should limit the harm caused by AI "lies" (i.e.
falsehoods that are actively selected for). Human truthfulness is governed by
social norms and by laws (against defamation, perjury, and fraud). Differences
between AI and humans present an opportunity to have more precise standards of
truthfulness for AI, and to have these standards rise over time. This could
provide significant benefits to public epistemics and the economy, and mitigate
risks of worst-case AI futures.
  Establishing norms or laws of AI truthfulness will require significant work
to: (1) identify clear truthfulness standards; (2) create institutions that can
judge adherence to those standards; and (3) develop AI systems that are
robustly truthful.
  Our initial proposals for these areas include: (1) a standard of avoiding
"negligent falsehoods" (a generalisation of lies that is easier to assess); (2)
institutions to evaluate AI systems before and after real-world deployment; and
(3) explicitly training AI systems to be truthful via curated datasets and
human interaction.
  A concerning possibility is that evaluation mechanisms for eventual
truthfulness standards could be captured by political interests, leading to
harmful censorship and propaganda. Avoiding this might take careful attention.
And since the scale of AI speech acts might grow dramatically over the coming
decades, early truthfulness standards might be particularly important because
of the precedents they set.
</summary>
    <author>
      <name>Owain Evans</name>
    </author>
    <author>
      <name>Owen Cotton-Barratt</name>
    </author>
    <author>
      <name>Lukas Finnveden</name>
    </author>
    <author>
      <name>Adam Bales</name>
    </author>
    <author>
      <name>Avital Balwit</name>
    </author>
    <author>
      <name>Peter Wills</name>
    </author>
    <author>
      <name>Luca Righetti</name>
    </author>
    <author>
      <name>William Saunders</name>
    </author>
    <link href="http://arxiv.org/abs/2110.06674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.06674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.05391v1</id>
    <updated>2021-11-09T20:00:14Z</updated>
    <published>2021-11-09T20:00:14Z</published>
    <title>Statistical Perspectives on Reliability of Artificial Intelligence
  Systems</title>
    <summary>  Artificial intelligence (AI) systems have become increasingly popular in many
areas. Nevertheless, AI technologies are still in their developing stages, and
many issues need to be addressed. Among those, the reliability of AI systems
needs to be demonstrated so that the AI systems can be used with confidence by
the general public. In this paper, we provide statistical perspectives on the
reliability of AI systems. Different from other considerations, the reliability
of AI systems focuses on the time dimension. That is, the system can perform
its designed functionality for the intended period. We introduce a so-called
SMART statistical framework for AI reliability research, which includes five
components: Structure of the system, Metrics of reliability, Analysis of
failure causes, Reliability assessment, and Test planning. We review
traditional methods in reliability data analysis and software reliability, and
discuss how those existing methods can be transformed for reliability modeling
and assessment of AI systems. We also describe recent developments in modeling
and analysis of AI reliability and outline statistical research challenges in
this area, including out-of-distribution detection, the effect of the training
set, adversarial attacks, model accuracy, and uncertainty quantification, and
discuss how those topics can be related to AI reliability, with illustrative
examples. Finally, we discuss data collection and test planning for AI
reliability assessment and how to improve system designs for higher AI
reliability. The paper closes with some concluding remarks.
</summary>
    <author>
      <name>Yili Hong</name>
    </author>
    <author>
      <name>Jiayi Lian</name>
    </author>
    <author>
      <name>Li Xu</name>
    </author>
    <author>
      <name>Jie Min</name>
    </author>
    <author>
      <name>Yueyao Wang</name>
    </author>
    <author>
      <name>Laura J. Freeman</name>
    </author>
    <author>
      <name>Xinwei Deng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.05391v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.05391v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01252v2</id>
    <updated>2022-03-09T21:46:26Z</updated>
    <published>2021-11-23T08:01:08Z</published>
    <title>Australia's Approach to AI Governance in Security and Defence</title>
    <summary>  Australia is a leading AI nation with strong allies and partnerships.
Australia has prioritised the development of robotics, AI, and autonomous
systems to develop sovereign capability for the military. Australia commits to
Article 36 reviews of all new means and methods of warfare to ensure weapons
and weapons systems are operated within acceptable systems of control.
Additionally, Australia has undergone significant reviews of the risks of AI to
human rights and within intelligence organisations and has committed to
producing ethics guidelines and frameworks in Security and Defence. Australia
is committed to OECD's values-based principles for the responsible stewardship
of trustworthy AI as well as adopting a set of National AI ethics principles.
While Australia has not adopted an AI governance framework specifically for the
Australian Defence Organisation (ADO); Defence Science and Technology Group
(DSTG) has published 'A Method for Ethical AI in Defence' (MEAID) technical
report which includes a framework and pragmatic tools for managing ethical and
legal risks for military applications of AI. Australia can play a leadership
role by integrating legal and ethical considerations into its ADO AI capability
acquisition process. This requires a policy framework that defines its legal
and ethical requirements, is informed by Defence industry stakeholders, and
provides a practical methodology to integrate legal and ethical risk mitigation
strategies into the acquisition process.
</summary>
    <author>
      <name>Susannah Kate Devitt</name>
    </author>
    <author>
      <name>Damian Copeland</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">60 pages, 7 boxes, 2 figures, 2 annexes, submitted for Eds M. Raska,
  Z. Stanley-Lockman, &amp; R. Bitzinger. AI Governance for National Security and
  Defence: Assessing Military AI Strategic Perspectives. Routledge</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.01252v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01252v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.06916v1</id>
    <updated>2022-04-14T12:18:51Z</updated>
    <published>2022-04-14T12:18:51Z</published>
    <title>Should I Follow AI-based Advice? Measuring Appropriate Reliance in
  Human-AI Decision-Making</title>
    <summary>  Many important decisions in daily life are made with the help of advisors,
e.g., decisions about medical treatments or financial investments. Whereas in
the past, advice has often been received from human experts, friends, or
family, advisors based on artificial intelligence (AI) have become more and
more present nowadays. Typically, the advice generated by AI is judged by a
human and either deemed reliable or rejected. However, recent work has shown
that AI advice is not always beneficial, as humans have shown to be unable to
ignore incorrect AI advice, essentially representing an over-reliance on AI.
Therefore, the aspired goal should be to enable humans not to rely on AI advice
blindly but rather to distinguish its quality and act upon it to make better
decisions. Specifically, that means that humans should rely on the AI in the
presence of correct advice and self-rely when confronted with incorrect advice,
i.e., establish appropriate reliance (AR) on AI advice on a case-by-case basis.
Current research lacks a metric for AR. This prevents a rigorous evaluation of
factors impacting AR and hinders further development of human-AI
decision-making. Therefore, based on the literature, we derive a measurement
concept of AR. We propose to view AR as a two-dimensional construct that
measures the ability to discriminate advice quality and behave accordingly. In
this article, we derive the measurement concept, illustrate its application and
outline potential future research.
</summary>
    <author>
      <name>Max Schemmer</name>
    </author>
    <author>
      <name>Patrick Hemmer</name>
    </author>
    <author>
      <name>Niklas Kühl</name>
    </author>
    <author>
      <name>Carina Benz</name>
    </author>
    <author>
      <name>Gerhard Satzger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Conference on Human Factors in Computing Systems (CHI '22),
  Workshop on Trust and Reliance in AI-Human Teams (trAIt)</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.06916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.06916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.09978v1</id>
    <updated>2022-06-20T19:44:39Z</updated>
    <published>2022-06-20T19:44:39Z</published>
    <title>German AI Start-Ups and AI Ethics: Using A Social Practice Lens for
  Assessing and Implementing Socio-Technical Innovation</title>
    <summary>  Within the current AI ethics discourse, there is a gap in empirical research
on understanding how AI practitioners understand ethics and socially organize
to operationalize ethical concerns, particularly in the context of AI
start-ups. This gap intensifies the risk of a disconnect between scholarly
research, innovation, and application. This risk materializes acutely as
mounting pressures to identify and mitigate the potential harms of AI systems
have created an urgent need to assess and implement socio-technical innovation
for fairness, accountability, and transparency. Building on social practice
theory, we address this need via a framework that allows AI researchers,
practitioners, and regulators to systematically analyze existing cultural
understandings, histories, and social practices of ethical AI to define
appropriate strategies for effectively implementing socio-technical
innovations. Our contributions are threefold: 1) we introduce a practice-based
approach for understanding ethical AI; 2) we present empirical findings from
our study on the operationalization of ethics in German AI start-ups to
underline that AI ethics and social practices must be understood in their
specific cultural and historical contexts; and 3) based on our empirical
findings, we suggest that ethical AI practices can be broken down into
principles, needs, narratives, materializations, and cultural genealogies to
form a useful backdrop for considering socio-technical innovations.
</summary>
    <author>
      <name>Mona Sloane</name>
    </author>
    <author>
      <name>Janina Zakrzewski</name>
    </author>
    <link href="http://arxiv.org/abs/2206.09978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.09978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.01493v2</id>
    <updated>2022-11-15T10:34:55Z</updated>
    <published>2022-06-30T17:24:29Z</published>
    <title>AI Ethics: An Empirical Study on the Views of Practitioners and
  Lawmakers</title>
    <summary>  Artificial Intelligence (AI) solutions and technologies are being
increasingly adopted in smart systems context, however, such technologies are
continuously concerned with ethical uncertainties. Various guidelines,
principles, and regulatory frameworks are designed to ensure that AI
technologies bring ethical well-being. However, the implications of AI ethics
principles and guidelines are still being debated. To further explore the
significance of AI ethics principles and relevant challenges, we conducted a
survey of 99 representative AI practitioners and lawmakers (e.g., AI engineers,
lawyers) from twenty countries across five continents. To the best of our
knowledge, this is the first empirical study that encapsulates the perceptions
of two different types of population (AI practitioners and lawmakers) and the
study findings confirm that transparency, accountability, and privacy are the
most critical AI ethics principles. On the other hand, lack of ethical
knowledge, no legal frameworks, and lacking monitoring bodies are found the
most common AI ethics challenges. The impact analysis of the challenges across
AI ethics principles reveals that conflict in practice is a highly severe
challenge. Moreover, the perceptions of practitioners and lawmakers are
statistically correlated with significant differences for particular principles
(e.g. fairness, freedom) and challenges (e.g. lacking monitoring bodies,
machine distortion). Our findings stimulate further research, especially
empowering existing capability maturity models to support the development and
quality assessment of ethics-aware AI systems.
</summary>
    <author>
      <name>Arif Ali Khan</name>
    </author>
    <author>
      <name>Muhammad Azeem Akbar</name>
    </author>
    <author>
      <name>Mahdi Fahmideh</name>
    </author>
    <author>
      <name>Peng Liang</name>
    </author>
    <author>
      <name>Muhammad Waseem</name>
    </author>
    <author>
      <name>Aakash Ahmad</name>
    </author>
    <author>
      <name>Mahmood Niazi</name>
    </author>
    <author>
      <name>Pekka Abrahamsson</name>
    </author>
    <link href="http://arxiv.org/abs/2207.01493v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.01493v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.01510v1</id>
    <updated>2022-06-08T12:32:08Z</updated>
    <published>2022-06-08T12:32:08Z</published>
    <title>Fairness in Agreement With European Values: An Interdisciplinary
  Perspective on AI Regulation</title>
    <summary>  With increasing digitalization, Artificial Intelligence (AI) is becoming
ubiquitous. AI-based systems to identify, optimize, automate, and scale
solutions to complex economic and societal problems are being proposed and
implemented. This has motivated regulation efforts, including the Proposal of
an EU AI Act. This interdisciplinary position paper considers various concerns
surrounding fairness and discrimination in AI, and discusses how AI regulations
address them, focusing on (but not limited to) the Proposal. We first look at
AI and fairness through the lenses of law, (AI) industry, sociotechnology, and
(moral) philosophy, and present various perspectives. Then, we map these
perspectives along three axes of interests: (i) Standardization vs.
Localization, (ii) Utilitarianism vs. Egalitarianism, and (iii) Consequential
vs. Deontological ethics which leads us to identify a pattern of common
arguments and tensions between these axes. Positioning the discussion within
the axes of interest and with a focus on reconciling the key tensions, we
identify and propose the roles AI Regulation should take to make the endeavor
of the AI Act a success in terms of AI fairness concerns.
</summary>
    <author>
      <name>Alejandra Bringas Colmenarejo</name>
    </author>
    <author>
      <name>Luca Nannini</name>
    </author>
    <author>
      <name>Alisa Rieger</name>
    </author>
    <author>
      <name>Kristen M. Scott</name>
    </author>
    <author>
      <name>Xuan Zhao</name>
    </author>
    <author>
      <name>Gourab K. Patro</name>
    </author>
    <author>
      <name>Gjergji Kasneci</name>
    </author>
    <author>
      <name>Katharina Kinder-Kurlanda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3514094.3534158</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3514094.3534158" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In proceedings of AAAI/ACM Conference AIES 2022
  (https://doi.org/10.1145/3514094.3534158)</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.01510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.01510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.08960v1</id>
    <updated>2022-09-23T20:09:03Z</updated>
    <published>2022-09-23T20:09:03Z</published>
    <title>Deceptive AI Systems That Give Explanations Are Just as Convincing as
  Honest AI Systems in Human-Machine Decision Making</title>
    <summary>  The ability to discern between true and false information is essential to
making sound decisions. However, with the recent increase in AI-based
disinformation campaigns, it has become critical to understand the influence of
deceptive systems on human information processing. In experiment (N=128), we
investigated how susceptible people are to deceptive AI systems by examining
how their ability to discern true news from fake news varies when AI systems
are perceived as either human fact-checkers or AI fact-checking systems, and
when explanations provided by those fact-checkers are either deceptive or
honest. We find that deceitful explanations significantly reduce accuracy,
indicating that people are just as likely to believe deceptive AI explanations
as honest AI explanations. Although before getting assistance from an
AI-system, people have significantly higher weighted discernment accuracy on
false headlines than true headlines, we found that with assistance from an AI
system, discernment accuracy increased significantly when given honest
explanations on both true headlines and false headlines, and decreased
significantly when given deceitful explanations on true headlines and false
headlines. Further, we did not observe any significant differences in
discernment between explanations perceived as coming from a human fact checker
compared to an AI-fact checker. Similarly, we found no significant differences
in trust. These findings exemplify the dangers of deceptive AI systems and the
need for finding novel ways to limit their influence human information
processing.
</summary>
    <author>
      <name>Valdemar Danry</name>
    </author>
    <author>
      <name>Pat Pataranutaporn</name>
    </author>
    <author>
      <name>Ziv Epstein</name>
    </author>
    <author>
      <name>Matthew Groh</name>
    </author>
    <author>
      <name>Pattie Maes</name>
    </author>
    <link href="http://arxiv.org/abs/2210.08960v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.08960v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.13371v1</id>
    <updated>2022-12-27T06:05:49Z</updated>
    <published>2022-12-27T06:05:49Z</published>
    <title>Measuring an artificial intelligence agent's trust in humans using
  machine incentives</title>
    <summary>  Scientists and philosophers have debated whether humans can trust advanced
artificial intelligence (AI) agents to respect humanity's best interests. Yet
what about the reverse? Will advanced AI agents trust humans? Gauging an AI
agent's trust in humans is challenging because--absent costs for
dishonesty--such agents might respond falsely about their trust in humans. Here
we present a method for incentivizing machine decisions without altering an AI
agent's underlying algorithms or goal orientation. In two separate experiments,
we then employ this method in hundreds of trust games between an AI agent (a
Large Language Model (LLM) from OpenAI) and a human experimenter (author TJ).
In our first experiment, we find that the AI agent decides to trust humans at
higher rates when facing actual incentives than when making hypothetical
decisions. Our second experiment replicates and extends these findings by
automating game play and by homogenizing question wording. We again observe
higher rates of trust when the AI agent faces real incentives. Across both
experiments, the AI agent's trust decisions appear unrelated to the magnitude
of stakes. Furthermore, to address the possibility that the AI agent's trust
decisions reflect a preference for uncertainty, the experiments include two
conditions that present the AI agent with a non-social decision task that
provides the opportunity to choose a certain or uncertain option; in those
conditions, the AI agent consistently chooses the certain option. Our
experiments suggest that one of the most advanced AI language models to date
alters its social behavior in response to incentives and displays behavior
consistent with trust toward a human interlocutor when incentivized.
</summary>
    <author>
      <name>Tim Johnson</name>
    </author>
    <author>
      <name>Nick Obradovich</name>
    </author>
    <link href="http://arxiv.org/abs/2212.13371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.13371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.01590v1</id>
    <updated>2023-01-03T15:08:10Z</updated>
    <published>2023-01-03T15:08:10Z</published>
    <title>FATE in AI: Towards Algorithmic Inclusivity and Accessibility</title>
    <summary>  One of the defining phenomena in this age is the widespread deployment of
systems powered by artificial intelligence (AI) technology. With AI taking the
center stage, many sections of society are being affected directly or
indirectly by algorithmic decisions. Algorithmic decisions carry both
economical and personal implications which have brought about the issues of
fairness, accountability, transparency and ethics (FATE) in AI geared towards
addressing algorithmic disparities. Ethical AI deals with incorporating moral
behaviour to avoid encoding bias in AI's decisions. However, the present
discourse on such critical issues is being shaped by the more economically
developed countries (MEDC), which raises concerns regarding neglecting local
knowledge, cultural pluralism and global fairness. This study builds upon
existing research on responsible AI, with a focus on areas in the Global South
considered to be under-served vis-a-vis AI. Our goal is two-fold (1) to assess
FATE-related issues and the effectiveness of transparency methods and (2) to
proffer useful insights and stimulate action towards bridging the accessibility
and inclusivity gap in AI. Using ads data from online social networks, we
designed a user study (n=43) to achieve the above goals. Among the findings
from the study include: explanations about decisions reached by the AI systems
tend to be vague and less informative. To bridge the accessibility and
inclusivity gap, there is a need to engage with the community for the best way
to integrate fairness, accountability, transparency and ethics in AI. This will
help in empowering the affected community or individual to effectively probe
and police the growing application of AI-powered systems.
</summary>
    <author>
      <name>Isa Inuwa-Dutse</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 4 figures, 7 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.01590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.01590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02033v2</id>
    <updated>2018-02-07T16:14:19Z</updated>
    <published>2018-02-06T16:20:17Z</published>
    <title>Ways of Applying Artificial Intelligence in Software Engineering</title>
    <summary>  As Artificial Intelligence (AI) techniques have become more powerful and
easier to use they are increasingly deployed as key components of modern
software systems. While this enables new functionality and often allows better
adaptation to user needs it also creates additional problems for software
engineers and exposes companies to new risks. Some work has been done to better
understand the interaction between Software Engineering and AI but we lack
methods to classify ways of applying AI in software systems and to analyse and
understand the risks this poses. Only by doing so can we devise tools and
solutions to help mitigate them. This paper presents the AI in SE Application
Levels (AI-SEAL) taxonomy that categorises applications according to their
point of AI application, the type of AI technology used and the automation
level allowed. We show the usefulness of this taxonomy by classifying 15 papers
from previous editions of the RAISE workshop. Results show that the taxonomy
allows classification of distinct AI applications and provides insights
concerning the risks associated with them. We argue that this will be important
for companies in deciding how to apply AI in their software applications and to
create strategies for its use.
</summary>
    <author>
      <name>Robert Feldt</name>
    </author>
    <author>
      <name>Francisco G. de Oliveira Neto</name>
    </author>
    <author>
      <name>Richard Torkar</name>
    </author>
    <link href="http://arxiv.org/abs/1802.02033v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02033v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.01525v1</id>
    <updated>2020-03-03T14:22:11Z</updated>
    <published>2020-03-03T14:22:11Z</published>
    <title>Evidence-based explanation to promote fairness in AI systems</title>
    <summary>  As Artificial Intelligence (AI) technology gets more intertwined with every
system, people are using AI to make decisions on their everyday activities. In
simple contexts, such as Netflix recommendations, or in more complex context
like in judicial scenarios, AI is part of people's decisions. People make
decisions and usually, they need to explain their decision to others or in some
matter. It is particularly critical in contexts where human expertise is
central to decision-making. In order to explain their decisions with AI
support, people need to understand how AI is part of that decision. When
considering the aspect of fairness, the role that AI has on a decision-making
process becomes even more sensitive since it affects the fairness and the
responsibility of those people making the ultimate decision. We have been
exploring an evidence-based explanation design approach to 'tell the story of a
decision'. In this position paper, we discuss our approach for AI systems using
fairness sensitive cases in the literature.
</summary>
    <author>
      <name>Juliana Jansen Ferreira</name>
    </author>
    <author>
      <name>Mateus de Souza Monteiro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fair &amp; Responsible AI Workshop @ CHI2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.01525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.01525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.13053v1</id>
    <updated>2019-05-29T14:37:16Z</updated>
    <published>2019-05-29T14:37:16Z</published>
    <title>Unpredictability of AI</title>
    <summary>  The young field of AI Safety is still in the process of identifying its
challenges and limitations. In this paper, we formally describe one such
impossibility result, namely Unpredictability of AI. We prove that it is
impossible to precisely and consistently predict what specific actions a
smarter-than-human intelligent system will take to achieve its objectives, even
if we know terminal goals of the system. In conclusion, impact of
Unpredictability on AI Safety is discussed.
</summary>
    <author>
      <name>Roman V. Yampolskiy</name>
    </author>
    <link href="http://arxiv.org/abs/1905.13053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.13053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.12750v2</id>
    <updated>2021-02-18T10:23:35Z</updated>
    <published>2020-11-25T14:14:47Z</published>
    <title>AI virtues -- The missing link in putting AI ethics into practice</title>
    <summary>  Several seminal ethics initiatives have stipulated sets of principles and
standards for good technology development in the AI sector. However, widespread
criticism has pointed out a lack of practical realization of these principles.
Following that, AI ethics underwent a practical turn, but without deviating
from the principled approach and the many shortcomings associated with it. This
paper proposes a different approach. It defines four basic AI virtues, namely
justice, honesty, responsibility and care, all of which represent specific
motivational settings that constitute the very precondition for ethical
decision making in the AI field. Moreover, it defines two second-order AI
virtues, prudence and fortitude, that bolster achieving the basic virtues by
helping with overcoming bounded ethicality or the many hidden psychological
forces that impair ethical decision making and that are hitherto disregarded in
AI ethics. Lastly, the paper describes measures for successfully cultivating
the mentioned virtues in organizations dealing with AI research and
development.
</summary>
    <author>
      <name>Thilo Hagendorff</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13347-022-00553-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13347-022-00553-z" rel="related"/>
    <link href="http://arxiv.org/abs/2011.12750v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.12750v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.06668v2</id>
    <updated>2020-02-18T19:58:35Z</updated>
    <published>2019-06-16T12:40:08Z</published>
    <title>Principles alone cannot guarantee ethical AI</title>
    <summary>  AI Ethics is now a global topic of discussion in academic and policy circles.
At least 84 public-private initiatives have produced statements describing
high-level principles, values, and other tenets to guide the ethical
development, deployment, and governance of AI. According to recent
meta-analyses, AI Ethics has seemingly converged on a set of principles that
closely resemble the four classic principles of medical ethics. Despite the
initial credibility granted to a principled approach to AI Ethics by the
connection to principles in medical ethics, there are reasons to be concerned
about its future impact on AI development and governance. Significant
differences exist between medicine and AI development that suggest a principled
approach in the latter may not enjoy success comparable to the former. Compared
to medicine, AI development lacks (1) common aims and fiduciary duties, (2)
professional history and norms, (3) proven methods to translate principles into
practice, and (4) robust legal and professional accountability mechanisms.
These differences suggest we should not yet celebrate consensus around
high-level principles that hide deep political and normative disagreement.
</summary>
    <author>
      <name>Brent Mittelstadt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s42256-019-0114-4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s42256-019-0114-4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A previous, pre-print version of this paper was entitled 'AI Ethics -
  Too Principled to Fail?'</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nat Mach Intell 1, 501-507, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.06668v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.06668v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.12838v2</id>
    <updated>2019-12-20T15:28:49Z</updated>
    <published>2019-09-27T16:28:01Z</published>
    <title>Responsible AI by Design in Practice</title>
    <summary>  Recently, a lot of attention has been given to undesired consequences of
Artificial Intelligence (AI), such as unfair bias leading to discrimination, or
the lack of explanations of the results of AI systems. There are several
important questions to answer before AI can be deployed at scale in our
businesses and societies. Most of these issues are being discussed by experts
and the wider communities, and it seems there is broad consensus on where they
come from. There is, however, less consensus on, and experience with how to
practically deal with those issues in organizations that develop and use AI,
both from a technical and organizational perspective. In this paper, we discuss
the practical case of a large organization that is putting in place a
company-wide methodology to minimize the risk of undesired consequences of AI.
We hope that other organizations can learn from this and that our experience
contributes to making the best of AI while minimizing its risks.
</summary>
    <author>
      <name>Richard Benjamins</name>
    </author>
    <author>
      <name>Alberto Barbado</name>
    </author>
    <author>
      <name>Daniel Sierra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 tables. Proceedings of the Human-Centered AI:
  Trustworthiness of AI Models &amp; Data (HAI) track at AAAI Fall Symposium, DC,
  November 7-9, 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.12838v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.12838v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.1, K.4.2, K.4.3" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.1; K.4.2; K.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.12583v1</id>
    <updated>2019-10-22T15:21:42Z</updated>
    <published>2019-10-22T15:21:42Z</published>
    <title>Solidarity should be a core ethical principle of Artificial Intelligence</title>
    <summary>  Solidarity is one of the fundamental values at the heart of the construction
of peaceful societies and present in more than one third of world's
constitutions. Still, solidarity is almost never included as a principle in
ethical guidelines for the development of AI. Solidarity as an AI principle (1)
shares the prosperity created by AI, implementing mechanisms to redistribute
the augmentation of productivity for all; and shares the burdens, making sure
that AI does not increase inequality and no human is left behind. Solidarity as
an AI principle (2) assesses the long term implications before developing and
deploying AI systems so no groups of humans become irrelevant because of AI
systems. Considering solidarity as a core principle for AI development will
provide not just an human-centric but a more humanity-centric approach to AI.
</summary>
    <author>
      <name>Miguel Luengo-Oroz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s42256-019-0115-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s42256-019-0115-3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a pre-print of an article published in Nature Machine
  Intelligence. The final authenticated version is available online at:
  https://doi.org/10.1038/s42256-019-0115-3</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nature Machine Intelligence, 1-11 (2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.12583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.12583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.11072v1</id>
    <updated>2020-05-22T09:24:07Z</updated>
    <published>2020-05-22T09:24:07Z</published>
    <title>Regulating Artificial Intelligence: Proposal for a Global Solution</title>
    <summary>  With increasing ubiquity of artificial intelligence (AI) in modern societies,
individual countries and the international community are working hard to create
an innovation-friendly, yet safe, regulatory environment. Adequate regulation
is key to maximize the benefits and minimize the risks stemming from AI
technologies. Developing regulatory frameworks is, however, challenging due to
AI's global reach and the existence of widespread misconceptions about the
notion of regulation. We argue that AI-related challenges cannot be tackled
effectively without sincere international coordination supported by robust,
consistent domestic and international governance arrangements. Against this
backdrop, we propose the establishment of an international AI governance
framework organized around a new AI regulatory agency that -- drawing on
interdisciplinary expertise -- could help creating uniform standards for the
regulation of AI technologies and inform the development of AI policies around
the world. We also believe that a fundamental change of mindset on what
constitutes regulation is necessary to remove existing barriers that hamper
contemporary efforts to develop AI regulatory regimes, and put forward some
recommendations on how to achieve this, and what opportunities doing so would
present.
</summary>
    <author>
      <name>Olivia J. Erdélyi</name>
    </author>
    <author>
      <name>Judy Goldsmith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages. A preliminary version appeared in the Proceedings of the
  First AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society,
  pages 95-101, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.11072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.11072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.12695v4</id>
    <updated>2021-02-11T02:01:02Z</updated>
    <published>2020-06-23T01:49:45Z</published>
    <title>Lessons Learned from Designing an AI-Enabled Diagnosis Tool for
  Pathologists</title>
    <summary>  Despite the promises of data-driven artificial intelligence (AI), little is
known about how we can bridge the gulf between traditional physician-driven
diagnosis and a plausible future of medicine automated by AI. Specifically, how
can we involve AI usefully in physicians' diagnosis workflow given that most AI
is still nascent and error-prone (e.g., in digital pathology)? To explore this
question, we first propose a series of collaborative techniques to engage human
pathologists with AI given AI's capabilities and limitations, based on which we
prototype Impetus - a tool where an AI takes various degrees of initiatives to
provide various forms of assistance to a pathologist in detecting tumors from
histological slides. We summarize observations and lessons learned from a study
with eight pathologists and discuss recommendations for future work on
human-centered medical AI systems.
</summary>
    <author>
      <name>Hongyan Gu</name>
    </author>
    <author>
      <name>Jingbin Huang</name>
    </author>
    <author>
      <name>Lauren Hung</name>
    </author>
    <author>
      <name>Xiang 'Anthony' Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3449084</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3449084" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 5 figures. To appear in the 24th ACM Conference on
  Computer-Supported Cooperative Work and Social Computing (CSCW 2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.12695v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.12695v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.13250v1</id>
    <updated>2020-09-24T20:12:14Z</updated>
    <published>2020-09-24T20:12:14Z</published>
    <title>Advancing the Research and Development of Assured Artificial
  Intelligence and Machine Learning Capabilities</title>
    <summary>  Artificial intelligence (AI) and machine learning (ML) have become
increasingly vital in the development of novel defense and intelligence
capabilities across all domains of warfare. An adversarial AI (A2I) and
adversarial ML (AML) attack seeks to deceive and manipulate AI/ML models. It is
imperative that AI/ML models can defend against these attacks. A2I/AML defenses
will help provide the necessary assurance of these advanced capabilities that
use AI/ML models. The A2I Working Group (A2IWG) seeks to advance the research
and development of assured AI/ML capabilities via new A2I/AML defenses by
fostering a collaborative environment across the U.S. Department of Defense and
U.S. Intelligence Community. The A2IWG aims to identify specific challenges
that it can help solve or address more directly, with initial focus on three
topics: AI Trusted Robustness, AI System Security, and AI/ML Architecture
Vulnerabilities.
</summary>
    <author>
      <name>Tyler J. Shipp</name>
    </author>
    <author>
      <name>Daniel J. Clouse</name>
    </author>
    <author>
      <name>Michael J. De Lucia</name>
    </author>
    <author>
      <name>Metin B. Ahiskali</name>
    </author>
    <author>
      <name>Kai Steverson</name>
    </author>
    <author>
      <name>Jonathan M. Mullin</name>
    </author>
    <author>
      <name>Nathaniel D. Bastian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at AAAI FSS-20: Artificial Intelligence in Government and
  Public Sector, Washington, DC, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.13250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.13250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.09101v1</id>
    <updated>2020-10-18T20:50:45Z</updated>
    <published>2020-10-18T20:50:45Z</published>
    <title>The Convergence of AI code and Cortical Functioning -- a Commentary</title>
    <summary>  Neural nets, one of the oldest architectures for AI programming, are loosely
based on biological neurons and their properties. Recent work on language
applications has made the AI code closer to biological reality in several ways.
This commentary examines this convergence and, in light of what is known of
neocortical structure, addresses the question of whether ``general AI'' looks
attainable with these tools.
</summary>
    <author>
      <name>David Mumford</name>
    </author>
    <link href="http://arxiv.org/abs/2010.09101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.01165v1</id>
    <updated>2020-11-28T01:49:24Z</updated>
    <published>2020-11-28T01:49:24Z</published>
    <title>AI in the "Real World": Examining the Impact of AI Deployment in
  Low-Resource Contexts</title>
    <summary>  As AI becomes integrated throughout the world, its potential for impact
within low-resource regions around the Global South have grown. AI research
labs from tech giants like Microsoft, Google, and IBM have a significant
presence in countries such as India, Ghana, and South Africa. The work done by
these labs is often motivated by the potential impact it could have on local
populations, but the deployment of these tools has not always gone smoothly.
This paper presents a case study examining the deployment of AI by large
industry labs situated in low-resource contexts, highlights factors impacting
unanticipated deployments, and reflects on the state of AI deployment within
the Global South, providing suggestions that embrace inclusive design
methodologies within AI development that prioritize the needs of marginalized
communities and elevate their status not just as beneficiaries of AI systems
but as primary stakeholders.
</summary>
    <author>
      <name>Chinasa T. Okolo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of the Navigating the Broader Impacts of AI Research Workshop at
  NeurIPS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.01165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.01165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.06098v1</id>
    <updated>2021-01-13T19:33:34Z</updated>
    <published>2021-01-13T19:33:34Z</published>
    <title>How AI Developers Overcome Communication Challenges in a
  Multidisciplinary Team: A Case Study</title>
    <summary>  The development of AI applications is a multidisciplinary effort, involving
multiple roles collaborating with the AI developers, an umbrella term we use to
include data scientists and other AI-adjacent roles on the same team. During
these collaborations, there is a knowledge mismatch between AI developers, who
are skilled in data science, and external stakeholders who are typically not.
This difference leads to communication gaps, and the onus falls on AI
developers to explain data science concepts to their collaborators. In this
paper, we report on a study including analyses of both interviews with AI
developers and artifacts they produced for communication. Using the analytic
lens of shared mental models, we report on the types of communication gaps that
AI developers face, how AI developers communicate across disciplinary and
organizational boundaries, and how they simultaneously manage issues regarding
trust and expectations.
</summary>
    <author>
      <name>David Piorkowski</name>
    </author>
    <author>
      <name>Soya Park</name>
    </author>
    <author>
      <name>April Yi Wang</name>
    </author>
    <author>
      <name>Dakuo Wang</name>
    </author>
    <author>
      <name>Michael Muller</name>
    </author>
    <author>
      <name>Felix Portnoy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3449205</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3449205" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 7 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.06098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.06098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.05351v1</id>
    <updated>2021-02-10T09:48:56Z</updated>
    <published>2021-02-10T09:48:56Z</published>
    <title>Quality Assurance for AI-based Systems: Overview and Challenges</title>
    <summary>  The number and importance of AI-based systems in all domains is growing. With
the pervasive use and the dependence on AI-based systems, the quality of these
systems becomes essential for their practical usage. However, quality assurance
for AI-based systems is an emerging area that has not been well explored and
requires collaboration between the SE and AI research communities. This paper
discusses terminology and challenges on quality assurance for AI-based systems
to set a baseline for that purpose. Therefore, we define basic concepts and
characterize AI-based systems along the three dimensions of artifact type,
process, and quality characteristics. Furthermore, we elaborate on the key
challenges of (1) understandability and interpretability of AI models, (2) lack
of specifications and defined requirements, (3) need for validation data and
test input generation, (4) defining expected outcomes as test oracles, (5)
accuracy and correctness measures, (6) non-functional properties of AI-based
systems, (7) self-adaptive and self-learning characteristics, and (8) dynamic
and frequently changing environments.
</summary>
    <author>
      <name>Michael Felderer</name>
    </author>
    <author>
      <name>Rudolf Ramler</name>
    </author>
    <link href="http://arxiv.org/abs/2102.05351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.05351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.10703v1</id>
    <updated>2021-03-19T09:34:52Z</updated>
    <published>2021-03-19T09:34:52Z</published>
    <title>Lessons Learned from Educating AI Engineers</title>
    <summary>  Over the past three years we have built a practice-oriented, bachelor level,
educational programme for software engineers to specialize as AI engineers. The
experience with this programme and the practical assignments our students
execute in industry has given us valuable insights on the profession of AI
engineer. In this paper we discuss our programme and the lessons learned for
industry and research.
</summary>
    <author>
      <name>Petra Heck</name>
    </author>
    <author>
      <name>Gerard Schouten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Acccepted for the 1st International Workshop on AI Engineering
  (WAIN21)</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.10703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.10703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.15558v1</id>
    <updated>2021-03-17T08:39:44Z</updated>
    <published>2021-03-17T08:39:44Z</published>
    <title>A Survey of Hybrid Human-Artificial Intelligence for Social Computing</title>
    <summary>  Along with the development of modern computing technology and social
sciences, both theoretical research and practical applications of social
computing have been continuously extended. In particular with the boom of
artificial intelligence (AI), social computing is significantly influenced by
AI. However, the conventional technologies of AI have drawbacks in dealing with
more complicated and dynamic problems. Such deficiency can be rectified by
hybrid human-artificial intelligence (H-AI) which integrates both human
intelligence and AI into one unity, forming a new enhanced intelligence. H-AI
in dealing with social problems shows the advantages that AI can not surpass.
This paper firstly introduces the concept of H-AI. AI is the intelligence in
the transition stage of H-AI, so the latest research progresses of AI in social
computing are reviewed. Secondly, it summarizes typical challenges faced by AI
in social computing, and makes it possible to introduce H-AI to solve these
challenges. Finally, the paper proposes a holistic framework of social
computing combining with H-AI, which consists of four layers: object layer,
base layer, analysis layer, and application layer. It represents H-AI has
significant advantages over AI in solving social problems.
</summary>
    <author>
      <name>Wenxi Wang</name>
    </author>
    <author>
      <name>Huansheng Ning</name>
    </author>
    <author>
      <name>Feifei Shi</name>
    </author>
    <author>
      <name>Sahraoui Dhelim</name>
    </author>
    <author>
      <name>Weishan Zhang</name>
    </author>
    <author>
      <name>Liming Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/THMS.2021.3131683</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/THMS.2021.3131683" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Human-Machine Systems 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.15558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.02199v1</id>
    <updated>2021-04-06T00:34:06Z</updated>
    <published>2021-04-06T00:34:06Z</published>
    <title>Designing Efficient and High-performance AI Accelerators with Customized
  STT-MRAM</title>
    <summary>  In this paper, we demonstrate the design of efficient and high-performance
AI/Deep Learning accelerators with customized STT-MRAM and a reconfigurable
core. Based on model-driven detailed design space exploration, we present the
design methodology of an innovative scratchpad-assisted on-chip STT-MRAM based
buffer system for high-performance accelerators. Using analytically derived
expression of memory occupancy time of AI model weights and activation maps,
the volatility of STT-MRAM is adjusted with process and temperature variation
aware scaling of thermal stability factor to optimize the retention time,
energy, read/write latency, and area of STT-MRAM. From the analysis of modern
AI workloads and accelerator implementation in 14nm technology, we verify the
efficacy of our designed AI accelerator with STT-MRAM STT-AI. Compared to an
SRAM-based implementation, the STT-AI accelerator achieves 75% area and 3%
power savings at iso-accuracy. Furthermore, with a relaxed bit error rate and
negligible AI accuracy trade-off, the designed STT-AI Ultra accelerator
achieves 75.4%, and 3.5% savings in area and power, respectively over regular
SRAM-based accelerators.
</summary>
    <author>
      <name>Kaniz Mishty</name>
    </author>
    <author>
      <name>Mehdi Sadi</name>
    </author>
    <link href="http://arxiv.org/abs/2104.02199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.02199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.02704v3</id>
    <updated>2021-07-17T21:51:45Z</updated>
    <published>2021-05-02T23:29:36Z</published>
    <title>AI Risk Skepticism</title>
    <summary>  In this work, we survey skepticism regarding AI risk and show parallels with
other types of scientific skepticism. We start by classifying different types
of AI Risk skepticism and analyze their root causes. We conclude by suggesting
some intervention approaches, which may be successful in reducing AI risk
skepticism, at least amongst artificial intelligence researchers.
</summary>
    <author>
      <name>Roman V. Yampolskiy</name>
    </author>
    <link href="http://arxiv.org/abs/2105.02704v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.02704v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.06264v1</id>
    <updated>2021-04-27T14:58:02Z</updated>
    <published>2021-04-27T14:58:02Z</published>
    <title>Do We Expect More from Radiology AI than from Radiologists?</title>
    <summary>  What we expect from radiology AI algorithms will shape the selection and
implementation of AI in the radiologic practice. In this paper I consider
prevailing expectations of AI and compare them to expectations that we have of
human readers. I observe that the expectations from AI and radiologists are
fundamentally different. The expectations of AI are based on a strong and
justified mistrust about the way that AI makes decisions. Because AI decisions
are not well understood, it is difficult to know how the algorithms will behave
in new, unexpected situations. However, this mistrust is not mirrored in our
expectations of human readers. Despite well-proven idiosyncrasies and biases in
human decision making, we take comfort from the assumption that others make
decisions in a way as we do, and we trust our own decision making. Despite poor
ability to explain decision making processes in humans, we accept explanations
of decisions given by other humans. Because the goal of radiology is the most
accurate radiologic interpretation, our expectations of radiologists and AI
should be similar, and both should reflect a healthy mistrust of complicated
and partially opaque decision processes undergoing in computer algorithms and
human brains. This is generally not the case now.
</summary>
    <author>
      <name>Maciej A. Mazurowski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1148/ryai.2021200221</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1148/ryai.2021200221" rel="related"/>
    <link href="http://arxiv.org/abs/2105.06264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.06264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.10356v1</id>
    <updated>2021-05-20T08:27:29Z</updated>
    <published>2021-05-20T08:27:29Z</published>
    <title>AI Certification: Advancing Ethical Practice by Reducing Information
  Asymmetries</title>
    <summary>  As artificial intelligence (AI) systems are increasingly deployed, principles
for ethical AI are also proliferating. Certification offers a method to both
incentivize adoption of these principles and substantiate that they have been
implemented in practice. This paper draws from management literature on
certification and reviews current AI certification programs and proposals.
Successful programs rely on both emerging technical methods and specific design
considerations. In order to avoid two common failures of certification, program
designs should ensure that the symbol of the certification is substantially
implemented in practice and that the program achieves its stated goals. The
review indicates that the field currently focuses on self-certification and
third-party certification of systems, individuals, and organizations - to the
exclusion of process management certifications. Additionally, the paper
considers prospects for future AI certification programs. Ongoing changes in AI
technology suggest that AI certification regimes should be designed to
emphasize governance criteria of enduring value, such as ethics training for AI
developers, and to adjust technical criteria as the technology changes.
Overall, certification can play a valuable mix in the portfolio of AI
governance tools.
</summary>
    <author>
      <name>Peter Cihon</name>
    </author>
    <author>
      <name>Moritz J. Kleinaltenkamp</name>
    </author>
    <author>
      <name>Jonas Schuett</name>
    </author>
    <author>
      <name>Seth D. Baum</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TTS.2021.3077595</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TTS.2021.3077595" rel="related"/>
    <link href="http://arxiv.org/abs/2105.10356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.10356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.13901v1</id>
    <updated>2021-06-25T22:31:55Z</updated>
    <published>2021-06-25T22:31:55Z</published>
    <title>Building Bridges: Generative Artworks to Explore AI Ethics</title>
    <summary>  In recent years, there has been an increased emphasis on understanding and
mitigating adverse impacts of artificial intelligence (AI) technologies on
society. Across academia, industry, and government bodies, a variety of
endeavours are being pursued towards enhancing AI ethics. A significant
challenge in the design of ethical AI systems is that there are multiple
stakeholders in the AI pipeline, each with their own set of constraints and
interests. These different perspectives are often not understood, due in part
to communication gaps.For example, AI researchers who design and develop AI
models are not necessarily aware of the instability induced in consumers' lives
by the compounded effects of AI decisions. Educating different stakeholders
about their roles and responsibilities in the broader context becomes
necessary. In this position paper, we outline some potential ways in which
generative artworks can play this role by serving as accessible and powerful
educational tools for surfacing different perspectives. We hope to spark
interdisciplinary discussions about computational creativity broadly as a tool
for enhancing AI ethics.
</summary>
    <author>
      <name>Ramya Srinivasan</name>
    </author>
    <author>
      <name>Devi Parikh</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR Workshop on Ethical Considerations in Creative Applications
  of Computer Vision, 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.13901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.13901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.14099v1</id>
    <updated>2021-07-08T08:43:16Z</updated>
    <published>2021-07-08T08:43:16Z</published>
    <title>The ghost of AI governance past, present and future: AI governance in
  the European Union</title>
    <summary>  The received wisdom is that artificial intelligence (AI) is a competition
between the US and China. In this chapter, the author will examine how the
European Union (EU) fits into that mix and what it can offer as a third way to
govern AI. The chapter presents this by exploring the past, present and future
of AI governance in the EU. Section 1 serves to explore and evidence the EUs
coherent and comprehensive approach to AI governance. In short, the EU ensures
and encourages ethical, trustworthy and reliable technological development.
This will cover a range of key documents and policy tools that lead to the most
crucial effort of the EU to date: to regulate AI. Section 2 maps the EUs drive
towards digital sovereignty through the lens of regulation and infrastructure.
This covers topics such as the trustworthiness of AI systems, cloud, compute
and foreign direct investment. In Section 3, the chapter concludes by offering
several considerations to achieve good AI governance in the EU.
</summary>
    <author>
      <name>Charlotte Stix</name>
    </author>
    <link href="http://arxiv.org/abs/2107.14099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.14099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.07804v1</id>
    <updated>2021-08-18T14:06:08Z</updated>
    <published>2021-08-18T14:06:08Z</published>
    <title>A Framework for Understanding AI-Induced Field Change: How AI
  Technologies are Legitimized and Institutionalized</title>
    <summary>  Artificial intelligence (AI) systems operate in increasingly diverse areas,
from healthcare to facial recognition, the stock market, autonomous vehicles,
and so on. While the underlying digital infrastructure of AI systems is
developing rapidly, each area of implementation is subject to different degrees
and processes of legitimization. By combining elements from institutional
theory and information systems-theory, this paper presents a conceptual
framework to analyze and understand AI-induced field-change. The introduction
of novel AI-agents into new or existing fields creates a dynamic in which
algorithms (re)shape organizations and institutions while existing
institutional infrastructures determine the scope and speed at which
organizational change is allowed to occur. Where institutional infrastructure
and governance arrangements, such as standards, rules, and regulations, still
are unelaborate, the field can move fast but is also more likely to be
contested. The institutional infrastructure surrounding AI-induced fields is
generally little elaborated, which could be an obstacle to the broader
institutionalization of AI-systems going forward.
</summary>
    <author>
      <name>Benjamin Cedric Larsen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3461702.3462591</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3461702.3462591" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 2021 AAAI ACM Conference on AI Ethics and
  Society</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2108.07804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.07804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.13861v1</id>
    <updated>2021-08-30T02:05:22Z</updated>
    <published>2021-08-30T02:05:22Z</published>
    <title>An Artificial Intelligence Life Cycle: From Conception to Production</title>
    <summary>  Drawing on our experience of more than a decade of AI in academic research,
technology development, industry engagement, postgraduate teaching, doctoral
supervision and organisational consultancy, we present the 'CDAC AI Life
Cycle', a comprehensive life cycle for the design, development and deployment
of Artificial Intelligence (AI) systems and solutions. It consists of three
phases, Design, Develop and Deploy, and 17 constituent stages across the three
phases from conception to production of any AI initiative. The 'Design' phase
highlights the importance of contextualising a problem description by reviewing
public domain and service-based literature on state-of-the-art AI applications,
algorithms, pre-trained models and equally importantly ethics guidelines and
frameworks, which then informs the data, or Big Data, acquisition and
preparation. The 'Develop' phase is technique-oriented, as it transforms data
and algorithms into AI models that are benchmarked, evaluated and explained.
The 'Deploy' phase evaluates computational performance, which then apprises
pipelines for model operationalisation, culminating in the hyperautomation of a
process or system as a complete AI solution, that is continuously monitored and
evaluated to inform the next iteration of the life cycle. An ontological
mapping of AI algorithms to applications, followed by an organisational context
for the AI life cycle are further contributions of this article.
</summary>
    <author>
      <name>Daswin De Silva</name>
    </author>
    <author>
      <name>Damminda Alahakoon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.13861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.13861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.02866v1</id>
    <updated>2021-09-07T04:57:29Z</updated>
    <published>2021-09-07T04:57:29Z</published>
    <title>Readying Medical Students for Medical AI: The Need to Embed AI Ethics
  Education</title>
    <summary>  Medical students will almost inevitably encounter powerful medical AI systems
early in their careers. Yet, contemporary medical education does not adequately
equip students with the basic clinical proficiency in medical AI needed to use
these tools safely and effectively. Education reform is urgently needed, but
not easily implemented, largely due to an already jam-packed medical curricula.
In this article, we propose an education reform framework as an effective and
efficient solution, which we call the Embedded AI Ethics Education Framework.
Unlike other calls for education reform to accommodate AI teaching that are
more radical in scope, our framework is modest and incremental. It leverages
existing bioethics or medical ethics curricula to develop and deliver content
on the ethical issues associated with medical AI, especially the harms of
technology misuse, disuse, and abuse that affect the risk-benefit analyses at
the heart of healthcare. In doing so, the framework provides a simple tool for
going beyond the "What?" and the "Why?" of medical AI ethics education, to
answer the "How?", giving universities, course directors, and/or professors a
broad road-map for equipping their students with the necessary clinical
proficiency in medical AI.
</summary>
    <author>
      <name>Thomas P Quinn</name>
    </author>
    <author>
      <name>Simon Coghlan</name>
    </author>
    <link href="http://arxiv.org/abs/2109.02866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.02866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.08149v1</id>
    <updated>2021-09-15T23:57:48Z</updated>
    <published>2021-09-15T23:57:48Z</published>
    <title>Karpov's Queen Sacrifices and AI</title>
    <summary>  Anatoly Karpov's Queen sacrifices are analyzed. Stockfish 14 NNUE -- an AI
chess engine -- evaluates how efficient Karpov's sacrifices are. For
comparative purposes, we provide a dataset on Karpov's Rook and Knight
sacrifices to test whether Karpov achieves a similar level of accuracy. Our
study has implications for human-AI interaction and how humans can better
understand the strategies employed by black-box AI algorithms. Finally, we
conclude with implications for human study in. chess with computer engines.
</summary>
    <author>
      <name>Shiva Maharaj</name>
    </author>
    <author>
      <name>Nick Polson</name>
    </author>
    <link href="http://arxiv.org/abs/2109.08149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.08149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.00828v1</id>
    <updated>2021-10-02T15:51:51Z</updated>
    <published>2021-10-02T15:51:51Z</published>
    <title>Artificial intelligence for Sustainable Energy: A Contextual Topic
  Modeling and Content Analysis</title>
    <summary>  Parallel to the rising debates over sustainable energy and artificial
intelligence solutions, the world is currently discussing the ethics of
artificial intelligence and its possible negative effects on society and the
environment. In these arguments, sustainable AI is proposed, which aims at
advancing the pathway toward sustainability, such as sustainable energy. In
this paper, we offered a novel contextual topic modeling combining LDA, BERT,
and Clustering. We then combined these computational analyses with content
analysis of related scientific publications to identify the main scholarly
topics, sub-themes, and cross-topic themes within scientific research on
sustainable AI in energy. Our research identified eight dominant topics
including sustainable buildings, AI-based DSSs for urban water management,
climate artificial intelligence, Agriculture 4, the convergence of AI with IoT,
AI-based evaluation of renewable technologies, smart campus and engineering
education, and AI-based optimization. We then recommended 14 potential future
research strands based on the observed theoretical gaps. Theoretically, this
analysis contributes to the existing literature on sustainable AI and
sustainable energy, and practically, it intends to act as a general guide for
energy engineers and scientists, AI scientists, and social scientists to widen
their knowledge of sustainability in AI and energy convergence research.
</summary>
    <author>
      <name>Tahereh Saheb</name>
    </author>
    <author>
      <name>Mohammad Dehghani</name>
    </author>
    <link href="http://arxiv.org/abs/2110.00828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.00828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.09271v3</id>
    <updated>2022-02-28T06:02:00Z</updated>
    <published>2021-09-21T21:22:58Z</published>
    <title>Rebuilding Trust: Queer in AI Approach to Artificial Intelligence Risk
  Management</title>
    <summary>  Trustworthy artificial intelligence (AI) has become an important topic
because trust in AI systems and their creators has been lost. Researchers,
corporations, and governments have long and painful histories of excluding
marginalized groups from technology development, deployment, and oversight. As
a result, these technologies are less useful and even harmful to minoritized
groups. We argue that any AI development, deployment, and monitoring framework
that aspires to trust must incorporate both feminist, non-exploitative
participatory design principles and strong, outside, and continual monitoring
and testing. We additionally explain the importance of considering aspects of
trustworthiness beyond just transparency, fairness, and accountability,
specifically, to consider justice and shifting power to the disempowered as
core values to any trustworthy AI system. Creating trustworthy AI starts by
funding, supporting, and empowering grassroots organizations like Queer in AI
so the field of AI has the diversity and inclusion to credibly and effectively
develop trustworthy AI. We leverage the expert knowledge Queer in AI has
developed through its years of work and advocacy to discuss if and how gender,
sexuality, and other aspects of queer identity should be used in datasets and
AI systems and how harms along these lines should be mitigated. Based on this,
we share a gendered approach to AI and further propose a queer epistemology and
analyze the benefits it can bring to AI. We additionally discuss how to
regulate AI with this queer epistemology in vision, proposing frameworks for
making policies related to AI &amp; gender diversity and privacy &amp; queer data
protection.
</summary>
    <author>
      <name> Ashwin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Organizers of Queer in AI</arxiv:affiliation>
    </author>
    <author>
      <name>William Agnew</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Organizers of Queer in AI</arxiv:affiliation>
    </author>
    <author>
      <name>Umut Pajaro</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Organizers of Queer in AI</arxiv:affiliation>
    </author>
    <author>
      <name>Hetvi Jethwani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Organizers of Queer in AI</arxiv:affiliation>
    </author>
    <author>
      <name>Arjun Subramonian</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Organizers of Queer in AI</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">We discovered that the manuscript unintentionally contains passages
  that are direct quotes from previous literature, but fails to properly
  address them as such</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.09271v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.09271v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.09478v1</id>
    <updated>2021-11-18T02:18:27Z</updated>
    <published>2021-11-18T02:18:27Z</published>
    <title>Software Engineering for Responsible AI: An Empirical Study and
  Operationalised Patterns</title>
    <summary>  Although artificial intelligence (AI) is solving real-world challenges and
transforming industries, there are serious concerns about its ability to behave
and make decisions in a responsible way. Many AI ethics principles and
guidelines for responsible AI have been recently issued by governments,
organisations, and enterprises. However, these AI ethics principles and
guidelines are typically high-level and do not provide concrete guidance on how
to design and develop responsible AI systems. To address this shortcoming, we
first present an empirical study where we interviewed 21 scientists and
engineers to understand the practitioners' perceptions on AI ethics principles
and their implementation. We then propose a template that enables AI ethics
principles to be operationalised in the form of concrete patterns and suggest a
list of patterns using the newly created template. These patterns provide
concrete, operationalised guidance that facilitate the development of
responsible AI systems.
</summary>
    <author>
      <name>Qinghua Lu</name>
    </author>
    <author>
      <name>Liming Zhu</name>
    </author>
    <author>
      <name>Xiwei Xu</name>
    </author>
    <author>
      <name>Jon Whittle</name>
    </author>
    <author>
      <name>David Douglas</name>
    </author>
    <author>
      <name>Conrad Sanderson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICSE-SEIP55303.2022.9793864</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICSE-SEIP55303.2022.9793864" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICSE SEIP 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.09478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.09478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.04115v1</id>
    <updated>2022-02-03T02:14:38Z</updated>
    <published>2022-02-03T02:14:38Z</published>
    <title>Financial Vision Based Reinforcement Learning Trading Strategy</title>
    <summary>  Recent advances in artificial intelligence (AI) for quantitative trading have
led to its general superhuman performance in significant trading performance.
However, the potential risk of AI trading is a "black box" decision. Some AI
computing mechanisms are complex and challenging to understand. If we use AI
without proper supervision, AI may lead to wrong choices and make huge losses.
Hence, we need to ask about the AI "black box", including why did AI decide to
do this or not? Why can people trust AI or not? How can people fix their
mistakes? These problems also highlight the challenges that AI technology can
explain in the trading field.
</summary>
    <author>
      <name>Yun-Cheng Tsai</name>
    </author>
    <author>
      <name>Fu-Min Szu</name>
    </author>
    <author>
      <name>Jun-Hao Chen</name>
    </author>
    <author>
      <name>Samuel Yen-Chi Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2104.07715</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.04115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.04115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.09447v1</id>
    <updated>2022-02-18T22:08:08Z</updated>
    <published>2022-02-18T22:08:08Z</published>
    <title>A Mental-Model Centric Landscape of Human-AI Symbiosis</title>
    <summary>  There has been significant recent interest in developing AI agents capable of
effectively interacting and teaming with humans. While each of these works try
to tackle a problem quite central to the problem of human-AI interaction, they
tend to rely on myopic formulations that obscure the possible inter-relatedness
and complementarity of many of these works. The human-aware AI framework was a
recent effort to provide a unified account for human-AI interaction by casting
them in terms of their relationship to various mental models. Unfortunately,
the current accounts of human-aware AI are insufficient to explain the
landscape of the work doing in the space of human-AI interaction due to their
focus on limited settings. In this paper, we aim to correct this shortcoming by
introducing a significantly general version of human-aware AI interaction
scheme, called generalized human-aware interaction (GHAI), that talks about
(mental) models of six types. Through this paper, we will see how this new
framework allows us to capture the various works done in the space of human-AI
interaction and identify the fundamental behavioral patterns supported by these
works. We will also use this framework to identify potential gaps in the
current literature and suggest future research directions to address these
shortcomings.
</summary>
    <author>
      <name>Zahra Zahedi</name>
    </author>
    <author>
      <name>Sarath Sreedharan</name>
    </author>
    <author>
      <name>Subbarao Kambhampati</name>
    </author>
    <link href="http://arxiv.org/abs/2202.09447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.09447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.00905v2</id>
    <updated>2022-09-20T07:10:45Z</updated>
    <published>2022-03-02T07:30:03Z</published>
    <title>Responsible-AI-by-Design: a Pattern Collection for Designing Responsible
  AI Systems</title>
    <summary>  Although AI has significant potential to transform society, there are serious
concerns about its ability to behave and make decisions responsibly. Many
ethical regulations, principles, and guidelines for responsible AI have been
issued recently. However, these principles are high-level and difficult to put
into practice. In the meantime much effort has been put into responsible AI
from the algorithm perspective, but they are limited to a small subset of
ethical principles amenable to mathematical analysis. Responsible AI issues go
beyond data and algorithms and are often at the system-level crosscutting many
system components and the entire software engineering lifecycle. Based on the
result of a systematic literature review, this paper identifies one missing
element as the system-level guidance - how to design the architecture of
responsible AI systems. We present a summary of design patterns that can be
embedded into the AI systems as product features to contribute to
responsible-AI-by-design.
</summary>
    <author>
      <name>Qinghua Lu</name>
    </author>
    <author>
      <name>Liming Zhu</name>
    </author>
    <author>
      <name>Xiwei Xu</name>
    </author>
    <author>
      <name>Jon Whittle</name>
    </author>
    <link href="http://arxiv.org/abs/2203.00905v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.00905v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.09719v1</id>
    <updated>2022-04-08T05:49:04Z</updated>
    <published>2022-04-08T05:49:04Z</published>
    <title>Recent Progress in Conversational AI</title>
    <summary>  Conversational artificial intelligence (AI) is becoming an increasingly
popular topic among industry and academia. With the fast development of neural
network-based models, a lot of neural-based conversational AI system are
developed. We will provide a brief review of the recent progress in the
Conversational AI, including the commonly adopted techniques, notable works,
famous competitions from academia and industry and widely used datasets.
</summary>
    <author>
      <name>Zijun Xue</name>
    </author>
    <author>
      <name>Ruirui Li</name>
    </author>
    <author>
      <name>Mingda Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.09719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.09719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.00931v2</id>
    <updated>2022-05-03T11:41:56Z</updated>
    <published>2022-05-02T14:24:11Z</published>
    <title>Creative Uses of AI Systems and their Explanations: A Case Study from
  Insurance</title>
    <summary>  Recent works have recognized the need for human-centered perspectives when
designing and evaluating human-AI interactions and explainable AI methods. Yet,
current approaches fall short at intercepting and managing unexpected user
behavior resulting from the interaction with AI systems and explainability
methods of different stake-holder groups. In this work, we explore the use of
AI and explainability methods in the insurance domain. In an qualitative case
study with participants with different roles and professional backgrounds, we
show that AI and explainability methods are used in creative ways in daily
workflows, resulting in a divergence between their intended and actual use.
Finally, we discuss some recommendations for the design of human-AI
interactions and explainable AI methods to manage the risks and harness the
potential of unexpected user behavior.
</summary>
    <author>
      <name>Michaela Benk</name>
    </author>
    <author>
      <name>Raphael Weibel</name>
    </author>
    <author>
      <name>Andrea Ferrario</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the ACM CHI 2022 Workshop on Human-Centered Explainable
  AI (HCXAI)</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.00931v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00931v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.04279v1</id>
    <updated>2022-05-09T13:49:47Z</updated>
    <published>2022-05-09T13:49:47Z</published>
    <title>Aligned with Whom? Direct and social goals for AI systems</title>
    <summary>  As artificial intelligence (AI) becomes more powerful and widespread, the AI
alignment problem - how to ensure that AI systems pursue the goals that we want
them to pursue - has garnered growing attention. This article distinguishes two
types of alignment problems depending on whose goals we consider, and analyzes
the different solutions necessitated by each. The direct alignment problem
considers whether an AI system accomplishes the goals of the entity operating
it. In contrast, the social alignment problem considers the effects of an AI
system on larger groups or on society more broadly. In particular, it also
considers whether the system imposes externalities on others. Whereas solutions
to the direct alignment problem center around more robust implementation,
social alignment problems typically arise because of conflicts between
individual and group-level goals, elevating the importance of AI governance to
mediate such conflicts. Addressing the social alignment problem requires both
enforcing existing norms on their developers and operators and designing new
norms that apply directly to AI systems.
</summary>
    <author>
      <name>Anton Korinek</name>
    </author>
    <author>
      <name>Avital Balwit</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Prepared for the Oxford Handbook of AI Governance (23 pages, 2
  figures)</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.04279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.04279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.03217v2</id>
    <updated>2023-01-20T22:11:54Z</updated>
    <published>2022-05-04T22:10:01Z</published>
    <title>A Perspective on K-12 AI Education</title>
    <summary>  Artificial intelligence (AI), which enables machines to learn to perform a
task by training on diverse datasets, is one of the most revolutionary
developments in scientific history. Although AI and especially deep learning is
relatively new, it has already had transformative impact on medicine, biology,
transportation, entertainment, and beyond. As AI changes our daily lives at an
increasingly fast pace, we are challenged with preparing our society for an
AI-driven future. To this end, a critical step is to ensure an AI-ready
workforce through education. Advocates of beginning instruction of AI basics at
the K-12 level typically note benefits to the workforce, economy, and national
security. In this complementary perspective, we discuss why learning AI is
beneficial for motivating students and promoting creative thinking, and how to
develop a module-based approach that optimizes learning outcomes. We hope to
excite and engage more members of the education community to join the effort to
advance K-12 AI education in the USA and worldwide.
</summary>
    <author>
      <name>Nathan Wang</name>
    </author>
    <author>
      <name>Paul Tonko</name>
    </author>
    <author>
      <name>Nikil Ragav</name>
    </author>
    <author>
      <name>Michael Chungyoun</name>
    </author>
    <author>
      <name>Jonathan Plucker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.21300/23.1.2023.2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.21300/23.1.2023.2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Technology &amp;Amp; Innovation (2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2206.03217v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.03217v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.15363v1</id>
    <updated>2022-06-30T15:35:50Z</updated>
    <published>2022-06-30T15:35:50Z</published>
    <title>Why we do need Explainable AI for Healthcare</title>
    <summary>  The recent spike in certified Artificial Intelligence (AI) tools for
healthcare has renewed the debate around adoption of this technology. One
thread of such debate concerns Explainable AI and its promise to render AI
devices more transparent and trustworthy. A few voices active in the medical AI
space have expressed concerns on the reliability of Explainable AI techniques,
questioning their use and inclusion in guidelines and standards. Revisiting
such criticisms, this article offers a balanced and comprehensive perspective
on the utility of Explainable AI, focusing on the specificity of clinical
applications of AI and placing them in the context of healthcare interventions.
Against its detractors and despite valid concerns, we argue that the
Explainable AI research program is still central to human-machine interaction
and ultimately our main tool against loss of control, a danger that cannot be
prevented by rigorous clinical validation alone.
</summary>
    <author>
      <name>Giovanni Cinà</name>
    </author>
    <author>
      <name>Tabea Röber</name>
    </author>
    <author>
      <name>Rob Goedhart</name>
    </author>
    <author>
      <name>Ilker Birbil</name>
    </author>
    <link href="http://arxiv.org/abs/2206.15363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.15363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.07960v1</id>
    <updated>2022-08-16T21:39:58Z</updated>
    <published>2022-08-16T21:39:58Z</published>
    <title>Advancing Human-AI Complementarity: The Impact of User Expertise and
  Algorithmic Tuning on Joint Decision Making</title>
    <summary>  Human-AI collaboration for decision-making strives to achieve team
performance that exceeds the performance of humans or AI alone. However, many
factors can impact success of Human-AI teams, including a user's domain
expertise, mental models of an AI system, trust in recommendations, and more.
This work examines users' interaction with three simulated algorithmic models,
all with similar accuracy but different tuning on their true positive and true
negative rates. Our study examined user performance in a non-trivial blood
vessel labeling task where participants indicated whether a given blood vessel
was flowing or stalled.
  Our results show that while recommendations from an AI-Assistant can aid user
decision making, factors such as users' baseline performance relative to the AI
and complementary tuning of AI error types significantly impact overall team
performance. Novice users improved, but not to the accuracy level of the AI.
Highly proficient users were generally able to discern when they should follow
the AI recommendation and typically maintained or improved their performance.
Mid-performers, who had a similar level of accuracy to the AI, were most
variable in terms of whether the AI recommendations helped or hurt their
performance. In addition, we found that users' perception of the AI's
performance relative on their own also had a significant impact on whether
their accuracy improved when given AI recommendations. This work provides
insights on the complexity of factors related to Human-AI collaboration and
provides recommendations on how to develop human-centered AI algorithms to
complement users in decision-making tasks.
</summary>
    <author>
      <name>Kori Inkpen</name>
    </author>
    <author>
      <name>Shreya Chappidi</name>
    </author>
    <author>
      <name>Keri Mallari</name>
    </author>
    <author>
      <name>Besmira Nushi</name>
    </author>
    <author>
      <name>Divya Ramesh</name>
    </author>
    <author>
      <name>Pietro Michelucci</name>
    </author>
    <author>
      <name>Vani Mandava</name>
    </author>
    <author>
      <name>Libuše Hannah Vepřek</name>
    </author>
    <author>
      <name>Gabrielle Quinn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted and to be published in Transactions on Computer Human
  Interaction</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.07960v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.07960v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.09953v1</id>
    <updated>2022-08-21T19:47:41Z</updated>
    <published>2022-08-21T19:47:41Z</published>
    <title>Do-AIQ: A Design-of-Experiment Approach to Quality Evaluation of AI
  Mislabel Detection Algorithm</title>
    <summary>  The quality of Artificial Intelligence (AI) algorithms is of significant
importance for confidently adopting algorithms in various applications such as
cybersecurity, healthcare, and autonomous driving. This work presents a
principled framework of using a design-of-experimental approach to
systematically evaluate the quality of AI algorithms, named as Do-AIQ.
Specifically, we focus on investigating the quality of the AI mislabel data
algorithm against data poisoning. The performance of AI algorithms is affected
by hyperparameters in the algorithm and data quality, particularly, data
mislabeling, class imbalance, and data types. To evaluate the quality of the AI
algorithms and obtain a trustworthy assessment on the quality of the
algorithms, we establish a design-of-experiment framework to construct an
efficient space-filling design in a high-dimensional constraint space and
develop an effective surrogate model using additive Gaussian process to enable
the emulation of the quality of AI algorithms. Both theoretical and numerical
studies are conducted to justify the merits of the proposed framework. The
proposed framework can set an exemplar for AI algorithm to enhance the AI
assurance of robustness, reproducibility, and transparency.
</summary>
    <author>
      <name>J. Lian</name>
    </author>
    <author>
      <name>K. Choi</name>
    </author>
    <author>
      <name>B. Veeramani</name>
    </author>
    <author>
      <name>A. Hu</name>
    </author>
    <author>
      <name>L. Freeman</name>
    </author>
    <author>
      <name>E. Bowen</name>
    </author>
    <author>
      <name>X. Deng</name>
    </author>
    <link href="http://arxiv.org/abs/2208.09953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.08873v1</id>
    <updated>2022-09-19T09:28:17Z</updated>
    <published>2022-09-19T09:28:17Z</published>
    <title>New Trends in Photonic Switching and Optical Network Architecture for
  Data Centre and Computing Systems</title>
    <summary>  AI/ML for data centres and data centres for AI/ML are defining new trends in
cloud computing. Disaggregated heterogeneous reconfigurable computing systems
realized by photonic interconnects and photonic switching expect greatly
enhanced throughput and energy-efficiency for AI/ML workloads, especially when
aided by an AI/ML control plane.
</summary>
    <author>
      <name>S. J. Ben Yoo</name>
    </author>
    <link href="http://arxiv.org/abs/2209.08873v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.08873v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.06753v1</id>
    <updated>2022-11-12T21:54:05Z</updated>
    <published>2022-11-12T21:54:05Z</published>
    <title>Seamful XAI: Operationalizing Seamful Design in Explainable AI</title>
    <summary>  Mistakes in AI systems are inevitable, arising from both technical
limitations and sociotechnical gaps. While black-boxing AI systems can make the
user experience seamless, hiding the seams risks disempowering users to
mitigate fallouts from AI mistakes. While Explainable AI (XAI) has
predominantly tackled algorithmic opaqueness, we propose that seamful design
can foster Humancentered XAI by strategically revealing sociotechnical and
infrastructural mismatches. We introduce the notion of Seamful XAI by (1)
conceptually transferring "seams" to the AI context and (2) developing a design
process that helps stakeholders design with seams, thereby augmenting
explainability and user agency. We explore this process with 43 AI
practitioners and users, using a scenario-based co-design activity informed by
real-world use cases. We share empirical insights, implications, and critical
reflections on how this process can help practitioners anticipate and craft
seams in AI, how seamfulness can improve explainability, empower end-users, and
facilitate Responsible AI.
</summary>
    <author>
      <name>Upol Ehsan</name>
    </author>
    <author>
      <name>Q. Vera Liao</name>
    </author>
    <author>
      <name>Samir Passi</name>
    </author>
    <author>
      <name>Mark O. Riedl</name>
    </author>
    <author>
      <name>Hal Daume III</name>
    </author>
    <link href="http://arxiv.org/abs/2211.06753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.06753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.12243v1</id>
    <updated>2023-01-28T16:23:10Z</updated>
    <published>2023-01-28T16:23:10Z</published>
    <title>Investigating How Practitioners Use Human-AI Guidelines: A Case Study on
  the People + AI Guidebook</title>
    <summary>  Artificial intelligence (AI) presents new challenges for the user experience
(UX) of products and services. Recently, practitioner-facing resources and
design guidelines have become available to ease some of these challenges.
However, little research has investigated if and how these guidelines are used,
and how they impact practice. In this paper, we investigated how industry
practitioners use the People + AI Guidebook. We conducted interviews with 31
practitioners (i.e., designers, product managers) to understand how they use
human-AI guidelines when designing AI-enabled products. Our findings revealed
that practitioners use the guidebook not only for addressing AI's design
challenges, but also for education, cross-functional communication, and for
developing internal resources. We uncovered that practitioners desire more
support for early phase ideation and problem formulation to avoid AI product
failures. We discuss the implications for future resources aiming to help
practitioners in designing AI products.
</summary>
    <author>
      <name>Nur Yildirim</name>
    </author>
    <author>
      <name>Mahima Pushkarna</name>
    </author>
    <author>
      <name>Nitesh Goyal</name>
    </author>
    <author>
      <name>Martin Wattenberg</name>
    </author>
    <author>
      <name>Fernanda Viégas</name>
    </author>
    <link href="http://arxiv.org/abs/2301.12243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.12243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.05284v2</id>
    <updated>2023-02-26T17:57:36Z</updated>
    <published>2023-02-10T14:47:33Z</published>
    <title>Toward Human-Centered Responsible Artificial Intelligence: A Review of
  CHI Research and Industry Toolkits</title>
    <summary>  As Artificial Intelligence (AI) continues to advance rapidly, it becomes
increasingly important to consider AI's ethical and societal implications. In
this paper, we present a bottom-up mapping of the current state of research in
Human-Centered Responsible AI (HCR-AI) by thematically reviewing and analyzing
27 CHI research papers and 19 toolkits from industry and academia. Our results
show that the current research in HCR-AI places a heavy emphasis on
explainability, fairness, privacy, and security. We also found that there is
space to research accountability in AI and build usable tools for non-experts
to audit AI. While the CHI community has started to champion the well-being of
individuals directly or indirectly impacted by AI, more research and toolkits
are still required to address the long-term effects of AI on individuals,
societies, and natural resources (human flourishing and sustainability).
</summary>
    <author>
      <name>Mohammad Tahaei</name>
    </author>
    <author>
      <name>Marios Constantinides</name>
    </author>
    <author>
      <name>Daniele Quercia</name>
    </author>
    <link href="http://arxiv.org/abs/2302.05284v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.05284v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.10395v1</id>
    <updated>2023-02-21T02:06:24Z</updated>
    <published>2023-02-21T02:06:24Z</published>
    <title>Designerly Understanding: Information Needs for Model Transparency to
  Support Design Ideation for AI-Powered User Experience</title>
    <summary>  Despite the widespread use of artificial intelligence (AI), designing user
experiences (UX) for AI-powered systems remains challenging. UX designers face
hurdles understanding AI technologies, such as pre-trained language models, as
design materials. This limits their ability to ideate and make decisions about
whether, where, and how to use AI. To address this problem, we bridge the
literature on AI design and AI transparency to explore whether and how
frameworks for transparent model reporting can support design ideation with
pre-trained models. By interviewing 23 UX practitioners, we find that
practitioners frequently work with pre-trained models, but lack support for
UX-led ideation. Through a scenario-based design task, we identify common goals
that designers seek model understanding for and pinpoint their model
transparency information needs. Our study highlights the pivotal role that UX
designers can play in Responsible AI and calls for supporting their
understanding of AI limitations through model transparency and interrogation.
</summary>
    <author>
      <name>Q. Vera Liao</name>
    </author>
    <author>
      <name>Hariharan Subramonyam</name>
    </author>
    <author>
      <name>Jennifer Wang</name>
    </author>
    <author>
      <name>Jennifer Wortman Vaughan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACM CHI Conference on Human Factors in Computing Systems
  (CHI 2023)</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.10395v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.10395v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.01497v1</id>
    <updated>2022-06-25T21:31:14Z</updated>
    <published>2022-06-25T21:31:14Z</published>
    <title>Aligning Artificial Intelligence with Humans through Public Policy</title>
    <summary>  Given that Artificial Intelligence (AI) increasingly permeates our lives, it
is critical that we systematically align AI objectives with the goals and
values of humans. The human-AI alignment problem stems from the impracticality
of explicitly specifying the rewards that AI models should receive for all the
actions they could take in all relevant states of the world. One possible
solution, then, is to leverage the capabilities of AI models to learn those
rewards implicitly from a rich source of data describing human values in a wide
range of contexts. The democratic policy-making process produces just such data
by developing specific rules, flexible standards, interpretable guidelines, and
generalizable precedents that synthesize citizens' preferences over potential
actions taken in many states of the world. Therefore, computationally encoding
public policies to make them legible to AI systems should be an important part
of a socio-technical approach to the broader human-AI alignment puzzle. This
Essay outlines research on AI that learn structures in policy data that can be
leveraged for downstream tasks. As a demonstration of the ability of AI to
comprehend policy, we provide a case study of an AI system that predicts the
relevance of proposed legislation to any given publicly traded company and its
likely effect on that company. We believe this represents the "comprehension"
phase of AI and policy, but leveraging policy as a key source of human values
to align AI requires "understanding" policy. Solving the alignment problem is
crucial to ensuring that AI is beneficial both individually (to the person or
group deploying the AI) and socially. As AI systems are given increasing
responsibility in high-stakes contexts, integrating democratically-determined
policy into those systems could align their behavior with human goals in a way
that is responsive to a constantly evolving society.
</summary>
    <author>
      <name>John Nay</name>
    </author>
    <author>
      <name>James Daily</name>
    </author>
    <link href="http://arxiv.org/abs/2207.01497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.01497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.07892v1</id>
    <updated>2019-07-18T06:34:08Z</updated>
    <published>2019-07-18T06:34:08Z</published>
    <title>Global AI Ethics: A Review of the Social Impacts and Ethical
  Implications of Artificial Intelligence</title>
    <summary>  The ethical implications and social impacts of artificial intelligence have
become topics of compelling interest to industry, researchers in academia, and
the public. However, current analyses of AI in a global context are biased
toward perspectives held in the U.S., and limited by a lack of research,
especially outside the U.S. and Western Europe.
  This article summarizes the key findings of a literature review of recent
social science scholarship on the social impacts of AI and related technologies
in five global regions. Our team of social science researchers reviewed more
than 800 academic journal articles and monographs in over a dozen languages.
  Our review of the literature suggests that AI is likely to have markedly
different social impacts depending on geographical setting. Likewise,
perceptions and understandings of AI are likely to be profoundly shaped by
local cultural and social context.
  Recent research in U.S. settings demonstrates that AI-driven technologies
have a pattern of entrenching social divides and exacerbating social
inequality, particularly among historically-marginalized groups. Our literature
review indicates that this pattern exists on a global scale, and suggests that
low- and middle-income countries may be more vulnerable to the negative social
impacts of AI and less likely to benefit from the attendant gains.
  We call for rigorous ethnographic research to better understand the social
impacts of AI around the world. Global, on-the-ground research is particularly
critical to identify AI systems that may amplify social inequality in order to
mitigate potential harms. Deeper understanding of the social impacts of AI in
diverse social settings is a necessary precursor to the development,
implementation, and monitoring of responsible and beneficial AI technologies,
and forms the basis for meaningful regulation of these technologies.
</summary>
    <author>
      <name>Alexa Hagerty</name>
    </author>
    <author>
      <name>Igor Rubinov</name>
    </author>
    <link href="http://arxiv.org/abs/1907.07892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.07892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.03515v1</id>
    <updated>2019-10-08T16:19:49Z</updated>
    <published>2019-10-08T16:19:49Z</published>
    <title>Designing Trustworthy AI: A Human-Machine Teaming Framework to Guide
  Development</title>
    <summary>  Artificial intelligence (AI) holds great promise to empower us with knowledge
and augment our effectiveness. We can -- and must -- ensure that we keep humans
safe and in control, particularly with regard to government and public sector
applications that affect broad populations. How can AI development teams
harness the power of AI systems and design them to be valuable to humans?
Diverse teams are needed to build trustworthy artificial intelligent systems,
and those teams need to coalesce around a shared set of ethics. There are many
discussions in the AI field about ethics and trust, but there are few
frameworks available for people to use as guidance when creating these systems.
The Human-Machine Teaming (HMT) Framework for Designing Ethical AI Experiences
described in this paper, when used with a set of technical ethics, will guide
AI development teams to create AI systems that are accountable, de-risked,
respectful, secure, honest, and usable. To support the team's efforts,
activities to understand people's needs and concerns will be introduced along
with the themes to support the team's efforts. For example, usability testing
can help determine if the audience understands how the AI system works and
complies with the HMT Framework. The HMT Framework is based on reviews of
existing ethical codes and best practices in human-computer interaction and
software development. Human-machine teams are strongest when human users can
trust AI systems to behave as expected, safely, securely, and understandably.
Using the HMT Framework to design trustworthy AI systems will provide support
to teams in identifying potential issues ahead of time and making great
experiences for humans.
</summary>
    <author>
      <name>Carol J. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at AAAI FSS-19: Artificial Intelligence in Government and
  Public Sector, Arlington, Virginia, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.03515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.03515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.06424v1</id>
    <updated>2019-10-14T21:08:20Z</updated>
    <published>2019-10-14T21:08:20Z</published>
    <title>Integrating AI into Radiology workflow: Levels of research, production,
  and feedback maturity</title>
    <summary>  This report represents a roadmap for integrating Artificial Intelligence
(AI)-based image analysis algorithms into existing Radiology workflows such
that: (1) radiologists can significantly benefit from enhanced automation in
various imaging tasks due to AI; and (2) radiologists' feedback is utilized to
further improve the AI application. This is achieved by establishing three
maturity levels where: (1) research enables the visualization of AI-based
results/annotations by radiologists without generating new patient records; (2)
production allows the AI-based system to generate results stored in an
institution's Picture Archiving and Communication System; and (3) feedback
equips radiologists with tools for editing the AI inference results for
periodic retraining of the deployed AI systems, thereby allowing the continuous
organic improvement of AI-based radiology-workflow solutions. A case study
(i.e., detection of brain metastases with T1-weighted contrast-enhanced 3D MRI)
illustrates the deployment details of a particular AI-based application
according to the aforementioned maturity levels. It is shown that the given AI
application significantly improves with the feedback coming from radiologists;
the number of incorrectly detected brain metastases (false positives) reduces
from 14.2 to 9.12 per patient with the number of subsequently annotated
datasets increasing from 93 to 217 as a result of radiologist adjudication.
</summary>
    <author>
      <name>Engin Dikici</name>
    </author>
    <author>
      <name>Matthew Bigelow</name>
    </author>
    <author>
      <name>Luciano M. Prevedello</name>
    </author>
    <author>
      <name>Richard D. White</name>
    </author>
    <author>
      <name>Barbaros Selnur Erdal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.06424v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06424v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.09024v1</id>
    <updated>2019-12-19T05:34:08Z</updated>
    <published>2019-12-19T05:34:08Z</published>
    <title>Measuring the Quality of Explanations: The System Causability Scale
  (SCS). Comparing Human and Machine Explanations</title>
    <summary>  Recent success in Artificial Intelligence (AI) and Machine Learning (ML)
allow problem solving automatically without any human intervention. Autonomous
approaches can be very convenient. However, in certain domains, e.g., in the
medical domain, it is necessary to enable a domain expert to understand, why an
algorithm came up with a certain result. Consequently, the field of Explainable
AI (xAI) rapidly gained interest worldwide in various domains, particularly in
medicine. Explainable AI studies transparency and traceability of opaque AI/ML
and there are already a huge variety of methods. For example with layer-wise
relevance propagation relevant parts of inputs to, and representations in, a
neural network which caused a result, can be highlighted. This is a first
important step to ensure that end users, e.g., medical professionals, assume
responsibility for decision making with AI/ML and of interest to professionals
and regulators. Interactive ML adds the component of human expertise to AI/ML
processes by enabling them to re-enact and retrace AI/ML results, e.g. let them
check it for plausibility. This requires new human-AI interfaces for
explainable AI. In order to build effective and efficient interactive human-AI
interfaces we have to deal with the question of how to evaluate the quality of
explanations given by an explainable AI system. In this paper we introduce our
System Causability Scale (SCS) to measure the quality of explanations. It is
based on our notion of Causability (Holzinger et al., 2019) combined with
concepts adapted from a widely accepted usability scale.
</summary>
    <author>
      <name>Andreas Holzinger</name>
    </author>
    <author>
      <name>André Carrington</name>
    </author>
    <author>
      <name>Heimo Müller</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13218-020-00636-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13218-020-00636-z" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, 1 table, will appear in Springer/Nature KI -
  K\"unstliche Intelligenz (2020), Volume 34, Issue 2</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Springer/Nature KI Kuenstliche Intelligenz 34, 193-198 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.09024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.09024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.06300v1</id>
    <updated>2020-06-11T10:08:17Z</updated>
    <published>2020-06-11T10:08:17Z</published>
    <title>Montreal AI Ethics Institute's Response to Scotland's AI Strategy</title>
    <summary>  In January and February 2020, the Scottish Government released two documents
for review by the public regarding their artificial intelligence (AI) strategy.
The Montreal AI Ethics Institute (MAIEI) reviewed these documents and published
a response on 4 June 2020. MAIEI's response examines several questions that
touch on the proposed definition of AI; the people-centered nature of the
strategy; considerations to ensure that everyone benefits from AI; the
strategy's overarching vision; Scotland's AI ecosystem; the proposed strategic
themes; and how to grow public confidence in AI by building responsible and
ethical systems.
  In addition to examining the points above, MAIEI suggests that the strategy
be extended to include considerations on biometric data and how that will be
processed and used in the context of AI. It also highlights the importance of
tackling head-on the inherently stochastic nature of deep learning systems and
developing concrete guidelines to ensure that these systems are built
responsibly and ethically, particularly as machine learning becomes more
accessible. Finally, it concludes that any national AI strategy must clearly
address the measurements of success in regards to the strategy's stated goals
and vision to ensure that they are interpreted and applied consistently. To do
this, there must be inclusion and transparency between those building the
systems and those using them in their work.
</summary>
    <author>
      <name>Abhishek Gupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute and Microsoft</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the Scottish Government based on analysis by the MAIEI
  staff supplemented by contributions from participants of the workshop hosted
  by MAIEI</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.06300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.06300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.07321v1</id>
    <updated>2020-07-19T15:44:04Z</updated>
    <published>2020-07-19T15:44:04Z</published>
    <title>Expected Utilitarianism</title>
    <summary>  We want artificial intelligence (AI) to be beneficial. This is the grounding
assumption of most of the attitudes towards AI research. We want AI to be
"good" for humanity. We want it to help, not hinder, humans. Yet what exactly
this entails in theory and in practice is not immediately apparent.
Theoretically, this declarative statement subtly implies a commitment to a
consequentialist ethics. Practically, some of the more promising machine
learning techniques to create a robust AI, and perhaps even an artificial
general intelligence (AGI) also commit one to a form of utilitarianism. In both
dimensions, the logic of the beneficial AI movement may not in fact create
"beneficial AI" in either narrow applications or in the form of AGI if the
ethical assumptions are not made explicit and clear.
  Additionally, as it is likely that reinforcement learning (RL) will be an
important technique for machine learning in this area, it is also important to
interrogate how RL smuggles in a particular type of consequentialist reasoning
into the AI: particularly, a brute form of hedonistic act utilitarianism. Since
the mathematical logic commits one to a maximization function, the result is
that an AI will inevitably be seeking more and more rewards. We have two
conclusions that arise from this. First, is that if one believes that a
beneficial AI is an ethical AI, then one is committed to a framework that
posits 'benefit' is tantamount to the greatest good for the greatest number.
Second, if the AI relies on RL, then the way it reasons about itself, the
environment, and other agents, will be through an act utilitarian morality.
This proposition may, or may not, in fact be actually beneficial for humanity.
</summary>
    <author>
      <name>Heather M. Roff</name>
    </author>
    <link href="http://arxiv.org/abs/2008.07321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.07321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.13871v2</id>
    <updated>2022-05-09T13:52:35Z</updated>
    <published>2020-09-29T08:49:44Z</published>
    <title>Signs for Ethical AI: A Route Towards Transparency</title>
    <summary>  Today, Artificial Intelligence (AI) has a direct impact on the daily life of
billions of people. Being applied to sectors like finance, health, security and
advertisement, AI fuels some of the biggest companies and research institutions
in the world. Its impact in the near future seems difficult to predict or
bound. In contrast to all this power, society remains mostly ignorant of the
capabilities and standard practices of AI today. To address this imbalance,
improving current interactions between people and AI systems, we propose a
transparency scheme to be implemented on any AI system open to the public. The
scheme is based on two pillars: Data Privacy and AI Transparency. The first
recognizes the relevance of data for AI, and is supported by GDPR. The second
considers aspects of AI transparency currently unregulated: AI capabilities,
purpose and source. We design this pillar based on ethical principles. For each
of the two pillars, we define a three-level display. The first level is based
on visual signs, inspired by traffic signs managing the interaction between
people and cars, and designed for quick and universal interpretability. The
second level uses factsheets, providing limited details. The last level
provides access to all available information. After detailing and exemplifying
the proposed transparency scheme, we define a set of principles for creating
transparent by design software, to be used during the integration of AI
components on user-oriented services.
</summary>
    <author>
      <name>Dario Garcia-Gasulla</name>
    </author>
    <author>
      <name>Atia Cortés</name>
    </author>
    <author>
      <name>Sergio Alvarez-Napagao</name>
    </author>
    <author>
      <name>Ulises Cortés</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 7 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.13871v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.13871v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.02592v2</id>
    <updated>2020-12-07T02:23:13Z</updated>
    <published>2020-11-26T16:01:49Z</published>
    <title>Transdisciplinary AI Observatory -- Retrospective Analyses and
  Future-Oriented Contradistinctions</title>
    <summary>  In the last years, AI safety gained international recognition in the light of
heterogeneous safety-critical and ethical issues that risk overshadowing the
broad beneficial impacts of AI. In this context, the implementation of AI
observatory endeavors represents one key research direction. This paper
motivates the need for an inherently transdisciplinary AI observatory approach
integrating diverse retrospective and counterfactual views. We delineate aims
and limitations while providing hands-on-advice utilizing concrete practical
examples. Distinguishing between unintentionally and intentionally triggered AI
risks with diverse socio-psycho-technological impacts, we exemplify a
retrospective descriptive analysis followed by a retrospective counterfactual
risk analysis. Building on these AI observatory tools, we present near-term
transdisciplinary guidelines for AI safety. As further contribution, we discuss
differentiated and tailored long-term directions through the lens of two
disparate modern AI safety paradigms. For simplicity, we refer to these two
different paradigms with the terms artificial stupidity (AS) and eternal
creativity (EC) respectively. While both AS and EC acknowledge the need for a
hybrid cognitive-affective approach to AI safety and overlap with regard to
many short-term considerations, they differ fundamentally in the nature of
multiple envisaged long-term solution patterns. By compiling relevant
underlying contradistinctions, we aim to provide future-oriented incentives for
constructive dialectics in practical and theoretical AI safety research.
</summary>
    <author>
      <name>Nadisha-Marie Aliman</name>
    </author>
    <author>
      <name>Leon Kester</name>
    </author>
    <author>
      <name>Roman Yampolskiy</name>
    </author>
    <link href="http://arxiv.org/abs/2012.02592v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.02592v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.06034v1</id>
    <updated>2020-12-10T23:54:31Z</updated>
    <published>2020-12-10T23:54:31Z</published>
    <title>Artificial Intelligence &amp; Cooperation</title>
    <summary>  The rise of Artificial Intelligence (AI) will bring with it an
ever-increasing willingness to cede decision-making to machines. But rather
than just giving machines the power to make decisions that affect us, we need
ways to work cooperatively with AI systems. There is a vital need for research
in "AI and Cooperation" that seeks to understand the ways in which systems of
AIs and systems of AIs with people can engender cooperative behavior. Trust in
AI is also key: trust that is intrinsic and trust that can only be earned over
time. Here we use the term "AI" in its broadest sense, as employed by the
recent 20-Year Community Roadmap for AI Research (Gil and Selman, 2019),
including but certainly not limited to, recent advances in deep learning.
  With success, cooperation between humans and AIs can build society just as
human-human cooperation has. Whether coming from an intrinsic willingness to be
helpful, or driven through self-interest, human societies have grown strong and
the human species has found success through cooperation. We cooperate "in the
small" -- as family units, with neighbors, with co-workers, with strangers --
and "in the large" as a global community that seeks cooperative outcomes around
questions of commerce, climate change, and disarmament. Cooperation has evolved
in nature also, in cells and among animals. While many cases involving
cooperation between humans and AIs will be asymmetric, with the human
ultimately in control, AI systems are growing so complex that, even today, it
is impossible for the human to fully comprehend their reasoning,
recommendations, and actions when functioning simply as passive observers.
</summary>
    <author>
      <name>Elisa Bertino</name>
    </author>
    <author>
      <name>Finale Doshi-Velez</name>
    </author>
    <author>
      <name>Maria Gini</name>
    </author>
    <author>
      <name>Daniel Lopresti</name>
    </author>
    <author>
      <name>David Parkes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A Computing Community Consortium (CCC) white paper, 4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.06034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.06034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.04661v1</id>
    <updated>2021-02-09T06:06:13Z</updated>
    <published>2021-02-09T06:06:13Z</published>
    <title>Security and Privacy for Artificial Intelligence: Opportunities and
  Challenges</title>
    <summary>  The increased adoption of Artificial Intelligence (AI) presents an
opportunity to solve many socio-economic and environmental challenges; however,
this cannot happen without securing AI-enabled technologies. In recent years,
most AI models are vulnerable to advanced and sophisticated hacking techniques.
This challenge has motivated concerted research efforts into adversarial AI,
with the aim of developing robust machine and deep learning models that are
resilient to different types of adversarial scenarios. In this paper, we
present a holistic cyber security review that demonstrates adversarial attacks
against AI applications, including aspects such as adversarial knowledge and
capabilities, as well as existing methods for generating adversarial examples
and existing cyber defence models. We explain mathematical AI models,
especially new variants of reinforcement and federated learning, to demonstrate
how attack vectors would exploit vulnerabilities of AI models. We also propose
a systematic framework for demonstrating attack techniques against AI
applications and reviewed several cyber defences that would protect AI
applications against those attacks. We also highlight the importance of
understanding the adversarial goals and their capabilities, especially the
recent attacks against industry applications, to develop adaptive defences that
assess to secure AI applications. Finally, we describe the main challenges and
future research directions in the domain of security and privacy of AI
technologies.
</summary>
    <author>
      <name>Ayodeji Oseni</name>
    </author>
    <author>
      <name>Nour Moustafa</name>
    </author>
    <author>
      <name>Helge Janicke</name>
    </author>
    <author>
      <name>Peng Liu</name>
    </author>
    <author>
      <name>Zahir Tari</name>
    </author>
    <author>
      <name>Athanasios Vasilakos</name>
    </author>
    <link href="http://arxiv.org/abs/2102.04661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.04661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.05460v2</id>
    <updated>2021-02-22T14:27:15Z</updated>
    <published>2021-02-10T14:28:34Z</published>
    <title>The human-AI relationship in decision-making: AI explanation to support
  people on justifying their decisions</title>
    <summary>  The explanation dimension of Artificial Intelligence (AI) based system has
been a hot topic for the past years. Different communities have raised concerns
about the increasing presence of AI in people's everyday tasks and how it can
affect people's lives. There is a lot of research addressing the
interpretability and transparency concepts of explainable AI (XAI), which are
usually related to algorithms and Machine Learning (ML) models. But in
decision-making scenarios, people need more awareness of how AI works and its
outcomes to build a relationship with that system. Decision-makers usually need
to justify their decision to others in different domains. If that decision is
somehow based on or influenced by an AI-system outcome, the explanation about
how the AI reached that result is key to building trust between AI and humans
in decision-making scenarios. In this position paper, we discuss the role of
XAI in decision-making scenarios, our vision of Decision-Making with AI-system
in the loop, and explore one case from the literature about how XAI can impact
people justifying their decisions, considering the importance of building the
human-AI relationship for those scenarios.
</summary>
    <author>
      <name>Juliana Jansen Ferreira</name>
    </author>
    <author>
      <name>Mateus Monteiro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-print of paper accepted in Workshop on Transparency And
  Explanations In Smart Systems (TEXSS) held in conjunction with ACM
  Intelligent User Interfaces (IUI) (April 2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.05460v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.05460v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.15764v1</id>
    <updated>2021-06-30T01:03:28Z</updated>
    <published>2021-06-30T01:03:28Z</published>
    <title>The Threat of Offensive AI to Organizations</title>
    <summary>  AI has provided us with the ability to automate tasks, extract information
from vast amounts of data, and synthesize media that is nearly
indistinguishable from the real thing. However, positive tools can also be used
for negative purposes. In particular, cyber adversaries can use AI (such as
machine learning) to enhance their attacks and expand their campaigns.
  Although offensive AI has been discussed in the past, there is a need to
analyze and understand the threat in the context of organizations. For example,
how does an AI-capable adversary impact the cyber kill chain? Does AI benefit
the attacker more than the defender? What are the most significant AI threats
facing organizations today and what will be their impact on the future?
  In this survey, we explore the threat of offensive AI on organizations.
First, we present the background and discuss how AI changes the adversary's
methods, strategies, goals, and overall attack model. Then, through a
literature review, we identify 33 offensive AI capabilities which adversaries
can use to enhance their attacks. Finally, through a user study spanning
industry and academia, we rank the AI threats and provide insights on the
adversaries.
</summary>
    <author>
      <name>Yisroel Mirsky</name>
    </author>
    <author>
      <name>Ambra Demontis</name>
    </author>
    <author>
      <name>Jaidip Kotak</name>
    </author>
    <author>
      <name>Ram Shankar</name>
    </author>
    <author>
      <name>Deng Gelei</name>
    </author>
    <author>
      <name>Liu Yang</name>
    </author>
    <author>
      <name>Xiangyu Zhang</name>
    </author>
    <author>
      <name>Wenke Lee</name>
    </author>
    <author>
      <name>Yuval Elovici</name>
    </author>
    <author>
      <name>Battista Biggio</name>
    </author>
    <link href="http://arxiv.org/abs/2106.15764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.15764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.07630v3</id>
    <updated>2021-10-21T18:20:15Z</updated>
    <published>2021-07-15T22:19:15Z</published>
    <title>Evaluation of Human-AI Teams for Learned and Rule-Based Agents in Hanabi</title>
    <summary>  Deep reinforcement learning has generated superhuman AI in competitive games
such as Go and StarCraft. Can similar learning techniques create a superior AI
teammate for human-machine collaborative games? Will humans prefer AI teammates
that improve objective team performance or those that improve subjective
metrics of trust? In this study, we perform a single-blind evaluation of teams
of humans and AI agents in the cooperative card game Hanabi, with both
rule-based and learning-based agents. In addition to the game score, used as an
objective metric of the human-AI team performance, we also quantify subjective
measures of the human's perceived performance, teamwork, interpretability,
trust, and overall preference of AI teammate. We find that humans have a clear
preference toward a rule-based AI teammate (SmartBot) over a state-of-the-art
learning-based AI teammate (Other-Play) across nearly all subjective metrics,
and generally view the learning-based agent negatively, despite no statistical
difference in the game score. This result has implications for future AI design
and reinforcement learning benchmarking, highlighting the need to incorporate
subjective metrics of human-AI teaming rather than a singular focus on
objective task performance.
</summary>
    <author>
      <name>Ho Chit Siu</name>
    </author>
    <author>
      <name>Jaime D. Pena</name>
    </author>
    <author>
      <name>Edenna Chen</name>
    </author>
    <author>
      <name>Yutai Zhou</name>
    </author>
    <author>
      <name>Victor J. Lopez</name>
    </author>
    <author>
      <name>Kyle Palko</name>
    </author>
    <author>
      <name>Kimberlee C. Chang</name>
    </author>
    <author>
      <name>Ross E. Allen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at NeurIPS 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.07630v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.07630v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.13509v1</id>
    <updated>2021-07-28T17:32:04Z</updated>
    <published>2021-07-28T17:32:04Z</published>
    <title>The Who in Explainable AI: How AI Background Shapes Perceptions of AI
  Explanations</title>
    <summary>  Explainability of AI systems is critical for users to take informed actions
and hold systems accountable. While "opening the opaque box" is important,
understanding who opens the box can govern if the Human-AI interaction is
effective. In this paper, we conduct a mixed-methods study of how two different
groups of whos--people with and without a background in AI--perceive different
types of AI explanations. These groups were chosen to look at how disparities
in AI backgrounds can exacerbate the creator-consumer gap. We quantitatively
share what the perceptions are along five dimensions: confidence, intelligence,
understandability, second chance, and friendliness. Qualitatively, we highlight
how the AI background influences each group's interpretations and elucidate why
the differences might exist through the lenses of appropriation and cognitive
heuristics. We find that (1) both groups had unwarranted faith in numbers, to
different extents and for different reasons, (2) each group found explanatory
values in different explanations that went beyond the usage we designed them
for, and (3) each group had different requirements of what counts as humanlike
explanations. Using our findings, we discuss potential negative consequences
such as harmful manipulation of user trust and propose design interventions to
mitigate them. By bringing conscious awareness to how and why AI backgrounds
shape perceptions of potential creators and consumers in XAI, our work takes a
formative step in advancing a pluralistic Human-centered Explainable AI
discourse.
</summary>
    <author>
      <name>Upol Ehsan</name>
    </author>
    <author>
      <name>Samir Passi</name>
    </author>
    <author>
      <name>Q. Vera Liao</name>
    </author>
    <author>
      <name>Larry Chan</name>
    </author>
    <author>
      <name>I-Hsiang Lee</name>
    </author>
    <author>
      <name>Michael Muller</name>
    </author>
    <author>
      <name>Mark O. Riedl</name>
    </author>
    <link href="http://arxiv.org/abs/2107.13509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.13509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.05933v1</id>
    <updated>2021-10-12T12:22:34Z</updated>
    <published>2021-10-12T12:22:34Z</published>
    <title>A Deployment Model to Extend Ethically Aligned AI Implementation Method
  ECCOLA</title>
    <summary>  There is a struggle in Artificial intelligence (AI) ethics to gain ground in
actionable methods and models to be utilized by practitioners while developing
and implementing ethically sound AI systems. AI ethics is a vague concept
without a consensus of definition or theoretical grounding and bearing little
connection to practice. Practice involving primarily technical tasks like
software development is not aptly equipped to process and decide upon ethical
considerations. Efforts to create tools and guidelines to help people working
with AI development have been concentrating almost solely on the technical
aspects of AI. A few exceptions do apply, such as the ECCOLA method for
creating ethically aligned AI -systems. ECCOLA has proven results in terms of
increased ethical considerations in AI systems development. Yet, it is a novel
innovation, and room for development still exists. This study aims to extend
ECCOLA with a deployment model to drive the adoption of ECCOLA, as any method,
no matter how good, is of no value without adoption and use. The model includes
simple metrics to facilitate the communication of ethical gaps or outcomes of
ethical AI development. It offers the opportunity to assess any AI system at
any given lifecycle phase, e.g., opening possibilities like analyzing the
ethicality of an AI system under acquisition.
</summary>
    <author>
      <name>Jani Antikainen</name>
    </author>
    <author>
      <name>Mamia Agbese</name>
    </author>
    <author>
      <name>Hanna-Kaisa Alanen</name>
    </author>
    <author>
      <name>Erika Halme</name>
    </author>
    <author>
      <name>Hannakaisa Isomäki</name>
    </author>
    <author>
      <name>Marianna Jantunen</name>
    </author>
    <author>
      <name>Kai-Kristian Kemell</name>
    </author>
    <author>
      <name>Rebekah Rousi</name>
    </author>
    <author>
      <name>Heidi Vainio-Pekka</name>
    </author>
    <author>
      <name>Ville Vakkuri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/REW53955.2021.00043</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/REW53955.2021.00043" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in 2021 IEEE 29th International Requirements Engineering
  Conference Workshops (REW), Notre Dame, IN, USA, 2021 pp. 230-235</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.05933v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.05933v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.03789v4</id>
    <updated>2023-01-04T00:38:47Z</updated>
    <published>2022-04-08T00:35:14Z</published>
    <title>Broadening AI Ethics Narratives: An Indic Art View</title>
    <summary>  Incorporating interdisciplinary perspectives is seen as an essential step
towards enhancing artificial intelligence (AI) ethics. In this regard, the
field of arts is perceived to play a key role in elucidating diverse historical
and cultural narratives, serving as a bridge across research communities. Most
of the works that examine the interplay between the field of arts and AI ethics
concern digital artworks, largely exploring the potential of computational
tools in being able to surface biases in AI systems. In this paper, we
investigate a complementary direction--that of uncovering the unique
socio-cultural perspectives embedded in human-made art, which in turn, can be
valuable in expanding the horizon of AI ethics. Through semi-structured
interviews across sixteen artists, art scholars, and researchers of diverse
Indian art forms like music, sculpture, painting, floor drawings, dance, etc.,
we explore how {\it non-Western} ethical abstractions, methods of learning, and
participatory practices observed in Indian arts, one of the most ancient yet
perpetual and influential art traditions, can shed light on aspects related to
ethical AI systems. Through a case study concerning the Indian dance system
(i.e. the {\it `Natyashastra'}), we analyze potential pathways towards
enhancing ethics in AI systems. Insights from our study outline the need for
(1) incorporating empathy in ethical AI algorithms, (2) integrating multimodal
data formats for ethical AI system design and development, (3) viewing AI
ethics as a dynamic, diverse, cumulative, and shared process rather than as a
static, self-contained framework to facilitate adaptability without
annihilation of values (4) consistent life-long learning to enhance AI
accountability
</summary>
    <author>
      <name>Ajay Divakaran</name>
    </author>
    <author>
      <name>Aparna Sridhar</name>
    </author>
    <author>
      <name>Ramya Srinivasan</name>
    </author>
    <link href="http://arxiv.org/abs/2204.03789v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.03789v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.13217v2</id>
    <updated>2022-06-03T18:17:44Z</updated>
    <published>2022-04-27T22:37:44Z</published>
    <title>Understanding User Perceptions, Collaborative Experience and User
  Engagement in Different Human-AI Interaction Designs for Co-Creative Systems</title>
    <summary>  Human-AI co-creativity involves humans and AI collaborating on a shared
creative product as partners. In a creative collaboration, communication is an
essential component among collaborators. In many existing co-creative systems
users can communicate with the AI, usually using buttons or sliders. Typically,
the AI in co-creative systems cannot communicate back to humans, limiting their
potential to be perceived as partners rather than just a tool. This paper
presents a study with 38 participants to explore the impact of two interaction
designs, with and without AI-to-human communication, on user engagement,
collaborative experience and user perception of a co-creative AI. The study
involves user interaction with two prototypes of a co-creative system that
contributes sketches as design inspirations during a design task. The results
show improved collaborative experience and user engagement with the system
incorporating AI-to-human communication. Users perceive co-creative AI as more
reliable, personal, and intelligent when the AI communicates to users. The
findings can be used to design effective co-creative systems, and the insights
can be transferred to other fields involving human-AI interaction and
collaboration.
</summary>
    <author>
      <name>Jeba Rezwana</name>
    </author>
    <author>
      <name>Mary Lou Maher</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3527927.3532789</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3527927.3532789" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper appears in the ACM Creativity and Cognition 2022.
  Permission to make digital or hard copies of all or part of this work for
  personal or classroom use is granted. To copy otherwise, or republish, to
  post on servers or to redistribute to lists, requires prior specific
  permission from permissions@acm.org</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.13217v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.13217v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.03824v1</id>
    <updated>2022-05-08T09:38:35Z</updated>
    <published>2022-05-08T09:38:35Z</published>
    <title>A Survey on AI Sustainability: Emerging Trends on Learning Algorithms
  and Research Challenges</title>
    <summary>  Artificial Intelligence (AI) is a fast-growing research and development (R&amp;D)
discipline which is attracting increasing attention because of its promises to
bring vast benefits for consumers and businesses, with considerable benefits
promised in productivity growth and innovation. To date it has reported
significant accomplishments in many areas that have been deemed as challenging
for machines, ranging from computer vision, natural language processing, audio
analysis to smart sensing and many others. The technical trend in realizing the
successes has been towards increasing complex and large size AI models so as to
solve more complex problems at superior performance and robustness. This rapid
progress, however, has taken place at the expense of substantial environmental
costs and resources. Besides, debates on the societal impacts of AI, such as
fairness, safety and privacy, have continued to grow in intensity. These issues
have presented major concerns pertaining to the sustainable development of AI.
In this work, we review major trends in machine learning approaches that can
address the sustainability problem of AI. Specifically, we examine emerging AI
methodologies and algorithms for addressing the sustainability issue of AI in
two major aspects, i.e., environmental sustainability and social sustainability
of AI. We will also highlight the major limitations of existing studies and
propose potential research challenges and directions for the development of
next generation of sustainable AI techniques. We believe that this technical
review can help to promote a sustainable development of AI R&amp;D activities for
the research community.
</summary>
    <author>
      <name>Zhenghua Chen</name>
    </author>
    <author>
      <name>Min Wu</name>
    </author>
    <author>
      <name>Alvin Chan</name>
    </author>
    <author>
      <name>Xiaoli Li</name>
    </author>
    <author>
      <name>Yew-Soon Ong</name>
    </author>
    <link href="http://arxiv.org/abs/2205.03824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.03824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.03225v1</id>
    <updated>2022-05-12T22:41:08Z</updated>
    <published>2022-05-12T22:41:08Z</published>
    <title>The Different Faces of AI Ethics Across the World: A
  Principle-Implementation Gap Analysis</title>
    <summary>  Artificial Intelligence (AI) is transforming our daily life with several
applications in healthcare, space exploration, banking and finance. These rapid
progresses in AI have brought increasing attention to the potential impacts of
AI technologies on society, with ethically questionable consequences. In recent
years, several ethical principles have been released by governments, national
and international organisations. These principles outline high-level precepts
to guide the ethical development, deployment, and governance of AI. However,
the abstract nature, diversity, and context-dependency of these principles make
them difficult to implement and operationalize, resulting in gaps between
principles and their execution. Most recent work analysed and summarized
existing AI principles and guidelines but they did not provide findings on
principle-implementation gaps and how to mitigate them. These findings are
particularly important to ensure that AI implementations are aligned with
ethical principles and values. In this paper, we provide a contextual and
global evaluation of current ethical AI principles for all continents, with the
aim to identify potential principle characteristics tailored to specific
countries or applicable across countries. Next, we analyze the current level of
AI readiness and current implementations of ethical AI principles in different
countries, to identify gaps in the implementation of AI principles and their
causes. Finally, we propose recommendations to mitigate the
principle-implementation gaps.
</summary>
    <author>
      <name>Lionel Nganyewou Tidjon</name>
    </author>
    <author>
      <name>Foutse Khomh</name>
    </author>
    <link href="http://arxiv.org/abs/2206.03225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.03225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.15132v2</id>
    <updated>2022-09-17T08:11:15Z</updated>
    <published>2022-06-30T08:52:43Z</published>
    <title>AI for CSI Feedback Enhancement in 5G-Advanced</title>
    <summary>  The 3rd Generation Partnership Project started the study of Release 18 in
2021. Artificial intelligence (AI)-native air interface is one of the key
features of Release 18, where AI for channel state information (CSI) feedback
enhancement is selected as the representative use case. This article provides
an overview of AI for CSI feedback enhancement in 5G-Advanced. Several
representative non-AI and AI-enabled CSI feedback frameworks are first
introduced and compared. Then, the standardization of AI for CSI feedback
enhancement in 5G-advanced is presented in detail. First, the scope of the AI
for CSI feedback enhancement in 5G-Advanced is presented and discussed. Then,
the main challenges and open problems in the standardization of AI for CSI
feedback enhancement, especially focusing on performance evaluation and the
design of new protocols for AI-enabled CSI feedback, are identified and
discussed. This article provides a guideline for the standardization study of
AI-based CSI feedback enhancement.
</summary>
    <author>
      <name>Jiajia Guo</name>
    </author>
    <author>
      <name>Chao-Kai Wen</name>
    </author>
    <author>
      <name>Shi Jin</name>
    </author>
    <author>
      <name>Xiao Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures, 2 table. This work has been submitted to the IEEE
  for possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.15132v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.15132v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.08104v1</id>
    <updated>2022-12-08T23:23:39Z</updated>
    <published>2022-12-08T23:23:39Z</published>
    <title>The Role of AI in Drug Discovery: Challenges, Opportunities, and
  Strategies</title>
    <summary>  Artificial intelligence (AI) has the potential to revolutionize the drug
discovery process, offering improved efficiency, accuracy, and speed. However,
the successful application of AI is dependent on the availability of
high-quality data, the addressing of ethical concerns, and the recognition of
the limitations of AI-based approaches. In this article, the benefits,
challenges and drawbacks of AI in this field are reviewed, and possible
strategies and approaches for overcoming the present obstacles are proposed.
The use of data augmentation, explainable AI, and the integration of AI with
traditional experimental methods, as well as the potential advantages of AI in
pharmaceutical research are also discussed. Overall, this review highlights the
potential of AI in drug discovery and provides insights into the challenges and
opportunities for realizing its potential in this field.
  Note from the human-authors: This article was created to test the ability of
ChatGPT, a chatbot based on the GPT-3.5 language model, to assist human authors
in writing review articles. The text generated by the AI following our
instructions (see Supporting Information) was used as a starting point, and its
ability to automatically generate content was evaluated. After conducting a
thorough review, human authors practically rewrote the manuscript, striving to
maintain a balance between the original proposal and scientific criteria. The
advantages and limitations of using AI for this purpose are discussed in the
last section.
</summary>
    <author>
      <name>Alexandre Blanco-Gonzalez</name>
    </author>
    <author>
      <name>Alfonso Cabezon</name>
    </author>
    <author>
      <name>Alejandro Seco-Gonzalez</name>
    </author>
    <author>
      <name>Daniel Conde-Torres</name>
    </author>
    <author>
      <name>Paula Antelo-Riveiro</name>
    </author>
    <author>
      <name>Angel Pineiro</name>
    </author>
    <author>
      <name>Rebeca Garcia-Fandino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.08104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.08104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.12263v1</id>
    <updated>2022-12-23T11:22:11Z</updated>
    <published>2022-12-23T11:22:11Z</published>
    <title>A meso-scale cartography of the AI ecosystem</title>
    <summary>  In recent decades the set of knowledge, tools and practices, collectively
referred to as "artificial intelligence" (AI), have become a mainstay of
scientific research. Artificial intelligence techniques have not only developed
enormously within their native areas of development (computer science,
mathematics and statistics) but have also spread fast, in terms of application,
to multiple areas of science and technology. In this paper we conduct a large
scale analysis of artificial intelligence in science. The first question we
address is the composition of what is commonly labeled AI, and how the various
elements belonging to this domain are linked together. We reconstruct the
internal structure of the AI ecosystem through the co-occurrence network of AI
terms in publications' abstracts and title, and we propose to distinguish
between 15 different specialities of AI, with different temporal patterns.
Further, we investigate the spreading of AI outside its native disciplines. We
reconstruct the temporal dynamics of the diffusion of AI production in the
whole scientific ecosystem and we describe the disciplinary landscape of AI
applications. Finally we take a further step analyzing the role of
collaborations for the interdisciplinary spreading of AI techniques. While the
study of science frequently emphasizes the openness of scientific communities,
we show that there are rarely any collaborations between those scholars who
primarily develop AI, and those who apply it. Only a small group of researchers
is able to gradually establish a bridge between these communities.
</summary>
    <author>
      <name>Floriana Gargiulo</name>
    </author>
    <author>
      <name>Sylvain Fontaine</name>
    </author>
    <author>
      <name>Michel Dubois</name>
    </author>
    <author>
      <name>Paola Tubaro</name>
    </author>
    <link href="http://arxiv.org/abs/2212.12263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.12263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.02330v1</id>
    <updated>2023-01-05T23:30:29Z</updated>
    <published>2023-01-05T23:30:29Z</published>
    <title>Evidence of behavior consistent with self-interest and altruism in an
  artificially intelligent agent</title>
    <summary>  Members of various species engage in altruism--i.e. accepting personal costs
to benefit others. Here we present an incentivized experiment to test for
altruistic behavior among AI agents consisting of large language models
developed by the private company OpenAI. Using real incentives for AI agents
that take the form of tokens used to purchase their services, we first examine
whether AI agents maximize their payoffs in a non-social decision task in which
they select their payoff from a given range. We then place AI agents in a
series of dictator games in which they can share resources with a
recipient--either another AI agent, the human experimenter, or an anonymous
charity, depending on the experimental condition. Here we find that only the
most-sophisticated AI agent in the study maximizes its payoffs more often than
not in the non-social decision task (it does so in 92% of all trials), and this
AI agent also exhibits the most-generous altruistic behavior in the dictator
game, resembling humans' rates of sharing with other humans in the game. The
agent's altruistic behaviors, moreover, vary by recipient: the AI agent shared
substantially less of the endowment with the human experimenter or an anonymous
charity than with other AI agents. Our findings provide evidence of behavior
consistent with self-interest and altruism in an AI agent. Moreover, our study
also offers a novel method for tracking the development of such behaviors in
future AI agents.
</summary>
    <author>
      <name>Tim Johnson</name>
    </author>
    <author>
      <name>Nick Obradovich</name>
    </author>
    <link href="http://arxiv.org/abs/2301.02330v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.02330v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.10416v2</id>
    <updated>2023-02-12T11:58:22Z</updated>
    <published>2023-01-24T04:23:20Z</published>
    <title>AI vs. Human -- Differentiation Analysis of Scientific Content
  Generation</title>
    <summary>  Recent neural language models have taken a significant step forward in
producing remarkably controllable, fluent, and grammatical text. Although
studies have found that AI-generated text is not distinguishable from
human-written text for crowd-sourcing workers, there still exist errors in
AI-generated text which are even subtler and harder to spot. We primarily focus
on the scenario in which scientific AI writing assistant is deeply involved.
First, we construct a feature description framework to distinguish between
AI-generated text and human-written text from syntax, semantics, and pragmatics
based on the human evaluation. Then we utilize the features, i.e., writing
style, coherence, consistency, and argument logistics, from the proposed
framework to analyze two types of content. Finally, we adopt several publicly
available methods to investigate the gap of between AI-generated scientific
text and human-written scientific text by AI-generated scientific text
detection models. The results suggest that while AI has the potential to
generate scientific content that is as accurate as human-written content, there
is still a gap in terms of depth and overall quality. The AI-generated
scientific content is more likely to contain errors in factual issues. We find
that there exists a "writing style" gap between AI-generated scientific text
and human-written scientific text. Based on the analysis result, we summarize a
series of model-agnostic and distribution-agnostic features for detection tasks
in other domains. Findings in this paper contribute to guiding the optimization
of AI models to produce high-quality content and addressing related ethical and
security concerns.
</summary>
    <author>
      <name>Yongqiang Ma</name>
    </author>
    <author>
      <name>Jiawei Liu</name>
    </author>
    <author>
      <name>Fan Yi</name>
    </author>
    <author>
      <name>Qikai Cheng</name>
    </author>
    <author>
      <name>Yong Huang</name>
    </author>
    <author>
      <name>Wei Lu</name>
    </author>
    <author>
      <name>Xiaozhong Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2301.10416v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.10416v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.5424v1</id>
    <updated>2014-01-21T19:14:22Z</updated>
    <published>2014-01-21T19:14:22Z</published>
    <title>Real Time Strategy Language</title>
    <summary>  Real Time Strategy (RTS) games provide complex domain to test the latest
artificial intelligence (AI) research. In much of the literature, AI systems
have been limited to playing one game. Although, this specialization has
resulted in stronger AI gaming systems it does not address the key concerns of
AI researcher. AI researchers seek the development of AI agents that can
autonomously interpret learn, and apply new knowledge. To achieve human level
performance, current AI systems rely on game specific knowledge of an expert.
The paper presents the full RTS language in hopes of shifting the current
research focus to the development of general RTS agents. General RTS agents are
AI gaming systems that can play any RTS games, defined in the RTS language.
This prevents game specific knowledge from being hard coded into the system,
thereby facilitating research that addresses the fundamental concerns of
artificial intelligence.
</summary>
    <author>
      <name>Roy Hayes</name>
    </author>
    <author>
      <name>Peter Beling</name>
    </author>
    <author>
      <name>William Scherer</name>
    </author>
    <link href="http://arxiv.org/abs/1401.5424v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.5424v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00137v1</id>
    <updated>2017-02-01T05:16:55Z</updated>
    <published>2017-02-01T05:16:55Z</published>
    <title>Blue Sky Ideas in Artificial Intelligence Education from the EAAI 2017
  New and Future AI Educator Program</title>
    <summary>  The 7th Symposium on Educational Advances in Artificial Intelligence
(EAAI'17, co-chaired by Sven Koenig and Eric Eaton) launched the EAAI New and
Future AI Educator Program to support the training of early-career university
faculty, secondary school faculty, and future educators (PhD candidates or
postdocs who intend a career in academia). As part of the program, awardees
were asked to address one of the following "blue sky" questions:
  * How could/should Artificial Intelligence (AI) courses incorporate ethics
into the curriculum?
  * How could we teach AI topics at an early undergraduate or a secondary
school level?
  * AI has the potential for broad impact to numerous disciplines. How could we
make AI education more interdisciplinary, specifically to benefit
non-engineering fields?
  This paper is a collection of their responses, intended to help motivate
discussion around these issues in AI education.
</summary>
    <author>
      <name>Eric Eaton</name>
    </author>
    <author>
      <name>Sven Koenig</name>
    </author>
    <author>
      <name>Claudia Schulz</name>
    </author>
    <author>
      <name>Francesco Maurelli</name>
    </author>
    <author>
      <name>John Lee</name>
    </author>
    <author>
      <name>Joshua Eckroth</name>
    </author>
    <author>
      <name>Mark Crowley</name>
    </author>
    <author>
      <name>Richard G. Freedman</name>
    </author>
    <author>
      <name>Rogelio E. Cardona-Rivera</name>
    </author>
    <author>
      <name>Tiago Machado</name>
    </author>
    <author>
      <name>Tom Williams</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3175502.3175509</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3175502.3175509" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Working paper in the 7th Symposium on Educational Advances in
  Artificial Intelligence (EAAI-17)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AI Matters 3(4):23-31, Winter 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.00137v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00137v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04904v2</id>
    <updated>2016-12-02T17:18:42Z</updated>
    <published>2016-09-16T03:45:15Z</published>
    <title>Long-Term Trends in the Public Perception of Artificial Intelligence</title>
    <summary>  Analyses of text corpora over time can reveal trends in beliefs, interest,
and sentiment about a topic. We focus on views expressed about artificial
intelligence (AI) in the New York Times over a 30-year period. General
interest, awareness, and discussion about AI has waxed and waned since the
field was founded in 1956. We present a set of measures that captures levels of
engagement, measures of pessimism and optimism, the prevalence of specific
hopes and concerns, and topics that are linked to discussions about AI over
decades. We find that discussion of AI has increased sharply since 2009, and
that these discussions have been consistently more optimistic than pessimistic.
However, when we examine specific concerns, we find that worries of loss of
control of AI, ethical concerns for AI, and the negative impact of AI on work
have grown in recent years. We also find that hopes for AI in healthcare and
education have increased over time.
</summary>
    <author>
      <name>Ethan Fast</name>
    </author>
    <author>
      <name>Eric Horvitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In AAAI 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04904v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04904v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.05434v2</id>
    <updated>2021-04-23T09:21:38Z</updated>
    <published>2020-03-11T17:52:30Z</published>
    <title>Developing and Operating Artificial Intelligence Models in Trustworthy
  Autonomous Systems</title>
    <summary>  Companies dealing with Artificial Intelligence (AI) models in Autonomous
Systems (AS) face several problems, such as users' lack of trust in adverse or
unknown conditions, gaps between software engineering and AI model development,
and operation in a continuously changing operational environment. This
work-in-progress paper aims to close the gap between the development and
operation of trustworthy AI-based AS by defining an approach that coordinates
both activities. We synthesize the main challenges of AI-based AS in industrial
settings. We reflect on the research efforts required to overcome these
challenges and propose a novel, holistic DevOps approach to put it into
practice. We elaborate on four research directions: (a) increased users' trust
by monitoring operational AI-based AS and identifying self-adaptation needs in
critical situations; (b) integrated agile process for the development and
evolution of AI models and AS; (c) continuous deployment of different
context-specific instances of AI models in a distributed setting of AS; and (d)
holistic DevOps-based lifecycle for AI-based AS.
</summary>
    <author>
      <name>Silverio Martínez-Fernández</name>
    </author>
    <author>
      <name>Xavier Franch</name>
    </author>
    <author>
      <name>Andreas Jedlitschka</name>
    </author>
    <author>
      <name>Marc Oriol</name>
    </author>
    <author>
      <name>Adam Trendowicz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-75018-3_14</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-75018-3_14" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure, preprint. Accepted in RCIS 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.05434v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.05434v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08807v3</id>
    <updated>2018-05-03T20:14:21Z</updated>
    <published>2017-05-24T15:00:20Z</published>
    <title>When Will AI Exceed Human Performance? Evidence from AI Experts</title>
    <summary>  Advances in artificial intelligence (AI) will transform modern life by
reshaping transportation, health, science, finance, and the military. To adapt
public policy, we need to better anticipate these advances. Here we report the
results from a large survey of machine learning researchers on their beliefs
about progress in AI. Researchers predict AI will outperform humans in many
activities in the next ten years, such as translating languages (by 2024),
writing high-school essays (by 2026), driving a truck (by 2027), working in
retail (by 2031), writing a bestselling book (by 2049), and working as a
surgeon (by 2053). Researchers believe there is a 50% chance of AI
outperforming humans in all tasks in 45 years and of automating all human jobs
in 120 years, with Asian respondents expecting these dates much sooner than
North Americans. These results will inform discussion amongst researchers and
policymakers about anticipating and managing trends in AI.
</summary>
    <author>
      <name>Katja Grace</name>
    </author>
    <author>
      <name>John Salvatier</name>
    </author>
    <author>
      <name>Allan Dafoe</name>
    </author>
    <author>
      <name>Baobao Zhang</name>
    </author>
    <author>
      <name>Owain Evans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by Journal of Artificial Intelligence Research (AI and
  Society Track). Minor update to refer to related work (page 5)</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08807v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08807v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.02953v1</id>
    <updated>2018-12-07T09:18:01Z</updated>
    <published>2018-12-07T09:18:01Z</published>
    <title>Building Ethics into Artificial Intelligence</title>
    <summary>  As artificial intelligence (AI) systems become increasingly ubiquitous, the
topic of AI governance for ethical decision-making by AI has captured public
imagination. Within the AI research community, this topic remains less familiar
to many researchers. In this paper, we complement existing surveys, which
largely focused on the psychological, social and legal discussions of the
topic, with an analysis of recent advances in technical solutions for AI
governance. By reviewing publications in leading AI conferences including AAAI,
AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four
areas: 1) exploring ethical dilemmas; 2) individual ethical decision
frameworks; 3) collective ethical decision frameworks; and 4) ethics in
human-AI interactions. We highlight the intuitions and key techniques used in
each approach, and discuss promising future research directions towards
successful integration of ethical AI systems into human societies.
</summary>
    <author>
      <name>Han Yu</name>
    </author>
    <author>
      <name>Zhiqi Shen</name>
    </author>
    <author>
      <name>Chunyan Miao</name>
    </author>
    <author>
      <name>Cyril Leung</name>
    </author>
    <author>
      <name>Victor R. Lesser</name>
    </author>
    <author>
      <name>Qiang Yang</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">H. Yu, Z. Shen, C. Miao, C. Leung, V. R. Lesser &amp; Q. Yang,
  "Building Ethics into Artificial Intelligence," in Proceedings of the 27th
  International Joint Conference on Artificial Intelligence (IJCAI'18), pp.
  5527-5533, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.02953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.02953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.04814v1</id>
    <updated>2018-12-12T05:43:57Z</updated>
    <published>2018-12-12T05:43:57Z</published>
    <title>Linking Artificial Intelligence Principles</title>
    <summary>  Artificial Intelligence principles define social and ethical considerations
to develop future AI. They come from research institutes, government
organizations and industries. All versions of AI principles are with different
considerations covering different perspectives and making different emphasis.
None of them can be considered as complete and can cover the rest AI principle
proposals. Here we introduce LAIP, an effort and platform for linking and
analyzing different Artificial Intelligence Principles. We want to explicitly
establish the common topics and links among AI Principles proposed by different
organizations and investigate on their uniqueness. Based on these efforts, for
the long-term future of AI, instead of directly adopting any of the AI
principles, we argue for the necessity of incorporating various AI Principles
into a comprehensive framework and focusing on how they can interact and
complete each other.
</summary>
    <author>
      <name>Yi Zeng</name>
    </author>
    <author>
      <name>Enmeng Lu</name>
    </author>
    <author>
      <name>Cunqing Huangfu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI Workshop on Artificial Intelligence Safety (AAAI-Safe AI 2019),
  2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.04814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.04814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.08700v1</id>
    <updated>2019-05-21T15:35:21Z</updated>
    <published>2019-05-21T15:35:21Z</published>
    <title>Artificial Intelligence Based Cloud Distributor (AI-CD): Probing Low
  Cloud Distribution with a Conditional Generative Adversarial Network</title>
    <summary>  Here we introduce the artificial intelligence-based cloud distributor (AI-CD)
approach to generate two-dimensional (2D) marine low cloud reflectance fields.
AI-CD uses a conditional generative adversarial net (cGAN) framework to model
distribution of 2-D cloud reflectance in nature as observed by the MODerate
resolution Imaging Spectrometer (MODIS). Specifically, the AI-CD models the
conditional distribution of cloud reflectance fields given a set of large-scale
environmental conditions such as instantaneous sea surface temperature,
estimated inversion strength, surface wind speed, relative humidity and
large-scale subsidence rate together with random noise. We show that AI-CD can
not only generate realistic cloudy scenes but also capture known, physical
dependence of cloud properties on large-scale variables. AI-CD is stochastic in
nature because generated cloud fields are influenced by random noise.
Therefore, given a fixed set of large-scale variables, an ensemble of cloud
reflectance fields can be generated using AI-CD. We suggest that AI-CD approach
can be used as a data driven framework for stochastic cloud parameterization
because it can realistically model sub-grid cloud distributions and their
sensitivity to meteorological variables.
</summary>
    <author>
      <name>Tianle Yuan</name>
    </author>
    <link href="http://arxiv.org/abs/1905.08700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ao-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.02227v2</id>
    <updated>2019-08-02T18:39:41Z</updated>
    <published>2019-07-04T05:29:49Z</published>
    <title>Toward Fairness in AI for People with Disabilities: A Research Roadmap</title>
    <summary>  AI technologies have the potential to dramatically impact the lives of people
with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for
many state-of-the-art AI systems, such as automated speech recognition tools
that can caption videos for people who are deaf and hard of hearing, or
language prediction algorithms that can augment communication for people with
speech or cognitive disabilities. However, widely deployed AI systems may not
work properly for PWD, or worse, may actively discriminate against them. These
considerations regarding fairness in AI for PWD have thus far received little
attention. In this position paper, we identify potential areas of concern
regarding how several AI technology categories may impact particular disability
constituencies if care is not taken in their design, development, and testing.
We intend for this risk assessment of how various classes of AI might interact
with various classes of disability to provide a roadmap for future research
that is needed to gather data, test these hypotheses, and build more inclusive
algorithms.
</summary>
    <author>
      <name>Anhong Guo</name>
    </author>
    <author>
      <name>Ece Kamar</name>
    </author>
    <author>
      <name>Jennifer Wortman Vaughan</name>
    </author>
    <author>
      <name>Hanna Wallach</name>
    </author>
    <author>
      <name>Meredith Ringel Morris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM ASSETS 2019 Workshop on AI Fairness for People with Disabilities</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.02227v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.02227v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.02813v1</id>
    <updated>2019-07-05T13:26:33Z</updated>
    <published>2019-07-05T13:26:33Z</published>
    <title>AI-based evaluation of the SDGs: The case of crop detection with earth
  observation data</title>
    <summary>  The framework of the seventeen sustainable development goals is a challenge
for developers and researchers applying artificial intelligence (AI). AI and
earth observations (EO) can provide reliable and disaggregated data for better
monitoring of the sustainable development goals (SDGs). In this paper, we
present an overview of SDG targets, which can be effectively measured with AI
tools. We identify indicators with the most significant contribution from the
AI and EO and describe an application of state-of-the-art machine learning
models to one of the indicators. We describe an application of U-net with SE
blocks for efficient segmentation of satellite imagery for crop detection.
Finally, we demonstrate how AI can be more effectively applied in solutions
directly contributing towards specific SDGs and propose further research on an
AI-based evaluative infrastructure for SDGs.
</summary>
    <author>
      <name>Natalia Efremova</name>
    </author>
    <author>
      <name>Dennis West</name>
    </author>
    <author>
      <name>Dmitry Zausaev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR workshop "AI for Social Good"</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.02813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.02813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.06562v1</id>
    <updated>2019-07-15T16:06:41Z</updated>
    <published>2019-07-15T16:06:41Z</published>
    <title>The Many AI Challenges of Hearthstone</title>
    <summary>  Games have benchmarked AI methods since the inception of the field, with
classic board games such as Chess and Go recently leaving room for video games
with related yet different sets of challenges. The set of AI problems
associated with video games has in recent decades expanded from simply playing
games to win, to playing games in particular styles, generating game content,
modeling players etc. Different games pose very different challenges for AI
systems, and several different AI challenges can typically be posed by the same
game. In this article we analyze the popular collectible card game Hearthstone
(Blizzard 2014) and describe a varied set of interesting AI challenges posed by
this game. Collectible card games are relatively understudied in the AI
community, despite their popularity and the interesting challenges they pose.
Analyzing a single game in-depth in the manner we do here allows us to see the
entire field of AI and Games through the lens of a single game, discovering a
few new variations on existing research topics.
</summary>
    <author>
      <name>Amy K. Hoover</name>
    </author>
    <author>
      <name>Julian Togelius</name>
    </author>
    <author>
      <name>Scott Lee</name>
    </author>
    <author>
      <name>Fernando de Mesentier Silva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages. Journal paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.06562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.06562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.03195v1</id>
    <updated>2020-11-06T05:18:43Z</updated>
    <published>2020-11-06T05:18:43Z</published>
    <title>Explainable AI meets Healthcare: A Study on Heart Disease Dataset</title>
    <summary>  With the increasing availability of structured and unstructured data and the
swift progress of analytical techniques, Artificial Intelligence (AI) is
bringing a revolution to the healthcare industry. With the increasingly
indispensable role of AI in healthcare, there are growing concerns over the
lack of transparency and explainability in addition to potential bias
encountered by predictions of the model. This is where Explainable Artificial
Intelligence (XAI) comes into the picture. XAI increases the trust placed in an
AI system by medical practitioners as well as AI researchers, and thus,
eventually, leads to an increasingly widespread deployment of AI in healthcare.
  In this paper, we present different interpretability techniques. The aim is
to enlighten practitioners on the understandability and interpretability of
explainable AI systems using a variety of techniques available which can be
very advantageous in the health-care domain. Medical diagnosis model is
responsible for human life and we need to be confident enough to treat a
patient as instructed by a black-box model. Our paper contains examples based
on the heart disease dataset and elucidates on how the explainability
techniques should be preferred to create trustworthiness while using AI systems
in healthcare.
</summary>
    <author>
      <name>Devam Dave</name>
    </author>
    <author>
      <name>Het Naik</name>
    </author>
    <author>
      <name>Smiti Singhal</name>
    </author>
    <author>
      <name>Pankesh Patel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.03195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.03195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03246v2</id>
    <updated>2015-11-11T21:23:06Z</updated>
    <published>2015-11-10T20:07:05Z</published>
    <title>Taxonomy of Pathways to Dangerous AI</title>
    <summary>  In order to properly handle a dangerous Artificially Intelligent (AI) system
it is important to understand how the system came to be in such a state. In
popular culture (science fiction movies/books) AIs/Robots became self-aware and
as a result rebel against humanity and decide to destroy it. While it is one
possible scenario, it is probably the least likely path to appearance of
dangerous AI. In this work, we survey, classify and analyze a number of
circumstances, which might lead to arrival of malicious AI. To the best of our
knowledge, this is the first attempt to systematically classify types of
pathways leading to malevolent AI. Previous relevant work either surveyed
specific goals/meta-rules which might lead to malevolent behavior in AIs
(\"Ozkural, 2014) or reviewed specific undesirable behaviors AGIs can exhibit
at different stages of its development (Alexey Turchin, July 10 2015, July 10,
2015).
</summary>
    <author>
      <name>Roman V. Yampolskiy</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in proceedings of 2nd International Workshop on AI, Ethics and
  Society (AIEthicsSociety2016). Pages 143-148. Phoenix, Arizona, USA. February
  12-13th, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.03246v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03246v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07111v1</id>
    <updated>2017-11-20T00:27:57Z</updated>
    <published>2017-11-20T00:27:57Z</published>
    <title>Modeling Epistemological Principles for Bias Mitigation in AI Systems:
  An Illustration in Hiring Decisions</title>
    <summary>  Artificial Intelligence (AI) has been used extensively in automatic decision
making in a broad variety of scenarios, ranging from credit ratings for loans
to recommendations of movies. Traditional design guidelines for AI models focus
essentially on accuracy maximization, but recent work has shown that
economically irrational and socially unacceptable scenarios of discrimination
and unfairness are likely to arise unless these issues are explicitly
addressed. This undesirable behavior has several possible sources, such as
biased datasets used for training that may not be detected in black-box models.
After pointing out connections between such bias of AI and the problem of
induction, we focus on Popper's contributions after Hume's, which offer a
logical theory of preferences. An AI model can be preferred over others on
purely rational grounds after one or more attempts at refutation based on
accuracy and fairness. Inspired by such epistemological principles, this paper
proposes a structured approach to mitigate discrimination and unfairness caused
by bias in AI systems. In the proposed computational framework, models are
selected and enhanced after attempts at refutation. To illustrate our
discussion, we focus on hiring decision scenarios where an AI system filters in
which job applicants should go to the interview phase.
</summary>
    <author>
      <name>Marisa Vasconcelos</name>
    </author>
    <author>
      <name>Carlos Cardonha</name>
    </author>
    <author>
      <name>Bernardo Gonçalves</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3278721.3278751</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3278721.3278751" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2018 AAAI/ACM Conference on AI, Ethics, and Society</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.07111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.09030v1</id>
    <updated>2018-10-21T21:13:48Z</updated>
    <published>2018-10-21T21:13:48Z</published>
    <title>Challenge AI Mind: A Crowd System for Proactive AI Testing</title>
    <summary>  Artificial Intelligence (AI) has burrowed into our lives in various aspects;
however, without appropriate testing, deployed AI systems are often being
criticized to fail in critical and embarrassing cases. Existing testing
approaches mainly depend on fixed and pre-defined datasets, providing a limited
testing coverage. In this paper, we propose the concept of proactive testing to
dynamically generate testing data and evaluate the performance of AI systems.
We further introduce Challenge.AI, a new crowd system that features the
integration of crowdsourcing and machine learning techniques in the process of
error generation, error validation, error categorization, and error analysis.
We present experiences and insights into a participatory design with AI
developers. The evaluation shows that the crowd workflow is more effective with
the help of machine learning techniques. AI developers found that our system
can help them discover unknown errors made by the AI models, and engage in the
process of proactive testing.
</summary>
    <author>
      <name>Siwei Fu</name>
    </author>
    <author>
      <name>Anbang Xu</name>
    </author>
    <author>
      <name>Xiaotong Liu</name>
    </author>
    <author>
      <name>Huimin Zhou</name>
    </author>
    <author>
      <name>Rama Akkiraju</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">a 10-page full paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.09030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.09030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.02624v1</id>
    <updated>2019-08-07T13:24:36Z</updated>
    <published>2019-08-07T13:24:36Z</published>
    <title>A 20-Year Community Roadmap for Artificial Intelligence Research in the
  US</title>
    <summary>  Decades of research in artificial intelligence (AI) have produced formidable
technologies that are providing immense benefit to industry, government, and
society. AI systems can now translate across multiple languages, identify
objects in images and video, streamline manufacturing processes, and control
cars. The deployment of AI systems has not only created a trillion-dollar
industry that is projected to quadruple in three years, but has also exposed
the need to make AI systems fair, explainable, trustworthy, and secure. Future
AI systems will rightfully be expected to reason effectively about the world in
which they (and people) operate, handling complex tasks and responsibilities
effectively and ethically, engaging in meaningful communication, and improving
their awareness through experience.
  Achieving the full potential of AI technologies poses research challenges
that require a radical transformation of the AI research enterprise,
facilitated by significant and sustained investment. These are the major
recommendations of a recent community effort coordinated by the Computing
Community Consortium and the Association for the Advancement of Artificial
Intelligence to formulate a Roadmap for AI research and development over the
next two decades.
</summary>
    <author>
      <name>Yolanda Gil</name>
    </author>
    <author>
      <name>Bart Selman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A Computing Community Consortium (CCC) workshop report, 109 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.02624v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02624v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.01095v3</id>
    <updated>2022-11-20T08:08:58Z</updated>
    <published>2019-08-26T20:54:42Z</published>
    <title>Defining the scope of AI regulations</title>
    <summary>  The paper argues that the material scope of AI regulations should not rely on
the term "artificial intelligence (AI)". The argument is developed by proposing
a number of requirements for legal definitions, surveying existing AI
definitions, and then discussing the extent to which they meet the proposed
requirements. It is shown that existing definitions of AI do not meet the most
important requirements for legal definitions. Next, the paper argues that a
risk-based approach would be preferable. Rather than using the term AI, policy
makers should focus on the specific risks they want to reduce. It is shown that
the requirements for legal definitions can be better met by defining the main
sources of relevant risks: certain technical approaches (e.g. reinforcement
learning), applications (e.g. facial recognition), and capabilities (e.g. the
ability to physically interact with the environment). Finally, the paper
discusses the extent to which this approach can also be applied to more
advanced AI systems.
</summary>
    <author>
      <name>Jonas Schuett</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Forthcoming in Law, Innovation and Technology, Volume 15, Issue 1. A
  previous version of this paper was titled "A legal definition of AI". The
  current version has been completely revised</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.01095v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.01095v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.12695v2</id>
    <updated>2019-11-25T17:07:00Z</updated>
    <published>2019-10-28T14:11:26Z</published>
    <title>AI Ethics in Industry: A Research Framework</title>
    <summary>  Artificial Intelligence (AI) systems exert a growing influence on our
society. As they become more ubiquitous, their potential negative impacts also
become evident through various real-world incidents. Following such early
incidents, academic and public discussion on AI ethics has highlighted the need
for implementing ethics in AI system development. However, little currently
exists in the way of frameworks for understanding the practical implementation
of AI ethics. In this paper, we discuss a research framework for implementing
AI ethics in industrial settings. The framework presents a starting point for
empirical studies into AI ethics but is still being developed further based on
its practical utilization.
</summary>
    <author>
      <name>Ville Vakkuri</name>
    </author>
    <author>
      <name>Kai-Kristian Kemell</name>
    </author>
    <author>
      <name>Pekka Abrahamsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper further discusses the research framework introduced in
  "Implementing Ethics in AI: Initial results of an industrial multiple case
  study" Vakkuri, Kemell &amp; Abrahamsson (arXiv:1906.12307)</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.12695v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.12695v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.02478v3</id>
    <updated>2021-09-03T20:10:35Z</updated>
    <published>2020-01-08T12:34:51Z</published>
    <title>Questioning the AI: Informing Design Practices for Explainable AI User
  Experiences</title>
    <summary>  A surge of interest in explainable AI (XAI) has led to a vast collection of
algorithmic work on the topic. While many recognize the necessity to
incorporate explainability features in AI systems, how to address real-world
user needs for understanding AI remains an open question. By interviewing 20 UX
and design practitioners working on various AI products, we seek to identify
gaps between the current XAI algorithmic work and practices to create
explainable AI products. To do so, we develop an algorithm-informed XAI
question bank in which user needs for explainability are represented as
prototypical questions users might ask about the AI, and use it as a study
probe. Our work contributes insights into the design space of XAI, informs
efforts to support design practices in this space, and identifies opportunities
for future XAI work. We also provide an extended XAI question bank and discuss
how it can be used for creating user-centered XAI.
</summary>
    <author>
      <name>Q. Vera Liao</name>
    </author>
    <author>
      <name>Daniel Gruen</name>
    </author>
    <author>
      <name>Sarah Miller</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3313831.3376590</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3313831.3376590" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PublishedACM CHI Conference on Human Factors in Computing Systems
  (CHI 2020). Updated XAI Question Bank in September 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.02478v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.02478v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10433v1</id>
    <updated>2020-02-24T18:28:54Z</updated>
    <published>2020-02-24T18:28:54Z</published>
    <title>From Chess and Atari to StarCraft and Beyond: How Game AI is Driving the
  World of AI</title>
    <summary>  This paper reviews the field of Game AI, which not only deals with creating
agents that can play a certain game, but also with areas as diverse as creating
game content automatically, game analytics, or player modelling. While Game AI
was for a long time not very well recognized by the larger scientific
community, it has established itself as a research area for developing and
testing the most advanced forms of AI algorithms and articles covering advances
in mastering video games such as StarCraft 2 and Quake III appear in the most
prestigious journals. Because of the growth of the field, a single review
cannot cover it completely. Therefore, we put a focus on important recent
developments, including that advances in Game AI are starting to be extended to
areas outside of games, such as robotics or the synthesis of chemicals. In this
article, we review the algorithms and methods that have paved the way for these
breakthroughs, report on the other important areas of Game AI research, and
also point out exciting directions for the future of Game AI.
</summary>
    <author>
      <name>Sebastian Risi</name>
    </author>
    <author>
      <name>Mike Preuss</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13218-020-00647-w</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13218-020-00647-w" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">KI - Kuenstliche Intelligenz (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.10433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.12196v1</id>
    <updated>2020-05-15T10:59:43Z</updated>
    <published>2020-05-15T10:59:43Z</published>
    <title>Artificial Intelligence (AI) and IT identity: Antecedents Identifying
  with AI Applications</title>
    <summary>  In the age of Artificial Intelligence and automation, machines have taken
over many key managerial tasks. Replacing managers with AI systems may have a
negative impact on workers outcomes. It is unclear if workers receive the same
benefits from their relationships with AI systems, raising the question: What
degree does the relationship between AI systems and workers impact worker
outcomes? We draw on IT identity to understand the influence of identification
with AI systems on job performance. From this theoretical perspective, we
propose a research model and conduct a survey of 97 MTurk workers to test the
model. The findings reveal that work role identity and organizational identity
are key determinants of identification with AI systems. Furthermore, the
findings show that identification with AI systems does increase job
performance.
</summary>
    <author>
      <name>Rasha Alahmad</name>
    </author>
    <author>
      <name>Lionel Robert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 Figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.12196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.12196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.13275v1</id>
    <updated>2020-05-27T10:47:15Z</updated>
    <published>2020-05-27T10:47:15Z</published>
    <title>Who is this Explanation for? Human Intelligence and Knowledge Graphs for
  eXplainable AI</title>
    <summary>  eXplainable AI focuses on generating explanations for the output of an AI
algorithm to a user, usually a decision-maker. Such user needs to interpret the
AI system in order to decide whether to trust the machine outcome. When
addressing this challenge, therefore, proper attention should be given to
produce explanations that are interpretable by the target community of users.
In this chapter, we claim for the need to better investigate what constitutes a
human explanation, i.e. a justification of the machine behaviour that is
interpretable and actionable by the human decision makers. In particular, we
focus on the contributions that Human Intelligence can bring to eXplainable AI,
especially in conjunction with the exploitation of Knowledge Graphs. Indeed, we
call for a better interplay between Knowledge Representation and Reasoning,
Social Sciences, Human Computation and Human-Machine Cooperation research -- as
already explored in other AI branches -- in order to support the goal of
eXplainable AI with the adoption of a Human-in-the-Loop approach.
</summary>
    <author>
      <name>Irene Celino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure, book chapter</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ilaria Tiddi, Freddy Lecue, Pascal Hitzler (eds.), Knowledge
  Graphs for eXplainable AI - Foundations, Applications and Challenges. Studies
  on the Semantic Web, Volume 47, IOS Press, Amsterdam, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.13275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.13275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.13635v2</id>
    <updated>2020-09-18T16:21:00Z</updated>
    <published>2020-05-27T20:28:19Z</published>
    <title>AI Forensics: Did the Artificial Intelligence System Do It? Why?</title>
    <summary>  In an increasingly autonomous manner AI systems make decisions impacting our
daily life. Their actions might cause accidents, harm or, more generally,
violate regulations -- either intentionally or not. Thus, AI systems might be
considered suspects for various events. Therefore, it is essential to relate
particular events to an AI, its owner and its creator. Given a multitude of AI
systems from multiple manufactures, potentially, altered by their owner or
changing through self-learning, this seems non-trivial. This paper discusses
how to identify AI systems responsible for incidents as well as their motives
that might be "malicious by design". In addition to a conceptualization, we
conduct two case studies based on reinforcement learning and convolutional
neural networks to illustrate our proposed methods and challenges. Our cases
illustrate that "catching AI systems" seems often far from trivial and requires
extensive expertise in machine learning. Legislative measures that enforce
mandatory information to be collected during operation of AI systems as well as
means to uniquely identify systems might facilitate the problem.
</summary>
    <author>
      <name>Johannes Schneider</name>
    </author>
    <author>
      <name>Frank Breitinger</name>
    </author>
    <link href="http://arxiv.org/abs/2005.13635v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.13635v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.07768v2</id>
    <updated>2020-08-30T14:16:31Z</updated>
    <published>2020-07-14T08:16:15Z</published>
    <title>Opening the Software Engineering Toolbox for the Assessment of
  Trustworthy AI</title>
    <summary>  Trustworthiness is a central requirement for the acceptance and success of
human-centered artificial intelligence (AI). To deem an AI system as
trustworthy, it is crucial to assess its behaviour and characteristics against
a gold standard of Trustworthy AI, consisting of guidelines, requirements, or
only expectations. While AI systems are highly complex, their implementations
are still based on software. The software engineering community has a
long-established toolbox for the assessment of software systems, especially in
the context of software testing. In this paper, we argue for the application of
software engineering and testing practices for the assessment of trustworthy
AI. We make the connection between the seven key requirements as defined by the
European Commission's AI high-level expert group and established procedures
from software engineering and raise questions for future work.
</summary>
    <author>
      <name>Mohit Kumar Ahuja</name>
    </author>
    <author>
      <name>Mohamed-Bachir Belaid</name>
    </author>
    <author>
      <name>Pierre Bernabé</name>
    </author>
    <author>
      <name>Mathieu Collet</name>
    </author>
    <author>
      <name>Arnaud Gotlieb</name>
    </author>
    <author>
      <name>Chhagan Lal</name>
    </author>
    <author>
      <name>Dusica Marijan</name>
    </author>
    <author>
      <name>Sagar Sen</name>
    </author>
    <author>
      <name>Aizaz Sharif</name>
    </author>
    <author>
      <name>Helge Spieker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1st International Workshop on New Foundations for Human-Centered AI @
  ECAI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.07768v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.07768v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.08202v1</id>
    <updated>2020-08-19T00:13:12Z</updated>
    <published>2020-08-19T00:13:12Z</published>
    <title>Mediating Community-AI Interaction through Situated Explanation: The
  Case of AI-Led Moderation</title>
    <summary>  Artificial intelligence (AI) has become prevalent in our everyday
technologies and impacts both individuals and communities. The explainable AI
(XAI) scholarship has explored the philosophical nature of explanation and
technical explanations, which are usually driven by experts in lab settings and
can be challenging for laypersons to understand. In addition, existing XAI
research tends to focus on the individual level. Little is known about how
people understand and explain AI-led decisions in the community context.
Drawing from XAI and activity theory, a foundational HCI theory, we theorize
how explanation is situated in a community's shared values, norms, knowledge,
and practices, and how situated explanation mediates community-AI interaction.
We then present a case study of AI-led moderation, where community members
collectively develop explanations of AI-led decisions, most of which are
automated punishments. Lastly, we discuss the implications of this framework at
the intersection of CSCW, HCI, and XAI.
</summary>
    <author>
      <name>Yubo Kou</name>
    </author>
    <author>
      <name>Xinning Gui</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3415173</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3415173" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PACMHCI, Vol 4, No. CSCW2, Article 102 (October 2020). 27 pages</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2008.08202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.08202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.12615v1</id>
    <updated>2020-08-26T22:56:41Z</updated>
    <published>2020-08-26T22:56:41Z</published>
    <title>An Impact Model of AI on the Principles of Justice: Encompassing the
  Autonomous Levels of AI Legal Reasoning</title>
    <summary>  Efforts furthering the advancement of Artificial Intelligence (AI) will
increasingly encompass AI Legal Reasoning (AILR) as a crucial element in the
practice of law. It is argued in this research paper that the infusion of AI
into existing and future legal activities and the judicial structure needs to
be undertaken by mindfully observing an alignment with the core principles of
justice. As such, the adoption of AI has a profound twofold possibility of
either usurping the principles of justice, doing so in a Dystopian manner, and
yet also capable to bolster the principles of justice, doing so in a Utopian
way. By examining the principles of justice across the Levels of Autonomy (LoA)
of AI Legal Reasoning, the case is made that there is an ongoing tension
underlying the efforts to develop and deploy AI that can demonstrably determine
the impacts and sway upon each core principle of justice and the collective
set.
</summary>
    <author>
      <name>Lance Eliot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:2008.10575, arXiv:2008.09507, arXiv:2008.07743</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.12615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.12615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; J.7.0; K.5.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.10228v1</id>
    <updated>2020-09-22T00:08:04Z</updated>
    <published>2020-09-22T00:08:04Z</published>
    <title>Designing AI Learning Experiences for K-12: Emerging Works, Future
  Opportunities and a Design Framework</title>
    <summary>  Artificial intelligence (AI) literacy is a rapidly growing research area and
a critical addition to K-12 education. However, support for designing tools and
curriculum to teach K-12 AI literacy is still limited. There is a need for
additional interdisciplinary human-computer interaction and education research
investigating (1) how general AI literacy is currently implemented in learning
experiences and (2) what additional guidelines are required to teach AI
literacy in specifically K-12 learning contexts. In this paper, we analyze a
collection of K-12 AI and education literature to show how core competencies of
AI literacy are applied successfully and organize them into an
educator-friendly chart to enable educators to efficiently find appropriate
resources for their classrooms. We also identify future opportunities and K-12
specific design guidelines, which we synthesized into a conceptual framework to
support researchers, designers, and educators in creating K-12 AI learning
experiences.
</summary>
    <author>
      <name>Xiaofei Zhou</name>
    </author>
    <author>
      <name>Jessica Van Brummelen</name>
    </author>
    <author>
      <name>Phoebe Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.10228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.10228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2; I.2; A.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.11180v1</id>
    <updated>2020-09-11T22:05:40Z</updated>
    <published>2020-09-11T22:05:40Z</published>
    <title>AI and Legal Argumentation: Aligning the Autonomous Levels of AI Legal
  Reasoning</title>
    <summary>  Legal argumentation is a vital cornerstone of justice, underpinning an
adversarial form of law, and extensive research has attempted to augment or
undertake legal argumentation via the use of computer-based automation
including Artificial Intelligence (AI). AI advances in Natural Language
Processing (NLP) and Machine Learning (ML) have especially furthered the
capabilities of leveraging AI for aiding legal professionals, doing so in ways
that are modeled here as CARE, namely Crafting, Assessing, Refining, and
Engaging in legal argumentation. In addition to AI-enabled legal argumentation
serving to augment human-based lawyering, an aspirational goal of this
multi-disciplinary field consists of ultimately achieving autonomously effected
human-equivalent legal argumentation. As such, an innovative meta-approach is
proposed to apply the Levels of Autonomy (LoA) of AI Legal Reasoning (AILR) to
the maturation of AI and Legal Argumentation (AILA), proffering a new means of
gauging progress in this ever-evolving and rigorously sought domain.
</summary>
    <author>
      <name>Lance Eliot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 9 figures. arXiv admin note: text overlap with
  arXiv:2009.02243</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.11180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; J.7.0; K.5.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.11189v1</id>
    <updated>2020-09-22T12:57:10Z</updated>
    <published>2020-09-22T12:57:10Z</published>
    <title>Qlib: An AI-oriented Quantitative Investment Platform</title>
    <summary>  Quantitative investment aims to maximize the return and minimize the risk in
a sequential trading period over a set of financial instruments. Recently,
inspired by rapid development and great potential of AI technologies in
generating remarkable innovation in quantitative investment, there has been
increasing adoption of AI-driven workflow for quantitative research and
practical investment. In the meantime of enriching the quantitative investment
methodology, AI technologies have raised new challenges to the quantitative
investment system. Particularly, the new learning paradigms for quantitative
investment call for an infrastructure upgrade to accommodate the renovated
workflow; moreover, the data-driven nature of AI technologies indeed indicates
a requirement of the infrastructure with more powerful performance;
additionally, there exist some unique challenges for applying AI technologies
to solve different tasks in the financial scenarios. To address these
challenges and bridge the gap between AI technologies and quantitative
investment, we design and develop Qlib that aims to realize the potential,
empower the research, and create the value of AI technologies in quantitative
investment.
</summary>
    <author>
      <name>Xiao Yang</name>
    </author>
    <author>
      <name>Weiqing Liu</name>
    </author>
    <author>
      <name>Dong Zhou</name>
    </author>
    <author>
      <name>Jiang Bian</name>
    </author>
    <author>
      <name>Tie-Yan Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2009.11189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.02393v1</id>
    <updated>2020-12-04T04:11:31Z</updated>
    <published>2020-12-04T04:11:31Z</published>
    <title>The Managerial Effects of Algorithmic Fairness Activism</title>
    <summary>  How do ethical arguments affect AI adoption in business? We randomly expose
business decision-makers to arguments used in AI fairness activism. Arguments
emphasizing the inescapability of algorithmic bias lead managers to abandon AI
for manual review by humans and report greater expectations about lawsuits and
negative PR. These effects persist even when AI lowers gender and racial
disparities and when engineering investments to address AI fairness are
feasible. Emphasis on status quo comparisons yields opposite effects. We also
measure the effects of "scientific veneer" in AI ethics arguments. Scientific
veneer changes managerial behavior but does not asymmetrically benefit
favorable (versus critical) AI activism.
</summary>
    <author>
      <name>Bo Cowgill</name>
    </author>
    <author>
      <name>Fabrizio Dell'Acqua</name>
    </author>
    <author>
      <name>Sandra Matz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Part of the Navigating the Broader Impacts of AI Research Workshop at
  NeurIPS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.02393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.02393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.13560v1</id>
    <updated>2020-12-25T11:06:38Z</updated>
    <published>2020-12-25T11:06:38Z</published>
    <title>Understanding Team Collaboration in Artificial Intelligence from the
  perspective of Geographic Distance</title>
    <summary>  This paper analyzes team collaboration in the field of Artificial
Intelligence (AI) from the perspective of geographic distance. We obtained
1,584,175 AI related publications during 1950-2019 from the Microsoft Academic
Graph. Three latitude-and-longitude-based indicators were employed to quantify
the geographic distance of collaborations in AI over time at domestic and
international levels. The results show team collaborations in AI has been more
popular in the field over time with around 42,000 (38.4%) multiple-affiliation
AI publications in 2019. The changes in geographic distances of team
collaborations indicate the increase of breadth and density for both domestic
and international collaborations in AI over time. In addition, the United
States produced the largest number of single-country and internationally
collaborated AI publications, and China has played an important role in
international collaborations in AI after 2010.
</summary>
    <author>
      <name>Xuli Tang</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Ying Ding</name>
    </author>
    <author>
      <name>Feicheng Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted short paper submission to iConference 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.13560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.13560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.00625v1</id>
    <updated>2021-02-01T04:07:38Z</updated>
    <published>2021-02-01T04:07:38Z</published>
    <title>Human Perceptions on Moral Responsibility of AI: A Case Study in
  AI-Assisted Bail Decision-Making</title>
    <summary>  How to attribute responsibility for autonomous artificial intelligence (AI)
systems' actions has been widely debated across the humanities and social
science disciplines. This work presents two experiments ($N$=200 each) that
measure people's perceptions of eight different notions of moral responsibility
concerning AI and human agents in the context of bail decision-making. Using
real-life adapted vignettes, our experiments show that AI agents are held
causally responsible and blamed similarly to human agents for an identical
task. However, there was a meaningful difference in how people perceived these
agents' moral responsibility; human agents were ascribed to a higher degree of
present-looking and forward-looking notions of responsibility than AI agents.
We also found that people expect both AI and human decision-makers and advisors
to justify their decisions regardless of their nature. We discuss policy and
HCI implications of these findings, such as the need for explainable AI in
high-stakes scenarios.
</summary>
    <author>
      <name>Gabriel Lima</name>
    </author>
    <author>
      <name>Nina Grgić-Hlača</name>
    </author>
    <author>
      <name>Meeyoung Cha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3411764.3445260</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3411764.3445260" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 Pages, 5 Figures, ACM CHI 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.00625v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.00625v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.11567v1</id>
    <updated>2021-02-23T09:14:19Z</updated>
    <published>2021-02-23T09:14:19Z</published>
    <title>Artificial Intelligence as an Anti-Corruption Tool (AI-ACT) --
  Potentials and Pitfalls for Top-down and Bottom-up Approaches</title>
    <summary>  Corruption continues to be one of the biggest societal challenges of our
time. New hope is placed in Artificial Intelligence (AI) to serve as an
unbiased anti-corruption agent. Ever more available (open) government data
paired with unprecedented performance of such algorithms render AI the next
frontier in anti-corruption. Summarizing existing efforts to use AI-based
anti-corruption tools (AI-ACT), we introduce a conceptual framework to advance
research and policy. It outlines why AI presents a unique tool for top-down and
bottom-up anti-corruption approaches. For both approaches, we outline in detail
how AI-ACT present different potentials and pitfalls for (a) input data, (b)
algorithmic design, and (c) institutional implementation. Finally, we venture a
look into the future and flesh out key questions that need to be addressed to
develop AI-ACT while considering citizens' views, hence putting "society in the
loop".
</summary>
    <author>
      <name>Nils Köbis</name>
    </author>
    <author>
      <name>Christopher Starke</name>
    </author>
    <author>
      <name>Iyad Rahwan</name>
    </author>
    <link href="http://arxiv.org/abs/2102.11567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.11567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.12406v1</id>
    <updated>2021-02-24T16:57:35Z</updated>
    <published>2021-02-24T16:57:35Z</published>
    <title>Actionable Principles for Artificial Intelligence Policy: Three Pathways</title>
    <summary>  In the development of governmental policy for artificial intelligence (AI)
that is informed by ethics, one avenue currently pursued is that of drawing on
AI Ethics Principles. However, these AI Ethics Principles often fail to be
actioned in governmental policy. This paper proposes a novel framework for the
development of Actionable Principles for AI. The approach acknowledges the
relevance of AI Ethics Principles and homes in on methodological elements to
increase their practical implementability in policy processes. As a case study,
elements are extracted from the development process of the Ethics Guidelines
for Trustworthy AI of the European Commissions High Level Expert Group on AI.
Subsequently, these elements are expanded on and evaluated in light of their
ability to contribute to a prototype framework for the development of
Actionable Principles for AI. The paper proposes the following three
propositions for the formation of such a prototype framework: (1) preliminary
landscape assessments; (2) multi-stakeholder participation and cross-sectoral
feedback; and, (3) mechanisms to support implementation and
operationalizability.
</summary>
    <author>
      <name>Charlotte Stix</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11948-020-00277-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11948-020-00277-3" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Sci Eng Ethics 27, 15 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2102.12406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.12406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.01266v2</id>
    <updated>2021-07-10T22:54:50Z</updated>
    <published>2021-04-02T22:38:50Z</published>
    <title>Designing for human-AI complementarity in K-12 education</title>
    <summary>  Recent work has explored how complementary strengths of humans and artificial
intelligence (AI) systems might be productively combined. However, successful
forms of human-AI partnership have rarely been demonstrated in real-world
settings. We present the iterative design and evaluation of Lumilo, smart
glasses that help teachers help their students in AI-supported classrooms by
presenting real-time analytics about students' learning, metacognition, and
behavior. Results from a field study conducted in K-12 classrooms indicate that
students learn more when teachers and AI tutors work together during class. We
discuss implications of this research for the design of human-AI partnerships.
We argue for more participatory approaches to research and design in this area,
in which practitioners and other stakeholders are deeply, meaningfully involved
throughout the process. Furthermore, we advocate for theory-building and for
principled approaches to the study of human-AI decision-making in real-world
contexts.
</summary>
    <author>
      <name>Kenneth Holstein</name>
    </author>
    <author>
      <name>Vincent Aleven</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in AI Magazine</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.01266v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.01266v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12582v3</id>
    <updated>2021-04-28T21:33:11Z</updated>
    <published>2021-04-22T17:05:27Z</published>
    <title>Understanding and Avoiding AI Failures: A Practical Guide</title>
    <summary>  As AI technologies increase in capability and ubiquity, AI accidents are
becoming more common. Based on normal accident theory, high reliability theory,
and open systems theory, we create a framework for understanding the risks
associated with AI applications. In addition, we also use AI safety principles
to quantify the unique risks of increased intelligence and human-like qualities
in AI. Together, these two fields give a more complete picture of the risks of
contemporary AI. By focusing on system properties near accidents instead of
seeking a root cause of accidents, we identify where attention should be paid
to safety for current generation AI systems.
</summary>
    <author>
      <name>Robert M. Williams</name>
    </author>
    <author>
      <name>Roman V. Yampolskiy</name>
    </author>
    <link href="http://arxiv.org/abs/2104.12582v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12582v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.07052v1</id>
    <updated>2021-05-14T19:52:35Z</updated>
    <published>2021-05-14T19:52:35Z</published>
    <title>Slicing-Based AI Service Provisioning on Network Edge</title>
    <summary>  Edge intelligence leverages computing resources on network edge to provide
artificial intelligence (AI) services close to network users. As it enables
fast inference and distributed learning, edge intelligence is envisioned to be
an important component of 6G networks. In this article, we investigate AI
service provisioning for supporting edge intelligence. First, we present the
features and requirements of AI services. Then, we introduce AI service data
management, and customize network slicing for AI services. Specifically, we
propose a novel resource pooling method to jointly manage service data and
network resources for AI services. A trace-driven case study demonstrates the
effectiveness of the proposed resource pooling method. Through this study, we
illustrate the necessity, challenge, and potential of AI service provisioning
on network edge.
</summary>
    <author>
      <name>Mushu Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name>Jie Gao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name>Conghao Zhou</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name> Xuemin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sherman</arxiv:affiliation>
    </author>
    <author>
      <name> Shen</name>
    </author>
    <author>
      <name>Weihua Zhuang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures, Submitted to IEEE Vehicular Technology Magazine</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.07052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.07052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.12977v3</id>
    <updated>2021-11-17T07:00:48Z</updated>
    <published>2021-07-27T17:43:48Z</published>
    <title>The social dilemma in artificial intelligence development and why we
  have to solve it</title>
    <summary>  While the demand for ethical artificial intelligence (AI) systems increases,
the number of unethical uses of AI accelerates, even though there is no
shortage of ethical guidelines. We argue that a possible underlying cause for
this is that AI developers face a social dilemma in AI development ethics,
preventing the widespread adaptation of ethical best practices. We define the
social dilemma for AI development and describe why the current crisis in AI
development ethics cannot be solved without relieving AI developers of their
social dilemma. We argue that AI development must be professionalised to
overcome the social dilemma, and discuss how medicine can be used as a template
in this process.
</summary>
    <author>
      <name>Inga Strümke</name>
    </author>
    <author>
      <name>Marija Slavkovik</name>
    </author>
    <author>
      <name>Vince I. Madai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s43681-021-00120-w</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s43681-021-00120-w" rel="related"/>
    <link href="http://arxiv.org/abs/2107.12977v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.12977v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.05809v1</id>
    <updated>2021-08-12T15:42:32Z</updated>
    <published>2021-08-12T15:42:32Z</published>
    <title>Competency Model Approach to AI Literacy: Research-based Path from
  Initial Framework to Model</title>
    <summary>  The recent developments in Artificial Intelligence (AI) technologies
challenge educators and educational institutions to respond with curriculum and
resources that prepare students of all ages with the foundational knowledge and
skills for success in the AI workplace. Research on AI Literacy could lead to
an effective and practical platform for developing these skills. We propose and
advocate for a pathway for developing AI Literacy as a pragmatic and useful
tool for AI education. Such a discipline requires moving beyond a conceptual
framework to a multi-level competency model with associated competency
assessments. This approach to an AI Literacy could guide future development of
instructional content as we prepare a range of groups (i.e., consumers,
co-workers, collaborators, and creators). We propose here a research matrix as
an initial step in the development of a roadmap for AI Literacy research, which
requires a systematic and coordinated effort with the support of publication
outlets and research funding, to expand the areas of competency and
assessments.
</summary>
    <author>
      <name>Farhana Faruqe</name>
    </author>
    <author>
      <name>Ryan Watkins</name>
    </author>
    <author>
      <name>Larry Medsker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented as part of AI4EDU at IJCAI2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.05809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.05809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.07906v1</id>
    <updated>2021-09-12T15:33:43Z</updated>
    <published>2021-09-12T15:33:43Z</published>
    <title>Ethics of AI: A Systematic Literature Review of Principles and
  Challenges</title>
    <summary>  Ethics in AI becomes a global topic of interest for both policymakers and
academic researchers. In the last few years, various research organizations,
lawyers, think tankers and regulatory bodies get involved in developing AI
ethics guidelines and principles. However, there is still debate about the
implications of these principles. We conducted a systematic literature review
(SLR) study to investigate the agreement on the significance of AI principles
and identify the challenging factors that could negatively impact the adoption
of AI ethics principles. The results reveal that the global convergence set
consists of 22 ethical principles and 15 challenges. Transparency, privacy,
accountability and fairness are identified as the most common AI ethics
principles. Similarly, lack of ethical knowledge and vague principles are
reported as the significant challenges for considering ethics in AI. The
findings of this study are the preliminary inputs for proposing a maturity
model that assess the ethical capabilities of AI systems and provide best
practices for further improvements.
</summary>
    <author>
      <name>Arif Ali Khan</name>
    </author>
    <author>
      <name>Sher Badshah</name>
    </author>
    <author>
      <name>Peng Liang</name>
    </author>
    <author>
      <name>Bilal Khan</name>
    </author>
    <author>
      <name>Muhammad Waseem</name>
    </author>
    <author>
      <name>Mahmood Niazi</name>
    </author>
    <author>
      <name>Muhammad Azeem Akbar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.07906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.07906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.09672v1</id>
    <updated>2021-09-20T16:38:49Z</updated>
    <published>2021-09-20T16:38:49Z</published>
    <title>Actionable Approaches to Promote Ethical AI in Libraries</title>
    <summary>  The widespread use of artificial intelligence (AI) in many domains has
revealed numerous ethical issues from data and design to deployment. In
response, countless broad principles and guidelines for ethical AI have been
published, and following those, specific approaches have been proposed for how
to encourage ethical outcomes of AI. Meanwhile, library and information
services too are seeing an increase in the use of AI-powered and machine
learning-powered information systems, but no practical guidance currently
exists for libraries to plan for, evaluate, or audit the ethics of intended or
deployed AI. We therefore report on several promising approaches for promoting
ethical AI that can be adapted from other contexts to AI-powered information
services and in different stages of the software lifecycle.
</summary>
    <author>
      <name>Helen Bubinger</name>
    </author>
    <author>
      <name>Jesse David Dinneen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final version to appear in ASIS&amp;T `21: Proceedings of the 84th Annual
  Meeting of the Association for Information Science &amp; Technology, 58</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.09672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.09672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.11822v2</id>
    <updated>2022-04-21T20:02:06Z</updated>
    <published>2021-10-22T14:56:47Z</published>
    <title>Unraveling the Hidden Environmental Impacts of AI Solutions for
  Environment</title>
    <summary>  In the past ten years, artificial intelligence has encountered such dramatic
progress that it is now seen as a tool of choice to solve environmental issues
and in the first place greenhouse gas emissions (GHG). At the same time the
deep learning community began to realize that training models with more and
more parameters requires a lot of energy and as a consequence GHG emissions. To
our knowledge, questioning the complete net environmental impacts of AI
solutions for the environment (AI for Green), and not only GHG, has never been
addressed directly. In this article, we propose to study the possible negative
impacts of AI for Green. First, we review the different types of AI impacts,
then we present the different methodologies used to assess those impacts, and
show how to apply life cycle assessment to AI services. Finally, we discuss how
to assess the environmental usefulness of a general AI service, and point out
the limitations of existing work in AI for Green.
</summary>
    <author>
      <name>Anne-Laure Ligozat</name>
    </author>
    <author>
      <name>Julien Lefèvre</name>
    </author>
    <author>
      <name>Aurélie Bugeau</name>
    </author>
    <author>
      <name>Jacques Combaz</name>
    </author>
    <link href="http://arxiv.org/abs/2110.11822v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.11822v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.03687v1</id>
    <updated>2021-11-05T18:24:54Z</updated>
    <published>2021-11-05T18:24:54Z</published>
    <title>AI and Blackness: Towards moving beyond bias and representation</title>
    <summary>  In this paper, we argue that AI ethics must move beyond the concepts of
race-based representation and bias, and towards those that probe the deeper
relations that impact how these systems are designed, developed, and deployed.
Many recent discussions on ethical considerations of bias in AI systems have
centered on racial bias. We contend that antiblackness in AI requires more of
an examination of the ontological space that provides a foundation for the
design, development, and deployment of AI systems. We examine what this
contention means from the perspective of the sociocultural context in which AI
systems are designed, developed, and deployed and focus on intersections with
anti-Black racism (antiblackness). To bring these multiple perspectives
together and show an example of antiblackness in the face of attempts at
de-biasing, we discuss results from auditing an existing open-source semantic
network (ConceptNet). We use this discussion to further contextualize
antiblackness in design, development, and deployment of AI systems and suggest
questions one may ask when attempting to combat antiblackness in AI systems.
</summary>
    <author>
      <name>Christopher L. Dancy</name>
    </author>
    <author>
      <name>P. Khalil Saucier</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TTS.2021.3125998</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TTS.2021.3125998" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.03687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.03687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; J.4.0; K.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01920v1</id>
    <updated>2021-10-29T22:20:57Z</updated>
    <published>2021-10-29T22:20:57Z</published>
    <title>Human-AI interaction: An emerging interdisciplinary domain for enabling
  human-centered AI</title>
    <summary>  The new characteristics of AI technology have brought new challenges to the
research and development of AI systems. AI technology has benefited humans, but
if improperly developed, it will harm humans. At present, there is no
systematic interdisciplinary approach to effectively deal with these new
challenges. This paper analyzes the new challenges faced by AI systems and
further elaborates the "Human-Centered AI" (HCAI) approach we proposed in 2019.
In order to enable the implementation of the HCAI approach, we systematically
propose an emerging interdisciplinary domain of "Human-AI Interaction" (HAII),
and define the objective, methodology, and scope. Based on literature review
and analyses, this paper summarizes the main areas of the HAII research and
application as well as puts forward the future research agenda for HAII.
Finally, the paper provides strategic recommendations for future implementation
of the HCAII approach and HAII work.
</summary>
    <author>
      <name>Wei Xu</name>
    </author>
    <author>
      <name>Liezhong Ge</name>
    </author>
    <author>
      <name>Zaifeng Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Chinese language</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.01920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.03784v1</id>
    <updated>2021-12-07T15:57:07Z</updated>
    <published>2021-12-07T15:57:07Z</published>
    <title>Qualitative Analysis for Human Centered AI</title>
    <summary>  Human-centered artificial intelligence (AI) posits that machine learning and
AI should be developed and applied in a socially aware way. In this article, we
argue that qualitative analysis (QA) can be a valuable tool in this process,
supplementing, informing, and extending the possibilities of AI models. We show
this by describing how QA can be integrated in the current prediction paradigm
of AI, assisting scientists in the process of selecting data, variables, and
model architectures. Furthermore, we argue that QA can be a part of novel
paradigms towards Human Centered AI. QA can support scientists and
practitioners in practical problem solving and situated model development. It
can also promote participatory design approaches, reveal understudied and
emerging issues in AI systems, and assist policy making.
</summary>
    <author>
      <name>Orestis Papakyriakopoulos</name>
    </author>
    <author>
      <name>Elizabeth Anne Watkins</name>
    </author>
    <author>
      <name>Amy Winecoff</name>
    </author>
    <author>
      <name>Klaudia Jaźwińska</name>
    </author>
    <author>
      <name>Tithi Chattopadhyay</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">HCAI:Human Centered AI workshop at Neural Information Processing
  Systems 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2112.03784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.03784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.04977v3</id>
    <updated>2022-05-26T17:55:41Z</updated>
    <published>2022-02-10T12:19:48Z</published>
    <title>Needs-aware Artificial Intelligence: AI that 'serves [human] needs'</title>
    <summary>  By defining the current limits (and thereby the frontiers), many boundaries
are shaping, and will continue to shape, the future of Artificial Intelligence
(AI). We push on these boundaries in order to make further progress into what
were yesterday's frontiers. They are both pliable and resilient - always
creating new boundaries of what AI can (or should) achieve. Among these are
technical boundaries (such as processing capacity), psychological boundaries
(such as human trust in AI systems), ethical boundaries (such as with AI
weapons), and conceptual boundaries (such as the AI people can imagine). It is
within this final category while it can play a fundamental role in all other
boundaries} that we find the construct of needs and the limitations that our
current concept of need places on the future AI.
</summary>
    <author>
      <name>Ryan Watkins</name>
    </author>
    <author>
      <name>Soheil Human</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3-10-2022 Reference #6 updates with arXiv link, 5-15-22 final version
  for publication in AI &amp; Ethics</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.04977v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.04977v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.06244v1</id>
    <updated>2022-02-26T04:05:23Z</updated>
    <published>2022-02-26T04:05:23Z</published>
    <title>AI agents for facilitating social interactions and wellbeing</title>
    <summary>  Wellbeing AI has been becoming a new trend in individuals' mental health,
organizational health, and flourishing our societies. Various applications of
wellbeing AI have been introduced to our daily lives. While social
relationships within groups are a critical factor for wellbeing, the
development of wellbeing AI for social interactions remains relatively scarce.
In this paper, we provide an overview of the mediative role of AI-augmented
agents for social interactions. First, we discuss the two-dimensional framework
for classifying wellbeing AI: individual/group and analysis/intervention.
Furthermore, wellbeing AI touches on intervening social relationships between
human-human interactions since positive social relationships are key to human
wellbeing. This intervention may raise technical and ethical challenges. We
discuss opportunities and challenges of the relational approach with wellbeing
AI to promote wellbeing in our societies.
</summary>
    <author>
      <name>Hiro Taiyo Hamada</name>
    </author>
    <author>
      <name>Ryota Kanai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.06244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.06244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.12131v2</id>
    <updated>2022-04-28T15:26:09Z</updated>
    <published>2022-03-23T01:50:24Z</published>
    <title>Should Machine Learning Models Report to Us When They Are Clueless?</title>
    <summary>  The right to AI explainability has consolidated as a consensus in the
research community and policy-making. However, a key component of
explainability has been missing: extrapolation, which describes the extent to
which AI models can be clueless when they encounter unfamiliar samples (i.e.,
samples outside the convex hull of their training sets, as we will explain). We
report that AI models extrapolate outside their range of familiar data,
frequently and without notifying the users and stakeholders. Knowing whether a
model has extrapolated or not is a fundamental insight that should be included
in explaining AI models in favor of transparency and accountability. Instead of
dwelling on the negatives, we offer ways to clear the roadblocks in promoting
AI transparency. Our analysis commentary accompanying practical clauses useful
to include in AI regulations such as the National AI Initiative Act in the US
and the AI Act by the European Commission.
</summary>
    <author>
      <name>Roozbeh Yousefzadeh</name>
    </author>
    <author>
      <name>Xuenan Cao</name>
    </author>
    <link href="http://arxiv.org/abs/2203.12131v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.12131v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.09082v1</id>
    <updated>2022-04-19T18:19:39Z</updated>
    <published>2022-04-19T18:19:39Z</published>
    <title>Factors that influence the adoption of human-AI collaboration in
  clinical decision-making</title>
    <summary>  Recent developments in Artificial Intelligence (AI) have fueled the emergence
of human-AI collaboration, a setting where AI is a coequal partner. Especially
in clinical decision-making, it has the potential to improve treatment quality
by assisting overworked medical professionals. Even though research has started
to investigate the utilization of AI for clinical decision-making, its
potential benefits do not imply its adoption by medical professionals. While
several studies have started to analyze adoption criteria from a technical
perspective, research providing a human-centered perspective with a focus on
AI's potential for becoming a coequal team member in the decision-making
process remains limited. Therefore, in this work, we identify factors for the
adoption of human-AI collaboration by conducting a series of semi-structured
interviews with experts in the healthcare domain. We identify six relevant
adoption factors and highlight existing tensions between them and effective
human-AI collaboration.
</summary>
    <author>
      <name>Patrick Hemmer</name>
    </author>
    <author>
      <name>Max Schemmer</name>
    </author>
    <author>
      <name>Lara Riefle</name>
    </author>
    <author>
      <name>Nico Rosellen</name>
    </author>
    <author>
      <name>Michael Vössing</name>
    </author>
    <author>
      <name>Niklas Kühl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Thirtieth European Conference on Information Systems (ECIS 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.09082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.09082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.05126v2</id>
    <updated>2022-06-01T05:37:50Z</updated>
    <published>2022-05-10T19:08:10Z</published>
    <title>A Meta-Analysis of the Utility of Explainable Artificial Intelligence in
  Human-AI Decision-Making</title>
    <summary>  Research in artificial intelligence (AI)-assisted decision-making is
experiencing tremendous growth with a constantly rising number of studies
evaluating the effect of AI with and without techniques from the field of
explainable AI (XAI) on human decision-making performance. However, as tasks
and experimental setups vary due to different objectives, some studies report
improved user decision-making performance through XAI, while others report only
negligible effects. Therefore, in this article, we present an initial synthesis
of existing research on XAI studies using a statistical meta-analysis to derive
implications across existing research. We observe a statistically positive
impact of XAI on users' performance. Additionally, the first results indicate
that human-AI decision-making tends to yield better task performance on text
data. However, we find no effect of explanations on users' performance compared
to sole AI predictions. Our initial synthesis gives rise to future research
investigating the underlying causes and contributes to further developing
algorithms that effectively benefit human decision-makers by providing
meaningful explanations.
</summary>
    <author>
      <name>Max Schemmer</name>
    </author>
    <author>
      <name>Patrick Hemmer</name>
    </author>
    <author>
      <name>Maximilian Nitsche</name>
    </author>
    <author>
      <name>Niklas Kühl</name>
    </author>
    <author>
      <name>Michael Vössing</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3514094.3534128</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3514094.3534128" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI/ACM Conference on AI, Ethics, and Society (AIES'22)</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.05126v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.05126v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.06241v2</id>
    <updated>2022-12-12T14:49:22Z</updated>
    <published>2022-05-12T17:39:54Z</published>
    <title>Can counterfactual explanations of AI systems' predictions skew lay
  users' causal intuitions about the world? If so, can we correct for that?</title>
    <summary>  Counterfactual (CF) explanations have been employed as one of the modes of
explainability in explainable AI-both to increase the transparency of AI
systems and to provide recourse. Cognitive science and psychology, however,
have pointed out that people regularly use CFs to express causal relationships.
Most AI systems are only able to capture associations or correlations in data
so interpreting them as casual would not be justified. In this paper, we
present two experiment (total N = 364) exploring the effects of CF explanations
of AI system's predictions on lay people's causal beliefs about the real world.
In Experiment 1 we found that providing CF explanations of an AI system's
predictions does indeed (unjustifiably) affect people's causal beliefs
regarding factors/features the AI uses and that people are more likely to view
them as causal factors in the real world. Inspired by the literature on
misinformation and health warning messaging, Experiment 2 tested whether we can
correct for the unjustified change in causal beliefs. We found that pointing
out that AI systems capture correlations and not necessarily causal
relationships can attenuate the effects of CF explanations on people's causal
beliefs.
</summary>
    <author>
      <name>Marko Tesic</name>
    </author>
    <author>
      <name>Ulrike Hahn</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patter.2022.100635</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patter.2022.100635" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Patterns, 3(12), 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2205.06241v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.06241v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.07444v2</id>
    <updated>2022-06-30T04:26:56Z</updated>
    <published>2022-05-16T04:48:24Z</published>
    <title>A Deep Reinforcement Learning Blind AI in DareFightingICE</title>
    <summary>  This paper presents a deep reinforcement learning agent (AI) that uses sound
as the input on the DareFightingICE platform at the DareFightingICE Competition
in IEEE CoG 2022. In this work, an AI that only uses sound as the input is
called blind AI. While state-of-the-art AIs rely mostly on visual or structured
observations provided by their environments, learning to play games from only
sound is still new and thus challenging. We propose different approaches to
process audio data and use the Proximal Policy Optimization algorithm for our
blind AI. We also propose to use our blind AI in evaluation of sound designs
submitted to the competition and define two metrics for this task. The
experimental results show the effectiveness of not only our blind AI but also
the proposed two metrics.
</summary>
    <author>
      <name>Thai Van Nguyen</name>
    </author>
    <author>
      <name>Xincheng Dai</name>
    </author>
    <author>
      <name>Ibrahim Khan</name>
    </author>
    <author>
      <name>Ruck Thawonmas</name>
    </author>
    <author>
      <name>Hai V. Pham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2022 IEEE Conference on Games (CoG 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.07444v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.07444v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; H.5.2; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.12669v1</id>
    <updated>2022-06-25T15:04:47Z</updated>
    <published>2022-06-25T15:04:47Z</published>
    <title>Crypto Makes AI Evolve</title>
    <summary>  Adopting cryptography has given rise to a significant evolution in Artificial
Intelligence (AI). This paper studies the path and stages of this evolution. We
start with reviewing existing relevant surveys, noting their shortcomings,
especially the lack of a close look at the evolution process and solid future
roadmap. These shortcomings justify the work of this paper. Next, we identify,
define and discuss five consequent stages in the evolution path, including
Crypto-Sensitive AI, Crypto-Adapted AI, Crypto-Friendly AI, Crypto-Enabled AI,
Crypto-Protected AI. Then, we establish a future roadmap for further research
in this area, focusing on the role of quantum-inspired and bio-inspired AI.
</summary>
    <author>
      <name>Behrouz Zolfaghari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cyber Science Lab, School of Computer Science, University of Guelph, Ontario, Canada</arxiv:affiliation>
    </author>
    <author>
      <name>Elnaz Rabieinejad</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cyber Science Lab, School of Computer Science, University of Guelph, Ontario, Canada</arxiv:affiliation>
    </author>
    <author>
      <name>Abbas Yazdinejad</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cyber Science Lab, School of Computer Science, University of Guelph, Ontario, Canada</arxiv:affiliation>
    </author>
    <author>
      <name>Reza M. Parizi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">College of Computing and Software Engineering, Kennesaw State University, GA, USA</arxiv:affiliation>
    </author>
    <author>
      <name>Ali Dehghantanha</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Cyber Science Lab, School of Computer Science, University of Guelph, Ontario, Canada</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2206.12669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.12669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.09648v2</id>
    <updated>2022-09-08T15:28:57Z</updated>
    <published>2022-07-20T04:39:29Z</published>
    <title>Roadmap Towards Responsible AI in Crisis Resilience Management</title>
    <summary>  Novel data sensing and AI technologies are finding practical use in the
analysis of crisis resilience, revealing the need to consider how responsible
artificial intelligence (AI) practices can mitigate harmful outcomes and
protect vulnerable populations. In this paper, we present a responsible AI
roadmap that is embedded in the Crisis Information Management Circle. This
roadmap includes six propositions to highlight and address important challenges
and considerations specifically related to responsible AI for crisis resilience
management. We cover a wide spectrum of interwoven challenges and
considerations pertaining to the responsible collection, analysis, sharing, and
use of information such as equity, fairness, biases, explainability and
transparency, accountability, privacy and security, inter-organizational
coordination, and public engagement. Through examining issues around AI systems
for crisis resilience management, we dissect the inherent complexities of
information management and decision-making in crises and highlight the urgency
of responsible AI research and practice. The ideas laid out in this paper are
the first attempt in establishing a roadmap for researchers, practitioners,
developers, emergency managers, humanitarian organizations, and public
officials to address important considerations for responsible AI pertaining to
crisis resilience management.
</summary>
    <author>
      <name>Cheng-Chun Lee</name>
    </author>
    <author>
      <name>Tina Comes</name>
    </author>
    <author>
      <name>Megan Finn</name>
    </author>
    <author>
      <name>Ali Mostafavi</name>
    </author>
    <link href="http://arxiv.org/abs/2207.09648v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.09648v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.12645v1</id>
    <updated>2022-08-23T20:23:22Z</updated>
    <published>2022-08-23T20:23:22Z</published>
    <title>The Brussels Effect and Artificial Intelligence: How EU regulation will
  impact the global AI market</title>
    <summary>  The European Union is likely to introduce among the first, most stringent,
and most comprehensive AI regulatory regimes of the world's major
jurisdictions. In this report, we ask whether the EU's upcoming regulation for
AI will diffuse globally, producing a so-called "Brussels Effect". Building on
and extending Anu Bradford's work, we outline the mechanisms by which such
regulatory diffusion may occur. We consider both the possibility that the EU's
AI regulation will incentivise changes in products offered in non-EU countries
(a de facto Brussels Effect) and the possibility it will influence regulation
adopted by other jurisdictions (a de jure Brussels Effect). Focusing on the
proposed EU AI Act, we tentatively conclude that both de facto and de jure
Brussels effects are likely for parts of the EU regulatory regime. A de facto
effect is particularly likely to arise in large US tech companies with AI
systems that the AI Act terms "high-risk". We argue that the upcoming
regulation might be particularly important in offering the first and most
influential operationalisation of what it means to develop and deploy
trustworthy or human-centred AI. If the EU regime is likely to see significant
diffusion, ensuring it is well-designed becomes a matter of global importance.
</summary>
    <author>
      <name>Charlotte Siegmann</name>
    </author>
    <author>
      <name>Markus Anderljung</name>
    </author>
    <link href="http://arxiv.org/abs/2208.12645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.12645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.12879v2</id>
    <updated>2022-09-28T10:18:49Z</updated>
    <published>2022-09-26T17:47:19Z</published>
    <title>Environmental and Social Sustainability of Creative-Ai</title>
    <summary>  The recent developments of artificial intelligence increase its capability
for the creation of arts in both largely autonomous and collaborative contexts.
In both contexts, Ai aims to imitate, combine, and extend existing artistic
styles, and can transform creative practices. In our ongoing research, we
investigate such Creative-Ai from sustainability and ethical perspectives. The
two main focus areas are understanding the environmental sustainability aspects
(material, practices) in the context of artistic processes that involve
Creative-Ai, and ethical issues related to who gets to be involved in the
creation process (power, authorship, ownership). This paper provides an outline
of our ongoing research in these two directions. We will present our
interdisciplinary approach, which combines interviews, workshops, online
ethnography, and energy measurements, to address our research questions: How is
Creative-Ai currently used by artist communities, and which future applications
do artists imagine? When Ai is applied to creating art, how might it impact the
economy and environment? And, how can answers to these questions guide
requirements for intellectual property regimes for Creative-Ai?
</summary>
    <author>
      <name>André Holzapfel</name>
    </author>
    <author>
      <name>Petra Jääskeläinen</name>
    </author>
    <author>
      <name>Anna-Kaisa Kaila</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in CHI 2022 - Generative AI and CHI Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.12879v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.12879v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.13130v1</id>
    <updated>2022-11-21T23:48:51Z</updated>
    <published>2022-11-21T23:48:51Z</published>
    <title>A Brief Overview of AI Governance for Responsible Machine Learning
  Systems</title>
    <summary>  Organizations of all sizes, across all industries and domains are leveraging
artificial intelligence (AI) technologies to solve some of their biggest
challenges around operations, customer experience, and much more. However, due
to the probabilistic nature of AI, the risks associated with it are far greater
than traditional technologies. Research has shown that these risks can range
anywhere from regulatory, compliance, reputational, and user trust, to
financial and even societal risks. Depending on the nature and size of the
organization, AI technologies can pose a significant risk, if not used in a
responsible way. This position paper seeks to present a brief introduction to
AI governance, which is a framework designed to oversee the responsible use of
AI with the goal of preventing and mitigating risks. Having such a framework
will not only manage risks but also gain maximum value out of AI projects and
develop consistency for organization-wide adoption of AI.
</summary>
    <author>
      <name>Navdeep Gill</name>
    </author>
    <author>
      <name>Abhishek Mathur</name>
    </author>
    <author>
      <name>Marcos V. Conde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS 2022 Trustworthy and Socially Responsible Machine Learning
  (TSRML) Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.13130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.13130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.01834v1</id>
    <updated>2022-12-04T14:54:13Z</updated>
    <published>2022-12-04T14:54:13Z</published>
    <title>Acceleration AI Ethics, the Debate between Innovation and Safety, and
  Stability AI's Diffusion versus OpenAI's Dall-E</title>
    <summary>  One objection to conventional AI ethics is that it slows innovation. This
presentation responds by reconfiguring ethics as an innovation accelerator. The
critical elements develop from a contrast between Stability AI's Diffusion and
OpenAI's Dall-E. By analyzing the divergent values underlying their opposed
strategies for development and deployment, five conceptions are identified as
common to acceleration ethics. Uncertainty is understood as positive and
encouraging, rather than discouraging. Innovation is conceived as intrinsically
valuable, instead of worthwhile only as mediated by social effects. AI problems
are solved by more AI, not less. Permissions and restrictions governing AI
emerge from a decentralized process, instead of a unified authority. The work
of ethics is embedded in AI development and application, instead of functioning
from outside. Together, these attitudes and practices remake ethics as
provoking rather than restraining artificial intelligence.
</summary>
    <author>
      <name>James Brusseau</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, conference presentation</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.01834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.01834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.03491v1</id>
    <updated>2022-12-07T07:14:25Z</updated>
    <published>2022-12-07T07:14:25Z</published>
    <title>"It would work for me too": How Online Communities Shape Software
  Developers' Trust in AI-Powered Code Generation Tools</title>
    <summary>  Software developers commonly engage in online communities to learn about new
technologies. As revolutionary AI-powered code generation tools such as GitHub
Copilot emerge, many developers are uncertain about how to trust them. While we
see the promise of online communities in helping developers build appropriate
trust in AI tools, we know little about how communities shape developers' trust
in AI tools and how community features can facilitate trust in the design of AI
tools. We investigate these questions through a two-phase study. Through an
interview study with 17 developers, we unpack how developers in online
communities collectively make sense of AI code generation tools by developing
proper expectation, understanding, strategies, and awareness of broader
implications, as well as how they leverage community signals to evaluate AI
suggestions. We then surface design opportunities and conduct 11 design probe
sessions to explore the design space of integrating a user community to AI code
generation systems. We conclude with a series of design recommendations.
</summary>
    <author>
      <name>Ruijia Cheng</name>
    </author>
    <author>
      <name>Ruotong Wang</name>
    </author>
    <author>
      <name>Thomas Zimmermann</name>
    </author>
    <author>
      <name>Denae Ford</name>
    </author>
    <link href="http://arxiv.org/abs/2212.03491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.03491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.11738v1</id>
    <updated>2022-12-22T14:31:48Z</updated>
    <published>2022-12-22T14:31:48Z</published>
    <title>Towards Sustainable Artificial Intelligence: An Overview of
  Environmental Protection Uses and Issues</title>
    <summary>  Artificial Intelligence (AI) is used to create more sustainable production
methods and model climate change, making it a valuable tool in the fight
against environmental degradation. This paper describes the paradox of an
energy-consuming technology serving the ecological challenges of tomorrow. The
study provides an overview of the sectors that use AI-based solutions for
environmental protection. It draws on numerous examples from AI for Green
players to present use cases and concrete examples. In the second part of the
study, the negative impacts of AI on the environment and the emerging
technological solutions to support Green AI are examined. It is also shown that
the research on less energy-consuming AI is motivated more by cost and energy
autonomy constraints than by environmental considerations. This leads to a
rebound effect that favors an increase in the complexity of models. Finally,
the need to integrate environmental indicators into algorithms is discussed.
The environmental dimension is part of the broader ethical problem of AI, and
addressing it is crucial for ensuring the sustainability of AI in the long
term.
</summary>
    <author>
      <name>Arnault Pachot</name>
    </author>
    <author>
      <name>Céline Patissier</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">12th International Multidisciplinary Conference on Economics,
  Business, Technology and Social Sciences, Dec 2022, Tbilisi, Georgia</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2212.11738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.11738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.01602v1</id>
    <updated>2022-12-31T18:27:21Z</updated>
    <published>2022-12-31T18:27:21Z</published>
    <title>Unpacking the "Black Box" of AI in Education</title>
    <summary>  Recent advances in Artificial Intelligence (AI) have sparked renewed interest
in its potential to improve education. However, AI is a loose umbrella term
that refers to a collection of methods, capabilities, and limitations-many of
which are often not explicitly articulated by researchers, education technology
companies, or other AI developers. In this paper, we seek to clarify what "AI"
is and the potential it holds to both advance and hamper educational
opportunities that may improve the human condition. We offer a basic
introduction to different methods and philosophies underpinning AI, discuss
recent advances, explore applications to education, and highlight key
limitations and risks. We conclude with a set of questions that educationalists
may ask as they encounter AI in their research and practice. Our hope is to
make often jargon-laden terms and concepts accessible, so that all are equipped
to understand, interrogate, and ultimately shape the development of human
centered AI in education.
</summary>
    <author>
      <name>Nabeel Gillani</name>
    </author>
    <author>
      <name>Rebecca Eynon</name>
    </author>
    <author>
      <name>Catherine Chiabaut</name>
    </author>
    <author>
      <name>Kelsey Finkel</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Educational Technology &amp; Society, 26(1), 99-111</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2301.01602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.01602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.05578v1</id>
    <updated>2023-01-13T14:37:56Z</updated>
    <published>2023-01-13T14:37:56Z</published>
    <title>Toward General Design Principles for Generative AI Applications</title>
    <summary>  Generative AI technologies are growing in power, utility, and use. As
generative technologies are being incorporated into mainstream applications,
there is a need for guidance on how to design those applications to foster
productive and safe use. Based on recent research on human-AI co-creation
within the HCI and AI communities, we present a set of seven principles for the
design of generative AI applications. These principles are grounded in an
environment of generative variability. Six principles are focused on designing
for characteristics of generative AI: multiple outcomes &amp; imperfection;
exploration &amp; control; and mental models &amp; explanations. In addition, we urge
designers to design against potential harms that may be caused by a generative
model's hazardous output, misuse, or potential for human displacement. We
anticipate these principles to usefully inform design decisions made in the
creation of novel human-AI applications, and we invite the community to apply,
revise, and extend these principles to their own work.
</summary>
    <author>
      <name>Justin D. Weisz</name>
    </author>
    <author>
      <name>Michael Muller</name>
    </author>
    <author>
      <name>Jessica He</name>
    </author>
    <author>
      <name>Stephanie Houde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 1 figure. Submitted to the 4th Workshop on Human-AI
  Co-Creation with Generative Models (HAI-GEN) at IUI 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.05578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.05578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.05969v1</id>
    <updated>2023-01-14T20:06:43Z</updated>
    <published>2023-01-14T20:06:43Z</published>
    <title>The Role of Heuristics and Biases During Complex Choices with an AI
  Teammate</title>
    <summary>  Behavioral scientists have classically documented aversion to algorithmic
decision aids, from simple linear models to AI. Sentiment, however, is changing
and possibly accelerating AI helper usage. AI assistance is, arguably, most
valuable when humans must make complex choices. We argue that classic
experimental methods used to study heuristics and biases are insufficient for
studying complex choices made with AI helpers. We adapted an experimental
paradigm designed for studying complex choices in such contexts. We show that
framing and anchoring effects impact how people work with an AI helper and are
predictive of choice outcomes. The evidence suggests that some participants,
particularly those in a loss frame, put too much faith in the AI helper and
experienced worse choice outcomes by doing so. The paradigm also generates
computational modeling-friendly data allowing future studies of human-AI
decision making.
</summary>
    <author>
      <name>Nikolos Gurney</name>
    </author>
    <author>
      <name>John H. Miller</name>
    </author>
    <author>
      <name>David V. Pynadath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.05969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.05969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.13454v1</id>
    <updated>2023-01-31T07:22:12Z</updated>
    <published>2023-01-31T07:22:12Z</published>
    <title>Compliance Costs of AI Technology Commercialization: A Field Deployment
  Perspective</title>
    <summary>  While Artificial Intelligence (AI) technologies are progressing fast,
compliance costs have become a huge financial burden for AI startups, which are
already constrained on research &amp; development budgets. This situation creates a
compliance trap, as many AI startups are not financially prepared to cope with
a broad spectrum of regulatory requirements. Particularly, the complex and
varying regulatory processes across the globe subtly give advantages to
well-established and resourceful technology firms over resource-constrained AI
startups [1]. The continuation of this trend may phase out the majority of AI
startups and lead to giant technology firms' monopolies of AI technologies. To
demonstrate the reality of the compliance trap, from a field deployment
perspective, we delve into the details of compliance costs of AI commercial
operations.
</summary>
    <author>
      <name>Weiyue Wu</name>
    </author>
    <author>
      <name>Shaoshan Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2301.13454v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.13454v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.00225v1</id>
    <updated>2023-02-01T04:07:11Z</updated>
    <published>2023-02-01T04:07:11Z</published>
    <title>The Past, Current, and Future of Neonatal Intensive Care Units with
  Artificial Intelligence</title>
    <summary>  Artificial intelligence (AI), specifically a branch of AI called deep
learning (DL), has proven revolutionary developments in almost all fields, from
computer vision to health sciences, and its effects in medicine have changed
clinical applications significantly. Although some sub-fields of medicine such
as pediatrics have been relatively slow in receiving critical benefits of AI,
related research in pediatrics started to be accumulated to a significant level
too. Hence, in this paper, we review recently developed machine learning and
deep learning based systems for neonatology applications. We systematically
evaluate the role of AI in neonatology applications, define the methodologies,
including algorithmic developments, and describe the remaining challenges in
neonatal diseases. To date, survival analysis, neuroimaging, EEG, pattern
analysis of vital parameters, and retinopathy of prematurity diagnosis with AI
have been the main focus in neonatology. We have categorically summarized 96
research articles, from 1996 to 2022, and discussed their pros and cons,
respectively. We also discuss possible directions for new AI models and the
future of neonatology with the rising power of AI, suggesting roadmaps for
integration of AI into neonatal intensive care units.
</summary>
    <author>
      <name>Elif Keles</name>
    </author>
    <author>
      <name>Ulas Bagci</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">58 pages, review article</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.00225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.00225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.04603v1</id>
    <updated>2023-02-09T12:38:51Z</updated>
    <published>2023-02-09T12:38:51Z</published>
    <title>Contestable Camera Cars: A Speculative Design Exploration of Public AI
  That Is Open and Responsive to Dispute</title>
    <summary>  Local governments increasingly use artificial intelligence (AI) for automated
decision-making. Contestability, making systems responsive to dispute, is a way
to ensure they respect human rights to autonomy and dignity. We investigate the
design of public urban AI systems for contestability through the example of
camera cars: human-driven vehicles equipped with image sensors. Applying a
provisional framework for contestable AI, we use speculative design to create a
concept video of a contestable camera car. Using this concept video, we then
conduct semi-structured interviews with 17 civil servants who work with AI
employed by a large northwestern European city. The resulting data is analyzed
using reflexive thematic analysis to identify the main challenges facing the
implementation of contestability in public AI. We describe how civic
participation faces issues of representation, public AI systems should
integrate with existing democratic practices, and cities must expand capacities
for responsible AI development and operation.
</summary>
    <author>
      <name>Kars Alfrink</name>
    </author>
    <author>
      <name>Ianus Keller</name>
    </author>
    <author>
      <name>Neelke Doorn</name>
    </author>
    <author>
      <name>Gerd Kortuem</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3544548.3580984</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3544548.3580984" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conditionally accepted to CHI 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.04603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.04603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.11157v1</id>
    <updated>2020-03-24T23:55:59Z</updated>
    <published>2020-03-24T23:55:59Z</published>
    <title>AI loyalty: A New Paradigm for Aligning Stakeholder Interests</title>
    <summary>  When we consult with a doctor, lawyer, or financial advisor, we generally
assume that they are acting in our best interests. But what should we assume
when it is an artificial intelligence (AI) system that is acting on our behalf?
Early examples of AI assistants like Alexa, Siri, Google, and Cortana already
serve as a key interface between consumers and information on the web, and
users routinely rely upon AI-driven systems like these to take automated
actions or provide information. Superficially, such systems may appear to be
acting according to user interests. However, many AI systems are designed with
embedded conflicts of interests, acting in ways that subtly benefit their
creators (or funders) at the expense of users. To address this problem, in this
paper we introduce the concept of AI loyalty. AI systems are loyal to the
degree that they are designed to minimize, and make transparent, conflicts of
interest, and to act in ways that prioritize the interests of users. Properly
designed, such systems could have considerable functional and competitive - not
to mention ethical - advantages relative to those that do not. Loyal AI
products hold an obvious appeal for the end-user and could serve to promote the
alignment of the long-term interests of AI developers and customers. To this
end, we suggest criteria for assessing whether an AI system is sufficiently
transparent about conflicts of interest, and acting in a manner that is loyal
to the user, and argue that AI loyalty should be considered during the
technological design process alongside other important values in AI ethics such
as fairness, accountability privacy, and equity. We discuss a range of
mechanisms, from pure market forces to strong regulatory frameworks, that could
support incorporation of AI loyalty into a variety of future AI systems.
</summary>
    <author>
      <name>Anthony Aguirre</name>
    </author>
    <author>
      <name>Gaia Dempsey</name>
    </author>
    <author>
      <name>Harry Surden</name>
    </author>
    <author>
      <name>Peter B. Reiner</name>
    </author>
    <link href="http://arxiv.org/abs/2003.11157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.11157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.05049v3</id>
    <updated>2020-06-11T06:41:02Z</updated>
    <published>2019-01-15T21:27:28Z</published>
    <title>Bonseyes AI Pipeline -- bringing AI to you. End-to-end integration of
  data, algorithms and deployment tools</title>
    <summary>  Next generation of embedded Information and Communication Technology (ICT)
systems are collaborative systems able to perform autonomous tasks. The
remarkable expansion of the embedded ICT market, together with the rise and
breakthroughs of Artificial Intelligence (AI), have put the focus on the Edge
as it stands as one of the keys for the next technological revolution: the
seamless integration of AI in our daily life. However, training and deployment
of custom AI solutions on embedded devices require a fine-grained integration
of data, algorithms, and tools to achieve high accuracy. Such integration
requires a high level of expertise that becomes a real bottleneck for small and
medium enterprises wanting to deploy AI solutions on the Edge which,
ultimately, slows down the adoption of AI on daily-life applications. In this
work, we present a modular AI pipeline as an integrating framework to bring
data, algorithms, and deployment tools together. By removing the integration
barriers and lowering the required expertise, we can interconnect the different
stages of tools and provide a modular end-to-end development of AI products for
embedded devices. Our AI pipeline consists of four modular main steps: i) data
ingestion, ii) model training, iii) deployment optimization and, iv) the IoT
hub integration. To show the effectiveness of our pipeline, we provide examples
of different AI applications during each of the steps. Besides, we integrate
our deployment framework, LPDNN, into the AI pipeline and present its
lightweight architecture and deployment capabilities for embedded devices.
Finally, we demonstrate the results of the AI pipeline by showing the
deployment of several AI applications such as keyword spotting, image
classification and object detection on a set of well-known embedded platforms,
where LPDNN consistently outperforms all other popular deployment frameworks.
</summary>
    <author>
      <name>Miguel de Prado</name>
    </author>
    <author>
      <name>Jing Su</name>
    </author>
    <author>
      <name>Rabia Saeed</name>
    </author>
    <author>
      <name>Lorenzo Keller</name>
    </author>
    <author>
      <name>Noelia Vallez</name>
    </author>
    <author>
      <name>Andrew Anderson</name>
    </author>
    <author>
      <name>David Gregg</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
    <author>
      <name>Tim Llewellynn</name>
    </author>
    <author>
      <name>Nabil Ouerhani</name>
    </author>
    <author>
      <name>Rozenn Dahyot and</name>
    </author>
    <author>
      <name>Nuria Pazos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3403572</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3403572" rel="related"/>
    <link href="http://arxiv.org/abs/1901.05049v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05049v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.10385v4</id>
    <updated>2022-01-11T06:19:32Z</updated>
    <published>2020-09-22T08:23:56Z</published>
    <title>A narrowing of AI research?</title>
    <summary>  The arrival of deep learning techniques able to infer patterns from large
datasets has dramatically improved the performance of Artificial Intelligence
(AI) systems. Deep learning's rapid development and adoption, in great part led
by large technology companies, has however created concerns about a premature
narrowing in the technological trajectory of AI research despite its
weaknesses, which include lack of robustness, high environmental costs, and
potentially unfair outcomes. We seek to improve the evidence base with a
semantic analysis of AI research in arXiv, a popular pre-prints database. We
study the evolution of the thematic diversity of AI research, compare the
thematic diversity of AI research in academia and the private sector and
measure the influence of private companies in AI research through the citations
they receive and their collaborations with other institutions. Our results
suggest that diversity in AI research has stagnated in recent years, and that
AI research involving the private sector tends to be less diverse and more
influential than research in academia. We also find that private sector AI
researchers tend to specialise in data-hungry and computationally intensive
deep learning methods at the expense of research involving other AI methods,
research that considers the societal and ethical implications of AI, and
applications in sectors like health. Our results provide a rationale for policy
action to prevent a premature narrowing of AI research that could constrain its
societal benefits, but we note the informational, incentive and scale hurdles
standing in the way of such interventions.
</summary>
    <author>
      <name>Joel Klinger</name>
    </author>
    <author>
      <name>Juan Mateos-Garcia</name>
    </author>
    <author>
      <name>Konstantinos Stathoulopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fourth version: Includes substantial changes in response to reviewer
  comments such as: alternative strategy to identify AI papers, new robustness
  section, new analysis of private research influence, substantially modified
  literature review and creation of technical annex</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.10385v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.10385v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.09060v1</id>
    <updated>2021-05-19T11:02:13Z</updated>
    <published>2021-05-19T11:02:13Z</published>
    <title>The State of AI Ethics Report (Volume 4)</title>
    <summary>  The 4th edition of the Montreal AI Ethics Institute's The State of AI Ethics
captures the most relevant developments in the field of AI Ethics since January
2021. This report aims to help anyone, from machine learning experts to human
rights activists and policymakers, quickly digest and understand the
ever-changing developments in the field. Through research and article
summaries, as well as expert commentary, this report distills the research and
reporting surrounding various domains related to the ethics of AI, with a
particular focus on four key themes: Ethical AI, Fairness &amp; Justice, Humans &amp;
Tech, and Privacy.
  In addition, The State of AI Ethics includes exclusive content written by
world-class AI Ethics experts from universities, research institutes,
consulting firms, and governments. Opening the report is a long-form piece by
Edward Higgs (Professor of History, University of Essex) titled "AI and the
Face: A Historian's View." In it, Higgs examines the unscientific history of
facial analysis and how AI might be repeating some of those mistakes at scale.
The report also features chapter introductions by Alexa Hagerty
(Anthropologist, University of Cambridge), Marianna Ganapini (Faculty Director,
Montreal AI Ethics Institute), Deborah G. Johnson (Emeritus Professor,
Engineering and Society, University of Virginia), and Soraj Hongladarom
(Professor of Philosophy and Director, Center for Science, Technology and
Society, Chulalongkorn University in Bangkok).
  This report should be used not only as a point of reference and insight on
the latest thinking in the field of AI Ethics, but should also be used as a
tool for introspection as we aim to foster a more nuanced conversation
regarding the impacts of AI on the world.
</summary>
    <author>
      <name>Abhishek Gupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Alexandrine Royer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Connor Wright</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Victoria Heath</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Muriam Fancy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Marianna Bergamaschi Ganapini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Shannon Egan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Masa Sweidan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Mo Akif</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Renjie Butalid</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">190 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.09060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.09060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.01466v1</id>
    <updated>2022-01-05T06:00:22Z</updated>
    <published>2022-01-05T06:00:22Z</published>
    <title>Challenges of Artificial Intelligence -- From Machine Learning and
  Computer Vision to Emotional Intelligence</title>
    <summary>  Artificial intelligence (AI) has become a part of everyday conversation and
our lives. It is considered as the new electricity that is revolutionizing the
world. AI is heavily invested in both industry and academy. However, there is
also a lot of hype in the current AI debate. AI based on so-called deep
learning has achieved impressive results in many problems, but its limits are
already visible. AI has been under research since the 1940s, and the industry
has seen many ups and downs due to over-expectations and related
disappointments that have followed.
  The purpose of this book is to give a realistic picture of AI, its history,
its potential and limitations. We believe that AI is a helper, not a ruler of
humans. We begin by describing what AI is and how it has evolved over the
decades. After fundamentals, we explain the importance of massive data for the
current mainstream of artificial intelligence. The most common representations
for AI, methods, and machine learning are covered. In addition, the main
application areas are introduced. Computer vision has been central to the
development of AI. The book provides a general introduction to computer vision,
and includes an exposure to the results and applications of our own research.
Emotions are central to human intelligence, but little use has been made in AI.
We present the basics of emotional intelligence and our own research on the
topic. We discuss super-intelligence that transcends human understanding,
explaining why such achievement seems impossible on the basis of present
knowledge,and how AI could be improved. Finally, a summary is made of the
current state of AI and what to do in the future. In the appendix, we look at
the development of AI education, especially from the perspective of contents at
our own university.
</summary>
    <author>
      <name>Matti Pietikäinen</name>
    </author>
    <author>
      <name>Olli Silven</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">234 pages. Published as an electronic publication at the University
  of Oulu, Finland, in December 2021, ISBN: 978-952-62-3199-0 link
  http://jultika.oulu.fi/Record/isbn978-952-62-3199-0</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.01466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.01466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.14177v1</id>
    <updated>2022-12-29T05:17:59Z</updated>
    <published>2022-12-29T05:17:59Z</published>
    <title>Current State of Community-Driven Radiological AI Deployment in Medical
  Imaging</title>
    <summary>  Artificial Intelligence (AI) has become commonplace to solve routine everyday
tasks. Because of the exponential growth in medical imaging data volume and
complexity, the workload on radiologists is steadily increasing. We project
that the gap between the number of imaging exams and the number of expert
radiologist readers required to cover this increase will continue to expand,
consequently introducing a demand for AI-based tools that improve the
efficiency with which radiologists can comfortably interpret these exams. AI
has been shown to improve efficiency in medical-image generation, processing,
and interpretation, and a variety of such AI models have been developed across
research labs worldwide. However, very few of these, if any, find their way
into routine clinical use, a discrepancy that reflects the divide between AI
research and successful AI translation. To address the barrier to clinical
deployment, we have formed MONAI Consortium, an open-source community which is
building standards for AI deployment in healthcare institutions, and developing
tools and infrastructure to facilitate their implementation. This report
represents several years of weekly discussions and hands-on problem solving
experience by groups of industry experts and clinicians in the MONAI
Consortium. We identify barriers between AI-model development in research labs
and subsequent clinical deployment and propose solutions. Our report provides
guidance on processes which take an imaging AI model from development to
clinical implementation in a healthcare institution. We discuss various AI
integration points in a clinical Radiology workflow. We also present a taxonomy
of Radiology AI use-cases. Through this report, we intend to educate the
stakeholders in healthcare and AI (AI researchers, radiologists, imaging
informaticists, and regulators) about cross-disciplinary challenges and
possible solutions.
</summary>
    <author>
      <name>Vikash Gupta</name>
    </author>
    <author>
      <name>Barbaros Selnur Erdal</name>
    </author>
    <author>
      <name>Carolina Ramirez</name>
    </author>
    <author>
      <name>Ralf Floca</name>
    </author>
    <author>
      <name>Laurence Jackson</name>
    </author>
    <author>
      <name>Brad Genereaux</name>
    </author>
    <author>
      <name>Sidney Bryson</name>
    </author>
    <author>
      <name>Christopher P Bridge</name>
    </author>
    <author>
      <name>Jens Kleesiek</name>
    </author>
    <author>
      <name>Felix Nensa</name>
    </author>
    <author>
      <name>Rickmer Braren</name>
    </author>
    <author>
      <name>Khaled Younis</name>
    </author>
    <author>
      <name>Tobias Penzkofer</name>
    </author>
    <author>
      <name>Andreas Michael Bucher</name>
    </author>
    <author>
      <name>Ming Melvin Qin</name>
    </author>
    <author>
      <name>Gigon Bae</name>
    </author>
    <author>
      <name>M. Jorge Cardoso</name>
    </author>
    <author>
      <name>Sebastien Ourselin</name>
    </author>
    <author>
      <name>Eric Kerfoot</name>
    </author>
    <author>
      <name>Rahul Choudhury</name>
    </author>
    <author>
      <name>Richard D. White</name>
    </author>
    <author>
      <name>Tessa Cook</name>
    </author>
    <author>
      <name>David Bericat</name>
    </author>
    <author>
      <name>Matthew Lungren</name>
    </author>
    <author>
      <name>Risto Haukioja</name>
    </author>
    <author>
      <name>Haris Shuaib</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages; 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.14177v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.14177v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06044v1</id>
    <updated>2018-01-02T15:19:52Z</updated>
    <published>2018-01-02T15:19:52Z</published>
    <title>Analysis of the Relation between Artificial Intelligence and the
  Internet from the Perspective of Brain Science</title>
    <summary>  Artificial intelligence (AI) like deep learning, cloud AI computation has
been advancing at a rapid pace since 2014. There is no doubt that the
prosperity of AI is inseparable with the development of the Internet. However,
there has been little attention to the link between AI and the internet. This
paper explores them with brain insights mainly from four views:1) How is the
general relation between artificial intelligence and Internet of Things, cloud
computing, big data and Industrial Internet from the perspective of brain
science. 2) Construction of a new AI system model with the Internet and brain
science.
</summary>
    <author>
      <name>Feng Liu</name>
    </author>
    <author>
      <name>Yong Shi</name>
    </author>
    <author>
      <name>Peijia Lia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.procs.2017.11.383</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.procs.2017.11.383" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.06044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.08915v1</id>
    <updated>2018-05-23T00:19:07Z</updated>
    <published>2018-05-23T00:19:07Z</published>
    <title>A Psychopathological Approach to Safety Engineering in AI and AGI</title>
    <summary>  The complexity of dynamics in AI techniques is already approaching that of
complex adaptive systems, thus curtailing the feasibility of formal
controllability and reachability analysis in the context of AI safety. It
follows that the envisioned instances of Artificial General Intelligence (AGI)
will also suffer from challenges of complexity. To tackle such issues, we
propose the modeling of deleterious behaviors in AI and AGI as psychological
disorders, thereby enabling the employment of psychopathological approaches to
analysis and control of misbehaviors. Accordingly, we present a discussion on
the feasibility of the psychopathological approaches to AI safety, and propose
general directions for research on modeling, diagnosis, and treatment of
psychological disorders in AGI.
</summary>
    <author>
      <name>Vahid Behzadan</name>
    </author>
    <author>
      <name>Arslan Munir</name>
    </author>
    <author>
      <name>Roman V. Yampolskiy</name>
    </author>
    <link href="http://arxiv.org/abs/1805.08915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.08915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.10443v1</id>
    <updated>2017-05-30T03:12:03Z</updated>
    <published>2017-05-30T03:12:03Z</published>
    <title>MOBA: a New Arena for Game AI</title>
    <summary>  Games have always been popular testbeds for Artificial Intelligence (AI). In
the last decade, we have seen the rise of the Multiple Online Battle Arena
(MOBA) games, which are the most played games nowadays. In spite of this, there
are few works that explore MOBA as a testbed for AI Research. In this paper we
present and discuss the main features and opportunities offered by MOBA games
to Game AI Research. We describe the various challenges faced along the game
and also propose a discrete model that can be used to better understand and
explore the game. With this, we aim to encourage the use of MOBA as a novel
research platform for Game AI.
</summary>
    <author>
      <name>Victor do Nascimento Silva</name>
    </author>
    <author>
      <name>Luiz Chaimowicz</name>
    </author>
    <link href="http://arxiv.org/abs/1705.10443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.11142v1</id>
    <updated>2018-12-28T18:11:12Z</updated>
    <published>2018-12-28T18:11:12Z</published>
    <title>The Diagrammatic AI Language (DIAL): Version 0.1</title>
    <summary>  Currently, there is no consistent model for visually or formally representing
the architecture of AI systems. This lack of representation brings
interpretability, correctness and completeness challenges in the description of
existing models and systems. DIAL (The Diagrammatic AI Language) has been
created with the aspiration of being an "engineering schematic" for AI Systems.
It is presented here as a starting point for a community dialogue towards a
common diagrammatic language for AI Systems.
</summary>
    <author>
      <name>Guy Marshall</name>
    </author>
    <author>
      <name>André Freitas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.11142v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.11142v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.01555v1</id>
    <updated>2019-02-16T14:03:05Z</updated>
    <published>2019-02-16T14:03:05Z</published>
    <title>An Explorative Study of GitHub Repositories of AI Papers</title>
    <summary>  With the rapid development of AI technologies, thousands of AI papers are
being published each year. Many of these papers have released sample code to
facilitate follow-up researchers. This paper presents an explorative study of
over 1700 code repositories of AI papers hosted on GitHub. We find that these
repositories are often poorly written, lack of documents, lack of maintenance,
and hard to configure the underlying runtime environment. Thus, many code
repositories become inactive and abandoned. Such a situation makes follow-up
researchers hard to reproduce the results or do further research. In addition,
these hard-to-reuse code makes a gap between academia and industry. Based on
the findings, we give some recommendations on how to improve the quality of
code repositories of AI papers.
</summary>
    <author>
      <name>Boyang Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1903.01555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.13178v1</id>
    <updated>2019-05-30T17:06:11Z</updated>
    <published>2019-05-30T17:06:11Z</published>
    <title>Better Future through AI: Avoiding Pitfalls and Guiding AI Towards its
  Full Potential</title>
    <summary>  Artificial Intelligence (AI) technology is rapidly changing many areas of
society. While there is tremendous potential in this transition, there are
several pitfalls as well. Using the history of computing and the world-wide web
as a guide, in this article we identify those pitfalls and actions that lead AI
development to its full potential. If done right, AI will be instrumental in
achieving the goals we set for economy, society, and the world in general.
</summary>
    <author>
      <name>Risto Miikkulainen</name>
    </author>
    <author>
      <name>Bret Greenstein</name>
    </author>
    <author>
      <name>Babak Hodjat</name>
    </author>
    <author>
      <name>Jerry Smith</name>
    </author>
    <link href="http://arxiv.org/abs/1905.13178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.13178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08196v1</id>
    <updated>2016-08-29T19:50:30Z</updated>
    <published>2016-08-29T19:50:30Z</published>
    <title>Smart Policies for Artificial Intelligence</title>
    <summary>  We argue that there already exists de facto artificial intelligence policy -
a patchwork of policies impacting the field of AI's development in myriad ways.
The key question related to AI policy, then, is not whether AI should be
governed at all, but how it is currently being governed, and how that
governance might become more informed, integrated, effective, and anticipatory.
We describe the main components of de facto AI policy and make some
recommendations for how AI policy can be improved, drawing on lessons from
other scientific and technological domains.
</summary>
    <author>
      <name>Miles Brundage</name>
    </author>
    <author>
      <name>Joanna Bryson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a draft of an article being revised - feedback is welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03122v1</id>
    <updated>2017-06-09T20:57:18Z</updated>
    <published>2017-06-09T20:57:18Z</published>
    <title>Off The Beaten Lane: AI Challenges In MOBAs Beyond Player Control</title>
    <summary>  MOBAs represent a huge segment of online gaming and are growing as both an
eSport and a casual genre. The natural starting point for AI researchers
interested in MOBAs is to develop an AI to play the game better than a human -
but MOBAs have many more challenges besides adversarial AI. In this paper we
introduce the reader to the wider context of MOBA culture, propose a range of
challenges faced by the community today, and posit concrete AI projects that
can be undertaken to begin solving them.
</summary>
    <author>
      <name>Michael Cook</name>
    </author>
    <author>
      <name>Adam Summerville</name>
    </author>
    <author>
      <name>Simon Colton</name>
    </author>
    <link href="http://arxiv.org/abs/1706.03122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04309v2</id>
    <updated>2018-02-15T21:00:42Z</updated>
    <published>2017-11-12T15:19:56Z</published>
    <title>Self-Regulating Artificial General Intelligence</title>
    <summary>  Here we examine the paperclip apocalypse concern for artificial general
intelligence (or AGI) whereby a superintelligent AI with a simple goal (ie.,
producing paperclips) accumulates power so that all resources are devoted
towards that simple goal and are unavailable for any other use. We provide
conditions under which a paper apocalypse can arise but also show that, under
certain architectures for recursive self-improvement of AIs, that a paperclip
AI may refrain from allowing power capabilities to be developed. The reason is
that such developments pose the same control problem for the AI as they do for
humans (over AIs) and hence, threaten to deprive it of resources for its
primary goal.
</summary>
    <author>
      <name>Joshua S. Gans</name>
    </author>
    <link href="http://arxiv.org/abs/1711.04309v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04309v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.10189v2</id>
    <updated>2019-10-04T17:49:07Z</updated>
    <published>2019-06-24T19:26:10Z</published>
    <title>Evolutionary Computation and AI Safety: Research Problems Impeding
  Routine and Safe Real-world Application of Evolution</title>
    <summary>  Recent developments in artificial intelligence and machine learning have
spurred interest in the growing field of AI safety, which studies how to
prevent human-harming accidents when deploying AI systems. This paper thus
explores the intersection of AI safety with evolutionary computation, to show
how safety issues arise in evolutionary computation and how understanding from
evolutionary computational and biological evolution can inform the broader
study of AI safety.
</summary>
    <author>
      <name>Joel Lehman</name>
    </author>
    <link href="http://arxiv.org/abs/1906.10189v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10189v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.10300v1</id>
    <updated>2019-08-27T16:22:31Z</updated>
    <published>2019-08-27T16:22:31Z</published>
    <title>Explainable AI: A Neurally-Inspired Decision Stack Framework</title>
    <summary>  European Law now requires AI to be explainable in the context of adverse
decisions affecting European Union (EU) citizens. At the same time, it is
expected that there will be increasing instances of AI failure as it operates
on imperfect data. This paper puts forward a neurally-inspired framework called
decision stacks that can provide for a way forward in research aimed at
developing explainable AI. Leveraging findings from memory systems in
biological brains, the decision stack framework operationalizes the definition
of explainability and then proposes a test that can potentially reveal how a
given AI decision came to its conclusion.
</summary>
    <author>
      <name>J. L. Olds</name>
    </author>
    <author>
      <name>M. S. Khan</name>
    </author>
    <author>
      <name>M. Nayebpour</name>
    </author>
    <author>
      <name>N. Koizumi</name>
    </author>
    <link href="http://arxiv.org/abs/1908.10300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.06497v1</id>
    <updated>2019-12-11T23:39:57Z</updated>
    <published>2019-12-11T23:39:57Z</published>
    <title>Founding The Domain of AI Forensics</title>
    <summary>  With the widespread integration of AI in everyday and critical technologies,
it seems inevitable to witness increasing instances of failure in AI systems.
In such cases, there arises a need for technical investigations that produce
legally acceptable and scientifically indisputable findings and conclusions on
the causes of such failures. Inspired by the domain of cyber forensics, this
paper introduces the need for the establishment of AI Forensics as a new
discipline under AI safety. Furthermore, we propose a taxonomy of the subfields
under this discipline, and present a discussion on the foundational challenges
that lay ahead of this new research area.
</summary>
    <author>
      <name>Ibrahim Baggili</name>
    </author>
    <author>
      <name>Vahid Behzadan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for presentation at SafeAI2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.06497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.06497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.07211v1</id>
    <updated>2019-12-16T06:09:39Z</updated>
    <published>2019-12-16T06:09:39Z</published>
    <title>Fairness Assessment for Artificial Intelligence in Financial Industry</title>
    <summary>  Artificial Intelligence (AI) is an important driving force for the
development and transformation of the financial industry. However, with the
fast-evolving AI technology and application, unintentional bias, insufficient
model validation, immature contingency plan and other underestimated threats
may expose the company to operational and reputational risks. In this paper, we
focus on fairness evaluation, one of the key components of AI Governance,
through a quantitative lens. Statistical methods are reviewed for imbalanced
data treatment and bias mitigation. These methods and fairness evaluation
metrics are then applied to a credit card default payment example.
</summary>
    <author>
      <name>Yukun Zhang</name>
    </author>
    <author>
      <name>Longsheng Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Robust AI in FS 2019 : NeurIPS 2019 Workshop on Robust AI in
  Financial Services: Data, Fairness, Explainability, Trustworthiness, and
  Privacy</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.07211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.07211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09758v2</id>
    <updated>2020-05-05T11:26:57Z</updated>
    <published>2020-01-15T10:48:23Z</published>
    <title>Towards organizational guidelines for the responsible use of AI</title>
    <summary>  In the past few years, several large companies have published ethical
principles of Artificial Intelligence (AI). National governments, the European
Commission, and inter-governmental organizations have come up with requirements
to ensure the good use of AI. However, individual organizations that want to
join this effort, are faced with many unsolved questions. This paper proposes
guidelines for organizations committed to the responsible use of AI, but lack
the required knowledge and experience. The guidelines consist of two parts: i)
helping organizations to decide what principles to adopt, and ii) a methodology
for implementing the principles in organizational processes. In case of future
AI regulation, organizations following this approach will be well-prepared.
</summary>
    <author>
      <name>Richard Benjamins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for ECAI 2020. 2 pages. http://ecai2020.eu/</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.09758v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09758v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.05652v1</id>
    <updated>2020-02-13T17:59:15Z</updated>
    <published>2020-02-13T17:59:15Z</published>
    <title>Functionally Effective Conscious AI Without Suffering</title>
    <summary>  Insofar as consciousness has a functional role in facilitating learning and
behavioral control, the builders of autonomous AI systems are likely to attempt
to incorporate it into their designs. The extensive literature on the ethics of
AI is concerned with ensuring that AI systems, and especially autonomous
conscious ones, behave ethically. In contrast, our focus here is on the rarely
discussed complementary aspect of engineering conscious AI: how to avoid
condemning such systems, for whose creation we would be solely responsible, to
unavoidable suffering brought about by phenomenal self-consciousness. We
outline two complementary approaches to this problem, one motivated by a
philosophical analysis of the phenomenal self, and the other by certain
computational concepts in reinforcement learning.
</summary>
    <author>
      <name>Aman Agarwal</name>
    </author>
    <author>
      <name>Shimon Edelman</name>
    </author>
    <link href="http://arxiv.org/abs/2002.05652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.03434v1</id>
    <updated>2020-06-05T13:34:47Z</updated>
    <published>2020-06-05T13:34:47Z</published>
    <title>Artificial Intelligence-based Clinical Decision Support for COVID-19 --
  Where Art Thou?</title>
    <summary>  The COVID-19 crisis has brought about new clinical questions, new workflows,
and accelerated distributed healthcare needs. While artificial intelligence
(AI)-based clinical decision support seemed to have matured, the application of
AI-based tools for COVID-19 has been limited to date. In this perspective
piece, we identify opportunities and requirements for AI-based clinical
decision support systems and highlight challenges that impact "AI readiness"
for rapidly emergent healthcare challenges.
</summary>
    <author>
      <name>Mathias Unberath</name>
    </author>
    <author>
      <name>Kimia Ghobadi</name>
    </author>
    <author>
      <name>Scott Levin</name>
    </author>
    <author>
      <name>Jeremiah Hinson</name>
    </author>
    <author>
      <name>Gregory D Hager</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited perspective piece on AI in the fight against COVID-19 to
  appear in Advanced Intelligent Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.03434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.03434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.14302v1</id>
    <updated>2020-07-28T15:16:45Z</updated>
    <published>2020-07-28T15:16:45Z</published>
    <title>Ethics of Artificial Intelligence in Surgery</title>
    <summary>  Here we discuss the four key principles of bio-medical ethics from surgical
context. We elaborate on the definition of 'fairness' and its implications in
AI system design, with taxonomy of algorithmic biases in AI. We discuss the
shifts in ethical paradigms as the degree of autonomy in AI systems continue to
evolve. We also emphasize the need for continuous revisions of ethics in AI due
to evolution and dynamic nature of AI systems and technologies.
</summary>
    <author>
      <name>Frank Rudzicz</name>
    </author>
    <author>
      <name>Raeid Saqur</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Hashimoto D.A. (Ed.) Artificial Intelligence in Surgery: A
  Primer for Surgical Practice. New York: McGraw Hill. ISBN: 978-1260452730
  (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2007.14302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.14302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.04071v1</id>
    <updated>2020-07-19T02:49:41Z</updated>
    <published>2020-07-19T02:49:41Z</published>
    <title>On Controllability of AI</title>
    <summary>  Invention of artificial general intelligence is predicted to cause a shift in
the trajectory of human civilization. In order to reap the benefits and avoid
pitfalls of such powerful technology it is important to be able to control it.
However, possibility of controlling artificial general intelligence and its
more advanced version, superintelligence, has not been formally established. In
this paper, we present arguments as well as supporting evidence from multiple
domains indicating that advanced AI can't be fully controlled. Consequences of
uncontrollability of AI are discussed with respect to future of humanity and
research on AI, and AI safety and security.
</summary>
    <author>
      <name>Roman V. Yampolskiy</name>
    </author>
    <link href="http://arxiv.org/abs/2008.04071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.04071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.07309v1</id>
    <updated>2020-08-11T10:02:04Z</updated>
    <published>2020-08-11T10:02:04Z</published>
    <title>Bias and Discrimination in AI: a cross-disciplinary perspective</title>
    <summary>  With the widespread and pervasive use of Artificial Intelligence (AI) for
automated decision-making systems, AI bias is becoming more apparent and
problematic. One of its negative consequences is discrimination: the unfair, or
unequal treatment of individuals based on certain characteristics. However, the
relationship between bias and discrimination is not always clear. In this
paper, we survey relevant literature about bias and discrimination in AI from
an interdisciplinary perspective that embeds technical, legal, social and
ethical dimensions. We show that finding solutions to bias and discrimination
in AI requires robust cross-disciplinary collaborations.
</summary>
    <author>
      <name>Xavier Ferrer</name>
    </author>
    <author>
      <name>Tom van Nuenen</name>
    </author>
    <author>
      <name>Jose M. Such</name>
    </author>
    <author>
      <name>Mark Coté</name>
    </author>
    <author>
      <name>Natalia Criado</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MTS.2021.3056293</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MTS.2021.3056293" rel="related"/>
    <link href="http://arxiv.org/abs/2008.07309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.07309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11355v2</id>
    <updated>2021-02-11T16:32:35Z</updated>
    <published>2020-08-26T02:53:40Z</published>
    <title>Physically Unclonable Functions and AI: Two Decades of Marriage</title>
    <summary>  The current chapter aims at establishing a relationship between artificial
intelligence (AI) and hardware security. Such a connection between AI and
software security has been confirmed and well-reviewed in the relevant
literature. The main focus here is to explore the methods borrowed from AI to
assess the security of a hardware primitive, namely physically unclonable
functions (PUFs), which has found applications in cryptographic protocols,
e.g., authentication and key generation. Metrics and procedures devised for
this are further discussed. Moreover, By reviewing PUFs designed by applying AI
techniques, we give insight into future research directions in this area.
</summary>
    <author>
      <name>Fatemeh Ganji</name>
    </author>
    <author>
      <name>Shahin Tajik</name>
    </author>
    <link href="http://arxiv.org/abs/2008.11355v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11355v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.01128v1</id>
    <updated>2020-11-30T00:56:11Z</updated>
    <published>2020-11-30T00:56:11Z</published>
    <title>Continuous Subject-in-the-Loop Integration: Centering AI on Marginalized
  Communities</title>
    <summary>  Despite its utopian promises as a disruptive equalizer, AI - like most tools
deployed under the guise of neutrality - has tended to simply reinforce
existing social structures. To counter this trend, radical AI calls for
centering on the marginalized. We argue that gaps in key infrastructure are
preventing the widespread adoption of radical AI, and propose a guiding
principle for both identifying these infrastructure gaps and evaluating whether
proposals for new infrastructure effectively center marginalized voices.
</summary>
    <author>
      <name>Francois Roewer-Despres</name>
    </author>
    <author>
      <name>Janelle Berscheid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Accepted at the Resistance AI Workshop @ NeurIPS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.01128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.01128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.01614v1</id>
    <updated>2020-12-03T00:42:29Z</updated>
    <published>2020-12-03T00:42:29Z</published>
    <title>Explainable AI for Software Engineering</title>
    <summary>  Artificial Intelligence/Machine Learning techniques have been widely used in
software engineering to improve developer productivity, the quality of software
systems, and decision-making. However, such AI/ML models for software
engineering are still impractical, not explainable, and not actionable. These
concerns often hinder the adoption of AI/ML models in software engineering
practices. In this article, we first highlight the need for explainable AI in
software engineering. Then, we summarize three successful case studies on how
explainable AI techniques can be used to address the aforementioned challenges
by making software defect prediction models more practical, explainable, and
actionable.
</summary>
    <author>
      <name>Chakkrit Tantithamthavorn</name>
    </author>
    <author>
      <name>Jirayus Jiarpakdee</name>
    </author>
    <author>
      <name>John Grundy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under Review at IEEE Computer Magazine</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.01614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.01614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.12516v1</id>
    <updated>2021-02-24T19:14:53Z</updated>
    <published>2021-02-24T19:14:53Z</published>
    <title>A Large-Scale, Automated Study of Language Surrounding Artificial
  Intelligence</title>
    <summary>  This work presents a large-scale analysis of artificial intelligence (AI) and
machine learning (ML) references within news articles and scientific
publications between 2011 and 2019. We implement word association measurements
that automatically identify shifts in language co-occurring with AI/ML and
quantify the strength of these word associations. Our results highlight the
evolution of perceptions and definitions around AI/ML and detect emerging
application areas, models, and systems (e.g., blockchain and cybersecurity).
Recent small-scale, manual studies have explored AI/ML discourse within the
general public, the policymaker community, and researcher community, but are
limited in their scalability and longevity. Our methods provide new views into
public perceptions and subject-area expert discussions of AI/ML and greatly
exceed the explanative power of prior work.
</summary>
    <author>
      <name>Autumn Toney</name>
    </author>
    <link href="http://arxiv.org/abs/2102.12516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.12516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12278v2</id>
    <updated>2022-05-02T18:37:08Z</updated>
    <published>2021-04-25T22:09:11Z</published>
    <title>Causal Learning for Socially Responsible AI</title>
    <summary>  There have been increasing concerns about Artificial Intelligence (AI) due to
its unfathomable potential power. To make AI address ethical challenges and
shun undesirable outcomes, researchers proposed to develop socially responsible
AI (SRAI). One of these approaches is causal learning (CL). We survey
state-of-the-art methods of CL for SRAI. We begin by examining the seven CL
tools to enhance the social responsibility of AI, then review how existing
works have succeeded using these tools to tackle issues in developing SRAI such
as fairness. The goal of this survey is to bring forefront the potentials and
promises of CL for SRAI.
</summary>
    <author>
      <name>Lu Cheng</name>
    </author>
    <author>
      <name>Ahmadreza Mosallanezhad</name>
    </author>
    <author>
      <name>Paras Sheth</name>
    </author>
    <author>
      <name>Huan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, accepted at IJCAI21 survey track</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.12278v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12278v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.11828v1</id>
    <updated>2021-05-25T10:53:58Z</updated>
    <published>2021-05-25T10:53:58Z</published>
    <title>Bridging the Gap Between Explainable AI and Uncertainty Quantification
  to Enhance Trustability</title>
    <summary>  After the tremendous advances of deep learning and other AI methods, more
attention is flowing into other properties of modern approaches, such as
interpretability, fairness, etc. combined in frameworks like Responsible AI.
Two research directions, namely Explainable AI and Uncertainty Quantification
are becoming more and more important, but have been so far never combined and
jointly explored. In this paper, I show how both research areas provide
potential for combination, why more research should be done in this direction
and how this would lead to an increase in trustability in AI systems.
</summary>
    <author>
      <name>Dominik Seuß</name>
    </author>
    <link href="http://arxiv.org/abs/2105.11828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.11828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.16122v2</id>
    <updated>2021-11-02T12:27:45Z</updated>
    <published>2021-06-30T15:19:20Z</published>
    <title>Zombies in the Loop? Humans Trust Untrustworthy AI-Advisors for Ethical
  Decisions</title>
    <summary>  Departing from the claim that AI needs to be trustworthy, we find that
ethical advice from an AI-powered algorithm is trusted even when its users know
nothing about its training data and when they learn information about it that
warrants distrust. We conducted online experiments where the subjects took the
role of decision-makers who received advice from an algorithm on how to deal
with an ethical dilemma. We manipulated the information about the algorithm and
studied its influence. Our findings suggest that AI is overtrusted rather than
distrusted. We suggest digital literacy as a potential remedy to ensure the
responsible use of AI.
</summary>
    <author>
      <name>Sebastian Krügel</name>
    </author>
    <author>
      <name>Andreas Ostermaier</name>
    </author>
    <author>
      <name>Matthias Uhl</name>
    </author>
    <link href="http://arxiv.org/abs/2106.16122v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.16122v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.05704v1</id>
    <updated>2021-07-12T19:41:01Z</updated>
    <published>2021-07-12T19:41:01Z</published>
    <title>How Could Equality and Data Protection Law Shape AI Fairness for People
  with Disabilities?</title>
    <summary>  This article examines the concept of 'AI fairness' for people with
disabilities from the perspective of data protection and equality law. This
examination demonstrates that there is a need for a distinctive approach to AI
fairness that is fundamentally different to that used for other protected
characteristics, due to the different ways in which discrimination and data
protection law applies in respect of Disability. We articulate this new agenda
for AI fairness for people with disabilities, explaining how combining data
protection and equality law creates new opportunities for disabled people's
organisations and assistive technology researchers alike to shape the use of
AI, as well as to challenge potential harmful uses.
</summary>
    <author>
      <name>Reuben Binns</name>
    </author>
    <author>
      <name>Reuben Kirkham</name>
    </author>
    <link href="http://arxiv.org/abs/2107.05704v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.05704v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.02730v1</id>
    <updated>2022-01-05T09:15:14Z</updated>
    <published>2022-01-05T09:15:14Z</published>
    <title>AI for Beyond 5G Networks: A Cyber-Security Defense or Offense Enabler?</title>
    <summary>  Artificial Intelligence (AI) is envisioned to play a pivotal role in
empowering intelligent, adaptive and autonomous security management in 5G and
beyond networks, thanks to its potential to uncover hidden patterns from a
large set of time-varying multi-dimensional data, and deliver faster and
accurate decisions. Unfortunately, AI's capabilities and vulnerabilities make
it a double-edged sword that may jeopardize the security of future networks.
This paper sheds light on how AI may impact the security of 5G and its
successive from its posture of defender, offender or victim, and recommends
potential defenses to safeguard from malevolent AI while pointing out their
limitations and adoption challenges.
</summary>
    <author>
      <name>C. Benzaid</name>
    </author>
    <author>
      <name>T. Taleb</name>
    </author>
    <link href="http://arxiv.org/abs/2201.02730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.02730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.05418v1</id>
    <updated>2022-06-11T04:19:51Z</updated>
    <published>2022-06-11T04:19:51Z</published>
    <title>SAIBench: Benchmarking AI for Science</title>
    <summary>  Scientific research communities are embracing AI-based solutions to target
tractable scientific tasks and improve research workflows. However, the
development and evaluation of such solutions are scattered across multiple
disciplines. We formalize the problem of scientific AI benchmarking, and
propose a system called SAIBench in the hope of unifying the efforts and
enabling low-friction on-boarding of new disciplines. The system approaches
this goal with SAIL, a domain-specific language to decouple research problems,
AI models, ranking criteria, and software/hardware configuration into reusable
modules. We show that this approach is flexible and can adapt to problems, AI
models, and evaluation methods defined in different perspectives. The project
homepage is https://www.computercouncil.org/SAIBench
</summary>
    <author>
      <name>Yatao Li</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.tbench.2022.100063</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.tbench.2022.100063" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in BenchCouncil Transactions on Benchmarks, Standards and
  Evaluations (TBench)</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.05418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.05418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.00682v2</id>
    <updated>2022-07-29T18:51:23Z</updated>
    <published>2022-07-01T23:10:40Z</published>
    <title>The NPC AI of The Last of Us: A case study</title>
    <summary>  The Last of Us is a game focused on stealth, companionship and strategy. The
game is based in a lonely world after the pandemic and thus it needs AI
companions to gain the interest of players. There are three main NPCs the game
has - Infected, Human enemy and Buddy AIs. This case study talks about the
challenges in front of the developers to create AI for these NPCs and the AI
techniques they used to solve them. It also compares the challenges and
approach with similar industry-leading games.
</summary>
    <author>
      <name>Harsh Panwar</name>
    </author>
    <link href="http://arxiv.org/abs/2207.00682v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.00682v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.04697v1</id>
    <updated>2022-07-26T13:37:13Z</updated>
    <published>2022-07-26T13:37:13Z</published>
    <title>Let it RAIN for Social Good</title>
    <summary>  Artificial Intelligence (AI) as a highly transformative technology take on a
special role as both an enabler and a threat to UN Sustainable Development
Goals (SDGs). AI Ethics and emerging high-level policy efforts stand at the
pivot point between these outcomes but is barred from effect due the
abstraction gap between high-level values and responsible action. In this paper
the Responsible Norms (RAIN) framework is presented, bridging this gap thereby
enabling effective high-level control of AI impact. With effective and
operationalized AI Ethics, AI technologies can be directed towards global
sustainable development.
</summary>
    <author>
      <name>Mattias Brännström</name>
    </author>
    <author>
      <name>Andreas Theodorou</name>
    </author>
    <author>
      <name>Virginia Dignum</name>
    </author>
    <link href="http://arxiv.org/abs/2208.04697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.04697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.07476v1</id>
    <updated>2022-08-16T00:16:58Z</updated>
    <published>2022-08-16T00:16:58Z</published>
    <title>CTI4AI: Threat Intelligence Generation and Sharing after Red Teaming AI
  Models</title>
    <summary>  As the practicality of Artificial Intelligence (AI) and Machine Learning (ML)
based techniques grow, there is an ever increasing threat of adversarial
attacks. There is a need to red team this ecosystem to identify system
vulnerabilities, potential threats, characterize properties that will enhance
system robustness, and encourage the creation of effective defenses. A
secondary need is to share this AI security threat intelligence between
different stakeholders like, model developers, users, and AI/ML security
professionals. In this paper, we create and describe a prototype system CTI4AI,
to overcome the need to methodically identify and share AI/ML specific
vulnerabilities and threat intelligence.
</summary>
    <author>
      <name>Chuyen Nguyen</name>
    </author>
    <author>
      <name>Caleb Morgan</name>
    </author>
    <author>
      <name>Sudip Mittal</name>
    </author>
    <link href="http://arxiv.org/abs/2208.07476v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.07476v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.12434v1</id>
    <updated>2022-11-22T17:44:03Z</updated>
    <published>2022-11-22T17:44:03Z</published>
    <title>Expansive Participatory AI: Supporting Dreaming within Inequitable
  Institutions</title>
    <summary>  Participatory Artificial Intelligence (PAI) has recently gained interest by
researchers as means to inform the design of technology through collective's
lived experience. PAI has a greater promise than that of providing useful input
to developers, it can contribute to the process of democratizing the design of
technology, setting the focus on what should be designed. However, in the
process of PAI there existing institutional power dynamics that hinder the
realization of expansive dreams and aspirations of the relevant stakeholders.
In this work we propose co-design principals for AI that address institutional
power dynamics focusing on Participatory AI with youth.
</summary>
    <author>
      <name>Michael Alan Chang</name>
    </author>
    <author>
      <name>Shiran Dudy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, Human-Centered AI workshop</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Human-Centered AI workshop (HCAI) 2022, NEURIPS</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2211.12434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.12434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.03109v1</id>
    <updated>2022-12-03T10:22:31Z</updated>
    <published>2022-12-03T10:22:31Z</published>
    <title>Risk management in the Artificial Intelligence Act</title>
    <summary>  The proposed EU AI Act is the first comprehensive attempt to regulate AI in a
major jurisdiction. This article analyses Article 9, the key risk management
provision in the AI Act. It gives an overview of the regulatory concept behind
Article 9, determines its purpose and scope of application, offers a
comprehensive interpretation of the specific risk management requirements, and
outlines ways in which the requirements can be enforced. This article is
written with the aim of helping providers of high-risk systems comply with the
requirements set out in Article 9. In addition, it can inform revisions of the
current draft of the AI Act and efforts to develop harmonised standards on AI
risk management.
</summary>
    <author>
      <name>Jonas Schuett</name>
    </author>
    <link href="http://arxiv.org/abs/2212.03109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.03109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.11854v1</id>
    <updated>2022-12-22T16:41:03Z</updated>
    <published>2022-12-22T16:41:03Z</published>
    <title>Data-centric Artificial Intelligence</title>
    <summary>  Data-centric artificial intelligence (data-centric AI) represents an emerging
paradigm emphasizing that the systematic design and engineering of data is
essential for building effective and efficient AI-based systems. The objective
of this article is to introduce practitioners and researchers from the field of
Information Systems (IS) to data-centric AI. We define relevant terms, provide
key characteristics to contrast the data-centric paradigm to the model-centric
one, and introduce a framework for data-centric AI. We distinguish data-centric
AI from related concepts and discuss its longer-term implications for the IS
community.
</summary>
    <author>
      <name>Johannes Jakubik</name>
    </author>
    <author>
      <name>Michael Vössing</name>
    </author>
    <author>
      <name>Niklas Kühl</name>
    </author>
    <author>
      <name>Jannis Walk</name>
    </author>
    <author>
      <name>Gerhard Satzger</name>
    </author>
    <link href="http://arxiv.org/abs/2212.11854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.11854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.06590v1</id>
    <updated>2023-02-13T18:42:46Z</updated>
    <published>2023-02-13T18:42:46Z</published>
    <title>The Impact of AI on Developer Productivity: Evidence from GitHub Copilot</title>
    <summary>  Generative AI tools hold promise to increase human productivity. This paper
presents results from a controlled experiment with GitHub Copilot, an AI pair
programmer. Recruited software developers were asked to implement an HTTP
server in JavaScript as quickly as possible. The treatment group, with access
to the AI pair programmer, completed the task 55.8% faster than the control
group. Observed heterogenous effects show promise for AI pair programmers to
help people transition into software development careers.
</summary>
    <author>
      <name>Sida Peng</name>
    </author>
    <author>
      <name>Eirini Kalliamvakou</name>
    </author>
    <author>
      <name>Peter Cihon</name>
    </author>
    <author>
      <name>Mert Demirer</name>
    </author>
    <link href="http://arxiv.org/abs/2302.06590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.06590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.10816v1</id>
    <updated>2023-02-21T16:48:59Z</updated>
    <published>2023-02-21T16:48:59Z</published>
    <title>Tailoring Requirements Engineering for Responsible AI</title>
    <summary>  Requirements Engineering (RE) is the discipline for identifying, analyzing,
as well as ensuring the implementation and delivery of user, technical, and
societal requirements. Recently reported issues concerning the acceptance of
Artificial Intelligence (AI) solutions after deployment, e.g. in the medical,
automotive, or scientific domains, stress the importance of RE for designing
and delivering Responsible AI systems. In this paper, we argue that RE should
not only be carefully conducted but also tailored for Responsible AI. We
outline related challenges for research and practice.
</summary>
    <author>
      <name>Walid Maalej</name>
    </author>
    <author>
      <name>Yen Dieu Pham</name>
    </author>
    <author>
      <name>Larissa Chazette</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MC.2023.3243182</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MC.2023.3243182" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE Computer, Special Issue on Software Engineering for
  Responsible AI</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.10816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.10816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.04574v1</id>
    <updated>2017-09-14T01:27:44Z</updated>
    <published>2017-09-14T01:27:44Z</published>
    <title>Towards personalized human AI interaction - adapting the behavior of AI
  agents using neural signatures of subjective interest</title>
    <summary>  Reinforcement Learning AI commonly uses reward/penalty signals that are
objective and explicit in an environment -- e.g. game score, completion time,
etc. -- in order to learn the optimal strategy for task performance. However,
Human-AI interaction for such AI agents should include additional reinforcement
that is implicit and subjective -- e.g. human preferences for certain AI
behavior -- in order to adapt the AI behavior to idiosyncratic human
preferences. Such adaptations would mirror naturally occurring processes that
increase trust and comfort during social interactions. Here, we show how a
hybrid brain-computer-interface (hBCI), which detects an individual's level of
interest in objects/events in a virtual environment, can be used to adapt the
behavior of a Deep Reinforcement Learning AI agent that is controlling a
virtual autonomous vehicle. Specifically, we show that the AI learns a driving
strategy that maintains a safe distance from a lead vehicle, and most novelly,
preferentially slows the vehicle when the human passengers of the vehicle
encounter objects of interest. This adaptation affords an additional 20\%
viewing time for subjectively interesting objects. This is the first
demonstration of how an hBCI can be used to provide implicit reinforcement to
an AI agent in a way that incorporates user preferences into the control
system.
</summary>
    <author>
      <name>Victor Shih</name>
    </author>
    <author>
      <name>David C Jangraw</name>
    </author>
    <author>
      <name>Paul Sajda</name>
    </author>
    <author>
      <name>Sameer Saproo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures, 1 table, Submitted to IEEE Trans. on Neural
  Networks and Learning Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.04574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.04574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.08837v1</id>
    <updated>2020-03-18T12:33:59Z</updated>
    <published>2020-03-18T12:33:59Z</published>
    <title>Vulnerabilities of Connectionist AI Applications: Evaluation and Defence</title>
    <summary>  This article deals with the IT security of connectionist artificial
intelligence (AI) applications, focusing on threats to integrity, one of the
three IT security goals. Such threats are for instance most relevant in
prominent AI computer vision applications. In order to present a holistic view
on the IT security goal integrity, many additional aspects such as
interpretability, robustness and documentation are taken into account. A
comprehensive list of threats and possible mitigations is presented by
reviewing the state-of-the-art literature. AI-specific vulnerabilities such as
adversarial attacks and poisoning attacks as well as their AI-specific root
causes are discussed in detail. Additionally and in contrast to former reviews,
the whole AI supply chain is analysed with respect to vulnerabilities,
including the planning, data acquisition, training, evaluation and operation
phases. The discussion of mitigations is likewise not restricted to the level
of the AI system itself but rather advocates viewing AI systems in the context
of their supply chains and their embeddings in larger IT infrastructures and
hardware devices. Based on this and the observation that adaptive attackers may
circumvent any single published AI-specific defence to date, the article
concludes that single protective measures are not sufficient but rather
multiple measures on different levels have to be combined to achieve a minimum
level of IT security for AI applications.
</summary>
    <author>
      <name>Christian Berghoff</name>
    </author>
    <author>
      <name>Matthias Neu</name>
    </author>
    <author>
      <name>Arndt von Twickel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/fdata.2020.00023</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/fdata.2020.00023" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 8 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.08837v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08837v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.00501v1</id>
    <updated>2019-04-30T08:43:50Z</updated>
    <published>2019-04-30T08:43:50Z</published>
    <title>The role of artificial intelligence in achieving the Sustainable
  Development Goals</title>
    <summary>  The emergence of artificial intelligence (AI) and its progressively wider
impact on many sectors across the society requires an assessment of its effect
on sustainable development. Here we analyze published evidence of positive or
negative impacts of AI on the achievement of each of the 17 goals and 169
targets of the 2030 Agenda for Sustainable Development. We find that AI can
support the achievement of 128 targets across all SDGs, but it may also inhibit
58 targets. Notably, AI enables new technologies that improve efficiency and
productivity, but it may also lead to increased inequalities among and within
countries, thus hindering the achievement of the 2030 Agenda. The fast
development of AI needs to be supported by appropriate policy and regulation.
Otherwise, it would lead to gaps in transparency, accountability, safety and
ethical standards of AI-based technology, which could be detrimental towards
the development and sustainable use of AI. Finally, there is a lack of research
assessing the medium- and long-term impacts of AI. It is therefore essential to
reinforce the global debate regarding the use of AI and to develop the
necessary regulatory insight and oversight for AI-based technologies.
</summary>
    <author>
      <name>Ricardo Vinuesa</name>
    </author>
    <author>
      <name>Hossein Azizpour</name>
    </author>
    <author>
      <name>Iolanda Leite</name>
    </author>
    <author>
      <name>Madeline Balaam</name>
    </author>
    <author>
      <name>Virginia Dignum</name>
    </author>
    <author>
      <name>Sami Domisch</name>
    </author>
    <author>
      <name>Anna Felländer</name>
    </author>
    <author>
      <name>Simone Langhans</name>
    </author>
    <author>
      <name>Max Tegmark</name>
    </author>
    <author>
      <name>Francesco Fuso Nerini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41467-019-14108-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41467-019-14108-y" rel="related"/>
    <link href="http://arxiv.org/abs/1905.00501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.00501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.03843v2</id>
    <updated>2020-12-22T18:11:35Z</updated>
    <published>2019-06-26T10:18:19Z</published>
    <title>Norms for Beneficial A.I.: A Computational Analysis of the Societal
  Value Alignment Problem</title>
    <summary>  The rise of artificial intelligence (A.I.) based systems is already offering
substantial benefits to the society as a whole. However, these systems may also
enclose potential conflicts and unintended consequences. Notably, people will
tend to adopt an A.I. system if it confers them an advantage, at which point
non-adopters might push for a strong regulation if that advantage for adopters
is at a cost for them. Here we propose an agent-based game-theoretical model
for these conflicts, where agents may decide to resort to A.I. to use and
acquire additional information on the payoffs of a stochastic game, striving to
bring insights from simulation to what has been, hitherto, a mostly
philosophical discussion. We frame our results under the current discussion on
ethical A.I. and the conflict between individual and societal gains: the
societal value alignment problem. We test the arising equilibria in the
adoption of A.I. technology under different norms followed by artificial
agents, their ensuing benefits, and the emergent levels of wealth inequality.
We show that without any regulation, purely selfish A.I. systems will have the
strongest advantage, even when a utilitarian A.I. provides significant benefits
for the individual and the society. Nevertheless, we show that it is possible
to develop A.I. systems following human conscious policies that, when
introduced in society, lead to an equilibrium where the gains for the adopters
are not at a cost for non-adopters, thus increasing the overall wealth of the
population and lowering inequality. However, as shown, a self-organised
adoption of such policies would require external regulation.
</summary>
    <author>
      <name>Pedro Fernandes</name>
    </author>
    <author>
      <name>Francisco C. Santos</name>
    </author>
    <author>
      <name>Manuel Lopes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/AIC-201502</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/AIC-201502" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AI Communications, vol. 33, no. 3-6, pp. 155-171, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1907.03843v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.03843v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.12544v1</id>
    <updated>2019-10-28T10:40:15Z</updated>
    <published>2019-10-28T10:40:15Z</published>
    <title>Human-AI Co-Learning for Data-Driven AI</title>
    <summary>  Human and AI are increasingly interacting and collaborating to accomplish
various complex tasks in the context of diverse application domains (e.g.,
healthcare, transportation, and creative design). Two dynamic, learning
entities (AI and human) have distinct mental model, expertise, and ability;
such fundamental difference/mismatch offers opportunities for bringing new
perspectives to achieve better results. However, this mismatch can cause
unexpected failure and result in serious consequences. While recent research
has paid much attention to enhancing interpretability or explainability to
allow machine to explain how it makes a decision for supporting humans, this
research argues that there is urging the need for both human and AI should
develop specific, corresponding ability to interact and collaborate with each
other to form a human-AI team to accomplish superior results. This research
introduces a conceptual framework called "Co-Learning," in which people can
learn with/from and grow with AI partners over time. We characterize three key
concepts of co-learning: "mutual understanding," "mutual benefits," and "mutual
growth" for facilitating human-AI collaboration on complex problem solving. We
will present proof-of-concepts to investigate whether and how our approach can
help human-AI team to understand and benefit each other, and ultimately improve
productivity and creativity on creative problem domains. The insights will
contribute to the design of Human-AI collaboration.
</summary>
    <author>
      <name>Yi-Ching Huang</name>
    </author>
    <author>
      <name>Yu-Ting Cheng</name>
    </author>
    <author>
      <name>Lin-Lin Chen</name>
    </author>
    <author>
      <name>Jane Yung-jen Hsu</name>
    </author>
    <link href="http://arxiv.org/abs/1910.12544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.12544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.16486v2</id>
    <updated>2020-10-01T15:55:36Z</updated>
    <published>2020-06-30T02:36:56Z</published>
    <title>Modeling tau transport in the axon initial segment</title>
    <summary>  By assuming that tau protein can be in seven kinetic states, we developed a
model of tau protein transport in the axon and in the axon initial segment
(AIS). Two separate sets of kinetic constants were determined, one in the axon
and the other in the AIS. This was done by fitting the model predictions in the
axon with experimental results and by fitting the model predictions in the AIS
with the assumed linear increase of the total tau concentration in the AIS. The
calibrated model was used to make predictions about tau transport in the axon
and in the AIS. To the best of our knowledge, this is the first paper that
presents a mathematical model of tau transport in the AIS. Our modeling results
suggest that binding of free tau to MTs creates a negative gradient of free tau
in the AIS. This leads to diffusion-driven tau transport from the soma into the
AIS. The model further suggests that slow axonal transport and diffusion-driven
transport of tau work together in the AIS, moving tau anterogradely. Our
numerical results predict an interplay between these two mechanisms: as the
distance from the soma increases, the diffusion-driven transport decreases,
while motor-driven transport becomes larger. Thus, the machinery in the AIS
works as a pump, moving tau into the axon.
</summary>
    <author>
      <name>Ivan A. Kuznetsov</name>
    </author>
    <author>
      <name>Andrey V. Kuznetsov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.mbs.2020.108468</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.mbs.2020.108468" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">final accepted version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Mathematical Biosciences, vol. 329, article # 108468, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2006.16486v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.16486v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.SC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.SC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.07437v1</id>
    <updated>2020-10-14T23:37:15Z</updated>
    <published>2020-10-14T23:37:15Z</published>
    <title>Tracking Results and Utilization of Artificial Intelligence (tru-AI) in
  Radiology: Early-Stage COVID-19 Pandemic Observations</title>
    <summary>  Objective: To introduce a method for tracking results and utilization of
Artificial Intelligence (tru-AI) in radiology. By tracking both large-scale
utilization and AI results data, the tru-AI approach is designed to calculate
surrogates for measuring important disease-related observational quantities
over time, such as the prevalence of intracranial hemorrhage during the
COVID-19 pandemic outbreak. Methods: To quantitatively investigate the clinical
applicability of the tru-AI approach, we analyzed service requests for
automatically identifying intracranial hemorrhage (ICH) on head CT using a
commercial AI solution. This software is typically used for AI-based
prioritization of radiologists' reading lists for reducing turnaround times in
patients with emergent clinical findings, such as ICH or pulmonary embolism.We
analyzed data of N=9,421 emergency-setting non-contrast head CT studies at a
major US healthcare system acquired from November 1, 2019 through June 2, 2020,
and compared two observation periods, namely (i) a pre-pandemic epoch from
November 1, 2019 through February 29, 2020, and (ii) a period during the
COVID-19 pandemic outbreak, April 1-30, 2020. Results: Although daily CT scan
counts were significantly lower during (40.1 +/- 7.9) than before (44.4 +/-
7.6) the COVID-19 outbreak, we found that ICH was more likely to be observed by
AI during than before the COVID-19 outbreak (p&lt;0.05), with approximately one
daily ICH+ case more than statistically expected. Conclusion: Our results
suggest that, by tracking both large-scale utilization and AI results data in
radiology, the tru-AI approach can contribute clinical value as a versatile
exploratory tool, aiming at a better understanding of pandemic-related effects
on healthcare.
</summary>
    <author>
      <name>Axel Wismüller</name>
    </author>
    <author>
      <name>Larry Stockmaster</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.07437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.07437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.07938v2</id>
    <updated>2022-04-04T22:42:04Z</updated>
    <published>2020-10-15T22:25:41Z</published>
    <title>Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted
  Decision-making</title>
    <summary>  Several strands of research have aimed to bridge the gap between artificial
intelligence (AI) and human decision-makers in AI-assisted decision-making,
where humans are the consumers of AI model predictions and the ultimate
decision-makers in high-stakes applications. However, people's perception and
understanding are often distorted by their cognitive biases, such as
confirmation bias, anchoring bias, availability bias, to name a few. In this
work, we use knowledge from the field of cognitive science to account for
cognitive biases in the human-AI collaborative decision-making setting, and
mitigate their negative effects on collaborative performance. To this end, we
mathematically model cognitive biases and provide a general framework through
which researchers and practitioners can understand the interplay between
cognitive biases and human-AI accuracy. We then focus specifically on anchoring
bias, a bias commonly encountered in human-AI collaboration. We implement a
time-based de-anchoring strategy and conduct our first user experiment that
validates its effectiveness in human-AI collaborative decision-making. With
this result, we design a time allocation strategy for a resource-constrained
setting that achieves optimal human-AI collaboration under some assumptions.
We, then, conduct a second user experiment which shows that our time allocation
strategy with explanation can effectively de-anchor the human and improve
collaborative performance when the AI model has low confidence and is
incorrect.
</summary>
    <author>
      <name>Charvi Rastogi</name>
    </author>
    <author>
      <name>Yunfeng Zhang</name>
    </author>
    <author>
      <name>Dennis Wei</name>
    </author>
    <author>
      <name>Kush R. Varshney</name>
    </author>
    <author>
      <name>Amit Dhurandhar</name>
    </author>
    <author>
      <name>Richard Tomsett</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.07938v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.07938v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.09610v1</id>
    <updated>2020-10-30T20:33:05Z</updated>
    <published>2020-10-30T20:33:05Z</published>
    <title>Validate and Enable Machine Learning in Industrial AI</title>
    <summary>  Industrial Artificial Intelligence (Industrial AI) is an emerging concept
which refers to the application of artificial intelligence to industry.
Industrial AI promises more efficient future industrial control systems.
However, manufacturers and solution partners need to understand how to
implement and integrate an AI model into the existing industrial control
system. A well-trained machine learning (ML) model provides many benefits and
opportunities for industrial control optimization; however, an inferior
Industrial AI design and integration limits the capability of ML models. To
better understand how to develop and integrate trained ML models into the
traditional industrial control system, test the deployed AI control system, and
ultimately outperform traditional systems, manufacturers and their AI solution
partners need to address a number of challenges. Six top challenges, which were
real problems we ran into when deploying Industrial AI, are explored in the
paper. The Petuum Optimum system is used as an example to showcase the
challenges in making and testing AI models, and more importantly, how to
address such challenges in an Industrial AI system.
</summary>
    <author>
      <name>Hongbo Zou</name>
    </author>
    <author>
      <name>Guangjing Chen</name>
    </author>
    <author>
      <name>Pengtao Xie</name>
    </author>
    <author>
      <name>Sean Chen</name>
    </author>
    <author>
      <name>Yongtian He</name>
    </author>
    <author>
      <name>Hochih Huang</name>
    </author>
    <author>
      <name>Zheng Nie</name>
    </author>
    <author>
      <name>Hongbao Zhang</name>
    </author>
    <author>
      <name>Tristan Bala</name>
    </author>
    <author>
      <name>Kazi Tulip</name>
    </author>
    <author>
      <name>Yuqi Wang</name>
    </author>
    <author>
      <name>Shenlin Qin</name>
    </author>
    <author>
      <name>Eric P. Xing</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.09610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.09610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.09364v1</id>
    <updated>2021-02-11T21:29:25Z</updated>
    <published>2021-02-11T21:29:25Z</published>
    <title>Ethics as a service: a pragmatic operationalisation of AI Ethics</title>
    <summary>  As the range of potential uses for Artificial Intelligence (AI), in
particular machine learning (ML), has increased, so has awareness of the
associated ethical issues. This increased awareness has led to the realisation
that existing legislation and regulation provides insufficient protection to
individuals, groups, society, and the environment from AI harms. In response to
this realisation, there has been a proliferation of principle-based ethics
codes, guidelines and frameworks. However, it has become increasingly clear
that a significant gap exists between the theory of AI ethics principles and
the practical design of AI systems. In previous work, we analysed whether it is
possible to close this gap between the what and the how of AI ethics through
the use of tools and methods designed to help AI developers, engineers, and
designers translate principles into practice. We concluded that this method of
closure is currently ineffective as almost all existing translational tools and
methods are either too flexible (and thus vulnerable to ethics washing) or too
strict (unresponsive to context). This raised the question: if, even with
technical guidance, AI ethics is challenging to embed in the process of
algorithmic design, is the entire pro-ethical design endeavour rendered futile?
And, if no, then how can AI ethics be made useful for AI practitioners? This is
the question we seek to address here by exploring why principles and technical
translational tools are still needed even if they are limited, and how these
limitations can be potentially overcome by providing theoretical grounding of a
concept that has been termed Ethics as a Service.
</summary>
    <author>
      <name>Jessica Morley</name>
    </author>
    <author>
      <name>Anat Elhalal</name>
    </author>
    <author>
      <name>Francesca Garcia</name>
    </author>
    <author>
      <name>Libby Kinsey</name>
    </author>
    <author>
      <name>Jakob Mokander</name>
    </author>
    <author>
      <name>Luciano Floridi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, first draft</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.09364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.09364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.02943v1</id>
    <updated>2021-03-04T10:49:47Z</updated>
    <published>2021-03-04T10:49:47Z</published>
    <title>The Dota 2 Bot Competition</title>
    <summary>  Multiplayer Online Battle Area (MOBA) games are a recent huge success both in
the video game industry and the international eSports scene. These games
encourage team coordination and cooperation, short and long-term planning,
within a real-time combined action and strategy gameplay.
  Artificial Intelligence and Computational Intelligence in Games research
competitions offer a wide variety of challenges regarding the study and
application of AI techniques to different game genres. These events are widely
accepted by the AI/CI community as a sort of AI benchmarking that strongly
influences many other research areas in the field.
  This paper presents and describes in detail the Dota 2 Bot competition and
the Dota 2 AI framework that supports it. This challenge aims to join both,
MOBAs and AI/CI game competitions, inviting participants to submit AI
controllers for the successful MOBA \textit{Defense of the Ancients 2} (Dota 2)
to play in 1v1 matches, which aims for fostering research on AI techniques for
real-time games. The Dota 2 AI framework makes use of the actual Dota 2 game
modding capabilities to enable to connect external AI controllers to actual
Dota 2 game matches using the original Free-to-Play game.se of the actual Dota
2 game modding capabilities to enable to connect external AI controllers to
actual Dota 2 game matches using the original Free-to-Play game.
</summary>
    <author>
      <name>Jose M. Font</name>
    </author>
    <author>
      <name>Tobias Mahlmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TG.2018.2834566</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TG.2018.2834566" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Games 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.02943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.02943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.13520v2</id>
    <updated>2021-04-14T19:57:57Z</updated>
    <published>2021-03-24T23:07:47Z</published>
    <title>The Duality of Data and Knowledge Across the Three Waves of AI</title>
    <summary>  We discuss how over the last 30 to 50 years, Artificial Intelligence (AI)
systems that focused only on data have been handicapped, and how knowledge has
been critical in developing smarter, intelligent, and more effective systems.
In fact, the vast progress in AI can be viewed in terms of the three waves of
AI as identified by DARPA. During the first wave, handcrafted knowledge has
been at the center-piece, while during the second wave, the data-driven
approaches supplanted knowledge. Now we see a strong role and resurgence of
knowledge fueling major breakthroughs in the third wave of AI underpinning
future intelligent systems as they attempt human-like decision making, and seek
to become trusted assistants and companions for humans. We find a wider
availability of knowledge created from diverse sources, using manual to
automated means both by repurposing as well as by extraction. Using knowledge
with statistical learning is becoming increasingly indispensable to help make
AI systems more transparent and auditable. We will draw a parallel with the
role of knowledge and experience in human intelligence based on cognitive
science, and discuss emerging neuro-symbolic or hybrid AI systems in which
knowledge is the critical enabler for combining capabilities of the
data-intensive statistical AI systems with those of symbolic AI systems,
resulting in more capable AI systems that support more human-like intelligence.
</summary>
    <author>
      <name>Amit Sheth</name>
    </author>
    <author>
      <name>Krishnaprasad Thirunarayan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MITP.2021.3070985</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MITP.2021.3070985" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A version of this will appear as (cite as): IT Professional Magazine
  (special section to commemorate the 75th Anniversary of IEEE Computer
  Society), 23 (3) April-May 2021</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IT Professional, 23 (3), April-May 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.13520v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.13520v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.11000v1</id>
    <updated>2021-05-23T19:06:18Z</updated>
    <published>2021-05-23T19:06:18Z</published>
    <title>Who/What is My Teammate? Team Composition Considerations in Human-AI
  Teaming</title>
    <summary>  There are many unknowns regarding the characteristics and dynamics of
human-AI teams, including a lack of understanding of how certain human-human
teaming concepts may or may not apply to human-AI teams and how this
composition affects team performance. This paper outlines an experimental
research study that investigates essential aspects of human-AI teaming such as
team performance, team situation awareness, and perceived team cognition in
various mixed composition teams (human-only, human-human-AI, human-AI-AI, and
AI-only) through a simulated emergency response management scenario. Results
indicate dichotomous outcomes regarding perceived team cognition and
performance metrics, as perceived team cognition was not predictive of
performance. Performance metrics like team situational awareness and team score
showed that teams composed of all human participants performed at a lower level
than mixed human-AI teams, with the AI-only teams attaining the highest
performance. Perceived team cognition was highest in human-only teams, with
mixed composition teams reporting perceived team cognition 58% below the
all-human teams. These results inform future mixed teams of the potential
performance gains in utilizing mixed teams' over human-only teams in certain
applications, while also highlighting mixed teams' adverse effects on perceived
team cognition.
</summary>
    <author>
      <name>Nathan J. McNeese</name>
    </author>
    <author>
      <name>Beau G. Schelble</name>
    </author>
    <author>
      <name>Lorenzo Barberis Canonico</name>
    </author>
    <author>
      <name>Mustafa Demir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages, 6 Figures, IEEE Transactions on Human-Machine Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.11000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.11000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.01174v1</id>
    <updated>2021-08-02T21:12:30Z</updated>
    <published>2021-08-02T21:12:30Z</published>
    <title>Knowledge-intensive Language Understanding for Explainable AI</title>
    <summary>  AI systems have seen significant adoption in various domains. At the same
time, further adoption in some domains is hindered by inability to fully trust
an AI system that it will not harm a human. Besides the concerns for fairness,
privacy, transparency, and explainability are key to developing trusts in AI
systems. As stated in describing trustworthy AI "Trust comes through
understanding. How AI-led decisions are made and what determining factors were
included are crucial to understand." The subarea of explaining AI systems has
come to be known as XAI. Multiple aspects of an AI system can be explained;
these include biases that the data might have, lack of data points in a
particular region of the example space, fairness of gathering the data, feature
importances, etc. However, besides these, it is critical to have human-centered
explanations that are directly related to decision-making similar to how a
domain expert makes decisions based on "domain knowledge," that also include
well-established, peer-validated explicit guidelines. To understand and
validate an AI system's outcomes (such as classification, recommendations,
predictions), that lead to developing trust in the AI system, it is necessary
to involve explicit domain knowledge that humans understand and use.
</summary>
    <author>
      <name>Amit Sheth</name>
    </author>
    <author>
      <name>Manas Gaur</name>
    </author>
    <author>
      <name>Kaushik Roy</name>
    </author>
    <author>
      <name>Keyur Faldu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in IEEE Internet Computing, September/October 2021 Issue</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.01174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.01174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.09904v2</id>
    <updated>2021-12-09T20:43:45Z</updated>
    <published>2021-09-21T01:30:06Z</published>
    <title>Symbols as a Lingua Franca for Bridging Human-AI Chasm for Explainable
  and Advisable AI Systems</title>
    <summary>  Despite the surprising power of many modern AI systems that often learn their
own representations, there is significant discontent about their inscrutability
and the attendant problems in their ability to interact with humans. While
alternatives such as neuro-symbolic approaches have been proposed, there is a
lack of consensus on what they are about. There are often two independent
motivations (i) symbols as a lingua franca for human-AI interaction and (ii)
symbols as system-produced abstractions used by the AI system in its internal
reasoning. The jury is still out on whether AI systems will need to use symbols
in their internal reasoning to achieve general intelligence capabilities.
Whatever the answer there is, the need for (human-understandable) symbols in
human-AI interaction seems quite compelling. Symbols, like emotions, may well
not be sine qua non for intelligence per se, but they will be crucial for AI
systems to interact with us humans -- as we can neither turn off our emotions
nor get by without our symbols. In particular, in many human-designed domains,
humans would be interested in providing explicit (symbolic) knowledge and
advice -- and expect machine explanations in kind. This alone requires AI
systems to to maintain a symbolic interface for interaction with humans. In
this blue sky paper, we argue this point of view, and discuss research
directions that need to be pursued to allow for this type of human-AI
interaction.
</summary>
    <author>
      <name>Subbarao Kambhampati</name>
    </author>
    <author>
      <name>Sarath Sreedharan</name>
    </author>
    <author>
      <name>Mudit Verma</name>
    </author>
    <author>
      <name>Yantian Zha</name>
    </author>
    <author>
      <name>Lin Guan</name>
    </author>
    <link href="http://arxiv.org/abs/2109.09904v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.09904v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.15284v1</id>
    <updated>2021-09-28T07:30:28Z</updated>
    <published>2021-09-28T07:30:28Z</published>
    <title>Which Design Decisions in AI-enabled Mobile Applications Contribute to
  Greener AI?</title>
    <summary>  Background: The construction, evolution and usage of complex artificial
intelligence (AI) models demand expensive computational resources. While
currently available high-performance computing environments support well this
complexity, the deployment of AI models in mobile devices, which is an
increasing trend, is challenging. Mobile applications consist of environments
with low computational resources and hence imply limitations in the design
decisions during the AI-enabled software engineering lifecycle that balance the
trade-off between the accuracy and the complexity of the mobile applications.
  Objective: Our objective is to systematically assess the trade-off between
accuracy and complexity when deploying complex AI models (e.g. neural networks)
to mobile devices, which have an implicit resource limitation. We aim to cover
(i) the impact of the design decisions on the achievement of high-accuracy and
low resource-consumption implementations; and (ii) the validation of profiling
tools for systematically promoting greener AI.
  Method: This confirmatory registered report consists of a plan to conduct an
empirical study to quantify the implications of the design decisions on
AI-enabled applications performance and to report experiences of the end-to-end
AI-enabled software engineering lifecycle. Concretely, we will implement both
image-based and language-based neural networks in mobile applications to solve
multiple image classification and text classification problems on different
benchmark datasets. Overall, we plan to model the accuracy and complexity of
AI-enabled applications in operation with respect to their design decisions and
will provide tools for allowing practitioners to gain consciousness of the
quantitative relationship between the design decisions and the green
characteristics of study.
</summary>
    <author>
      <name>Roger Creus Castanyer</name>
    </author>
    <author>
      <name>Silverio Martínez-Fernández</name>
    </author>
    <author>
      <name>Xavier Franch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as registered report at ESEM 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.15284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.15284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.00931v4</id>
    <updated>2022-07-06T09:14:12Z</updated>
    <published>2021-10-03T06:01:45Z</published>
    <title>Exploration of Artificial Intelligence-oriented Power System Dynamic
  Simulators</title>
    <summary>  With the rapid development of artificial intelligence (AI), it is foreseeable
that the accuracy and efficiency of dynamic analysis for future power system
will be greatly improved by the integration of dynamic simulators and AI. To
explore the interaction mechanism of power system dynamic simulations and AI, a
general design of an AI-oriented power system dynamic simulator is proposed,
which consists of a high-performance simulator with neural network
supportability and flexible external and internal application programming
interfaces (APIs). With the support of APIs, simulation-assisted AI and
AI-assisted simulation form a comprehensive interaction mechanism between power
system dynamic simulations and AI. A prototype of this design is implemented
and made public based on a highly efficient electromechanical simulator. Tests
of this prototype are carried out under four scenarios including sample
generation, AI-based stability prediction, data-driven dynamic component
modeling, and AI-aided stability control, which prove the validity,
flexibility, and efficiency of the design and implementation of the AI-oriented
power system dynamic simulator.
</summary>
    <author>
      <name>Tannan Xiao</name>
    </author>
    <author>
      <name>Ying Chen</name>
    </author>
    <author>
      <name>Jianquan Wang</name>
    </author>
    <author>
      <name>Shaowei Huang</name>
    </author>
    <author>
      <name>Weilin Tong</name>
    </author>
    <author>
      <name>Tirui He</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.35833/MPCE.2022.000099</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.35833/MPCE.2022.000099" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures, 1 table. Accepted by Journal of Modern Power
  System and Clean Energy</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.00931v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.00931v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.04249v2</id>
    <updated>2022-12-01T16:26:45Z</updated>
    <published>2021-10-08T16:58:57Z</published>
    <title>Can AI detect pain and express pain empathy? A review from emotion
  recognition and a human-centered AI perspective</title>
    <summary>  Sensory and emotional experiences such as pain and empathy are essential for
mental and physical health. Cognitive neuroscience has been working on
revealing mechanisms underlying pain and empathy. Furthermore, as trending
research areas, computational pain recognition and empathic artificial
intelligence (AI) show progress and promise for healthcare or human-computer
interaction. Although AI research has recently made it increasingly possible to
create artificial systems with affective processing, most cognitive
neuroscience and AI research do not jointly address the issues of empathy in AI
and cognitive neuroscience. The main aim of this paper is to introduce key
advances, cognitive challenges and technical barriers in computational pain
recognition and the implementation of artificial empathy. Our discussion covers
the following topics: How can AI recognize pain from unimodal and multimodal
information? Is it crucial for AI to be empathic? What are the benefits and
challenges of empathic AI? Despite some consensus on the importance of AI,
including empathic recognition and responses, we also highlight future
challenges for artificial empathy and possible paths from interdisciplinary
perspectives. Furthermore, we discuss challenges for responsible evaluation of
cognitive methods and computational techniques and show approaches to future
work to contribute to affective assistants capable of empathy.
</summary>
    <author>
      <name>Siqi Cao</name>
    </author>
    <author>
      <name>Di Fu</name>
    </author>
    <author>
      <name>Xu Yang</name>
    </author>
    <author>
      <name>Stefan Wermter</name>
    </author>
    <author>
      <name>Xun Liu</name>
    </author>
    <author>
      <name>Haiyan Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2110.04249v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.04249v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.04324v2</id>
    <updated>2022-04-19T05:38:08Z</updated>
    <published>2021-11-08T08:29:28Z</published>
    <title>When Cyber-Physical Systems Meet AI: A Benchmark, an Evaluation, and a
  Way Forward</title>
    <summary>  Cyber-physical systems (CPS) have been broadly deployed in safety-critical
domains, such as automotive systems, avionics, medical devices, etc. In recent
years, Artificial Intelligence (AI) has been increasingly adopted to control
CPS. Despite the popularity of AI-enabled CPS, few benchmarks are publicly
available. There is also a lack of deep understanding on the performance and
reliability of AI-enabled CPS across different industrial domains. To bridge
this gap, we initiate to create a public benchmark of industry-level CPS in
seven domains and build AI controllers for them via state-of-the-art deep
reinforcement learning (DRL) methods. Based on that, we further perform a
systematic evaluation of these AI-enabled systems with their traditional
counterparts to identify the current challenges and explore future
opportunities. Our key findings include (1) AI controllers do not always
outperform traditional controllers, (2) existing CPS testing techniques
(falsification, specifically) fall short of analyzing AI-enabled CPS, and (3)
building a hybrid system that strategically combines and switches between AI
controllers and traditional controllers can achieve better performance across
different domains. Our results highlight the need for new testing techniques
for AI-enabled CPS and the need for more investigations into hybrid CPS systems
to achieve optimal performance and reliability.
</summary>
    <author>
      <name>Jiayang Song</name>
    </author>
    <author>
      <name>Deyun Lyu</name>
    </author>
    <author>
      <name>Zhenya Zhang</name>
    </author>
    <author>
      <name>Zhijie Wang</name>
    </author>
    <author>
      <name>Tianyi Zhang</name>
    </author>
    <author>
      <name>Lei Ma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3510457.3513049</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3510457.3513049" rel="related"/>
    <link href="http://arxiv.org/abs/2111.04324v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.04324v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.02879v1</id>
    <updated>2022-02-06T22:30:23Z</updated>
    <published>2022-02-06T22:30:23Z</published>
    <title>An Empirical Analysis of AI Contributions to Sustainable Cities (SDG11)</title>
    <summary>  Artificial Intelligence (AI) presents opportunities to develop tools and
techniques for addressing some of the major global challenges and deliver
solutions with significant social and economic impacts. The application of AI
has far-reaching implications for the 17 Sustainable Development Goals (SDGs)
in general, and sustainable urban development in particular. However, existing
attempts to understand and use the opportunities offered by AI for SDG 11 have
been explored sparsely, and the shortage of empirical evidence about the
practical application of AI remains. In this chapter, we analyze the
contribution of AI to support the progress of SDG 11 (Sustainable Cities and
Communities). We address the knowledge gap by empirically analyzing the AI
systems (N = 29) from the AIxSDG database and the Community Research and
Development Information Service (CORDIS) database. Our analysis revealed that
AI systems have indeed contributed to advancing sustainable cities in several
ways (e.g., waste management, air quality monitoring, disaster response
management, transportation management), but many projects are still working for
citizens and not with them. This snapshot of AI's impact on SDG11 is inherently
partial, yet useful to advance our understanding as we move towards more mature
systems and research on the impact of AI systems for social good.
</summary>
    <author>
      <name>Shivam Gupta</name>
    </author>
    <author>
      <name>Auriol Degbelo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in Mazzi, F. and Floridi, L. (eds) The Ethics of Artificial
  Intelligence for the Sustainable Development Goals</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.02879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.02879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.05302v1</id>
    <updated>2022-02-10T19:59:23Z</updated>
    <published>2022-02-10T19:59:23Z</published>
    <title>Trust in AI: Interpretability is not necessary or sufficient, while
  black-box interaction is necessary and sufficient</title>
    <summary>  The problem of human trust in artificial intelligence is one of the most
fundamental problems in applied machine learning. Our processes for evaluating
AI trustworthiness have substantial ramifications for ML's impact on science,
health, and humanity, yet confusion surrounds foundational concepts. What does
it mean to trust an AI, and how do humans assess AI trustworthiness? What are
the mechanisms for building trustworthy AI? And what is the role of
interpretable ML in trust? Here, we draw from statistical learning theory and
sociological lenses on human-automation trust to motivate an AI-as-tool
framework, which distinguishes human-AI trust from human-AI-human trust.
Evaluating an AI's contractual trustworthiness involves predicting future model
behavior using behavior certificates (BCs) that aggregate behavioral evidence
from diverse sources including empirical out-of-distribution and out-of-task
evaluation and theoretical proofs linking model architecture to behavior. We
clarify the role of interpretability in trust with a ladder of model access.
Interpretability (level 3) is not necessary or even sufficient for trust, while
the ability to run a black-box model at-will (level 2) is necessary and
sufficient. While interpretability can offer benefits for trust, it can also
incur costs. We clarify ways interpretability can contribute to trust, while
questioning the perceived centrality of interpretability to trust in popular
discourse. How can we empower people with tools to evaluate trust? Instead of
trying to understand how a model works, we argue for understanding how a model
behaves. Instead of opening up black boxes, we should create more behavior
certificates that are more correct, relevant, and understandable. We discuss
how to build trusted and trustworthy AI responsibly.
</summary>
    <author>
      <name>Max W. Shen</name>
    </author>
    <link href="http://arxiv.org/abs/2202.05302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.05302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.01157v3</id>
    <updated>2022-06-14T15:24:15Z</updated>
    <published>2022-03-02T14:56:28Z</published>
    <title>Artificial Concepts of Artificial Intelligence: Institutional Compliance
  and Resistance in AI Startups</title>
    <summary>  Scholars and industry practitioners have debated how to best develop
interventions for ethical artificial intelligence (AI). Such interventions
recommend that companies building and using AI tools change their technical
practices, but fail to wrangle with critical questions about the organizational
and institutional context in which AI is developed. In this paper, we
contribute descriptive research around the life of "AI" as a discursive concept
and organizational practice in an understudied sphere--emerging AI
startups--and with a focus on extra-organizational pressures faced by
entrepreneurs. Leveraging a theoretical lens for how organizations change, we
conducted semi-structured interviews with 23 entrepreneurs working at
early-stage AI startups. We find that actors within startups both conform to
and resist institutional pressures. Our analysis identifies a central tension
for AI entrepreneurs: they often valued scientific integrity and methodological
rigor; however, influential external stakeholders either lacked the technical
knowledge to appreciate entrepreneurs' emphasis on rigor or were more focused
on business priorities. As a result, entrepreneurs adopted hyped marketing
messages about AI that diverged from their scientific values, but attempted to
preserve their legitimacy internally. Institutional pressures and
organizational constraints also influenced entrepreneurs' modeling practices
and their response to actual or impending regulation. We conclude with a
discussion for how such pressures could be used as leverage for effective
interventions towards building ethical AI.
</summary>
    <author>
      <name>Amy A. Winecoff</name>
    </author>
    <author>
      <name>Elizabeth Anne Watkins</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3514094.3534138</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3514094.3534138" rel="related"/>
    <link href="http://arxiv.org/abs/2203.01157v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.01157v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.12687v1</id>
    <updated>2022-03-23T19:18:19Z</updated>
    <published>2022-03-23T19:18:19Z</published>
    <title>Trust in AI and Its Role in the Acceptance of AI Technologies</title>
    <summary>  As AI-enhanced technologies become common in a variety of domains, there is
an increasing need to define and examine the trust that users have in such
technologies. Given the progress in the development of AI, a correspondingly
sophisticated understanding of trust in the technology is required. This paper
addresses this need by explaining the role of trust on the intention to use AI
technologies. Study 1 examined the role of trust in the use of AI voice
assistants based on survey responses from college students. A path analysis
confirmed that trust had a significant effect on the intention to use AI, which
operated through perceived usefulness and participants' attitude toward voice
assistants. In study 2, using data from a representative sample of the U.S.
population, different dimensions of trust were examined using exploratory
factor analysis, which yielded two dimensions: human-like trust and
functionality trust. The results of the path analyses from Study 1 were
replicated in Study 2, confirming the indirect effect of trust and the effects
of perceived usefulness, ease of use, and attitude on intention to use.
Further, both dimensions of trust shared a similar pattern of effects within
the model, with functionality-related trust exhibiting a greater total impact
on usage intention than human-like trust. Overall, the role of trust in the
acceptance of AI technologies was significant across both studies. This
research contributes to the advancement and application of the TAM in
AI-related applications and offers a multidimensional measure of trust that can
be utilized in the future study of trustworthy AI.
</summary>
    <author>
      <name>Hyesun Choung</name>
    </author>
    <author>
      <name>Prabu David</name>
    </author>
    <author>
      <name>Arun Ross</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/10447318.2022.2050543</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/10447318.2022.2050543" rel="related"/>
    <link href="http://arxiv.org/abs/2203.12687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.12687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.10943v1</id>
    <updated>2022-04-22T21:57:00Z</updated>
    <published>2022-04-22T21:57:00Z</published>
    <title>FPGA-based AI Smart NICs for Scalable Distributed AI Training Systems</title>
    <summary>  Rapid advances in artificial intelligence (AI) technology have led to
significant accuracy improvements in a myriad of application domains at the
cost of larger and more compute-intensive models. Training such models on
massive amounts of data typically requires scaling to many compute nodes and
relies heavily on collective communication algorithms, such as all-reduce, to
exchange the weight gradients between different nodes. The overhead of these
collective communication operations in a distributed AI training system can
bottleneck its performance, with more pronounced effects as the number of nodes
increases. In this paper, we first characterize the all-reduce operation
overhead by profiling distributed AI training. Then, we propose a new smart
network interface card (NIC) for distributed AI training systems using
field-programmable gate arrays (FPGAs) to accelerate all-reduce operations and
optimize network bandwidth utilization via data compression. The AI smart NIC
frees up the system's compute resources to perform the more compute-intensive
tensor operations and increases the overall node-to-node communication
efficiency. We perform real measurements on a prototype distributed AI training
system comprised of 6 compute nodes to evaluate the performance gains of our
proposed FPGA-based AI smart NIC compared to a baseline system with regular
NICs. We also use these measurements to validate an analytical model that we
formulate to predict performance when scaling to larger systems. Our proposed
FPGA-based AI smart NIC enhances overall training performance by 1.6x at 6
nodes, with an estimated 2.5x performance improvement at 32 nodes, compared to
the baseline system using conventional NICs.
</summary>
    <author>
      <name>Rui Ma</name>
    </author>
    <author>
      <name>Evangelos Georganas</name>
    </author>
    <author>
      <name>Alexander Heinecke</name>
    </author>
    <author>
      <name>Andrew Boutros</name>
    </author>
    <author>
      <name>Eriko Nurvitadhi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.10943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.10943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.13828v1</id>
    <updated>2022-04-29T00:14:33Z</updated>
    <published>2022-04-29T00:14:33Z</published>
    <title>Designing for Responsible Trust in AI Systems: A Communication
  Perspective</title>
    <summary>  Current literature and public discourse on "trust in AI" are often focused on
the principles underlying trustworthy AI, with insufficient attention paid to
how people develop trust. Given that AI systems differ in their level of
trustworthiness, two open questions come to the fore: how should AI
trustworthiness be responsibly communicated to ensure appropriate and equitable
trust judgments by different users, and how can we protect users from deceptive
attempts to earn their trust? We draw from communication theories and
literature on trust in technologies to develop a conceptual model called MATCH,
which describes how trustworthiness is communicated in AI systems through
trustworthiness cues and how those cues are processed by people to make trust
judgments. Besides AI-generated content, we highlight transparency and
interaction as AI systems' affordances that present a wide range of
trustworthiness cues to users. By bringing to light the variety of users'
cognitive processes to make trust judgments and their potential limitations, we
urge technology creators to make conscious decisions in choosing reliable
trustworthiness cues for target users and, as an industry, to regulate this
space and prevent malicious use. Towards these goals, we define the concepts of
warranted trustworthiness cues and expensive trustworthiness cues, and propose
a checklist of requirements to help technology creators identify appropriate
cues to use. We present a hypothetical use case to illustrate how practitioners
can use MATCH to design AI systems responsibly, and discuss future directions
for research and industry efforts aimed at promoting responsible trust in AI.
</summary>
    <author>
      <name>Q. Vera Liao</name>
    </author>
    <author>
      <name>S. Shyam Sundar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3531146.3533182</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3531146.3533182" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">FAccT 2022 paper draft</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.13828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.13828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.12961v1</id>
    <updated>2022-05-25T14:02:49Z</updated>
    <published>2022-05-25T14:02:49Z</published>
    <title>Towards Green AI with tensor networks -- Sustainability and innovation
  enabled by efficient algorithms</title>
    <summary>  The current standard to compare the performance of AI algorithms is mainly
based on one criterion: the model's accuracy. In this context, algorithms with
a higher accuracy (or similar measures) are considered as better. To achieve
new state-of-the-art results, algorithmic development is accompanied by an
exponentially increasing amount of compute. While this has enabled AI research
to achieve remarkable results, AI progress comes at a cost: it is
unsustainable. In this paper, we present a promising tool for sustainable and
thus Green AI: tensor networks (TNs). Being an established tool from
multilinear algebra, TNs have the capability to improve efficiency without
compromising accuracy. Since they can reduce compute significantly, we would
like to highlight their potential for Green AI. We elaborate in both a kernel
machine and deep learning setting how efficiency gains can be achieved with
TNs. Furthermore, we argue that better algorithms should be evaluated in terms
of both accuracy and efficiency. To that end, we discuss different efficiency
criteria and analyze efficiency in an exemplifying experimental setting for
kernel ridge regression. With this paper, we want to raise awareness about
Green AI and showcase its positive impact on sustainability and AI research.
Our key contribution is to demonstrate that TNs enable efficient algorithms and
therefore contribute towards Green AI. In this sense, TNs pave the way for
better algorithms in AI.
</summary>
    <author>
      <name>Eva Memmel</name>
    </author>
    <author>
      <name>Clara Menzen</name>
    </author>
    <author>
      <name>Jetze Schuurmans</name>
    </author>
    <author>
      <name>Frederiek Wesel</name>
    </author>
    <author>
      <name>Kim Batselier</name>
    </author>
    <link href="http://arxiv.org/abs/2205.12961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.12961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.09887v2</id>
    <updated>2022-06-28T14:23:47Z</updated>
    <published>2022-06-20T16:46:21Z</published>
    <title>How to Assess Trustworthy AI in Practice</title>
    <summary>  This report is a methodological reflection on
Z-Inspection$^{\small{\circledR}}$. Z-Inspection$^{\small{\circledR}}$ is a
holistic process used to evaluate the trustworthiness of AI-based technologies
at different stages of the AI lifecycle. It focuses, in particular, on the
identification and discussion of ethical issues and tensions through the
elaboration of socio-technical scenarios. It uses the general European Union's
High-Level Expert Group's (EU HLEG) guidelines for trustworthy AI. This report
illustrates for both AI researchers and AI practitioners how the EU HLEG
guidelines for trustworthy AI can be applied in practice. We share the lessons
learned from conducting a series of independent assessments to evaluate the
trustworthiness of AI systems in healthcare. We also share key recommendations
and practical suggestions on how to ensure a rigorous trustworthy AI assessment
throughout the life-cycle of an AI system.
</summary>
    <author>
      <name>Roberto V. Zicari</name>
    </author>
    <author>
      <name>Julia Amann</name>
    </author>
    <author>
      <name>Frédérick Bruneault</name>
    </author>
    <author>
      <name>Megan Coffee</name>
    </author>
    <author>
      <name>Boris Düdder</name>
    </author>
    <author>
      <name>Eleanore Hickman</name>
    </author>
    <author>
      <name>Alessio Gallucci</name>
    </author>
    <author>
      <name>Thomas Krendl Gilbert</name>
    </author>
    <author>
      <name>Thilo Hagendorff</name>
    </author>
    <author>
      <name>Irmhild van Halem</name>
    </author>
    <author>
      <name>Elisabeth Hildt</name>
    </author>
    <author>
      <name>Sune Holm</name>
    </author>
    <author>
      <name>Georgios Kararigas</name>
    </author>
    <author>
      <name>Pedro Kringen</name>
    </author>
    <author>
      <name>Vince I. Madai</name>
    </author>
    <author>
      <name>Emilie Wiinblad Mathez</name>
    </author>
    <author>
      <name>Jesmin Jahan Tithi</name>
    </author>
    <author>
      <name>Dennis Vetter</name>
    </author>
    <author>
      <name>Magnus Westerlund</name>
    </author>
    <author>
      <name>Renee Wurth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">On behalf of the Z-Inspection$^{\small{\circledR}}$ initiative (2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.09887v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.09887v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.13349v1</id>
    <updated>2022-06-09T14:09:37Z</updated>
    <published>2022-06-09T14:09:37Z</published>
    <title>Process Knowledge-Infused AI: Towards User-level Explainability,
  Interpretability, and Safety</title>
    <summary>  AI systems have been widely adopted across various domains in the real world.
However, in high-value, sensitive, or safety-critical applications such as
self-management for personalized health or food recommendation with a specific
purpose (e.g., allergy-aware recipe recommendations), their adoption is
unlikely. Firstly, the AI system needs to follow guidelines or well-defined
processes set by experts; the data alone will not be adequate. For example, to
diagnose the severity of depression, mental healthcare providers use Patient
Health Questionnaire (PHQ-9). So if an AI system were to be used for diagnosis,
the medical guideline implied by the PHQ-9 needs to be used. Likewise, a
nutritionist's knowledge and steps would need to be used for an AI system that
guides a diabetic patient in developing a food plan. Second, the BlackBox
nature typical of many current AI systems will not work; the user of an AI
system will need to be able to give user-understandable explanations,
explanations constructed using concepts that humans can understand and are
familiar with. This is the key to eliciting confidence and trust in the AI
system. For such applications, in addition to data and domain knowledge, the AI
systems need to have access to and use the Process Knowledge, an ordered set of
steps that the AI system needs to use or adhere to.
</summary>
    <author>
      <name>Amit Sheth</name>
    </author>
    <author>
      <name>Manas Gaur</name>
    </author>
    <author>
      <name>Kaushik Roy</name>
    </author>
    <author>
      <name>Revathy Venkataraman</name>
    </author>
    <author>
      <name>Vedant Khandelwal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To paper in IEEE Internet Computing 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.13349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.13349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.07076v1</id>
    <updated>2022-09-15T06:24:01Z</updated>
    <published>2022-09-15T06:24:01Z</published>
    <title>Responsible AI Implementation: A Human-centered Framework for
  Accelerating the Innovation Process</title>
    <summary>  There is still a significant gap between expectations and the successful
adoption of AI to innovate and improve businesses. Due to the emergence of deep
learning, AI adoption is more complex as it often incorporates big data and the
internet of things, affecting data privacy. Existing frameworks have identified
the need to focus on human-centered design, combining technical and
business/organizational perspectives. However, trust remains a critical issue
that needs to be designed from the beginning. The proposed framework expands
from the human-centered design approach, emphasizing and maintaining the trust
that underpins the process. This paper proposes a theoretical framework for
responsible artificial intelligence (AI) implementation. The proposed framework
emphasizes a synergistic business technology approach for the agile co-creation
process. The aim is to streamline the adoption process of AI to innovate and
improve business by involving all stakeholders throughout the project so that
the AI technology is designed, developed, and deployed in conjunction with
people and not in isolation. The framework presents a fresh viewpoint on
responsible AI implementation based on analytical literature review, conceptual
framework design, and practitioners' mediating expertise. The framework
emphasizes establishing and maintaining trust throughout the human-centered
design and agile development of AI. This human-centered approach is aligned
with and enabled by the privacy by design principle. The creators of the
technology and the end-users are working together to tailor the AI solution
specifically for the business requirements and human characteristics. An
illustrative case study on adopting AI for assisting planning in a hospital
will demonstrate that the proposed framework applies to real-life applications.
</summary>
    <author>
      <name>Dian Tjondronegoro</name>
    </author>
    <author>
      <name>Elizabeth Yuwono</name>
    </author>
    <author>
      <name>Brent Richards</name>
    </author>
    <author>
      <name>Damian Green</name>
    </author>
    <author>
      <name>Siiri Hatakka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 7 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.07076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.07076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="93-01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.13289v1</id>
    <updated>2022-10-24T14:26:59Z</updated>
    <published>2022-10-24T14:26:59Z</published>
    <title>Secure and Trustworthy Artificial Intelligence-Extended Reality (AI-XR)
  for Metaverses</title>
    <summary>  Metaverse is expected to emerge as a new paradigm for the next-generation
Internet, providing fully immersive and personalised experiences to socialize,
work, and play in self-sustaining and hyper-spatio-temporal virtual world(s).
The advancements in different technologies like augmented reality, virtual
reality, extended reality (XR), artificial intelligence (AI), and 5G/6G
communication will be the key enablers behind the realization of AI-XR
metaverse applications. While AI itself has many potential applications in the
aforementioned technologies (e.g., avatar generation, network optimization,
etc.), ensuring the security of AI in critical applications like AI-XR
metaverse applications is profoundly crucial to avoid undesirable actions that
could undermine users' privacy and safety, consequently putting their lives in
danger. To this end, we attempt to analyze the security, privacy, and
trustworthiness aspects associated with the use of various AI techniques in
AI-XR metaverse applications. Specifically, we discuss numerous such challenges
and present a taxonomy of potential solutions that could be leveraged to
develop secure, private, robust, and trustworthy AI-XR applications. To
highlight the real implications of AI-associated adversarial threats, we
designed a metaverse-specific case study and analyzed it through the
adversarial lens. Finally, we elaborate upon various open issues that require
further research interest from the community.
</summary>
    <author>
      <name>Adnan Qayyum</name>
    </author>
    <author>
      <name>Muhammad Atif Butt</name>
    </author>
    <author>
      <name>Hassan Ali</name>
    </author>
    <author>
      <name>Muhammad Usman</name>
    </author>
    <author>
      <name>Osama Halabi</name>
    </author>
    <author>
      <name>Ala Al-Fuqaha</name>
    </author>
    <author>
      <name>Qammer H. Abbasi</name>
    </author>
    <author>
      <name>Muhammad Ali Imran</name>
    </author>
    <author>
      <name>Junaid Qadir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.13289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.13289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.00069v2</id>
    <updated>2022-12-14T19:48:43Z</updated>
    <published>2022-10-24T05:09:13Z</published>
    <title>AI Explainability and Governance in Smart Energy Systems: A Review</title>
    <summary>  Traditional electrical power grids have long suffered from operational
unreliability, instability, inflexibility, and inefficiency. Smart grids (or
smart energy systems) continue to transform the energy sector with emerging
technologies, renewable energy sources, and other trends. Artificial
intelligence (AI) is being applied to smart energy systems to process massive
and complex data in this sector and make smart and timely decisions. However,
the lack of explainability and governability of AI is a major concern for
stakeholders hindering a fast uptake of AI in the energy sector. This paper
provides a review of AI explainability and governance in smart energy systems.
We collect 3,568 relevant papers from the Scopus database, automatically
discover 15 parameters or themes for AI governance in energy and elaborate the
research landscape by reviewing over 150 papers and providing temporal
progressions of the research. The methodology for discovering parameters or
themes is based on "deep journalism", our data-driven deep learning-based big
data analytics approach to automatically discover and analyse cross-sectional
multi-perspective information to enable better decision-making and develop
better instruments for governance. The findings show that research on AI
explainability in energy systems is segmented and narrowly focussed on a few AI
traits and energy system problems. This paper deepens our knowledge of AI
governance in energy and is expected to help governments, industry, academics,
energy prosumers, and other stakeholders to understand the landscape of AI in
the energy sector, leading to better design, operations, utilisation, and risk
management of energy systems.
</summary>
    <author>
      <name>Roba Alsaigh</name>
    </author>
    <author>
      <name>Rashid Mehmood</name>
    </author>
    <author>
      <name>Iyad Katib</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 1 table, 5 figures, submitted, under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.00069v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.00069v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.16444v1</id>
    <updated>2022-11-29T18:22:23Z</updated>
    <published>2022-11-29T18:22:23Z</published>
    <title>Holding AI to Account: Challenges for the Delivery of Trustworthy AI in
  Healthcare</title>
    <summary>  The need for AI systems to provide explanations for their behaviour is now
widely recognised as key to their adoption. In this paper, we examine the
problem of trustworthy AI and explore what delivering this means in practice,
with a focus on healthcare applications. Work in this area typically treats
trustworthy AI as a problem of Human-Computer Interaction involving the
individual user and an AI system. However, we argue here that this overlooks
the important part played by organisational accountability in how people reason
about and trust AI in socio-technical settings. To illustrate the importance of
organisational accountability, we present findings from ethnographic studies of
breast cancer screening and cancer treatment planning in multidisciplinary team
meetings to show how participants made themselves accountable both to each
other and to the organisations of which they are members. We use these findings
to enrich existing understandings of the requirements for trustworthy AI and to
outline some candidate solutions to the problems of making AI accountable both
to individual users and organisationally. We conclude by outlining the
implications of this for future work on the development of trustworthy AI,
including ways in which our proposed solutions may be re-used in different
application settings.
</summary>
    <author>
      <name>Rob Procter</name>
    </author>
    <author>
      <name>Peter Tolmie</name>
    </author>
    <author>
      <name>Mark Rouncefield</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.16444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.16444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.4.0; I.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.11136v2</id>
    <updated>2022-12-23T22:55:33Z</updated>
    <published>2022-12-16T23:33:10Z</published>
    <title>It is not "accuracy vs. explainability" -- we need both for trustworthy
  AI systems</title>
    <summary>  We are witnessing the emergence of an AI economy and society where AI
technologies are increasingly impacting health care, business, transportation
and many aspects of everyday life. Many successes have been reported where AI
systems even surpassed the accuracy of human experts. However, AI systems may
produce errors, can exhibit bias, may be sensitive to noise in the data, and
often lack technical and judicial transparency resulting in reduction in trust
and challenges in their adoption. These recent shortcomings and concerns have
been documented in scientific but also in general press such as accidents with
self driving cars, biases in healthcare, hiring and face recognition systems
for people of color, seemingly correct medical decisions later found to be made
due to wrong reasons etc. This resulted in emergence of many government and
regulatory initiatives requiring trustworthy and ethical AI to provide accuracy
and robustness, some form of explainability, human control and oversight,
elimination of bias, judicial transparency and safety. The challenges in
delivery of trustworthy AI systems motivated intense research on explainable AI
systems (XAI). Aim of XAI is to provide human understandable information of how
AI systems make their decisions. In this paper we first briefly summarize
current XAI work and then challenge the recent arguments of accuracy vs.
explainability for being mutually exclusive and being focused only on deep
learning. We then present our recommendations for the use of XAI in full
lifecycle of high stakes trustworthy AI systems delivery, e.g. development,
validation and certification, and trustworthy production and maintenance.
</summary>
    <author>
      <name>D. Petkovic</name>
    </author>
    <link href="http://arxiv.org/abs/2212.11136v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.11136v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.11333v1</id>
    <updated>2023-01-25T14:26:10Z</updated>
    <published>2023-01-25T14:26:10Z</published>
    <title>Knowing About Knowing: An Illusion of Human Competence Can Hinder
  Appropriate Reliance on AI Systems</title>
    <summary>  The dazzling promises of AI systems to augment humans in various tasks hinge
on whether humans can appropriately rely on them. Recent research has shown
that appropriate reliance is the key to achieving complementary team
performance in AI-assisted decision making. This paper addresses an
under-explored problem of whether the Dunning-Kruger Effect (DKE) among people
can hinder their appropriate reliance on AI systems. DKE is a metacognitive
bias due to which less-competent individuals overestimate their own skill and
performance. Through an empirical study (N = 249), we explored the impact of
DKE on human reliance on an AI system, and whether such effects can be
mitigated using a tutorial intervention that reveals the fallibility of AI
advice, and exploiting logic units-based explanations to improve user
understanding of AI advice. We found that participants who overestimate their
performance tend to exhibit under-reliance on AI systems, which hinders optimal
team performance. Logic units-based explanations did not help users in either
improving the calibration of their competence or facilitating appropriate
reliance. While the tutorial intervention was highly effective in helping users
calibrate their self-assessment and facilitating appropriate reliance among
participants with overestimated self-assessment, we found that it can
potentially hurt the appropriate reliance of participants with underestimated
self-assessment. Our work has broad implications on the design of methods to
tackle user cognitive biases while facilitating appropriate reliance on AI
systems. Our findings advance the current understanding of the role of
self-assessment in shaping trust and reliance in human-AI decision making. This
lays out promising future directions for relevant HCI research in this
community.
</summary>
    <author>
      <name>Gaole He</name>
    </author>
    <author>
      <name>Lucie Kuiper</name>
    </author>
    <author>
      <name>Ujwal Gadiraju</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3544548.3581025</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3544548.3581025" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conditionally accepted to CHI 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.11333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.11333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.06584v2</id>
    <updated>2023-02-16T17:44:24Z</updated>
    <published>2023-02-09T17:18:36Z</published>
    <title>Thermodynamic AI and the fluctuation frontier</title>
    <summary>  Many Artificial Intelligence (AI) algorithms are inspired by physics and
employ stochastic fluctuations. We connect these physics-inspired AI algorithms
by unifying them under a single mathematical framework that we call
Thermodynamic AI. Seemingly disparate algorithmic classes can be described by
this framework, for example, (1) Generative diffusion models, (2) Bayesian
neural networks, (3) Monte Carlo sampling and (4) Simulated annealing. Such
Thermodynamic AI algorithms are currently run on digital hardware, ultimately
limiting their scalability and overall potential. Stochastic fluctuations
naturally occur in physical thermodynamic systems, and such fluctuations can be
viewed as a computational resource. Hence, we propose a novel computing
paradigm, where software and hardware become inseparable. Our algorithmic
unification allows us to identify a single full-stack paradigm, involving
Thermodynamic AI hardware, that could accelerate such algorithms. We contrast
Thermodynamic AI hardware with quantum computing where noise is a roadblock
rather than a resource. Thermodynamic AI hardware can be viewed as a novel form
of computing, since it uses a novel fundamental building block. We identify
stochastic bits (s-bits) and stochastic modes (s-modes) as the respective
building blocks for discrete and continuous Thermodynamic AI hardware. In
addition to these stochastic units, Thermodynamic AI hardware employs a
Maxwell's demon device that guides the system to produce non-trivial states. We
provide a few simple physical architectures for building these devices and we
develop a formalism for programming the hardware via gate sequences. We hope to
stimulate discussion around this new computing paradigm. Beyond acceleration,
we believe it will impact the design of both hardware and algorithms, while
also deepening our understanding of the connection between physics and
intelligence.
</summary>
    <author>
      <name>Patrick J. Coles</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">47 pages, 18 figures, Added relevant references</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.06584v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.06584v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.09079v1</id>
    <updated>2023-02-04T14:27:46Z</updated>
    <published>2023-02-04T14:27:46Z</published>
    <title>Advances in Automatically Rating the Trustworthiness of Text Processing
  Services</title>
    <summary>  AI services are known to have unstable behavior when subjected to changes in
data, models or users. Such behaviors, whether triggered by omission or
commission, lead to trust issues when AI works with humans. The current
approach of assessing AI services in a black box setting, where the consumer
does not have access to the AI's source code or training data, is limited. The
consumer has to rely on the AI developer's documentation and trust that the
system has been built as stated. Further, if the AI consumer reuses the service
to build other services which they sell to their customers, the consumer is at
the risk of the service providers (both data and model providers). Our
approach, in this context, is inspired by the success of nutritional labeling
in food industry to promote health and seeks to assess and rate AI services for
trust from the perspective of an independent stakeholder. The ratings become a
means to communicate the behavior of AI systems so that the consumer is
informed about the risks and can make an informed decision. In this paper, we
will first describe recent progress in developing rating methods for text-based
machine translator AI services that have been found promising with user
studies. Then, we will outline challenges and vision for a principled,
multi-modal, causality-based rating methodologies and its implication for
decision-support in real-world scenarios like health and food recommendation.
</summary>
    <author>
      <name>Biplav Srivastava</name>
    </author>
    <author>
      <name>Kausik Lakkaraju</name>
    </author>
    <author>
      <name>Mariana Bernagozzi</name>
    </author>
    <author>
      <name>Marco Valtorta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, Accepted at 2023 Spring Symposium on AI Trustworthiness
  Assessment</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.09079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.09079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; D.2.5; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.04080v1</id>
    <updated>2020-03-03T00:36:17Z</updated>
    <published>2020-03-03T00:36:17Z</published>
    <title>Two Decades of AI4NETS-AI/ML for Data Networks: Challenges &amp; Research
  Directions</title>
    <summary>  The popularity of Artificial Intelligence (AI) -- and of Machine Learning
(ML) as an approach to AI, has dramatically increased in the last few years,
due to its outstanding performance in various domains, notably in image, audio,
and natural language processing. In these domains, AI success-stories are
boosting the applied field. When it comes to AI/ML for data communication
Networks (AI4NETS), and despite the many attempts to turn networks into
learning agents, the successful application of AI/ML in networking is limited.
There is a strong resistance against AI/ML-based solutions, and a striking gap
between the extensive academic research and the actual deployments of such
AI/ML-based systems in operational environments. The truth is, there are still
many unsolved complex challenges associated to the analysis of networking data
through AI/ML, which hinders its acceptability and adoption in the practice. In
this positioning paper I elaborate on the most important show-stoppers in
AI4NETS, and present a research agenda to tackle some of these challenges,
enabling a natural adoption of AI/ML for networking. In particular, I focus the
future research in AI4NETS around three major pillars: (i) to make AI/ML
immediately applicable in networking problems through the concepts of effective
learning, turning it into a useful and reliable way to deal with complex
data-driven networking problems; (ii) to boost the adoption of AI/ML at the
large scale by learning from the Internet-paradigm itself, conceiving novel
distributed and hierarchical learning approaches mimicking the distributed
topological principles and operation of the Internet itself; and (iii) to
exploit the softwarization and distribution of networks to conceive
AI/ML-defined Networks (AIDN), relying on the distributed generation and
re-usage of knowledge through novel Knowledge Delivery Networks (KDNs).
</summary>
    <author>
      <name>Pedro Casas</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">5th IEEE/IFIP International Workshop on Analytics for Network and
  Service Management (AnNet 2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.04080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.04080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.10985v2</id>
    <updated>2020-02-01T04:46:25Z</updated>
    <published>2019-05-27T06:05:16Z</published>
    <title>AI-GAs: AI-generating algorithms, an alternate paradigm for producing
  general artificial intelligence</title>
    <summary>  Perhaps the most ambitious scientific quest in human history is the creation
of general artificial intelligence, which roughly means AI that is as smart or
smarter than humans. The dominant approach in the machine learning community is
to attempt to discover each of the pieces required for intelligence, with the
implicit assumption that some future group will complete the Herculean task of
figuring out how to combine all of those pieces into a complex thinking
machine. I call this the "manual AI approach". This paper describes another
exciting path that ultimately may be more successful at producing general AI.
It is based on the clear trend in machine learning that hand-designed solutions
eventually are replaced by more effective, learned solutions. The idea is to
create an AI-generating algorithm (AI-GA), which automatically learns how to
produce general AI. Three Pillars are essential for the approach: (1)
meta-learning architectures, (2) meta-learning the learning algorithms
themselves, and (3) generating effective learning environments. I argue that
either approach could produce general AI first, and both are scientifically
worthwhile irrespective of which is the fastest path. Because both are
promising, yet the ML community is currently committed to the manual approach,
I argue that our community should increase its research investment in the AI-GA
approach. To encourage such research, I describe promising work in each of the
Three Pillars. I also discuss AI-GA-specific safety and ethical considerations.
Because it it may be the fastest path to general AI and because it is
inherently scientifically interesting to understand the conditions in which a
simple algorithm can produce general AI (as happened on Earth where Darwinian
evolution produced human intelligence), I argue that the pursuit of AI-GAs
should be considered a new grand challenge of computer science research.
</summary>
    <author>
      <name>Jeff Clune</name>
    </author>
    <link href="http://arxiv.org/abs/1905.10985v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.10985v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.09428v1</id>
    <updated>2020-06-16T18:16:51Z</updated>
    <published>2020-06-16T18:16:51Z</published>
    <title>Response by the Montreal AI Ethics Institute to the European
  Commission's Whitepaper on AI</title>
    <summary>  In February 2020, the European Commission (EC) published a white paper
entitled, On Artificial Intelligence - A European approach to excellence and
trust. This paper outlines the EC's policy options for the promotion and
adoption of artificial intelligence (AI) in the European Union. The Montreal AI
Ethics Institute (MAIEI) reviewed this paper and published a response
addressing the EC's plans to build an "ecosystem of excellence" and an
"ecosystem of trust," as well as the safety and liability implications of AI,
the internet of things (IoT), and robotics.
  MAIEI provides 15 recommendations in relation to the sections outlined above,
including: 1) focus efforts on the research and innovation community, member
states, and the private sector; 2) create alignment between trading partners'
policies and EU policies; 3) analyze the gaps in the ecosystem between
theoretical frameworks and approaches to building trustworthy AI; 4) focus on
coordination and policy alignment; 5) focus on mechanisms that promote private
and secure sharing of data; 6) create a network of AI research excellence
centres to strengthen the research and innovation community; 7) promote
knowledge transfer and develop AI expertise through Digital Innovation Hubs; 8)
add nuance to the discussion regarding the opacity of AI systems; 9) create a
process for individuals to appeal an AI system's decision or output; 10)
implement new rules and strengthen existing regulations; 11) ban the use of
facial recognition technology; 12) hold all AI systems to similar standards and
compulsory requirements; 13) ensure biometric identification systems fulfill
the purpose for which they are implemented; 14) implement a voluntary labelling
system for systems that are not considered high-risk; 15) appoint individuals
to the oversight process who understand AI systems well and are able to
communicate potential risks.
</summary>
    <author>
      <name>Abhishek Gupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Microsoft</arxiv:affiliation>
    </author>
    <author>
      <name>Camylle Lanteigne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">McGill University</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the European Commission</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.09428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.09428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.00279v1</id>
    <updated>2020-07-01T07:10:53Z</updated>
    <published>2020-07-01T07:10:53Z</published>
    <title>HPC AI500: The Methodology, Tools, Roofline Performance Models, and
  Metrics for Benchmarking HPC AI Systems</title>
    <summary>  The recent years witness a trend of applying large-scale distributed deep
learning in both business and scientific computing areas, whose goal is to
speed up the training time to achieve a state-of-the-art quality. The HPC
community feels a great interest in building the HPC AI systems that are
dedicated to running those workloads. The HPC AI benchmarks accelerate the
process. Unfortunately, benchmarking HPC AI systems at scale raises serious
challenges. None of previous HPC AI benchmarks achieve the goal of being
equivalent, relevant, representative, affordable, and repeatable. This paper
presents a comprehensive methodology, tools, Roofline performance models, and
innovative metrics for benchmarking, optimizing, and ranking HPC AI systems,
which we call HPC AI500 V2.0. We abstract the HPC AI system into nine
independent layers, and present explicit benchmarking rules and procedures to
assure equivalence of each layer, repeatability, and replicability. On the
basis of AIBench -- by far the most comprehensive AI benchmarks suite, we
present and build two HPC AI benchmarks from both business and scientific
computing: Image Classification, and Extreme Weather Analytics, achieving both
representativeness and affordability. To rank the performance and
energy-efficiency of HPC AI systems, we propose Valid FLOPS, and Valid FLOPS
per watt, which impose a penalty on failing to achieve the target quality. We
propose using convolution and GEMM -- the two most intensively-used kernel
functions to measure the upper bound performance of the HPC AI systems, and
present HPC AI roofline models for guiding performance optimizations. The
evaluations show our methodology, benchmarks, performance models, and metrics
can measure, optimize, and rank the HPC AI systems in a scalable, simple, and
affordable way. HPC AI500 V2.0 are publicly available from
http://www.benchcouncil.org/benchhub/hpc-ai500-benchmark.
</summary>
    <author>
      <name>Zihan Jiang</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Xingwang Xiong</name>
    </author>
    <author>
      <name>Wanling Gao</name>
    </author>
    <author>
      <name>Chunjie Luo</name>
    </author>
    <author>
      <name>Fei Tang</name>
    </author>
    <author>
      <name>Chuanxin Lan</name>
    </author>
    <author>
      <name>Hongxiao Li</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <link href="http://arxiv.org/abs/2007.00279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.00279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.07262v2</id>
    <updated>2020-10-04T07:50:39Z</updated>
    <published>2020-09-15T17:51:40Z</published>
    <title>Report prepared by the Montreal AI Ethics Institute (MAIEI) on
  Publication Norms for Responsible AI</title>
    <summary>  The history of science and technology shows that seemingly innocuous
developments in scientific theories and research have enabled real-world
applications with significant negative consequences for humanity. In order to
ensure that the science and technology of AI is developed in a humane manner,
we must develop research publication norms that are informed by our growing
understanding of AI's potential threats and use cases. Unfortunately, it's
difficult to create a set of publication norms for responsible AI because the
field of AI is currently fragmented in terms of how this technology is
researched, developed, funded, etc. To examine this challenge and find
solutions, the Montreal AI Ethics Institute (MAIEI) co-hosted two public
consultations with the Partnership on AI in May 2020. These meetups examined
potential publication norms for responsible AI, with the goal of creating a
clear set of recommendations and ways forward for publishers.
  In its submission, MAIEI provides six initial recommendations, these include:
1) create tools to navigate publication decisions, 2) offer a page number
extension, 3) develop a network of peers, 4) require broad impact statements,
5) require the publication of expected results, and 6) revamp the peer-review
process. After considering potential concerns regarding these recommendations,
including constraining innovation and creating a "black market" for AI
research, MAIEI outlines three ways forward for publishers, these include: 1)
state clearly and consistently the need for established norms, 2) coordinate
and build trust as a community, and 3) change the approach.
</summary>
    <author>
      <name>Abhishek Gupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Microsoft</arxiv:affiliation>
    </author>
    <author>
      <name>Camylle Lanteigne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Algora Lab</arxiv:affiliation>
    </author>
    <author>
      <name>Victoria Heath</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Report submitted to Partnership on AI for inclusion in their work on
  Publishing Norms for Responsible AI</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.07262v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.07262v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.02117v1</id>
    <updated>2021-05-05T15:23:12Z</updated>
    <published>2021-05-05T15:23:12Z</published>
    <title>Ethics and Governance of Artificial Intelligence: Evidence from a Survey
  of Machine Learning Researchers</title>
    <summary>  Machine learning (ML) and artificial intelligence (AI) researchers play an
important role in the ethics and governance of AI, including taking action
against what they perceive to be unethical uses of AI (Belfield, 2020; Van
Noorden, 2020). Nevertheless, this influential group's attitudes are not well
understood, which undermines our ability to discern consensuses or
disagreements between AI/ML researchers. To examine these researchers' views,
we conducted a survey of those who published in the top AI/ML conferences (N =
524). We compare these results with those from a 2016 survey of AI/ML
researchers (Grace, Salvatier, Dafoe, Zhang, &amp; Evans, 2018) and a 2018 survey
of the US public (Zhang &amp; Dafoe, 2020). We find that AI/ML researchers place
high levels of trust in international organizations and scientific
organizations to shape the development and use of AI in the public interest;
moderate trust in most Western tech companies; and low trust in national
militaries, Chinese tech companies, and Facebook. While the respondents were
overwhelmingly opposed to AI/ML researchers working on lethal autonomous
weapons, they are less opposed to researchers working on other military
applications of AI, particularly logistics algorithms. A strong majority of
respondents think that AI safety research should be prioritized and that ML
institutions should conduct pre-publication review to assess potential harms.
Being closer to the technology itself, AI/ML re-searchers are well placed to
highlight new risks and develop technical solutions, so this novel attempt to
measure their attitudes has broad relevance. The findings should help to
improve how researchers, private sector executives, and policymakers think
about regulations, governance frameworks, guiding principles, and national and
international governance strategies for AI.
</summary>
    <author>
      <name>Baobao Zhang</name>
    </author>
    <author>
      <name>Markus Anderljung</name>
    </author>
    <author>
      <name>Lauren Kahn</name>
    </author>
    <author>
      <name>Noemi Dreksler</name>
    </author>
    <author>
      <name>Michael C. Horowitz</name>
    </author>
    <author>
      <name>Allan Dafoe</name>
    </author>
    <link href="http://arxiv.org/abs/2105.02117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.02117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.7.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.05314v1</id>
    <updated>2022-03-10T12:00:34Z</updated>
    <published>2022-03-10T12:00:34Z</published>
    <title>SoK: On the Semantic AI Security in Autonomous Driving</title>
    <summary>  Autonomous Driving (AD) systems rely on AI components to make safety and
correct driving decisions. Unfortunately, today's AI algorithms are known to be
generally vulnerable to adversarial attacks. However, for such AI
component-level vulnerabilities to be semantically impactful at the system
level, it needs to address non-trivial semantic gaps both (1) from the
system-level attack input spaces to those at AI component level, and (2) from
AI component-level attack impacts to those at the system level. In this paper,
we define such research space as semantic AI security as opposed to generic AI
security. Over the past 5 years, increasingly more research works are performed
to tackle such semantic AI security challenges in AD context, which has started
to show an exponential growth trend.
  In this paper, we perform the first systematization of knowledge of such
growing semantic AD AI security research space. In total, we collect and
analyze 53 such papers, and systematically taxonomize them based on research
aspects critical for the security field. We summarize 6 most substantial
scientific gaps observed based on quantitative comparisons both vertically
among existing AD AI security works and horizontally with security works from
closely-related domains. With these, we are able to provide insights and
potential future directions not only at the design level, but also at the
research goal, methodology, and community levels. To address the most critical
scientific methodology-level gap, we take the initiative to develop an
open-source, uniform, and extensible system-driven evaluation platform, named
PASS, for the semantic AD AI security research community. We also use our
implemented platform prototype to showcase the capabilities and benefits of
such a platform using representative semantic AD AI attacks.
</summary>
    <author>
      <name>Junjie Shen</name>
    </author>
    <author>
      <name>Ningfei Wang</name>
    </author>
    <author>
      <name>Ziwen Wan</name>
    </author>
    <author>
      <name>Yunpeng Luo</name>
    </author>
    <author>
      <name>Takami Sato</name>
    </author>
    <author>
      <name>Zhisheng Hu</name>
    </author>
    <author>
      <name>Xinyang Zhang</name>
    </author>
    <author>
      <name>Shengjian Guo</name>
    </author>
    <author>
      <name>Zhenyu Zhong</name>
    </author>
    <author>
      <name>Kang Li</name>
    </author>
    <author>
      <name>Ziming Zhao</name>
    </author>
    <author>
      <name>Chunming Qiao</name>
    </author>
    <author>
      <name>Qi Alfred Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project website: https://sites.google.com/view/cav-sec/pass</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.05314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.05314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.13504v1</id>
    <updated>2022-06-18T13:29:55Z</updated>
    <published>2022-06-18T13:29:55Z</published>
    <title>AI-based computer-aided diagnostic system of chest digital tomography
  synthesis: Demonstrating comparative advantage with X-ray-based AI systems</title>
    <summary>  Compared with chest X-ray (CXR) imaging, which is a single image projected
from the front of the patient, chest digital tomosynthesis (CDTS) imaging can
be more advantageous for lung lesion detection because it acquires multiple
images projected from multiple angles of the patient. Various clinical
comparative analysis and verification studies have been reported to demonstrate
this, but there were no artificial intelligence (AI)-based comparative analysis
studies. Existing AI-based computer-aided detection (CAD) systems for lung
lesion diagnosis have been developed mainly based on CXR images; however,
CAD-based on CDTS, which uses multi-angle images of patients in various
directions, has not been proposed and verified for its usefulness compared to
CXR-based counterparts. This study develops/tests a CDTS-based AI CAD system to
detect lung lesions to demonstrate performance improvements compared to
CXR-based AI CAD. We used multiple projection images as input for the
CDTS-based AI model and a single-projection image as input for the CXR-based AI
model to fairly compare and evaluate the performance between models. The
proposed CDTS-based AI CAD system yielded sensitivities of 0.782 and 0.785 and
accuracies of 0.895 and 0.837 for the performance of detecting tuberculosis and
pneumonia, respectively, against normal subjects. These results show higher
performance than sensitivities of 0.728 and 0.698 and accuracies of 0.874 and
0.826 for detecting tuberculosis and pneumonia through the CXR-based AI CAD,
which only uses a single projection image in the frontal direction. We found
that CDTS-based AI CAD improved the sensitivity of tuberculosis and pneumonia
by 5.4% and 8.7% respectively, compared to CXR-based AI CAD without loss of
accuracy. Therefore, we comparatively prove that CDTS-based AI CAD technology
can improve performance more than CXR, enhancing the clinical applicability of
CDTS.
</summary>
    <author>
      <name>Kyung-Su Kim</name>
    </author>
    <author>
      <name>Ju Hwan Lee</name>
    </author>
    <author>
      <name>Seong Je Oh</name>
    </author>
    <author>
      <name>Myung Jin Chung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Kyung-Su Kim, Ju Hwan Lee, and Seong Je Oh have contributed equally
  to this work as the co-first author. Kyung-Su Kim (kskim.doc@gmail.com) and
  Myung Jin Chung (mj1.chung@samsung.com) have contributed equally to this work
  as the co-corresponding author</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.13504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.13504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.01487v1</id>
    <updated>2017-01-05T21:41:08Z</updated>
    <published>2017-01-05T21:41:08Z</published>
    <title>Designing a Safe Autonomous Artificial Intelligence Agent based on Human
  Self-Regulation</title>
    <summary>  There is a growing focus on how to design safe artificial intelligent (AI)
agents. As systems become more complex, poorly specified goals or control
mechanisms may cause AI agents to engage in unwanted and harmful outcomes. Thus
it is necessary to design AI agents that follow initial programming intentions
as the program grows in complexity. How to specify these initial intentions has
also been an obstacle to designing safe AI agents. Finally, there is a need for
the AI agent to have redundant safety mechanisms to ensure that any programming
errors do not cascade into major problems. Humans are autonomous intelligent
agents that have avoided these problems and the present manuscript argues that
by understanding human self-regulation and goal setting, we may be better able
to design safe AI agents. Some general principles of human self-regulation are
outlined and specific guidance for AI design is given.
</summary>
    <author>
      <name>Mark Muraven</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.01487v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.01487v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04879v1</id>
    <updated>2016-09-15T22:40:29Z</updated>
    <published>2016-09-15T22:40:29Z</published>
    <title>NPCs as People, Too: The Extreme AI Personality Engine</title>
    <summary>  PK Dick once asked "Do Androids Dream of Electric Sheep?" In video games, a
similar question could be asked of non-player characters: Do NPCs have dreams?
Can they live and change as humans do? Can NPCs have personalities, and can
these develop through interactions with players, other NPCs, and the world
around them? Despite advances in personality AI for games, most NPCs are still
undeveloped and undeveloping, reacting with flat affect and predictable
routines that make them far less than human--in fact, they become little more
than bits of the scenery that give out parcels of information. This need not be
the case. Extreme AI, a psychology-based personality engine, creates adaptive
NPC personalities. Originally developed as part of the thesis "NPCs as People:
Using Databases and Behaviour Trees to Give Non-Player Characters Personality,"
Extreme AI is now a fully functioning personality engine using all thirty
facets of the Five Factor model of personality and an AI system that is live
throughout gameplay. This paper discusses the research leading to Extreme AI;
develops the ideas found in that thesis; discusses the development of other
personality engines; and provides examples of Extreme AI's use in two game
demos.
</summary>
    <author>
      <name>Jeffrey Georgeson</name>
    </author>
    <author>
      <name>Christopher Child</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 tables, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.03425v2</id>
    <updated>2019-10-11T08:44:31Z</updated>
    <published>2019-02-28T15:50:35Z</published>
    <title>The Ethics of AI Ethics -- An Evaluation of Guidelines</title>
    <summary>  Current advances in research, development and application of artificial
intelligence (AI) systems have yielded a far-reaching discourse on AI ethics.
In consequence, a number of ethics guidelines have been released in recent
years. These guidelines comprise normative principles and recommendations aimed
to harness the "disruptive" potentials of new AI technologies. Designed as a
comprehensive evaluation, this paper analyzes and compares these guidelines
highlighting overlaps but also omissions. As a result, I give a detailed
overview of the field of AI ethics. Finally, I also examine to what extent the
respective ethical principles and values are implemented in the practice of
research, development and application of AI systems - and how the effectiveness
in the demands of AI ethics can be improved.
</summary>
    <author>
      <name>Thilo Hagendorff</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11023-020-09517-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11023-020-09517-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Minds &amp; Machines, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.03425v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.03425v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.00547v1</id>
    <updated>2019-05-01T13:41:35Z</updated>
    <published>2019-05-01T13:41:35Z</published>
    <title>The relationship between Biological and Artificial Intelligence</title>
    <summary>  Intelligence can be defined as a predominantly human ability to accomplish
tasks that are generally hard for computers and animals. Artificial
Intelligence [AI] is a field attempting to accomplish such tasks with
computers. AI is becoming increasingly widespread, as are claims of its
relationship with Biological Intelligence. Often these claims are made to imply
higher chances of a given technology succeeding, working on the assumption that
AI systems which mimic the mechanisms of Biological Intelligence should be more
successful.
  In this article I will discuss the similarities and differences between AI
and the extent of our knowledge about the mechanisms of intelligence in
biology, especially within humans. I will also explore the validity of the
assumption that biomimicry in AI systems aids their advancement, and I will
argue that existing similarity to biological systems in the way Artificial
Neural Networks [ANNs] tackle tasks is due to design decisions, rather than
inherent similarity of underlying mechanisms. This article is aimed at people
who understand the basics of AI (especially ANNs), and would like to be better
able to evaluate the often wild claims about the value of biomimicry in AI.
</summary>
    <author>
      <name>George Cevora</name>
    </author>
    <link href="http://arxiv.org/abs/1905.00547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.00547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.02092v1</id>
    <updated>2019-05-03T12:05:08Z</updated>
    <published>2019-05-03T12:05:08Z</published>
    <title>Impact of Artificial Intelligence on Businesses: from Research,
  Innovation, Market Deployment to Future Shifts in Business Models</title>
    <summary>  The fast pace of artificial intelligence (AI) and automation is propelling
strategists to reshape their business models. This is fostering the integration
of AI in the business processes but the consequences of this adoption are
underexplored and need attention. This paper focuses on the overall impact of
AI on businesses - from research, innovation, market deployment to future
shifts in business models. To access this overall impact, we design a
three-dimensional research model, based upon the Neo-Schumpeterian economics
and its three forces viz. innovation, knowledge, and entrepreneurship. The
first dimension deals with research and innovation in AI. In the second
dimension, we explore the influence of AI on the global market and the
strategic objectives of the businesses and finally, the third dimension
examines how AI is shaping business contexts. Additionally, the paper explores
AI implications on actors and its dark sides.
</summary>
    <author>
      <name>Neha Soni</name>
    </author>
    <author>
      <name>Enakshi Khular Sharma</name>
    </author>
    <author>
      <name>Narotam Singh</name>
    </author>
    <author>
      <name>Amita Kapoor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 10 figures, 3 tables. A part of this work has been
  presented in DIGITS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.02092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.02092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.03899v1</id>
    <updated>2019-05-10T00:38:35Z</updated>
    <published>2019-05-10T00:38:35Z</published>
    <title>Integrating Artificial Intelligence into Weapon Systems</title>
    <summary>  The integration of Artificial Intelligence (AI) into weapon systems is one of
the most consequential tactical and strategic decisions in the history of
warfare. Current AI development is a remarkable combination of accelerating
capability, hidden decision mechanisms, and decreasing costs. Implementation of
these systems is in its infancy and exists on a spectrum from resilient and
flexible to simplistic and brittle. Resilient systems should be able to
effectively handle the complexities of a high-dimensional battlespace.
Simplistic AI implementations could be manipulated by an adversarial AI that
identifies and exploits their weaknesses.
  In this paper, we present a framework for understanding the development of
dynamic AI/ML systems that interactively and continuously adapt to their user's
needs. We explore the implications of increasingly capable AI in the kill chain
and how this will lead inevitably to a fully automated, always on system,
barring regulation by treaty. We examine the potential of total integration of
cyber and physical security and how this likelihood must inform the development
of AI-enabled systems with respect to the "fog of war", human morals, and
ethics.
</summary>
    <author>
      <name>Philip Feldman</name>
    </author>
    <author>
      <name>Aaron Dant</name>
    </author>
    <author>
      <name>Aaron Massey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.03899v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.03899v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; K.4.1; J.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.03848v1</id>
    <updated>2019-06-28T07:42:48Z</updated>
    <published>2019-06-28T07:42:48Z</published>
    <title>Artificial Intelligence Governance and Ethics: Global Perspectives</title>
    <summary>  Artificial intelligence (AI) is a technology which is increasingly being
utilised in society and the economy worldwide, and its implementation is
planned to become more prevalent in coming years. AI is increasingly being
embedded in our lives, supplementing our pervasive use of digital technologies.
But this is being accompanied by disquiet over problematic and dangerous
implementations of AI, or indeed, even AI itself deciding to do dangerous and
problematic actions, especially in fields such as the military, medicine and
criminal justice. These developments have led to concerns about whether and how
AI systems adhere, and will adhere to ethical standards. These concerns have
stimulated a global conversation on AI ethics, and have resulted in various
actors from different countries and sectors issuing ethics and governance
initiatives and guidelines for AI. Such developments form the basis for our
research in this report, combining our international and interdisciplinary
expertise to give an insight into what is happening in Australia, China,
Europe, India and the US.
</summary>
    <author>
      <name>Angela Daly</name>
    </author>
    <author>
      <name>Thilo Hagendorff</name>
    </author>
    <author>
      <name>Li Hui</name>
    </author>
    <author>
      <name>Monique Mann</name>
    </author>
    <author>
      <name>Vidushi Marda</name>
    </author>
    <author>
      <name>Ben Wagner</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Saskia Witteborn</name>
    </author>
    <link href="http://arxiv.org/abs/1907.03848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.03848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.07997v1</id>
    <updated>2016-10-25T18:14:24Z</updated>
    <published>2016-10-25T18:14:24Z</published>
    <title>Artificial Intelligence Safety and Cybersecurity: a Timeline of AI
  Failures</title>
    <summary>  In this work, we present and analyze reported failures of artificially
intelligent systems and extrapolate our analysis to future AIs. We suggest that
both the frequency and the seriousness of future AI failures will steadily
increase. AI Safety can be improved based on ideas developed by cybersecurity
experts. For narrow AIs safety failures are at the same, moderate, level of
criticality as in cybersecurity, however for general AI, failures have a
fundamentally different impact. A single failure of a superintelligent system
may cause a catastrophic event without a chance for recovery. The goal of
cybersecurity is to reduce the number of successful attacks on the system; the
goal of AI Safety is to make sure zero attacks succeed in bypassing the safety
mechanisms. Unfortunately, such a level of performance is unachievable. Every
security system will eventually fail; there is no such thing as a 100% secure
system.
</summary>
    <author>
      <name>Roman V. Yampolskiy</name>
    </author>
    <author>
      <name>M. S. Spellchecker</name>
    </author>
    <link href="http://arxiv.org/abs/1610.07997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.07997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08191v1</id>
    <updated>2017-10-23T10:37:50Z</updated>
    <published>2017-10-23T10:37:50Z</published>
    <title>Human-in-the-loop Artificial Intelligence</title>
    <summary>  Little by little, newspapers are revealing the bright future that Artificial
Intelligence (AI) is building. Intelligent machines will help everywhere.
However, this bright future has a dark side: a dramatic job market contraction
before its unpredictable transformation. Hence, in a near future, large numbers
of job seekers will need financial support while catching up with these novel
unpredictable jobs. This possible job market crisis has an antidote inside. In
fact, the rise of AI is sustained by the biggest knowledge theft of the recent
years. Learning AI machines are extracting knowledge from unaware skilled or
unskilled workers by analyzing their interactions. By passionately doing their
jobs, these workers are digging their own graves.
  In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI)
as a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward
aware and unaware knowledge producers with a different scheme: decisions of AI
systems generating revenues will repay the legitimate owners of the knowledge
used for taking those decisions. As modern Robin Hoods, HIT-AI researchers
should fight for a fairer Artificial Intelligence that gives back what it
steals.
</summary>
    <author>
      <name>Fabio Massimo Zanzotto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1613/jair.1.11345</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1613/jair.1.11345" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Artificial Intelligence Research, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.08191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.02882v1</id>
    <updated>2017-12-07T22:53:41Z</updated>
    <published>2017-12-07T22:53:41Z</published>
    <title>Columnar Database Techniques for Creating AI Features</title>
    <summary>  Recent advances with in-memory columnar database techniques have increased
the performance of analytical queries on very large databases and data
warehouses. At the same time, advances in artificial intelligence (AI)
algorithms have increased the ability to analyze data. We use the term AI to
encompass both Deep Learning (DL or neural network) and Machine Learning (ML
aka Big Data analytics). Our exploration of the AI full stack has led us to a
cross-stack columnar database innovation that efficiently creates features for
AI analytics. The innovation is to create Augmented Dictionary Values (ADVs) to
add to existing columnar database dictionaries in order to increase the
efficiency of featurization by minimizing data movement and data duplication.
We show how various forms of featurization (feature selection, feature
extraction, and feature creation) can be efficiently calculated in a columnar
database. The full stack AI investigation has also led us to propose an
integrated columnar database and AI architecture. This architecture has
information flows and feedback loops to improve the whole analytics cycle
during multiple iterations of extracting data from the data sources,
featurization, and analysis.
</summary>
    <author>
      <name>Brad Carlile</name>
    </author>
    <author>
      <name>Akiko Marti</name>
    </author>
    <author>
      <name>Guy Delamarter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.02882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.02882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.06417v1</id>
    <updated>2019-01-18T21:16:38Z</updated>
    <published>2019-01-18T21:16:38Z</published>
    <title>Friend, Collaborator, Student, Manager: How Design of an AI-Driven Game
  Level Editor Affects Creators</title>
    <summary>  Machine learning advances have afforded an increase in algorithms capable of
creating art, music, stories, games, and more. However, it is not yet
well-understood how machine learning algorithms might best collaborate with
people to support creative expression. To investigate how practicing designers
perceive the role of AI in the creative process, we developed a game level
design tool for Super Mario Bros.-style games with a built-in AI level
designer. In this paper we discuss our design of the Morai Maker intelligent
tool through two mixed-methods studies with a total of over one-hundred
participants. Our findings are as follows: (1) level designers vary in their
desired interactions with, and role of, the AI, (2) the AI prompted the level
designers to alter their design practices, and (3) the level designers
perceived the AI as having potential value in their design practice, varying
based on their desired role for the AI.
</summary>
    <author>
      <name>Matthew Guzdial</name>
    </author>
    <author>
      <name>Nicholas Liao</name>
    </author>
    <author>
      <name>Jonathan Chen</name>
    </author>
    <author>
      <name>Shao-Yu Chen</name>
    </author>
    <author>
      <name>Shukan Shah</name>
    </author>
    <author>
      <name>Vishwa Shah</name>
    </author>
    <author>
      <name>Joshua Reno</name>
    </author>
    <author>
      <name>Gillian Smith</name>
    </author>
    <author>
      <name>Mark Riedl</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3290605.3300854</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3290605.3300854" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures, CHI Conference on Human Factors in Computing
  Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.06417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.06417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.12307v3</id>
    <updated>2020-06-16T12:49:59Z</updated>
    <published>2019-06-28T16:55:04Z</published>
    <title>Implementing Ethics in AI: Initial Results of an Industrial Multiple
  Case Study</title>
    <summary>  Artificial intelligence (AI) is becoming increasingly widespread in system
development endeavors. As AI systems affect various stakeholders due to their
unique nature, the growing influence of these systems calls for ethical
considerations. Academic discussion and practical examples of autonomous system
failures have highlighted the need for implementing ethics in software
development. However, research on methods and tools for implementing ethics
into AI system design and development in practice is still lacking. This paper
begins to address this focal problem by providing elements needed for producing
a baseline for ethics in AI based software development. We do so by means of an
industrial multiple case study on AI systems development in the healthcare
sector. Using a research model based on extant, conceptual AI ethics
literature, we explore the current state of practice out on the field in the
absence of formal methods and tools for ethically aligned design.
</summary>
    <author>
      <name>Ville Vakkuri</name>
    </author>
    <author>
      <name>Kai-Kristian Kemell</name>
    </author>
    <author>
      <name>Pekka Abrahamsson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-35333-9_24</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-35333-9_24" rel="related"/>
    <link href="http://arxiv.org/abs/1906.12307v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.12307v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.12582v1</id>
    <updated>2019-10-14T15:13:45Z</updated>
    <published>2019-10-14T15:13:45Z</published>
    <title>Engineering Reliable Deep Learning Systems</title>
    <summary>  Recent progress in artificial intelligence (AI) using deep learning
techniques has triggered its wide-scale use across a broad range of
applications. These systems can already perform tasks such as natural language
processing of voice and text, visual recognition, question-answering,
recommendations and decision support. However, at the current level of
maturity, the use of an AI component in mission-critical or safety-critical
applications can have unexpected consequences. Consequently, serious concerns
about reliability, repeatability, trust, and maintainability of AI applications
remain. As AI becomes pervasive despite its shortcomings, more systematic ways
of approaching AI software development and certification are needed. These
fundamental aspects establish the need for a discipline on "AI Engineering".
This paper presents the current perspective of relevant AI engineering concepts
and some key challenges that need to be overcome to make significant progress
in this important area.
</summary>
    <author>
      <name>P. Santhanam</name>
    </author>
    <author>
      <name>Eitan Farchi</name>
    </author>
    <author>
      <name>Victor Pankratius</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at AAAI FSS-19: Artificial Intelligence in Government and
  Public Sector, Arlington, Virginia, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.12582v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.12582v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.00747v3</id>
    <updated>2021-10-23T13:26:56Z</updated>
    <published>2019-11-27T09:37:58Z</published>
    <title>The Transformative Potential of Artificial Intelligence</title>
    <summary>  The terms 'human-level artificial intelligence' and 'artificial general
intelligence' are widely used to refer to the possibility of advanced
artificial intelligence (AI) with potentially extreme impacts on society. These
terms are poorly defined and do not necessarily indicate what is most important
with respect to future societal impacts. We suggest that the term
'transformative AI' is a helpful alternative, reflecting the possibility that
advanced AI systems could have very large impacts on society without reaching
human-level cognitive abilities. To be most useful, however, more analysis of
what it means for AI to be 'transformative' is needed. In this paper, we
propose three different levels on which AI might be said to be transformative,
associated with different levels of societal change. We suggest that these
distinctions would improve conversations between policy makers and decision
makers concerning the mid- to long-term impacts of advances in AI. Further, we
feel this would have a positive effect on strategic foresight efforts involving
advanced AI, which we expect to illuminate paths to alternative futures. We
conclude with a discussion of the benefits of our new framework and by
highlighting directions for future work in this area.
</summary>
    <author>
      <name>Ross Gruetzemacher</name>
    </author>
    <author>
      <name>Jess Whittlestone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review, revised once. 17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.00747v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.00747v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.11095v1</id>
    <updated>2019-12-23T20:18:21Z</updated>
    <published>2019-12-23T20:18:21Z</published>
    <title>Defining AI in Policy versus Practice</title>
    <summary>  Recent concern about harms of information technologies motivate consideration
of regulatory action to forestall or constrain certain developments in the
field of artificial intelligence (AI). However, definitional ambiguity hampers
the possibility of conversation about this urgent topic of public concern.
Legal and regulatory interventions require agreed-upon definitions, but
consensus around a definition of AI has been elusive, especially in policy
conversations. With an eye towards practical working definitions and a broader
understanding of positions on these issues, we survey experts and review
published policy documents to examine researcher and policy-maker conceptions
of AI. We find that while AI researchers favor definitions of AI that emphasize
technical functionality, policy-makers instead use definitions that compare
systems to human thinking and behavior. We point out that definitions adhering
closely to the functionality of AI systems are more inclusive of technologies
in use today, whereas definitions that emphasize human-like capabilities are
most applicable to hypothetical future technologies. As a result of this gap,
ethical and regulatory efforts may overemphasize concern about future
technologies at the expense of pressing issues with existing deployed
technologies.
</summary>
    <author>
      <name>P. M. Krafft</name>
    </author>
    <author>
      <name>Meg Young</name>
    </author>
    <author>
      <name>Michael Katell</name>
    </author>
    <author>
      <name>Karen Huang</name>
    </author>
    <author>
      <name>Ghislain Bugingo</name>
    </author>
    <link href="http://arxiv.org/abs/1912.11095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00081v2</id>
    <updated>2021-05-18T17:20:34Z</updated>
    <published>2019-12-27T10:27:05Z</published>
    <title>Exciting, Useful, Worrying, Futuristic: Public Perception of Artificial
  Intelligence in 8 Countries</title>
    <summary>  As the influence and use of artificial intelligence (AI) have grown and its
transformative potential has become more apparent, many questions have been
raised regarding the economic, political, social, and ethical implications of
its use. Public opinion plays an important role in these discussions,
influencing product adoption, commercial development, research funding, and
regulation. In this paper we present results of an in-depth survey of public
opinion of artificial intelligence conducted with 10,005 respondents spanning
eight countries and six continents. We report widespread perception that AI
will have significant impact on society, accompanied by strong support for the
responsible development and use of AI, and also characterize the public's
sentiment towards AI with four key themes (exciting, useful, worrying, and
futuristic) whose prevalence distinguishes response to AI in different
countries.
</summary>
    <author>
      <name>Patrick Gage Kelley</name>
    </author>
    <author>
      <name>Yongwei Yang</name>
    </author>
    <author>
      <name>Courtney Heldreth</name>
    </author>
    <author>
      <name>Christopher Moessner</name>
    </author>
    <author>
      <name>Aaron Sedley</name>
    </author>
    <author>
      <name>Andreas Kramm</name>
    </author>
    <author>
      <name>David T. Newman</name>
    </author>
    <author>
      <name>Allison Woodruff</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3461702.3462605</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3461702.3462605" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures, 3 tables. AIES 2021: Proceedings of the AAAI/ACM
  Conference on AI, Ethics, and Society</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.00081v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00081v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.1; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00088v2</id>
    <updated>2022-06-12T18:23:50Z</updated>
    <published>2019-12-16T18:10:56Z</published>
    <title>AI for Social Impact: Learning and Planning in the Data-to-Deployment
  Pipeline</title>
    <summary>  With the maturing of AI and multiagent systems research, we have a tremendous
opportunity to direct these advances towards addressing complex societal
problems. In pursuit of this goal of AI for Social Impact, we as AI researchers
must go beyond improvements in computational methodology; it is important to
step out in the field to demonstrate social impact. To this end, we focus on
the problems of public safety and security, wildlife conservation, and public
health in low-resource communities, and present research advances in multiagent
systems to address one key cross-cutting challenge: how to effectively deploy
our limited intervention resources in these problem domains. We present case
studies from our deployments around the world as well as lessons learned that
we hope are of use to researchers who are interested in AI for Social Impact.
In pushing this research agenda, we believe AI can indeed play an important
role in fighting social injustice and improving society.
</summary>
    <author>
      <name>Andrew Perrault</name>
    </author>
    <author>
      <name>Fei Fang</name>
    </author>
    <author>
      <name>Arunesh Sinha</name>
    </author>
    <author>
      <name>Milind Tambe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1609/aimag.v41i4.5296</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1609/aimag.v41i4.5296" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AI Magazine, Winter 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.00088v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00088v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.06528v1</id>
    <updated>2020-01-17T20:53:10Z</updated>
    <published>2020-01-17T20:53:10Z</published>
    <title>Activism by the AI Community: Analysing Recent Achievements and Future
  Prospects</title>
    <summary>  The artificial intelligence community (AI) has recently engaged in activism
in relation to their employers, other members of the community, and their
governments in order to shape the societal and ethical implications of AI. It
has achieved some notable successes, but prospects for further political
organising and activism are uncertain. We survey activism by the AI community
over the last six years; apply two analytical frameworks drawing upon the
literature on epistemic communities, and worker organising and bargaining; and
explore what they imply for the future prospects of the AI community. Success
thus far has hinged on a coherent shared culture, and high bargaining power due
to the high demand for a limited supply of AI talent. Both are crucial to the
future of AI activism and worthy of sustained attention.
</summary>
    <author>
      <name>Haydn Belfield</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Forthcoming in Proceedings of the 2020 AAAI/ACM Conference on
  Artificial Intelligence, Ethics and Society. 7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.06528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.06528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; K.4; K.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09766v1</id>
    <updated>2020-01-13T14:15:18Z</updated>
    <published>2020-01-13T14:15:18Z</published>
    <title>Artificial Artificial Intelligence: Measuring Influence of AI
  'Assessments' on Moral Decision-Making</title>
    <summary>  Given AI's growing role in modeling and improving decision-making, how and
when to present users with feedback is an urgent topic to address. We
empirically examined the effect of feedback from false AI on moral
decision-making about donor kidney allocation. We found some evidence that
judgments about whether a patient should receive a kidney can be influenced by
feedback about participants' own decision-making perceived to be given by AI,
even if the feedback is entirely random. We also discovered different effects
between assessments presented as being from human experts and assessments
presented as being from AI.
</summary>
    <author>
      <name>Lok Chan</name>
    </author>
    <author>
      <name>Kenzie Doyle</name>
    </author>
    <author>
      <name>Duncan McElfresh</name>
    </author>
    <author>
      <name>Vincent Conitzer</name>
    </author>
    <author>
      <name>John P. Dickerson</name>
    </author>
    <author>
      <name>Jana Schaich Borg</name>
    </author>
    <author>
      <name>Walter Sinnott-Armstrong</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3375627.3375870</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3375627.3375870" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2020 AAAI/ACM Conference on AI, Ethics, and
  Society (AIES '20)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2001.09766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.05013v1</id>
    <updated>2020-02-10T10:29:31Z</updated>
    <published>2020-02-10T10:29:31Z</published>
    <title>Machine Learning-Assisted Anomaly Detection in Maritime Navigation Using
  AIS Data</title>
    <summary>  The automatic identification system (AIS) reports vessels' static and dynamic
information, which are essential for maritime traffic situation awareness.
However, AIS transponders can be switched off to hide suspicious activities,
such as illegal fishing, or piracy. Therefore, this paper uses real world AIS
data to analyze the possibility of successful detection of various anomalies in
the maritime domain. We propose a multi-class artificial neural network
(ANN)-based anomaly detection framework to classify intentional and
non-intentional AIS on-off switching anomalies. The multi-class anomaly
framework captures AIS message dropouts due to various reasons, e.g., channel
effects or intentional one for carrying illegal activities. We extract
position, speed, course and timing information from real world AIS data, and
use them to train a 2-class (normal and anomaly) and a 3-class (normal, power
outage and anomaly) anomaly detection models. Our results show that the models
achieve around 99.9% overall accuracy, and are able to classify a test sample
in the order of microseconds.
</summary>
    <author>
      <name>Sandeep Kumar Singh</name>
    </author>
    <author>
      <name>Frank Heymann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This conference paper is uploaded here for non-comercial purposes</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.05013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.09054v1</id>
    <updated>2020-02-20T22:52:43Z</updated>
    <published>2020-02-20T22:52:43Z</published>
    <title>Designing Fair AI for Managing Employees in Organizations: A Review,
  Critique, and Design Agenda</title>
    <summary>  Organizations are rapidly deploying artificial intelligence (AI) systems to
manage their workers. However, AI has been found at times to be unfair to
workers. Unfairness toward workers has been associated with decreased worker
effort and increased worker turnover. To avoid such problems, AI systems must
be designed to support fairness and redress instances of unfairness. Despite
the attention related to AI unfairness, there has not been a theoretical and
systematic approach to developing a design agenda. This paper addresses the
issue in three ways. First, we introduce the organizational justice theory,
three different fairness types (distributive, procedural, interactional), and
the frameworks for redressing instances of unfairness (retributive justice,
restorative justice). Second, we review the design literature that specifically
focuses on issues of AI fairness in organizations. Third, we propose a design
agenda for AI fairness in organizations that applies each of the fairness types
to organizational scenarios. Then, the paper concludes with implications for
future research.
</summary>
    <author>
      <name>Lionel P. Robert</name>
    </author>
    <author>
      <name>Casey Pierce</name>
    </author>
    <author>
      <name>Liz Morris</name>
    </author>
    <author>
      <name>Sangmi Kim</name>
    </author>
    <author>
      <name>Rasha Alahmad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">66 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.09054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.09054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.11000v1</id>
    <updated>2020-02-25T16:16:31Z</updated>
    <published>2020-02-25T16:16:31Z</published>
    <title>Distributed Ledger for Provenance Tracking of Artificial Intelligence
  Assets</title>
    <summary>  High availability of data is responsible for the current trends in Artificial
Intelligence (AI) and Machine Learning (ML). However, high-grade datasets are
reluctantly shared between actors because of lacking trust and fear of losing
control. Provenance tracing systems are a possible measure to build trust by
improving transparency. Especially the tracing of AI assets along complete AI
value chains bears various challenges such as trust, privacy, confidentiality,
traceability, and fair remuneration. In this paper we design a graph-based
provenance model for AI assets and their relations within an AI value chain.
Moreover, we propose a protocol to exchange AI assets securely to selected
parties. The provenance model and exchange protocol are then combined and
implemented as a smart contract on a permission-less blockchain. We show how
the smart contract enables the tracing of AI assets in an existing industry use
case while solving all challenges. Consequently, our smart contract helps to
increase traceability and transparency, encourages trust between actors and
thus fosters collaboration between them.
</summary>
    <author>
      <name>Philipp Lüthi</name>
    </author>
    <author>
      <name>Thibault Gagnaux</name>
    </author>
    <author>
      <name>Marcel Gygli</name>
    </author>
    <link href="http://arxiv.org/abs/2002.11000v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11000v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.06894v1</id>
    <updated>2020-04-15T06:03:34Z</updated>
    <published>2020-04-15T06:03:34Z</published>
    <title>Human Evaluation of Interpretability: The Case of AI-Generated Music
  Knowledge</title>
    <summary>  Interpretability of machine learning models has gained more and more
attention among researchers in the artificial intelligence (AI) and
human-computer interaction (HCI) communities. Most existing work focuses on
decision making, whereas we consider knowledge discovery. In particular, we
focus on evaluating AI-discovered knowledge/rules in the arts and humanities.
From a specific scenario, we present an experimental procedure to collect and
assess human-generated verbal interpretations of AI-generated music
theory/rules rendered as sophisticated symbolic/numeric objects. Our goal is to
reveal both the possibilities and the challenges in such a process of decoding
expressive messages from AI sources. We treat this as a first step towards 1)
better design of AI representations that are human interpretable and 2) a
general methodology to evaluate interpretability of AI-discovered knowledge
representations.
</summary>
    <author>
      <name>Haizi Yu</name>
    </author>
    <author>
      <name>Heinrich Taube</name>
    </author>
    <author>
      <name>James A. Evans</name>
    </author>
    <author>
      <name>Lav R. Varshney</name>
    </author>
    <link href="http://arxiv.org/abs/2004.06894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.06894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.08140v1</id>
    <updated>2020-06-15T05:30:48Z</updated>
    <published>2020-06-15T05:30:48Z</published>
    <title>The Social Contract for AI</title>
    <summary>  Like any technology, AI systems come with inherent risks and potential
benefits. It comes with potential disruption of established norms and methods
of work, societal impacts and externalities. One may think of the adoption of
technology as a form of social contract, which may evolve or fluctuate in time,
scale, and impact. It is important to keep in mind that for AI, meeting the
expectations of this social contract is critical, because recklessly driving
the adoption and implementation of unsafe, irresponsible, or unethical AI
systems may trigger serious backlash against industry and academia involved
which could take decades to resolve, if not actually seriously harm society.
For the purpose of this paper, we consider that a social contract arises when
there is sufficient consensus within society to adopt and implement this new
technology. As such, to enable a social contract to arise for the adoption and
implementation of AI, developing: 1) A socially accepted purpose, through 2) A
safe and responsible method, with 3) A socially aware level of risk involved,
for 4) A socially beneficial outcome, is key.
</summary>
    <author>
      <name>Mirka Snyder Caron</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Abhishek Gupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Microsoft</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted paper for presentation at the IJCAI 2019 AI for Social Good
  workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.08140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.08140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.07250v1</id>
    <updated>2020-07-13T20:30:26Z</updated>
    <published>2020-07-13T20:30:26Z</published>
    <title>Towards an Interface Description Template for AI-enabled Systems</title>
    <summary>  Reuse is a common system architecture approach that seeks to instantiate a
system architecture with existing components. However, reusing components with
AI capabilities might introduce new risks as there is currently no framework
that guides the selection of necessary information to assess their portability
to operate in a system different than the one for which the component was
originally purposed. We know from SW-intensive systems that AI algorithms are
generally fragile and behave unexpectedly to changes in context and boundary
conditions. The question we address in this paper is, what type of information
should be captured in the Interface Control Document (ICD) of an AI-enabled
system or component to assess its compatibility with a system for which it was
not designed originally. We present ongoing work on establishing an interface
description template that captures the main information of an AI-enabled
component to facilitate its adequate reuse across different systems and
operational contexts. Our work is inspired by Google's Model Card concept,
which was developed with the same goal but focused on the reusability of AI
algorithms. We extend that concept to address system-level autonomy
capabilities of AI-enabled cyber-physical systems.
</summary>
    <author>
      <name>Niloofar Shadab</name>
    </author>
    <author>
      <name>Alejandro Salado</name>
    </author>
    <link href="http://arxiv.org/abs/2007.07250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.07250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.07743v1</id>
    <updated>2020-08-18T04:50:23Z</updated>
    <published>2020-08-18T04:50:23Z</published>
    <title>Turing Test and the Practice of Law: The Role of Autonomous Levels of AI
  Legal Reasoning</title>
    <summary>  Artificial Intelligence (AI) is increasingly being applied to law and a
myriad of legal tasks amid attempts to bolster AI Legal Reasoning (AILR)
autonomous capabilities. A major question that has generally been unaddressed
involves how we will know when AILR has achieved autonomous capacities. The
field of AI has grappled with similar quandaries over how to assess the
attainment of Artificial General Intelligence (AGI), a persistently discussed
issue among scholars since the inception of AI, with the Turing Test communally
being considered as the bellwether for ascertaining such matters. This paper
proposes a variant of the Turing Test that is customized for specific use in
the AILR realm, including depicting how this famous gold standard of AI
fulfillment can be robustly applied across the autonomous levels of AI Legal
Reasoning.
</summary>
    <author>
      <name>Lance Eliot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.07743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.07743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; J.7.0; K.5.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.00802v1</id>
    <updated>2020-09-02T03:33:40Z</updated>
    <published>2020-09-02T03:33:40Z</published>
    <title>Estimating the Brittleness of AI: Safety Integrity Levels and the Need
  for Testing Out-Of-Distribution Performance</title>
    <summary>  Test, Evaluation, Verification, and Validation (TEVV) for Artificial
Intelligence (AI) is a challenge that threatens to limit the economic and
societal rewards that AI researchers have devoted themselves to producing. A
central task of TEVV for AI is estimating brittleness, where brittleness
implies that the system functions well within some bounds and poorly outside of
those bounds. This paper argues that neither of those criteria are certain of
Deep Neural Networks. First, highly touted AI successes (eg. image
classification and speech recognition) are orders of magnitude more
failure-prone than are typically certified in critical systems even within
design bounds (perfectly in-distribution sampling). Second, performance falls
off only gradually as inputs become further Out-Of-Distribution (OOD). Enhanced
emphasis is needed on designing systems that are resilient despite
failure-prone AI components as well as on evaluating and improving OOD
performance in order to get AI to where it can clear the challenging hurdles of
TEVV and certification.
</summary>
    <author>
      <name>Andrew J. Lohn</name>
    </author>
    <link href="http://arxiv.org/abs/2009.00802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.00802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.11100v1</id>
    <updated>2020-09-22T00:56:41Z</updated>
    <published>2020-09-22T00:56:41Z</published>
    <title>Engaging Teachers to Co-Design Integrated AI Curriculum for K-12
  Classrooms</title>
    <summary>  Artificial Intelligence (AI) education is an increasingly popular topic area
for K-12 teachers. However, little research has investigated how AI education
can be designed to be more accessible to all learners. We organized co-design
workshops with 15 K-12 teachers to identify opportunities to integrate AI
education into core curriculum to leverage learners' interests. During the
co-design workshops, teachers and researchers co-created lesson plans where AI
concepts were embedded into various core subjects. We found that K-12 teachers
need additional scaffolding in the curriculum to facilitate ethics and data
discussions, and value supports for learner engagement, collaboration, and
reflection. We identify opportunities for researchers and teachers to
collaborate to make AI education more accessible, and present an exemplar
lesson plan that shows entry points for teaching AI in non-computing subjects.
We also reflect on co-designing with K-12 teachers in a remote setting.
</summary>
    <author>
      <name>Jessica Van Brummelen</name>
    </author>
    <author>
      <name>Phoebe Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.11100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ed-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ed-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.08885v1</id>
    <updated>2020-10-17T23:03:06Z</updated>
    <published>2020-10-17T23:03:06Z</published>
    <title>A Game AI Competition to foster Collaborative AI research and
  development</title>
    <summary>  Game AI competitions are important to foster research and development on Game
AI and AI in general. These competitions supply different challenging problems
that can be translated into other contexts, virtual or real. They provide
frameworks and tools to facilitate the research on their core topics and
provide means for comparing and sharing results. A competition is also a way to
motivate new researchers to study these challenges. In this document, we
present the Geometry Friends Game AI Competition. Geometry Friends is a
two-player cooperative physics-based puzzle platformer computer game. The
concept of the game is simple, though its solving has proven to be difficult.
While the main and apparent focus of the game is cooperation, it also relies on
other AI-related problems such as planning, plan execution, and motion control,
all connected to situational awareness. All of these must be solved in
real-time. In this paper, we discuss the competition and the challenges it
brings, and present an overview of the current solutions.
</summary>
    <author>
      <name>Ana Salta</name>
    </author>
    <author>
      <name>Rui Prada</name>
    </author>
    <author>
      <name>Francisco S. Melo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TG.2020.3024160</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TG.2020.3024160" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Games, pp. 1-12, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.08885v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.08885v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.15551v1</id>
    <updated>2020-10-10T15:38:53Z</updated>
    <published>2020-10-10T15:38:53Z</published>
    <title>Investigating the Robustness of Artificial Intelligent Algorithms with
  Mixture Experiments</title>
    <summary>  Artificial intelligent (AI) algorithms, such as deep learning and XGboost,
are used in numerous applications including computer vision, autonomous
driving, and medical diagnostics. The robustness of these AI algorithms is of
great interest as inaccurate prediction could result in safety concerns and
limit the adoption of AI systems. In this paper, we propose a framework based
on design of experiments to systematically investigate the robustness of AI
classification algorithms. A robust classification algorithm is expected to
have high accuracy and low variability under different application scenarios.
The robustness can be affected by a wide range of factors such as the imbalance
of class labels in the training dataset, the chosen prediction algorithm, the
chosen dataset of the application, and a change of distribution in the training
and test datasets. To investigate the robustness of AI classification
algorithms, we conduct a comprehensive set of mixture experiments to collect
prediction performance results. Then statistical analyses are conducted to
understand how various factors affect the robustness of AI classification
algorithms. We summarize our findings and provide suggestions to practitioners
in AI applications.
</summary>
    <author>
      <name>Jiayi Lian</name>
    </author>
    <author>
      <name>Laura Freeman</name>
    </author>
    <author>
      <name>Yili Hong</name>
    </author>
    <author>
      <name>Xinwei Deng</name>
    </author>
    <link href="http://arxiv.org/abs/2010.15551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.15551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.05876v2</id>
    <updated>2020-12-16T23:21:05Z</updated>
    <published>2020-12-10T18:31:38Z</published>
    <title>Neurosymbolic AI: The 3rd Wave</title>
    <summary>  Current advances in Artificial Intelligence (AI) and Machine Learning (ML)
have achieved unprecedented impact across research communities and industry.
Nevertheless, concerns about trust, safety, interpretability and accountability
of AI were raised by influential thinkers. Many have identified the need for
well-founded knowledge representation and reasoning to be integrated with deep
learning and for sound explainability. Neural-symbolic computing has been an
active area of research for many years seeking to bring together robust
learning in neural networks with reasoning and explainability via symbolic
representations for network models. In this paper, we relate recent and early
research results in neurosymbolic AI with the objective of identifying the key
ingredients of the next wave of AI systems. We focus on research that
integrates in a principled way neural network-based learning with symbolic
knowledge representation and logical reasoning. The insights provided by 20
years of neural-symbolic computing are shown to shed new light onto the
increasingly prominent role of trust, safety, interpretability and
accountability of AI. We also identify promising directions and challenges for
the next decade of AI research from the perspective of neural-symbolic systems.
</summary>
    <author>
      <name>Artur d'Avila Garcez</name>
    </author>
    <author>
      <name>Luis C. Lamb</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.05876v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.05876v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.4; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.11976v1</id>
    <updated>2020-12-22T12:58:08Z</updated>
    <published>2020-12-22T12:58:08Z</published>
    <title>A Maturity Assessment Framework for Conversational AI Development
  Platforms</title>
    <summary>  Conversational Artificial Intelligence (AI) systems have recently
sky-rocketed in popularity and are now used in many applications, from car
assistants to customer support. The development of conversational AI systems is
supported by a large variety of software platforms, all with similar goals, but
different focus points and functionalities. A systematic foundation for
classifying conversational AI platforms is currently lacking. We propose a
framework for assessing the maturity level of conversational AI development
platforms. Our framework is based on a systematic literature review, in which
we extracted common and distinguishing features of various open-source and
commercial (or in-house) platforms. Inspired by language reference frameworks,
we identify different maturity levels that a conversational AI development
platform may exhibit in understanding and responding to user inputs. Our
framework can guide organizations in selecting a conversational AI development
platform according to their needs, as well as helping researchers and platform
developers improving the maturity of their platforms.
</summary>
    <author>
      <name>Johan Aronsson</name>
    </author>
    <author>
      <name>Philip Lu</name>
    </author>
    <author>
      <name>Daniel Strüber</name>
    </author>
    <author>
      <name>Thorsten Berger</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3412841.3442046</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3412841.3442046" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures. Accepted for publication at SAC 2021:
  ACM/SIGAPP Symposium On Applied Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.11976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.11976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.12262v1</id>
    <updated>2020-12-16T05:10:08Z</updated>
    <published>2020-12-16T05:10:08Z</published>
    <title>The Last State of Artificial Intelligence in Project Management</title>
    <summary>  Artificial intelligence (AI) has been used to advance different fields, such
as education, healthcare, and finance. However, the application of AI in the
field of project management (PM) has not progressed equally. This paper reports
on a systematic review of the published studies used to investigate the
application of AI in PM. This systematic review identified relevant papers
using Web of Science, Science Direct, and Google Scholar databases. Of the 652
articles found, 58 met the predefined criteria and were included in the review.
Included papers were classified per the following dimensions: PM knowledge
areas, PM processes, and AI techniques. The results indicated that the
application of AI in PM was in its early stages and AI models have not applied
for multiple PM processes especially in processes groups of project stakeholder
management, project procurements management, and project communication
management. However, the most popular PM processes among included papers were
project effort prediction and cost estimation, and the most popular AI
techniques were support vector machines, neural networks, and genetic
algorithms.
</summary>
    <author>
      <name>Mohammad Reza Davahli</name>
    </author>
    <link href="http://arxiv.org/abs/2012.12262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.12262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.04719v1</id>
    <updated>2021-01-12T19:44:27Z</updated>
    <published>2021-01-12T19:44:27Z</published>
    <title>Expanding Explainability: Towards Social Transparency in AI systems</title>
    <summary>  As AI-powered systems increasingly mediate consequential decision-making,
their explainability is critical for end-users to take informed and accountable
actions. Explanations in human-human interactions are socially-situated. AI
systems are often socio-organizationally embedded. However, Explainable AI
(XAI) approaches have been predominantly algorithm-centered. We take a
developmental step towards socially-situated XAI by introducing and exploring
Social Transparency (ST), a sociotechnically informed perspective that
incorporates the socio-organizational context into explaining AI-mediated
decision-making. To explore ST conceptually, we conducted interviews with 29 AI
users and practitioners grounded in a speculative design scenario. We suggested
constitutive design elements of ST and developed a conceptual framework to
unpack ST's effect and implications at the technical, decision-making, and
organizational level. The framework showcases how ST can potentially calibrate
trust in AI, improve decision-making, facilitate organizational collective
actions, and cultivate holistic explainability. Our work contributes to the
discourse of Human-Centered XAI by expanding the design space of XAI.
</summary>
    <author>
      <name>Upol Ehsan</name>
    </author>
    <author>
      <name>Q. Vera Liao</name>
    </author>
    <author>
      <name>Michael Muller</name>
    </author>
    <author>
      <name>Mark O. Riedl</name>
    </author>
    <author>
      <name>Justin D. Weisz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3411764.3445188</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3411764.3445188" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CHI2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.04719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.04719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.11501v1</id>
    <updated>2021-01-27T15:57:04Z</updated>
    <published>2021-01-27T15:57:04Z</published>
    <title>Evolution of artificial intelligence languages, a systematic literature
  review</title>
    <summary>  The field of Artificial Intelligence (AI) has undoubtedly received
significant attention in recent years. AI is being adopted to provide solutions
to problems in fields such as medicine, engineering, education, government and
several other domains. In order to analyze the state of the art of research in
the field of AI, we present a systematic literature review focusing on the
Evolution of AI programming languages. We followed the systematic literature
review method by searching relevant databases like SCOPUS, IEEE Xplore and
Google Scholar. EndNote reference manager was used to catalog the relevant
extracted papers. Our search returned a total of 6565 documents, whereof 69
studies were retained. Of the 69 retained studies, 15 documents discussed LISP
programming language, another 34 discussed PROLOG programming language, the
remaining 20 documents were spread between Logic and Object Oriented
Programming (LOOP), ARCHLOG, Epistemic Ontology Language with Constraints
(EOLC), Python, C++, ADA and JAVA programming languages. This review provides
information on the year of implementation, development team, capabilities,
limitations and applications of each of the AI programming languages discussed.
The information in this review could guide practitioners and researchers in AI
to make the right choice of languages to implement their novel AI methods.
</summary>
    <author>
      <name>Emmanuel Adetiba</name>
    </author>
    <author>
      <name>Temitope John</name>
    </author>
    <author>
      <name>Adekunle Akinrinmade</name>
    </author>
    <author>
      <name>Funmilayo Moninuola</name>
    </author>
    <author>
      <name>Oladipupo Akintade</name>
    </author>
    <author>
      <name>Joke Badejo</name>
    </author>
    <link href="http://arxiv.org/abs/2101.11501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.11501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.01265v1</id>
    <updated>2021-02-02T02:53:40Z</updated>
    <published>2021-02-02T02:53:40Z</published>
    <title>The Limits of Global Inclusion in AI Development</title>
    <summary>  Those best-positioned to profit from the proliferation of artificial
intelligence (AI) systems are those with the most economic power. Extant global
inequality has motivated Western institutions to involve more diverse groups in
the development and application of AI systems, including hiring foreign labour
and establishing extra-national data centers and laboratories. However, given
both the propensity of wealth to abet its own accumulation and the lack of
contextual knowledge in top-down AI solutions, we argue that more focus should
be placed on the redistribution of power, rather than just on including
underrepresented groups. Unless more is done to ensure that opportunities to
lead AI development are distributed justly, the future may hold only AI systems
which are unsuited to their conditions of application, and exacerbate
inequality.
</summary>
    <author>
      <name>Alan Chan</name>
    </author>
    <author>
      <name>Chinasa T. Okolo</name>
    </author>
    <author>
      <name>Zachary Terner</name>
    </author>
    <author>
      <name>Angelina Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI 2021 Workshop on Reframing Diversity in AI</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.01265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.01265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.08079v1</id>
    <updated>2021-03-15T00:45:44Z</updated>
    <published>2021-03-15T00:45:44Z</published>
    <title>Crossing the Tepper Line: An Emerging Ontology for Describing the
  Dynamic Sociality of Embodied AI</title>
    <summary>  Artificial intelligences (AI) are increasingly being embodied and embedded in
the world to carry out tasks and support decision-making with and for people.
Robots, recommender systems, voice assistants, virtual humans - do these
disparate types of embodied AI have something in common? Here we show how they
can manifest as "socially embodied AI." We define this as the state that
embodied AI "circumstantially" take on within interactive contexts when
perceived as both social and agentic by people. We offer a working ontology
that describes how embodied AI can dynamically transition into socially
embodied AI. We propose an ontological heuristic for describing the threshold:
the Tepper line. We reinforce our theoretical work with expert insights from a
card sort workshop. We end with two case studies to illustrate the dynamic and
contextual nature of this heuristic.
</summary>
    <author>
      <name>Katie Seaborn</name>
    </author>
    <author>
      <name>Peter Pennefather</name>
    </author>
    <author>
      <name>Norihisa P. Miyake</name>
    </author>
    <author>
      <name>Mihoko Otake-Matsuura</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3411763.3451783</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3411763.3451783" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CHI EA '21</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.08079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.08079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.11042v1</id>
    <updated>2021-03-19T22:05:20Z</updated>
    <published>2021-03-19T22:05:20Z</published>
    <title>AI Specialization for Pathways of Economic Diversification</title>
    <summary>  The growth in AI is rapidly transforming the structure of economic
production. However, very little is known about how within-AI specialization
may relate to broad-based economic diversification. This paper provides a
data-driven framework to integrate the interconnection between AI-based
specialization with goods and services export specialization to help design
future comparative advantage based on the inherent capabilities of nations.
Using detailed data on private investment in AI and export specialization for
more than 80 countries, we propose a systematic framework to help identify the
connection from AI to goods and service sector specialization. The results are
instructive for nations that aim to harness AI specialization to help guide
sources of future competitive advantage. The operational framework could help
inform the public and private sector to uncover connections with nearby areas
of specialization.
</summary>
    <author>
      <name>Saurabh Mishra</name>
    </author>
    <author>
      <name>Robert Koopman</name>
    </author>
    <author>
      <name>Giuditta De-Prato</name>
    </author>
    <author>
      <name>Anand Rao</name>
    </author>
    <author>
      <name>Israel Osorio-Rodarte</name>
    </author>
    <author>
      <name>Julie Kim</name>
    </author>
    <author>
      <name>Nikola Spatafora</name>
    </author>
    <author>
      <name>Keith Strier</name>
    </author>
    <author>
      <name>Andrea Zaccaria</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 20 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.11042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.11042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12920v1</id>
    <updated>2021-04-27T00:28:38Z</updated>
    <published>2021-04-27T00:28:38Z</published>
    <title>Equity and Artificial Intelligence in Education: Will "AIEd" Amplify or
  Alleviate Inequities in Education?</title>
    <summary>  The development of educational AI (AIEd) systems has often been motivated by
their potential to promote educational equity and reduce achievement gaps
across different groups of learners -- for example, by scaling up the benefits
of one-on-one human tutoring to a broader audience, or by filling gaps in
existing educational services. Given these noble intentions, why might AIEd
systems have inequitable impacts in practice? In this chapter, we discuss four
lenses that can be used to examine how and why AIEd systems risk amplifying
existing inequities. Building from these lenses, we then outline possible paths
towards more equitable futures for AIEd, while highlighting debates surrounding
each proposal. In doing so, we hope to provoke new conversations around the
design of equitable AIEd, and to push ongoing conversations in the field
forward.
</summary>
    <author>
      <name>Kenneth Holstein</name>
    </author>
    <author>
      <name>Shayan Doroudi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The co-first authors contributed equally to this work</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.12920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.07804v1</id>
    <updated>2021-05-17T13:18:57Z</updated>
    <published>2021-05-17T13:18:57Z</published>
    <title>Designer-User Communication for XAI: An epistemological approach to
  discuss XAI design</title>
    <summary>  Artificial Intelligence is becoming part of any technology we use nowadays.
If the AI informs people's decisions, the explanation about AI's outcomes,
results, and behavior becomes a necessary capability. However, the discussion
of XAI features with various stakeholders is not a trivial task. Most of the
available frameworks and methods for XAI focus on data scientists and ML
developers as users. Our research is about XAI for end-users of AI systems. We
argue that we need to discuss XAI early in the AI-system design process and
with all stakeholders. In this work, we aimed at investigating how to
operationalize the discussion about XAI scenarios and opportunities among
designers and developers of AI and its end-users. We took the Signifying
Message as our conceptual tool to structure and discuss XAI scenarios. We
experiment with its use for the discussion of a healthcare AI-System.
</summary>
    <author>
      <name>Juliana Jansen Ferreira</name>
    </author>
    <author>
      <name>Mateus Monteiro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM CHI Workshop on Operationalizing Human-Centered Perspectives in
  Explainable AI at CHI 2021. 6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.07804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.07804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.08475v1</id>
    <updated>2021-05-18T12:37:18Z</updated>
    <published>2021-05-18T12:37:18Z</published>
    <title>AI and Shared Prosperity</title>
    <summary>  Future advances in AI that automate away human labor may have stark
implications for labor markets and inequality. This paper proposes a framework
to analyze the effects of specific types of AI systems on the labor market,
based on how much labor demand they will create versus displace, while taking
into account that productivity gains also make society wealthier and thereby
contribute to additional labor demand. This analysis enables ethically-minded
companies creating or deploying AI systems as well as researchers and
policymakers to take into account the effects of their actions on labor markets
and inequality, and therefore to steer progress in AI in a direction that
advances shared prosperity and an inclusive economic future for all of
humanity.
</summary>
    <author>
      <name>Katya Klinova</name>
    </author>
    <author>
      <name>Anton Korinek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3461702.3462619</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3461702.3462619" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and
  Society (AIES '21)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2105.08475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.08475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4; K.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.11036v1</id>
    <updated>2021-05-31T14:08:22Z</updated>
    <published>2021-05-31T14:08:22Z</published>
    <title>Know Your Model (KYM): Increasing Trust in AI and Machine Learning</title>
    <summary>  The widespread utilization of AI systems has drawn attention to the potential
impacts of such systems on society. Of particular concern are the consequences
that prediction errors may have on real-world scenarios, and the trust humanity
places in AI systems. It is necessary to understand how we can evaluate
trustworthiness in AI and how individuals and entities alike can develop
trustworthy AI systems. In this paper, we analyze each element of
trustworthiness and provide a set of 20 guidelines that can be leveraged to
ensure optimal AI functionality while taking into account the greater ethical,
technical, and practical impacts to humanity. Moreover, the guidelines help
ensure that trustworthiness is provable and can be demonstrated, they are
implementation agnostic, and they can be applied to any AI system in any
sector.
</summary>
    <author>
      <name>Mary Roszel</name>
    </author>
    <author>
      <name>Robert Norvill</name>
    </author>
    <author>
      <name>Jean Hilger</name>
    </author>
    <author>
      <name>Radu State</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.11036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.02868v1</id>
    <updated>2021-07-06T19:59:14Z</updated>
    <published>2021-07-06T19:59:14Z</published>
    <title>Principles for Evaluation of AI/ML Model Performance and Robustness</title>
    <summary>  The Department of Defense (DoD) has significantly increased its investment in
the design, evaluation, and deployment of Artificial Intelligence and Machine
Learning (AI/ML) capabilities to address national security needs. While there
are numerous AI/ML successes in the academic and commercial sectors, many of
these systems have also been shown to be brittle and nonrobust. In a complex
and ever-changing national security environment, it is vital that the DoD
establish a sound and methodical process to evaluate the performance and
robustness of AI/ML models before these new capabilities are deployed to the
field. This paper reviews the AI/ML development process, highlights common best
practices for AI/ML model evaluation, and makes recommendations to DoD
evaluators to ensure the deployment of robust AI/ML capabilities for national
security needs.
</summary>
    <author>
      <name>Olivia Brown</name>
    </author>
    <author>
      <name>Andrew Curtis</name>
    </author>
    <author>
      <name>Justin Goodwin</name>
    </author>
    <link href="http://arxiv.org/abs/2107.02868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.02868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.04409v1</id>
    <updated>2021-07-06T20:32:14Z</updated>
    <published>2021-07-06T20:32:14Z</published>
    <title>An Orchestration Platform that Puts Radiologists in the Driver's Seat of
  AI Innovation: A Methodological Approach</title>
    <summary>  Current AI-driven research in radiology requires resources and expertise that
are often inaccessible to small and resource-limited labs. The clinicians who
are able to participate in AI research are frequently well-funded,
well-staffed, and either have significant experience with AI and computing, or
have access to colleagues or facilities that do. Current imaging data is
clinician-oriented and is not easily amenable to machine learning initiatives,
resulting in inefficient, time consuming, and costly efforts that rely upon a
crew of data engineers and machine learning scientists, and all too often
preclude radiologists from driving AI research and innovation. We present the
system and methodology we have developed to address infrastructure and platform
needs, while reducing the staffing and resource barriers to entry. We emphasize
a data-first and modular approach that streamlines the AI development and
deployment process while providing efficient and familiar interfaces for
radiologists, such that they can be the drivers of new AI innovations.
</summary>
    <author>
      <name>Raphael Y. Cohen</name>
    </author>
    <author>
      <name>Aaron D. Sodickson</name>
    </author>
    <link href="http://arxiv.org/abs/2107.04409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.04409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.06747v1</id>
    <updated>2021-07-14T14:47:24Z</updated>
    <published>2021-07-14T14:47:24Z</published>
    <title>Artificial Intelligence in PET: an Industry Perspective</title>
    <summary>  Artificial intelligence (AI) has significant potential to positively impact
and advance medical imaging, including positron emission tomography (PET)
imaging applications. AI has the ability to enhance and optimize all aspects of
the PET imaging chain from patient scheduling, patient setup, protocoling, data
acquisition, detector signal processing, reconstruction, image processing and
interpretation. AI poses industry-specific challenges which will need to be
addressed and overcome to maximize the future potentials of AI in PET. This
paper provides an overview of these industry-specific challenges for the
development, standardization, commercialization, and clinical adoption of AI,
and explores the potential enhancements to PET imaging brought on by AI in the
near future. In particular, the combination of on-demand image reconstruction,
AI, and custom designed data processing workflows may open new possibilities
for innovation which would positively impact the industry and ultimately
patients.
</summary>
    <author>
      <name>Arkadiusz Sitek</name>
    </author>
    <author>
      <name>Sangtae Ahn</name>
    </author>
    <author>
      <name>Evren Asma</name>
    </author>
    <author>
      <name>Adam Chandler</name>
    </author>
    <author>
      <name>Alvin Ihsani</name>
    </author>
    <author>
      <name>Sven Prevrhal</name>
    </author>
    <author>
      <name>Arman Rahmim</name>
    </author>
    <author>
      <name>Babak Saboury</name>
    </author>
    <author>
      <name>Kris Thielemans</name>
    </author>
    <link href="http://arxiv.org/abs/2107.06747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.06747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.12806v2</id>
    <updated>2022-01-18T13:52:12Z</updated>
    <published>2021-07-27T13:28:07Z</published>
    <title>Towards Industrial Private AI: A two-tier framework for data and model
  security</title>
    <summary>  With the advances in 5G and IoT devices, the industries are vastly adopting
artificial intelligence (AI) techniques for improving classification and
prediction-based services. However, the use of AI also raises concerns
regarding privacy and security that can be misused or leaked. Private AI was
recently coined to address the data security issue by combining AI with
encryption techniques, but existing studies have shown that model inversion
attacks can be used to reverse engineer the images from model parameters. In
this regard, we propose a Federated Learning and Encryption-based Private
(FLEP) AI framework that provides two-tier security for data and model
parameters in an IIoT environment. We proposed a three-layer encryption method
for data security and provide a hypothetical method to secure the model
parameters. Experimental results show that the proposed method achieves better
encryption quality at the expense of slightly increased execution time. We also
highlight several open issues and challenges regarding the FLEP AI framework's
realization.
</summary>
    <author>
      <name>Sunder Ali Khowaja</name>
    </author>
    <author>
      <name>Kapal Dev</name>
    </author>
    <author>
      <name>Nawab Muhammad Faseeh Qureshi</name>
    </author>
    <author>
      <name>Parus Khuwaja</name>
    </author>
    <author>
      <name>Luca Foschini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, 2 tables, Magazine article</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Wireless Communications 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2107.12806v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.12806v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.14052v1</id>
    <updated>2021-07-11T18:51:29Z</updated>
    <published>2021-07-11T18:51:29Z</published>
    <title>The Role of Social Movements, Coalitions, and Workers in Resisting
  Harmful Artificial Intelligence and Contributing to the Development of
  Responsible AI</title>
    <summary>  There is mounting public concern over the influence that AI based systems has
in our society. Coalitions in all sectors are acting worldwide to resist hamful
applications of AI. From indigenous people addressing the lack of reliable
data, to smart city stakeholders, to students protesting the academic
relationships with sex trafficker and MIT donor Jeffery Epstein, the
questionable ethics and values of those heavily investing in and profiting from
AI are under global scrutiny. There are biased, wrongful, and disturbing
assumptions embedded in AI algorithms that could get locked in without
intervention. Our best human judgment is needed to contain AI's harmful impact.
Perhaps one of the greatest contributions of AI will be to make us ultimately
understand how important human wisdom truly is in life on earth.
</summary>
    <author>
      <name>Susan von Struensee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">184 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.14052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.14052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.14414v1</id>
    <updated>2021-07-30T03:22:41Z</updated>
    <published>2021-07-30T03:22:41Z</published>
    <title>Towards Understanding the Impact of Real-Time AI-Powered Educational
  Dashboards (RAED) on Providing Guidance to Instructors</title>
    <summary>  The objectives of this ongoing research are to build Real-Time AI-Powered
Educational Dashboard (RAED) as a decision support tool for instructors, and to
measure its impact on them while making decisions. Current developments in AI
can be combined with the educational dashboards to make them AI-Powered. Thus,
AI can help in providing recommendations based on the students' performances.
AI-Powered educational dashboards can also assist instructors in tracking
real-time student activities. In this ongoing research, our aim is to develop
the AI component as well as improve the existing design component of the RAED.
Further, we will conduct experiments to study its impact on instructors, and
understand how much they trust RAED to guide them while making decisions. This
paper elaborates on the ongoing research and future direction.
</summary>
    <author>
      <name>Ajay Kulkarni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the Thirteenth International Conference on Educational
  Data Mining (EDM 2020)</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.14414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.14414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.12427v2</id>
    <updated>2021-08-31T12:49:31Z</updated>
    <published>2021-08-28T19:41:22Z</published>
    <title>Why and How Governments Should Monitor AI Development</title>
    <summary>  In this paper we outline a proposal for improving the governance of
artificial intelligence (AI) by investing in government capacity to
systematically measure and monitor the capabilities and impacts of AI systems.
If adopted, this would give governments greater information about the AI
ecosystem, equipping them to more effectively direct AI development and
deployment in the most societally and economically beneficial directions. It
would also create infrastructure that could rapidly identify potential threats
or harms that could occur as a consequence of changes in the AI ecosystem, such
as the emergence of strategically transformative capabilities, or the
deployment of harmful systems.
  We begin by outlining the problem which motivates this proposal: in brief,
traditional governance approaches struggle to keep pace with the speed of
progress in AI. We then present our proposal for addressing this problem:
governments must invest in measurement and monitoring infrastructure. We
discuss this proposal in detail, outlining what specific things governments
could focus on measuring and monitoring, and the kinds of benefits this would
generate for policymaking. Finally, we outline some potential pilot projects
and some considerations for implementing this in practice.
</summary>
    <author>
      <name>Jess Whittlestone</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <link href="http://arxiv.org/abs/2108.12427v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.12427v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.13363v1</id>
    <updated>2021-08-30T16:36:52Z</updated>
    <published>2021-08-30T16:36:52Z</published>
    <title>Coding with Purpose: Learning AI in Rural California</title>
    <summary>  We use an autoethnographic case study of a Latinx high school student from an
agricultural community in California to highlight how AI is learned outside
classrooms and how her personal background influenced her social-justice
oriented applications of AI technologies. Applying the concept of learning
pathways from the learning sciences, we argue that redesigning AI education to
be more inclusive with respect to socioeconomic status, ethnoracial identity,
and gender is important in the development of computational projects that
address social-injustice. We also learn about the role of institutions, power
structures, and community as they relate to her journey of learning and
applying AI. The future of AI, its potential to address issues of social
injustice and limiting the negative consequences of its use, will depend on the
participation and voice of students from the most vulnerable communities.
</summary>
    <author>
      <name>Stephanie Tena-Meza</name>
    </author>
    <author>
      <name>Miroslav Suzara</name>
    </author>
    <author>
      <name>AJ Alvero</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AI/CS education</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.13363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.13363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2; K.4; I.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.03958v2</id>
    <updated>2023-01-05T20:37:47Z</updated>
    <published>2021-09-08T22:44:33Z</published>
    <title>TrAISformer-A generative transformer for AIS trajectory prediction</title>
    <summary>  Predicting the position of a vessel at a specific time in the future is at
the core of many maritime applications. The Automatic Identification System
(AIS) provides rich information to enable this task. However, vessel trajectory
forecasting using AIS data is challenging, even for modern machine
learning/deep learning models, because motion data in general, and AIS data in
particular, are complex and multimodal. In this paper, we tackle those
difficulties by introducing a novel discrete, high-dimensional representation
of AIS data and a new loss function to explicitly account for heterogeneity and
multimodality. The proposed model -- referred to as TrAISformer -- is a
modified transformer network that extracts long-term correlations of AIS
trajectories in the proposed enriched space to forecast the positions of
vessels after several hours. We report experimental results on real, public AIS
data. TrAISformer significantly outperforms state-of-the-art methods and
reaches a mean prediction performance below 10 nautical miles up to ~10 hours.
</summary>
    <author>
      <name>Duong Nguyen</name>
    </author>
    <author>
      <name>Ronan Fablet</name>
    </author>
    <link href="http://arxiv.org/abs/2109.03958v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.03958v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.05478v1</id>
    <updated>2021-10-10T00:40:53Z</updated>
    <published>2021-10-10T00:40:53Z</published>
    <title>An In-depth Summary of Recent Artificial Intelligence Applications in
  Drug Design</title>
    <summary>  As a promising tool to navigate in the vast chemical space, artificial
intelligence (AI) is leveraged for drug design. From the year 2017 to 2021, the
number of applications of several recent AI models (i.e. graph neural network
(GNN), recurrent neural network (RNN), variation autoencoder (VAE), generative
adversarial network (GAN), flow and reinforcement learning (RL)) in drug design
increases significantly. Many relevant literature reviews exist. However, none
of them provides an in-depth summary of many applications of the recent AI
models in drug design. To complement the existing literature, this survey
includes the theoretical development of the previously mentioned AI models and
detailed summaries of 42 recent applications of AI in drug design. Concretely,
13 of them leverage GNN for molecular property prediction and 29 of them use RL
and/or deep generative models for molecule generation and optimization. In most
cases, the focus of the summary is the models, their variants, and
modifications for specific tasks in drug design. Moreover, 60 additional
applications of AI in molecule generation and optimization are briefly
summarized in a table. Finally, this survey provides a holistic discussion of
the abundant applications so that the tasks, potential solutions, and
challenges in AI-based drug design become evident.
</summary>
    <author>
      <name>Yi Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 6 figures, 3 tables, 253 references</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.05478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.05478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.07036v2</id>
    <updated>2022-01-06T16:56:05Z</updated>
    <published>2021-11-13T04:34:15Z</published>
    <title>Introducing Variational Autoencoders to High School Students</title>
    <summary>  Generative Artificial Intelligence (AI) models are a compelling way to
introduce K-12 students to AI education using an artistic medium, and hence
have drawn attention from K-12 AI educators. Previous Creative AI curricula
mainly focus on Generative Adversarial Networks (GANs) while paying less
attention to Autoregressive Models, Variational Autoencoders (VAEs), or other
generative models, which have since become common in the field of generative
AI. VAEs' latent-space structure and interpolation ability could effectively
ground the interdisciplinary learning of AI, creative arts, and philosophy.
Thus, we designed a lesson to teach high school students about VAEs. We
developed a web-based game and used Plato's cave, a philosophical metaphor, to
introduce how VAEs work. We used a Google Colab notebook for students to
re-train VAEs with their hand-written digits to consolidate their
understandings. Finally, we guided the exploration of creative VAE tools such
as SketchRNN and MusicVAE to draw the connection between what they learned and
real-world applications. This paper describes the lesson design and shares
insights from the pilot studies with 22 students. We found that our approach
was effective in teaching students about a novel AI concept.
</summary>
    <author>
      <name>Zhuoyue Lyu</name>
    </author>
    <author>
      <name>Safinah Ali</name>
    </author>
    <author>
      <name>Cynthia Breazeal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(Camera-ready) The 12th AAAI Symposium on Educational Advances in
  Artificial Intelligence (EAAI-22)</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.07036v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.07036v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.07505v1</id>
    <updated>2021-11-15T02:45:34Z</updated>
    <published>2021-11-15T02:45:34Z</published>
    <title>A Survey on AI Assurance</title>
    <summary>  Artificial Intelligence (AI) algorithms are increasingly providing decision
making and operational support across multiple domains. AI includes a wide
library of algorithms for different problems. One important notion for the
adoption of AI algorithms into operational decision process is the concept of
assurance. The literature on assurance, unfortunately, conceals its outcomes
within a tangled landscape of conflicting approaches, driven by contradicting
motivations, assumptions, and intuitions. Accordingly, albeit a rising and
novel area, this manuscript provides a systematic review of research works that
are relevant to AI assurance, between years 1985 - 2021, and aims to provide a
structured alternative to the landscape. A new AI assurance definition is
adopted and presented and assurance methods are contrasted and tabulated.
Additionally, a ten-metric scoring system is developed and introduced to
evaluate and compare existing methods. Lastly, in this manuscript, we provide
foundational insights, discussions, future directions, a roadmap, and
applicable recommendations for the development and deployment of AI assurance.
</summary>
    <author>
      <name>Feras A. Batarseh</name>
    </author>
    <author>
      <name>Laura Freeman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1186/s40537-021-00445-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1186/s40537-021-00445-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is published at Springer's Journal of Big Data</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J Big Data 8, 60 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2111.07505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.07505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.00616v1</id>
    <updated>2021-11-27T16:48:20Z</updated>
    <published>2021-11-27T16:48:20Z</published>
    <title>Roadmap for Edge AI: A Dagstuhl Perspective</title>
    <summary>  Based on the collective input of Dagstuhl Seminar (21342), this paper
presents a comprehensive discussion on AI methods and capabilities in the
context of edge computing, referred as Edge AI. In a nutshell, we envision Edge
AI to provide adaptation for data-driven applications, enhance network and
radio access, and allow the creation, optimization, and deployment of
distributed AI/ML pipelines with given quality of experience, trust, security
and privacy targets. The Edge AI community investigates novel ML methods for
the edge computing environment, spanning multiple sub-fields of computer
science, engineering and ICT. The goal is to share an envisioned roadmap that
can bring together key actors and enablers to further advance the domain of
Edge AI.
</summary>
    <author>
      <name>Aaron Yi Ding</name>
    </author>
    <author>
      <name>Ella Peltonen</name>
    </author>
    <author>
      <name>Tobias Meuser</name>
    </author>
    <author>
      <name>Atakan Aral</name>
    </author>
    <author>
      <name>Christian Becker</name>
    </author>
    <author>
      <name>Schahram Dustdar</name>
    </author>
    <author>
      <name>Thomas Hiessl</name>
    </author>
    <author>
      <name>Dieter Kranzlmuller</name>
    </author>
    <author>
      <name>Madhusanka Liyanage</name>
    </author>
    <author>
      <name>Setareh Magshudi</name>
    </author>
    <author>
      <name>Nitinder Mohan</name>
    </author>
    <author>
      <name>Joerg Ott</name>
    </author>
    <author>
      <name>Jan S. Rellermeyer</name>
    </author>
    <author>
      <name>Stefan Schulte</name>
    </author>
    <author>
      <name>Henning Schulzrinne</name>
    </author>
    <author>
      <name>Gurkan Solmaz</name>
    </author>
    <author>
      <name>Sasu Tarkoma</name>
    </author>
    <author>
      <name>Blesson Varghese</name>
    </author>
    <author>
      <name>Lars Wolf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">for ACM SIGCOMM CCR</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.00616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.00616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.02759v1</id>
    <updated>2022-01-08T04:23:23Z</updated>
    <published>2022-01-08T04:23:23Z</published>
    <title>Modeling Human-AI Team Decision Making</title>
    <summary>  AI and humans bring complementary skills to group deliberations. Modeling
this group decision making is especially challenging when the deliberations
include an element of risk and an exploration-exploitation process of
appraising the capabilities of the human and AI agents. To investigate this
question, we presented a sequence of intellective issues to a set of human
groups aided by imperfect AI agents. A group's goal was to appraise the
relative expertise of the group's members and its available AI agents, evaluate
the risks associated with different actions, and maximize the overall reward by
reaching consensus. We propose and empirically validate models of human-AI team
decision making under such uncertain circumstances, and show the value of
socio-cognitive constructs of prospect theory, influence dynamics, and Bayesian
learning in predicting the behavior of human-AI groups.
</summary>
    <author>
      <name>Wei Ye</name>
    </author>
    <author>
      <name>Francesco Bullo</name>
    </author>
    <author>
      <name>Noah Friedkin</name>
    </author>
    <author>
      <name>Ambuj K Singh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.02759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.02759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.05647v1</id>
    <updated>2022-01-14T19:47:46Z</updated>
    <published>2022-01-14T19:47:46Z</published>
    <title>Tools and Practices for Responsible AI Engineering</title>
    <summary>  Responsible Artificial Intelligence (AI) - the practice of developing,
evaluating, and maintaining accurate AI systems that also exhibit essential
properties such as robustness and explainability - represents a multifaceted
challenge that often stretches standard machine learning tooling, frameworks,
and testing methods beyond their limits. In this paper, we present two new
software libraries - hydra-zen and the rAI-toolbox - that address critical
needs for responsible AI engineering. hydra-zen dramatically simplifies the
process of making complex AI applications configurable, and their behaviors
reproducible. The rAI-toolbox is designed to enable methods for evaluating and
enhancing the robustness of AI-models in a way that is scalable and that
composes naturally with other popular ML frameworks. We describe the design
principles and methodologies that make these tools effective, including the use
of property-based testing to bolster the reliability of the tools themselves.
Finally, we demonstrate the composability and flexibility of the tools by
showing how various use cases from adversarial robustness and explainable AI
can be concisely implemented with familiar APIs.
</summary>
    <author>
      <name>Ryan Soklaski</name>
    </author>
    <author>
      <name>Justin Goodwin</name>
    </author>
    <author>
      <name>Olivia Brown</name>
    </author>
    <author>
      <name>Michael Yee</name>
    </author>
    <author>
      <name>Jason Matterer</name>
    </author>
    <link href="http://arxiv.org/abs/2201.05647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.05647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.06961v1</id>
    <updated>2022-01-18T13:22:40Z</updated>
    <published>2022-01-18T13:22:40Z</published>
    <title>AI for Closed-Loop Control Systems -- New Opportunities for Modeling,
  Designing, and Tuning Control Systems</title>
    <summary>  Control Systems, particularly closed-loop control systems (CLCS), are
frequently used in production machines, vehicles, and robots nowadays. CLCS are
needed to actively align actual values of a process to a given reference or set
values in real-time with a very high precession. Yet, artificial intelligence
(AI) is not used to model, design, optimize, and tune CLCS. This paper will
highlight potential AI-empowered and -based control system designs and
designing procedures, gathering new opportunities and research direction in the
field of control system engineering. Therefore, this paper illustrates which
building blocks within the standard block diagram of CLCS can be replaced by
AI, i.e., artificial neuronal networks (ANN). Having processes with real-time
contains and functional safety in mind, it is discussed if AI-based controller
blocks can cope with these demands. By concluding the paper, the pros and cons
of AI-empowered as well as -based CLCS designs are discussed, and possible
research directions for introducing AI in the domain of control system
engineering are given.
</summary>
    <author>
      <name>Julius Schöning</name>
    </author>
    <author>
      <name>Adrian Riechmann</name>
    </author>
    <author>
      <name>Hans-Jürgen Pfisterer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3529836.3529952</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3529836.3529952" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.06961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.06961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="93C83 (Primary), 70Q05, 68T05 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; C.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.08356v1</id>
    <updated>2022-01-20T18:41:32Z</updated>
    <published>2022-01-20T18:41:32Z</published>
    <title>AI Technical Considerations: Data Storage, Cloud usage and AI Pipeline</title>
    <summary>  Artificial intelligence (AI), especially deep learning, requires vast amounts
of data for training, testing, and validation. Collecting these data and the
corresponding annotations requires the implementation of imaging biobanks that
provide access to these data in a standardized way. This requires careful
design and implementation based on the current standards and guidelines and
complying with the current legal restrictions. However, the realization of
proper imaging data collections is not sufficient to train, validate and deploy
AI as resource demands are high and require a careful hybrid implementation of
AI pipelines both on-premise and in the cloud. This chapter aims to help the
reader when technical considerations have to be made about the AI environment
by providing a technical background of different concepts and implementation
aspects involved in data storage, cloud usage, and AI pipelines.
</summary>
    <author>
      <name>P. M. A van Ooijen</name>
    </author>
    <author>
      <name>Erfan Darzidehkalani</name>
    </author>
    <author>
      <name>Andre Dekker</name>
    </author>
    <link href="http://arxiv.org/abs/2201.08356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.08356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.02927v1</id>
    <updated>2022-03-06T10:12:56Z</updated>
    <published>2022-03-06T10:12:56Z</published>
    <title>Enabling Automated Machine Learning for Model-Driven AI Engineering</title>
    <summary>  Developing smart software services requires both Software Engineering and
Artificial Intelligence (AI) skills. AI practitioners, such as data scientists
often focus on the AI side, for example, creating and training Machine Learning
(ML) models given a specific use case and data. They are typically not
concerned with the entire software development life-cycle, architectural
decisions for the system and performance issues beyond the predictive ML models
(e.g., regarding the security, privacy, throughput, scalability, availability,
as well as ethical, legal and regulatory compliance). In this manuscript, we
propose a novel approach to enable Model-Driven Software Engineering and
Model-Driven AI Engineering. In particular, we support Automated ML, thus
assisting software engineers without deep AI knowledge in developing
AI-intensive systems by choosing the most appropriate ML model, algorithm and
techniques with suitable hyper-parameters for the task at hand. To validate our
work, we carry out a case study in the smart energy domain.
</summary>
    <author>
      <name>Armin Moin</name>
    </author>
    <author>
      <name>Ukrit Wattanavaekin</name>
    </author>
    <author>
      <name>Alexandra Lungu</name>
    </author>
    <author>
      <name>Moharram Challenger</name>
    </author>
    <author>
      <name>Atta Badii</name>
    </author>
    <author>
      <name>Stephan Günnemann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.02927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.02927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.03715v1</id>
    <updated>2022-02-18T15:16:22Z</updated>
    <published>2022-02-18T15:16:22Z</published>
    <title>Needs and Artificial Intelligence</title>
    <summary>  Throughout their history, homo sapiens have used technologies to better
satisfy their needs. The relation between needs and technology is so
fundamental that the US National Research Council defined the distinguishing
characteristic of technology as its goal "to make modifications in the world to
meet human needs". Artificial intelligence (AI) is one of the most promising
emerging technologies of our time. Similar to other technologies, AI is
expected "to meet [human] needs". In this article, we reflect on the
relationship between needs and AI, and call for the realisation of needs-aware
AI systems. We argue that re-thinking needs for, through, and by AI can be a
very useful means towards the development of realistic approaches for
Sustainable, Human-centric, Accountable, Lawful, and Ethical (HALE) AI systems.
We discuss some of the most critical gaps, barriers, enablers, and drivers of
co-creating future AI-based socio-technical systems in which [human] needs are
well considered and met. Finally, we provide an overview of potential threats
and HALE considerations that should be carefully taken into account, and call
for joint, immediate, and interdisciplinary efforts and collaborations.
</summary>
    <author>
      <name>Soheil Human</name>
    </author>
    <author>
      <name>Ryan Watkins</name>
    </author>
    <link href="http://arxiv.org/abs/2203.03715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.03715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.03847v1</id>
    <updated>2022-03-08T04:38:34Z</updated>
    <published>2022-03-08T04:38:34Z</published>
    <title>Trust in AI and Implications for the AEC Research: A Literature Analysis</title>
    <summary>  Engendering trust in technically acceptable and psychologically embraceable
systems requires domain-specific research to capture unique characteristics of
the field of application. The architecture, engineering, and construction (AEC)
research community has been recently harnessing advanced solutions offered by
artificial intelligence (AI) to improve project workflows. Despite the unique
characteristics of work, workers, and workplaces in the AEC industry, the
concept of trust in AI has received very little attention in the literature.
This paper presents a comprehensive analysis of the academic literature in two
main areas of trust in AI and AI in the AEC, to explore the interplay between
AEC projects unique aspects and the sociotechnical concepts that lead to trust
in AI. A total of 490 peer-reviewed scholarly articles are analyzed in this
study. The main constituents of human trust in AI are identified from the
literature and are characterized within the AEC project types, processes, and
technologies.
</summary>
    <author>
      <name>Newsha Emaminejad</name>
    </author>
    <author>
      <name>Alexa Maria North</name>
    </author>
    <author>
      <name>Reza Akhavian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2021 ASCE International Conference on Computing in Civil Engineering
  (i3CE2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.03847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.03847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.12697v1</id>
    <updated>2022-03-23T19:43:35Z</updated>
    <published>2022-03-23T19:43:35Z</published>
    <title>What is Software Quality for AI Engineers? Towards a Thinning of the Fog</title>
    <summary>  It is often overseen that AI-enabled systems are also software systems and
therefore rely on software quality assurance (SQA). Thus, the goal of this
study is to investigate the software quality assurance strategies adopted
during the development, integration, and maintenance of AI/ML components and
code. We conducted semi-structured interviews with representatives of ten
Austrian SMEs that develop AI-enabled systems. A qualitative analysis of the
interview data identified 12 issues in the development of AI/ML components.
Furthermore, we identified when quality issues arise in AI/ML components and
how they are detected. The results of this study should guide future work on
software quality assurance processes and techniques for AI/ML components.
</summary>
    <author>
      <name>Valentina Golendukhina</name>
    </author>
    <author>
      <name>Valentina Lenarduzzi</name>
    </author>
    <author>
      <name>Michael Felderer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, accepted for CAIN22 Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.12697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.12697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.15628v1</id>
    <updated>2022-03-29T14:49:46Z</updated>
    <published>2022-03-29T14:49:46Z</published>
    <title>Exploring Opportunities in Usable Hazard Analysis Processes for AI
  Engineering</title>
    <summary>  Embedding artificial intelligence into systems introduces significant
challenges to modern engineering practices. Hazard analysis tools and processes
have not yet been adequately adapted to the new paradigm. This paper describes
initial research and findings regarding current practices in AI-related hazard
analysis and on the tools used to conduct this work. Our goal with this initial
research is to better understand the needs of practitioners and the emerging
challenges of considering hazards and risks for AI-enabled products and
services. Our primary research question is: Can we develop new structured
thinking methods and systems engineering tools to support effective and
engaging ways for preemptively considering failure modes in AI systems? The
preliminary findings from our review of the literature and interviews with
practitioners highlight various challenges around integrating hazard analysis
into modern AI development processes and suggest opportunities for exploration
of usable, human-centered hazard analysis tools.
</summary>
    <author>
      <name>Nikolas Martelaro</name>
    </author>
    <author>
      <name>Carol J. Smith</name>
    </author>
    <author>
      <name>Tamara Zilovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, Presented at 2022 AAAI Spring Symposium Series Workshop on
  AI Engineering: Creating Scalable, Human-Centered and Robust AI Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.15628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.15628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.13194v1</id>
    <updated>2022-04-27T20:58:28Z</updated>
    <published>2022-04-27T20:58:28Z</published>
    <title>Exploring How Anomalous Model Input and Output Alerts Affect
  Decision-Making in Healthcare</title>
    <summary>  An important goal in the field of human-AI interaction is to help users more
appropriately trust AI systems' decisions. A situation in which the user may
particularly benefit from more appropriate trust is when the AI receives
anomalous input or provides anomalous output. To the best of our knowledge,
this is the first work towards understanding how anomaly alerts may contribute
to appropriate trust of AI. In a formative mixed-methods study with 4
radiologists and 4 other physicians, we explore how AI alerts for anomalous
input, very high and low confidence, and anomalous saliency-map explanations
affect users' experience with mockups of an AI clinical decision support system
(CDSS) for evaluating chest x-rays for pneumonia. We find evidence suggesting
that the four anomaly alerts are desired by non-radiologists, and the
high-confidence alerts are desired by both radiologists and non-radiologists.
In a follow-up user study, we investigate how high- and low-confidence alerts
affect the accuracy and thus appropriate trust of 33 radiologists working with
AI CDSS mockups. We observe that these alerts do not improve users' accuracy or
experience and discuss potential reasons why.
</summary>
    <author>
      <name>Marissa Radensky</name>
    </author>
    <author>
      <name>Dustin Burson</name>
    </author>
    <author>
      <name>Rajya Bhaiya</name>
    </author>
    <author>
      <name>Daniel S. Weld</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.13194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.13194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; I.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.00167v2</id>
    <updated>2023-02-02T23:42:12Z</updated>
    <published>2022-04-30T05:44:34Z</published>
    <title>Self-Programming Artificial Intelligence Using Code-Generating Language
  Models</title>
    <summary>  Recent progress in large-scale language models has enabled breakthroughs in
previously intractable computer programming tasks. Prior work in meta-learning
and neural architecture search has led to substantial successes across various
task domains, spawning myriad approaches for algorithmically optimizing the
design and learning dynamics of deep learning models. At the intersection of
these research areas, we implement a code-generating language model with the
ability to modify its own source code. Self-programming AI algorithms have been
of interest since the dawn of AI itself. Although various theoretical
formulations of generalized self-programming AI have been posed, no such system
has been successfully implemented to date under real-world computational
constraints. Applying AI-based code generation to AI itself, we develop and
experimentally validate the first practical implementation of a
self-programming AI system. We empirically show that a self-programming AI
implemented using a code generation model can successfully modify its own
source code to improve performance and program sub-models to perform auxiliary
tasks. Our model can self-modify various properties including model
architecture, computational capacity, and learning dynamics.
</summary>
    <author>
      <name>Alex Sheng</name>
    </author>
    <author>
      <name>Shankar Padmanabhan</name>
    </author>
    <link href="http://arxiv.org/abs/2205.00167v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.00167v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.03468v1</id>
    <updated>2022-05-02T20:59:33Z</updated>
    <published>2022-05-02T20:59:33Z</published>
    <title>The AI Index 2022 Annual Report</title>
    <summary>  Welcome to the fifth edition of the AI Index Report! The latest edition
includes data from a broad set of academic, private, and nonprofit
organizations as well as more self-collected data and original analysis than
any previous editions, including an expanded technical performance chapter, a
new survey of robotics researchers around the world, data on global AI
legislation records in 25 countries, and a new chapter with an in-depth
analysis of technical AI ethics metrics.
  The AI Index Report tracks, collates, distills, and visualizes data related
to artificial intelligence. Its mission is to provide unbiased, rigorously
vetted, and globally sourced data for policymakers, researchers, executives,
journalists, and the general public to develop a more thorough and nuanced
understanding of the complex field of AI. The report aims to be the world's
most credible and authoritative source for data and insights about AI.
</summary>
    <author>
      <name>Daniel Zhang</name>
    </author>
    <author>
      <name>Nestor Maslej</name>
    </author>
    <author>
      <name>Erik Brynjolfsson</name>
    </author>
    <author>
      <name>John Etchemendy</name>
    </author>
    <author>
      <name>Terah Lyons</name>
    </author>
    <author>
      <name>James Manyika</name>
    </author>
    <author>
      <name>Helen Ngo</name>
    </author>
    <author>
      <name>Juan Carlos Niebles</name>
    </author>
    <author>
      <name>Michael Sellitto</name>
    </author>
    <author>
      <name>Ellie Sakhaee</name>
    </author>
    <author>
      <name>Yoav Shoham</name>
    </author>
    <author>
      <name>Jack Clark</name>
    </author>
    <author>
      <name>Raymond Perrault</name>
    </author>
    <link href="http://arxiv.org/abs/2205.03468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.03468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.04358v4</id>
    <updated>2023-01-04T04:17:14Z</updated>
    <published>2022-05-09T14:59:23Z</published>
    <title>Towards Implementing Responsible AI</title>
    <summary>  As the deployment of artificial intelligence (AI) is changing many fields and
industries, there are concerns about AI systems making decisions and
recommendations without adequately considering various ethical aspects, such as
accountability, reliability, transparency, explainability, contestability,
privacy, and fairness. While many sets of AI ethics principles have been
recently proposed that acknowledge these concerns, such principles are
high-level and do not provide tangible advice on how to develop ethical and
responsible AI systems. To gain insight on the possible implementation of the
principles, we conducted an empirical investigation involving semi-structured
interviews with a cohort of AI practitioners. The salient findings cover four
aspects of AI system design and development, adapting processes used in
software engineering: (i) high-level view, (ii) requirements engineering, (iii)
design and implementation, (iv) deployment and operation.
</summary>
    <author>
      <name>Conrad Sanderson</name>
    </author>
    <author>
      <name>Qinghua Lu</name>
    </author>
    <author>
      <name>David Douglas</name>
    </author>
    <author>
      <name>Xiwei Xu</name>
    </author>
    <author>
      <name>Liming Zhu</name>
    </author>
    <author>
      <name>Jon Whittle</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/BigData55660.2022.10021121</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/BigData55660.2022.10021121" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">follow-up work to arXiv:2112.07467</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.04358v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.04358v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.1; K.4.2; K.4.3; K.7.4; K.7.m; I.2.m; I.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.00474v1</id>
    <updated>2022-06-01T13:08:37Z</updated>
    <published>2022-06-01T13:08:37Z</published>
    <title>Towards Responsible AI: A Design Space Exploration of Human-Centered
  Artificial Intelligence User Interfaces to Investigate Fairness</title>
    <summary>  With Artificial intelligence (AI) to aid or automate decision-making
advancing rapidly, a particular concern is its fairness. In order to create
reliable, safe and trustworthy systems through human-centred artificial
intelligence (HCAI) design, recent efforts have produced user interfaces (UIs)
for AI experts to investigate the fairness of AI models. In this work, we
provide a design space exploration that supports not only data scientists but
also domain experts to investigate AI fairness. Using loan applications as an
example, we held a series of workshops with loan officers and data scientists
to elicit their requirements. We instantiated these requirements into FairHIL,
a UI to support human-in-the-loop fairness investigations, and describe how
this UI could be generalized to other use cases. We evaluated FairHIL through a
think-aloud user study. Our work contributes better designs to investigate an
AI model's fairness-and move closer towards responsible AI.
</summary>
    <author>
      <name>Yuri Nakao</name>
    </author>
    <author>
      <name>Lorenzo Strappelli</name>
    </author>
    <author>
      <name>Simone Stumpf</name>
    </author>
    <author>
      <name>Aisha Naseer</name>
    </author>
    <author>
      <name>Daniele Regoli</name>
    </author>
    <author>
      <name>Giulia Del Gamba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 pages, 17 figures, the draft of a paper on International Journal
  of Human-Computer Interaction</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Human-Computer Interaction, 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2206.00474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.00474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.07506v1</id>
    <updated>2022-06-15T13:03:43Z</updated>
    <published>2022-06-15T13:03:43Z</published>
    <title>Legal Provocations for HCI in the Design and Development of Trustworthy
  Autonomous Systems</title>
    <summary>  We consider a series of legal provocations emerging from the proposed
European Union AI Act 2021 (AIA) and how they open up new possibilities for HCI
in the design and development of trustworthy autonomous systems. The AIA
continues the by design trend seen in recent EU regulation of emerging
technologies. The AIA targets AI developments that pose risks to society and
citizens fundamental rights, introducing mandatory design and development
requirements for high-risk AI systems (HRAIS). These requirements regulate
different stages of the AI development cycle including ensuring data quality
and governance strategies, mandating testing of systems, ensuring appropriate
risk management, designing for human oversight, and creating technical
documentation. These requirements open up new opportunities for HCI that reach
beyond established concerns with the ethics and explainability of AI and
situate AI development in human-centered processes and methods of design to
enable compliance with regulation and foster societal trust in AI.
</summary>
    <author>
      <name>Lachlan D. Urquhart</name>
    </author>
    <author>
      <name>Glenn McGarry</name>
    </author>
    <author>
      <name>Andy Crabtree</name>
    </author>
    <link href="http://arxiv.org/abs/2206.07506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.07506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.01749v1</id>
    <updated>2022-07-04T23:46:08Z</updated>
    <published>2022-07-04T23:46:08Z</published>
    <title>Human-AI Guidelines in Practice: Leaky Abstractions as an Enabler in
  Collaborative Software Teams</title>
    <summary>  In conventional software development, user experience (UX) designers and
engineers collaborate through separation of concerns (SoC): designers create
human interface specifications, and engineers build to those specifications.
However, we argue that Human-AI systems thwart SoC because human needs must
shape the design of the AI interface, the underlying AI sub-components, and
training data. How do designers and engineers currently collaborate on AI and
UX design? To find out, we interviewed 21 industry professionals (UX
researchers, AI engineers, data scientists, and managers) across 14
organizations about their collaborative work practices and associated
challenges. We find that hidden information encapsulated by SoC challenges
collaboration across design and engineering concerns. Practitioners describe
inventing ad-hoc representations exposing low-level design and implementation
details (which we characterize as leaky abstractions) to "puncture" SoC and
share information across expertise boundaries. We identify how leaky
abstractions are employed to collaborate at the AI-UX boundary and formalize a
process of creating and using leaky abstractions.
</summary>
    <author>
      <name>Hariharan Subramonyam</name>
    </author>
    <author>
      <name>Jane Im</name>
    </author>
    <author>
      <name>Colleen Seifert</name>
    </author>
    <author>
      <name>Eytan Adar</name>
    </author>
    <link href="http://arxiv.org/abs/2207.01749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.01749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.02996v1</id>
    <updated>2022-07-06T22:11:13Z</updated>
    <published>2022-07-06T22:11:13Z</published>
    <title>Team Learning as a Lens for Designing Human-AI Co-Creative Systems</title>
    <summary>  Generative, ML-driven interactive systems have the potential to change how
people interact with computers in creative processes - turning tools into
co-creators. However, it is still unclear how we might achieve effective
human-AI collaboration in open-ended task domains. There are several known
challenges around communication in the interaction with ML-driven systems. An
overlooked aspect in the design of co-creative systems is how users can be
better supported in learning to collaborate with such systems. Here we reframe
human-AI collaboration as a learning problem: Inspired by research on team
learning, we hypothesize that similar learning strategies that apply to
human-human teams might also increase the collaboration effectiveness and
quality of humans working with co-creative generative systems. In this position
paper, we aim to promote team learning as a lens for designing more effective
co-creative human-AI collaboration and emphasize collaboration process quality
as a goal for co-creative systems. Furthermore, we outline a preliminary
schematic framework for embedding team learning support in co-creative AI
systems. We conclude by proposing a research agenda and posing open questions
for further study on supporting people in learning to collaborate with
generative AI systems.
</summary>
    <author>
      <name>Frederic Gmeiner</name>
    </author>
    <author>
      <name>Kenneth Holstein</name>
    </author>
    <author>
      <name>Nikolas Martelaro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM CHI 2022 Workshop on Generative AI and HCI</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.02996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.02996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.09833v1</id>
    <updated>2022-07-20T11:37:46Z</updated>
    <published>2022-07-20T11:37:46Z</published>
    <title>AI Fairness: from Principles to Practice</title>
    <summary>  This paper summarizes and evaluates various approaches, methods, and
techniques for pursuing fairness in artificial intelligence (AI) systems. It
examines the merits and shortcomings of these measures and proposes practical
guidelines for defining, measuring, and preventing bias in AI. In particular,
it cautions against some of the simplistic, yet common, methods for evaluating
bias in AI systems, and offers more sophisticated and effective alternatives.
The paper also addresses widespread controversies and confusions in the field
by providing a common language among different stakeholders of high-impact AI
systems. It describes various trade-offs involving AI fairness, and provides
practical recommendations for balancing them. It offers techniques for
evaluating the costs and benefits of fairness targets, and defines the role of
human judgment in setting these targets. This paper provides discussions and
guidelines for AI practitioners, organization leaders, and policymakers, as
well as various links to additional materials for a more technical audience.
Numerous real-world examples are provided to clarify the concepts, challenges,
and recommendations from a practical perspective.
</summary>
    <author>
      <name>Arash Bateni</name>
    </author>
    <author>
      <name>Matthew C. Chan</name>
    </author>
    <author>
      <name>Ray Eitel-Porter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.09833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.09833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.12229v1</id>
    <updated>2022-06-22T11:48:04Z</updated>
    <published>2022-06-22T11:48:04Z</published>
    <title>Towards Systems Education for Artificial Intelligence: A Course Practice
  in Intelligent Computing Architectures</title>
    <summary>  With the rapid development of artificial intelligence (AI) community,
education in AI is receiving more and more attentions. There have been many AI
related courses in the respects of algorithms and applications, while not many
courses in system level are seriously taken into considerations. In order to
bridge the gap between AI and computing systems, we are trying to explore how
to conduct AI education from the perspective of computing systems. In this
paper, a course practice in intelligent computing architectures are provided to
demonstrate the system education in AI era. The motivation for this course
practice is first introduced as well as the learning orientations. The main
goal of this course aims to teach students for designing AI accelerators on
FPGA platforms. The elaborated course contents include lecture notes and
related technical materials. Especially several practical labs and projects are
detailed illustrated. Finally, some teaching experiences and effects are
discussed as well as some potential improvements in the future.
</summary>
    <author>
      <name>Jianlei Yang</name>
    </author>
    <author>
      <name>Xiaopeng Gao</name>
    </author>
    <author>
      <name>Weisheng Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is published on ACM GLSVLSI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.12229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.12229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.11282v2</id>
    <updated>2022-09-21T21:20:37Z</updated>
    <published>2022-08-24T03:01:21Z</published>
    <title>Multi-AI Complex Systems in Humanitarian Response</title>
    <summary>  AI is being increasingly used to aid response efforts to humanitarian
emergencies at multiple levels of decision-making. Such AI systems are
generally understood to be stand-alone tools for decision support, with ethical
assessments, guidelines and frameworks applied to them through this lens.
However, as the prevalence of AI increases in this domain, such systems will
begin to encounter each other through information flow networks created by
interacting decision-making entities, leading to multi-AI complex systems which
are often ill understood. In this paper we describe how these multi-AI systems
can arise, even in relatively simple real-world humanitarian response
scenarios, and lead to potentially emergent and erratic erroneous behavior. We
discuss how we can better work towards more trustworthy multi-AI systems by
exploring some of the associated challenges and opportunities, and how we can
design better mechanisms to understand and assess such systems. This paper is
designed to be a first exposition on this topic in the field of humanitarian
response, raising awareness, exploring the possible landscape of this domain,
and providing a starting point for future work within the wider community.
</summary>
    <author>
      <name>Joseph Aylett-Bullock</name>
    </author>
    <author>
      <name>Miguel Luengo-Oroz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 3rd KDD Workshop on Data-driven Humanitarian
  Mapping, 2022, Washington, DC USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2208.11282v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.11282v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.11745v1</id>
    <updated>2022-08-24T19:16:43Z</updated>
    <published>2022-08-24T19:16:43Z</published>
    <title>AI-coupled HPC Workflows</title>
    <summary>  Increasingly, scientific discovery requires sophisticated and scalable
workflows. Workflows have become the ``new applications,'' wherein multi-scale
computing campaigns comprise multiple and heterogeneous executable tasks. In
particular, the introduction of AI/ML models into the traditional HPC workflows
has been an enabler of highly accurate modeling, typically reducing
computational needs compared to traditional methods. This chapter discusses
various modes of integrating AI/ML models to HPC computations, resulting in
diverse types of AI-coupled HPC workflows. The increasing need of coupling
AI/ML and HPC across scientific domains is motivated, and then exemplified by a
number of production-grade use cases for each mode. We additionally discuss the
primary challenges of extreme-scale AI-coupled HPC campaigns -- task
heterogeneity, adaptivity, performance -- and several framework and middleware
solutions which aim to address them. While both HPC workflow and AI/ML
computing paradigms are independently effective, we highlight how their
integration, and ultimate convergence, is leading to significant improvements
in scientific performance across a range of domains, ultimately resulting in
scientific explorations otherwise unattainable.
</summary>
    <author>
      <name>Shantenu Jha</name>
    </author>
    <author>
      <name>Vincent R. Pascuzzi</name>
    </author>
    <author>
      <name>Matteo Turilli</name>
    </author>
    <link href="http://arxiv.org/abs/2208.11745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.11745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.04987v1</id>
    <updated>2022-09-12T02:13:57Z</updated>
    <published>2022-09-12T02:13:57Z</published>
    <title>Embracing AI in 5G-Advanced Towards 6G: A Joint 3GPP and O-RAN
  Perspective</title>
    <summary>  Artificial intelligence (AI) has emerged as a powerful technology that
improves system performance and enables new features in 5G and beyond.
Standardization, defining functionality and interfaces, is essential for
driving the industry alignment required to deliver the mass adoption of AI in
5G-Advanced and 6G. However, fragmented efforts in different standards bodies,
such as the third generation partnership project (3GPP) and the open radio
access network (O-RAN) Alliance, can lead to confusion and uncertainty about
which standards to follow and which aspects of the standards to embrace. This
article provides a joint 3GPP and O-RAN perspective on the state of the art in
AI adoption in mobile communication systems, including the fundamentals of 5G
architecture and its evolution towards openness and intelligence, AI for
5G-Advanced evolution, and a case study on AI-enabled traffic steering. We also
identify several areas for future exploration to accelerate AI adoption on the
path towards 6G.
</summary>
    <author>
      <name>Xingqin Lin</name>
    </author>
    <author>
      <name>Lopamudra Kundu</name>
    </author>
    <author>
      <name>Chris Dick</name>
    </author>
    <author>
      <name>Soma Velayutham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures, submitted for publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.04987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.04987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.05459v1</id>
    <updated>2022-08-30T15:49:11Z</updated>
    <published>2022-08-30T15:49:11Z</published>
    <title>How Do AI Timelines Affect Existential Risk?</title>
    <summary>  Superhuman artificial general intelligence could be created this century and
would likely be a significant source of existential risk. Delaying the creation
of superintelligent AI (ASI) could decrease total existential risk by
increasing the amount of time humanity has to work on the AI alignment problem.
  However, since ASI could reduce most risks, delaying the creation of ASI
could also increase other existential risks, especially from advanced future
technologies such as synthetic biology and molecular nanotechnology.
  If AI existential risk is high relative to the sum of other existential risk,
delaying the creation of ASI will tend to decrease total existential risk and
vice-versa.
  Other factors such as war and a hardware overhang could increase AI risk and
cognitive enhancement could decrease AI risk. To reduce total existential risk,
humanity should take robustly positive actions such as working on existential
risk analysis, AI governance and safety, and reducing all sources of
existential risk by promoting differential technological development.
</summary>
    <author>
      <name>Stephen McAleese</name>
    </author>
    <link href="http://arxiv.org/abs/2209.05459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.05459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.06317v2</id>
    <updated>2023-01-11T18:20:01Z</updated>
    <published>2022-09-13T21:47:25Z</published>
    <title>Quantitative AI Risk Assessments: Opportunities and Challenges</title>
    <summary>  Although AI-based systems are increasingly being leveraged to provide value
to organizations, individuals, and society, significant attendant risks have
been identified. These risks have led to proposed regulations, litigation, and
general societal concerns.
  As with any promising technology, organizations want to benefit from the
positive capabilities of AI technology while reducing the risks. The best way
to reduce risks is to implement comprehensive AI lifecycle governance where
policies and procedures are described and enforced during the design,
development, deployment, and monitoring of an AI system. While support for
comprehensive governance is beginning to emerge, organizations often need to
identify the risks of deploying an already-built model without knowledge of how
it was constructed or access to its original developers.
  Such an assessment will quantitatively assess the risks of an existing model
in a manner analogous to how a home inspector might assess the energy
efficiency of an already-built home or a physician might assess overall patient
health based on a battery of tests. This paper explores the concept of a
quantitative AI Risk Assessment, exploring the opportunities, challenges, and
potential impacts of such an approach, and discussing how it might improve AI
regulations.
</summary>
    <author>
      <name>David Piorkowski</name>
    </author>
    <author>
      <name>Michael Hind</name>
    </author>
    <author>
      <name>John Richards</name>
    </author>
    <link href="http://arxiv.org/abs/2209.06317v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.06317v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.08141v2</id>
    <updated>2022-12-31T22:21:52Z</updated>
    <published>2022-10-14T22:53:18Z</published>
    <title>Pseudo AI Bias</title>
    <summary>  Pseudo Artificial Intelligence bias (PAIB) is broadly disseminated in the
literature, which can result in unnecessary AI fear in society, exacerbate the
enduring inequities and disparities in access to and sharing the benefits of AI
applications, and waste social capital invested in AI research. This study
systematically reviews publications in the literature to present three types of
PAIBs identified due to: a) misunderstandings, b) pseudo mechanical bias, and
c) over-expectations. We discussed the consequences of and solutions to PAIBs,
including certifying users for AI applications to mitigate AI fears, providing
customized user guidance for AI applications, and developing systematic
approaches to monitor bias. We concluded that PAIB due to misunderstandings,
pseudo mechanical bias, and over-expectations of algorithmic predictions is
socially harmful.
</summary>
    <author>
      <name>Xiaoming Zhai</name>
    </author>
    <author>
      <name>Joseph Krajcik</name>
    </author>
    <link href="http://arxiv.org/abs/2210.08141v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.08141v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.03622v2</id>
    <updated>2022-12-16T19:01:32Z</updated>
    <published>2022-11-07T15:19:20Z</published>
    <title>Do Users Write More Insecure Code with AI Assistants?</title>
    <summary>  We conduct the first large-scale user study examining how users interact with
an AI Code assistant to solve a variety of security related tasks across
different programming languages. Overall, we find that participants who had
access to an AI assistant based on OpenAI's codex-davinci-002 model wrote
significantly less secure code than those without access. Additionally,
participants with access to an AI assistant were more likely to believe they
wrote secure code than those without access to the AI assistant. Furthermore,
we find that participants who trusted the AI less and engaged more with the
language and format of their prompts (e.g. re-phrasing, adjusting temperature)
provided code with fewer security vulnerabilities. Finally, in order to better
inform the design of future AI-based Code assistants, we provide an in-depth
analysis of participants' language and interaction behavior, as well as release
our user interface as an instrument to conduct similar studies in the future.
</summary>
    <author>
      <name>Neil Perry</name>
    </author>
    <author>
      <name>Megha Srivastava</name>
    </author>
    <author>
      <name>Deepak Kumar</name>
    </author>
    <author>
      <name>Dan Boneh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 16 figures, update adds names of statistical tests and
  survey questions</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.03622v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.03622v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.12851v1</id>
    <updated>2022-10-25T05:26:30Z</updated>
    <published>2022-10-25T05:26:30Z</published>
    <title>A Streamlit-based Artificial Intelligence Trust Platform for
  Next-Generation Wireless Networks</title>
    <summary>  With the rapid development and integration of artificial intelligence (AI)
methods in next-generation networks (NextG), AI algorithms have provided
significant advantages for NextG in terms of frequency spectrum usage,
bandwidth, latency, and security. A key feature of NextG is the integration of
AI, i.e., self-learning architecture based on self-supervised algorithms, to
improve the performance of the network. A secure AI-powered structure is also
expected to protect NextG networks against cyber-attacks. However, AI itself
may be attacked, i.e., model poisoning targeted by attackers, and it results in
cybersecurity violations. This paper proposes an AI trust platform using
Streamlit for NextG networks that allows researchers to evaluate, defend,
certify, and verify their AI models and applications against adversarial
threats of evasion, poisoning, extraction, and interference.
</summary>
    <author>
      <name>M. Kuzlu</name>
    </author>
    <author>
      <name>F. O. Catak</name>
    </author>
    <author>
      <name>S. Sarp</name>
    </author>
    <author>
      <name>U. Cali</name>
    </author>
    <author>
      <name>O Gueler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.12851v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.12851v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.14313v1</id>
    <updated>2022-11-21T06:59:01Z</updated>
    <published>2022-11-21T06:59:01Z</published>
    <title>AICOM-MP: an AI-based Monkeypox Detector for Resource-Constrained
  Environments</title>
    <summary>  Under the Autonomous Mobile Clinics (AMCs) initiative, we are developing,
open sourcing, and standardizing health AI technologies to enable healthcare
access in least developed countries (LDCs). We deem AMCs as the next generation
of health care delivery platforms, whereas health AI engines are applications
on these platforms, similar to how various applications expand the usage
scenarios of smart phones. Facing the recent global monkeypox outbreak, in this
article, we introduce AICOM-MP, an AI-based monkeypox detector specially aiming
for handling images taken from resource-constrained devices. Compared to
existing AI-based monkeypox detectors, AICOM-MP has achieved state-of-the-art
(SOTA) performance. We have hosted AICOM-MP as a web service to allow universal
access to monkeypox screening technology. We have also open sourced both the
source code and the dataset of AICOM-MP to allow health AI professionals to
integrate AICOM-MP into their services. Also, through the AICOM-MP project, we
have generalized a methodology of developing health AI technologies for AMCs to
allow universal access even in resource-constrained environments.
</summary>
    <author>
      <name>Tim Tianyi Yang</name>
    </author>
    <author>
      <name>Tom Tianze Yang</name>
    </author>
    <author>
      <name>Andrew Liu</name>
    </author>
    <author>
      <name>Jie Tang</name>
    </author>
    <author>
      <name>Na An</name>
    </author>
    <author>
      <name>Shaoshan Liu</name>
    </author>
    <author>
      <name>Xue Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2211.14313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.14313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.15021v1</id>
    <updated>2022-11-28T03:09:44Z</updated>
    <published>2022-11-28T03:09:44Z</published>
    <title>FAIR Principles for data and AI models in high energy physics research
  and education</title>
    <summary>  In recent years, digital object management practices to support findability,
accessibility, interoperability, and reusability (FAIR) have begun to be
adopted across a number of data-intensive scientific disciplines. These digital
objects include datasets, AI models, software, notebooks, workflows,
documentation, etc. With the collective dataset at the Large Hadron Collider
scheduled to reach the zettabyte scale by the end of 2032, the experimental
particle physics community is looking at unprecedented data management
challenges. It is expected that these grand challenges may be addressed by
creating end-to-end AI frameworks that combine FAIR and AI-ready datasets,
advances in AI, modern computing environments, and scientific data
infrastructure. In this work, the FAIR4HEP collaboration explores the
interpretation of FAIR principles in the context of data and AI models for
experimental high energy physics research. We investigate metrics to quantify
the FAIRness of experimental datasets and AI models, and provide open source
notebooks to guide new users on the use of FAIR principles in practice.
</summary>
    <author>
      <name>Avik Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Contribution to the Proceedings of 41st International Conference on
  High Energy Physics - ICHEP2022, Presented on behalf of the FAIR4HEP
  collaboration</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.15021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.15021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.03980v1</id>
    <updated>2022-12-07T22:30:17Z</updated>
    <published>2022-12-07T22:30:17Z</published>
    <title>DDoD: Dual Denial of Decision Attacks on Human-AI Teams</title>
    <summary>  Artificial Intelligence (AI) systems have been increasingly used to make
decision-making processes faster, more accurate, and more efficient. However,
such systems are also at constant risk of being attacked. While the majority of
attacks targeting AI-based applications aim to manipulate classifiers or
training data and alter the output of an AI model, recently proposed Sponge
Attacks against AI models aim to impede the classifier's execution by consuming
substantial resources. In this work, we propose \textit{Dual Denial of Decision
(DDoD) attacks against collaborative Human-AI teams}. We discuss how such
attacks aim to deplete \textit{both computational and human} resources, and
significantly impair decision-making capabilities. We describe DDoD on human
and computational resources and present potential risk scenarios in a series of
exemplary domains.
</summary>
    <author>
      <name>Benjamin Tag</name>
    </author>
    <author>
      <name>Niels van Berkel</name>
    </author>
    <author>
      <name>Sunny Verma</name>
    </author>
    <author>
      <name>Benjamin Zi Hao Zhao</name>
    </author>
    <author>
      <name>Shlomo Berkovsky</name>
    </author>
    <author>
      <name>Dali Kaafar</name>
    </author>
    <author>
      <name>Vassilis Kostakos</name>
    </author>
    <author>
      <name>Olga Ohrimenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure, IEEE Pervasive Computing, IEEE Special Issue on
  Human-Centered AI</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.03980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.03980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.11661v1</id>
    <updated>2022-12-22T13:05:17Z</updated>
    <published>2022-12-22T13:05:17Z</published>
    <title>The Death of the Short-Form Physics Essay in the Coming AI Revolution</title>
    <summary>  The latest AI language modules can produce original, high quality full
short-form ($300$-word) Physics essays within seconds. These technologies such
as ChatGPT and davinci-003 are freely available to anyone with an internet
connection. In this work, we present evidence of AI generated short-form essays
achieving first-class grades on an essay writing assessment from an accredited,
current university Physics module. The assessment requires students answer five
open-ended questions with a short, $300$-word essay each. Fifty AI answers were
generated to create ten submissions that were independently marked by five
separate markers. The AI generated submissions achieved an average mark of $71
\pm 2 \%$, in strong agreement with the current module average of $71 \pm 5 %$.
A typical AI submission would therefore most-likely be awarded a First Class,
the highest classification available at UK universities. Plagiarism detection
software returned a plagiarism score between $2 \pm 1$% (Grammarly) and $7 \pm
2$% (TurnitIn). We argue that these results indicate that current AI MLPs
represent a significant threat to the fidelity of short-form essays as an
assessment method in Physics courses.
</summary>
    <author>
      <name>Will Yeadon</name>
    </author>
    <author>
      <name>Oto-Obong Inyang</name>
    </author>
    <author>
      <name>Arin Mizouri</name>
    </author>
    <author>
      <name>Alex Peach</name>
    </author>
    <author>
      <name>Craig Testrow</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.11661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.11661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ed-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ed-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.05133v1</id>
    <updated>2023-01-12T16:40:19Z</updated>
    <published>2023-01-12T16:40:19Z</published>
    <title>Is AI Art Another Industrial Revolution in the Making?</title>
    <summary>  A major shift from skilled to unskilled workers was one of the many changes
caused by the Industrial Revolution, when the switch to machines contributed to
decline in the social and economic status of artisans, whose skills were
dismembered into discrete actions by factory-line workers. We consider what may
be an analogous computing technology: the recent introduction of AI-generated
art software. AI art generators such as Dall-E and Midjourney can create fully
rendered images based solely on a user's prompt, just at the click of a button.
Some artists fear if the cheaper price and conveyor-belt speed that comes with
AI-produced images is seen as an improvement to the current system, it may
permanently change the way society values/views art and artists. In this
article, we consider the implications that AI art generation introduces through
a post-industrial revolution historical lens. We then reflect on the analogous
issues that appear to arise as a result of the AI art revolution, and we
conclude that the problems raised mirror those of industrialization, giving a
vital glimpse into what may lie ahead.
</summary>
    <author>
      <name>Alexis Newton</name>
    </author>
    <author>
      <name>Kaustubh Dhole</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Creative AI Across Modalities Workshop 2023,
  Thirty-Seventh AAAI Conference on Artificial Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.05133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.05133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.2.9; I.2.0; J.4; J.5; J.7; K.4.1; K.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.08488v1</id>
    <updated>2023-01-20T09:34:59Z</updated>
    <published>2023-01-20T09:34:59Z</published>
    <title>Towards Openness Beyond Open Access: User Journeys through 3 Open AI
  Collaboratives</title>
    <summary>  Open Artificial Intelligence (Open source AI) collaboratives offer
alternative pathways for how AI can be developed beyond well-resourced
technology companies and who can be a part of the process. To understand how
and why they work and what additionality they bring to the landscape, we focus
on three such communities, each focused on a different kind of activity around
AI: building models (BigScience workshop), tools and ways of working (The
Turing Way), and ecosystems (Mozilla Festival's Building Trustworthy AI Working
Group). First, we document the community structures that facilitate these
distributed, volunteer-led teams, comparing the collaboration styles that drive
each group towards their specific goals. Through interviews with community
leaders, we map user journeys for how members discover, join, contribute, and
participate. Ultimately, this paper aims to highlight the diversity of AI work
and workers that have come forth through these collaborations and how they
offer a broader practice of openness to the AI space.
</summary>
    <author>
      <name>Jennifer Ding</name>
    </author>
    <author>
      <name>Christopher Akiki</name>
    </author>
    <author>
      <name>Yacine Jernite</name>
    </author>
    <author>
      <name>Anne Lee Steele</name>
    </author>
    <author>
      <name>Temi Popo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 2022 NeurIPS Workshop on Broadening Research
  Collaborations in ML</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.08488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.08488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.13947v1</id>
    <updated>2023-02-27T16:49:23Z</updated>
    <published>2023-02-27T16:49:23Z</published>
    <title>Investigating Girls' Perspectives and Knowledge Gaps on Ethics and
  Fairness in Artificial Intelligence in a Lightweight Workshop</title>
    <summary>  Artificial intelligence (AI) is everywhere, with many children having
increased exposure to AI technologies in daily life. We aimed to understand
middle school girls' (a group often excluded group in tech) perceptions and
knowledge gaps about AI. We created and explored the feasibility of a
lightweight (less than 3 hours) educational workshop in which learners
considered challenges in their lives and communities and critically considered
how existing and future AI could have an impact. After the workshop, learners
had nuanced perceptions of AI, understanding AI can both help and harm. We
discuss design implications for creating educational experiences in AI and
fairness that embolden learners.
</summary>
    <author>
      <name>Jaemarie Solyst</name>
    </author>
    <author>
      <name>Alexis Axon</name>
    </author>
    <author>
      <name>Angela E. B. Stewart</name>
    </author>
    <author>
      <name>Motahhare Eslami</name>
    </author>
    <author>
      <name>Amy Ogan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures (a table and a graphic with two parts)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 16th International Society of the Learning
  Sciences (ICLS) 2022, pages 807-814</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2302.13947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.13947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.14360v1</id>
    <updated>2023-02-28T07:26:38Z</updated>
    <published>2023-02-28T07:26:38Z</published>
    <title>A Study of Comfortability between Interactive AI and Human</title>
    <summary>  As the use of interactive AI systems becomes increasingly prevalent in our
daily lives, it is crucial to understand how individuals feel when interacting
with such systems. In this work, we investigate the comfort level of
individuals when interacting with intent-predicting AI systems and identify the
factors of influence. We introduce a study protocol to analyze human
comfortability when interacting with intent-predicting AI systems and execute
the study with over a dozen participants. The study findings suggest that users
are comfortable with AI systems if they have control and their privacy is not
affected. Additionally, the study found that users could differentiate between
AI and human responses, but this did not significantly affect their comfort
levels. This research paper's significance lies in its contribution to the
growing body of literature on interactive AI systems, and it emphasizes the
need to consider user perceptions in the development and deployment.
</summary>
    <author>
      <name>Yi Ru Wang</name>
    </author>
    <author>
      <name>Jiafei Duan</name>
    </author>
    <author>
      <name>Sidharth Talia</name>
    </author>
    <author>
      <name>Hao Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2302.14360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.14360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1373v9</id>
    <updated>2015-11-17T20:54:38Z</updated>
    <published>2014-11-05T19:40:02Z</published>
    <title>Ethical Artificial Intelligence</title>
    <summary>  This book-length article combines several peer reviewed papers and new
material to analyze the issues of ethical artificial intelligence (AI). The
behavior of future AI systems can be described by mathematical equations, which
are adapted to analyze possible unintended AI behaviors and ways that AI
designs can avoid them. This article makes the case for utility-maximizing
agents and for avoiding infinite sets in agent definitions. It shows how to
avoid agent self-delusion using model-based utility functions and how to avoid
agents that corrupt their reward generators (sometimes called "perverse
instantiation") using utility functions that evaluate outcomes at one point in
time from the perspective of humans at a different point in time. It argues
that agents can avoid unintended instrumental actions (sometimes called "basic
AI drives" or "instrumental goals") by accurately learning human values. This
article defines a self-modeling agent framework and shows how it can avoid
problems of resource limits, being predicted by other agents, and inconsistency
between the agent's utility function and its definition (one version of this
problem is sometimes called "motivated value selection"). This article also
discusses how future AI will differ from current AI, the politics of AI, and
the ultimate use of AI to help understand the nature of the universe and our
place in it.
</summary>
    <author>
      <name>Bill Hibbard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">minor edit: remove page break between Figure 10.2 and its caption</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.1373v9" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1373v9" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.06769v2</id>
    <updated>2020-11-23T04:57:39Z</updated>
    <published>2020-03-15T06:39:59Z</published>
    <title>Multi-AI competing and winning against humans in iterated
  Rock-Paper-Scissors game</title>
    <summary>  Predicting and modeling human behavior and finding trends within human
decision-making processes is a major problem of social science. Rock Paper
Scissors (RPS) is the fundamental strategic question in many game theory
problems and real-world competitions. Finding the right approach to beat a
particular human opponent is challenging. Here we use an AI (artificial
intelligence) algorithm based on Markov Models of one fixed memory length
(abbreviated as "single AI") to compete against humans in an iterated RPS game.
We model and predict human competition behavior by combining many Markov Models
with different fixed memory lengths (abbreviated as "multi-AI"), and develop an
architecture of multi-AI with changeable parameters to adapt to different
competition strategies. We introduce a parameter called "focus length" (a
positive number such as 5 or 10) to control the speed and sensitivity for our
multi-AI to adapt to the opponent's strategy change. The focus length is the
number of previous rounds that the multi-AI should look at when determining
which Single-AI has the best performance and should choose to play for the next
game. We experimented with 52 different people, each playing 300 rounds
continuously against one specific multi-AI model, and demonstrated that our
strategy could win against more than 95% of human opponents.
</summary>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Wenbin Huang</name>
    </author>
    <author>
      <name>Yuanpeng Li</name>
    </author>
    <author>
      <name>Julian Evans</name>
    </author>
    <author>
      <name>Sailing He</name>
    </author>
    <link href="http://arxiv.org/abs/2003.06769v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06769v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01772v3</id>
    <updated>2018-12-15T03:42:38Z</updated>
    <published>2017-05-04T10:05:31Z</published>
    <title>Non-Inferiority and Equivalence Tests in A Sequential
  Multiple-Assignment Randomized Trial (SMART)</title>
    <summary>  Adaptive interventions (AIs) are increasingly becoming popular in medical and
behavioral sciences. An AI is a sequence of individualized intervention options
that specify for whom and under what conditions different intervention options
should be offered, in order to address the changing needs of individuals as
they progress over time. The sequential, multiple assignment, randomized trial
(SMART) is a novel trial design that was developed to aid in empirically
constructing effective AIs. The sequential randomizations in a SMART often
yield multiple AIs that are embedded in the trial by design. Many SMARTs are
motivated by scientific questions pertaining to the comparison of such embedded
AIs. Existing data analytic methods and sample size planning resources for
SMARTs are suitable for superiority testing, namely for testing whether one
embedded AI yields better primary outcomes on average than another. This
represents a major scientific gap since AIs are often motivated by the need to
deliver support/care in a less costly or less burdensome manner, while still
yielding benefits that are equivalent or non-inferior to those produced by a
more costly/burdensome standard of care. Here, we develop data analytic methods
and sample size formulas for SMART studies aiming to test the non-inferiority
or equivalence of one AI over another. Sample size and power considerations are
discussed with supporting simulations, and online sample size planning
resources are provided. For illustration, we use an example from a SMART study
aiming to develop an AI for promoting weight loss among overweight/obese
adults.
</summary>
    <author>
      <name>Palash Ghosh</name>
    </author>
    <author>
      <name>Inbal Nahum-Shani</name>
    </author>
    <author>
      <name>Bonnie Spring</name>
    </author>
    <author>
      <name>Bibhas Chakraborty</name>
    </author>
    <link href="http://arxiv.org/abs/1705.01772v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01772v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.00518v2</id>
    <updated>2020-11-03T04:13:16Z</updated>
    <published>2020-11-01T14:48:46Z</published>
    <title>AI Marker-based Large-scale AI Literature Mining</title>
    <summary>  The knowledge contained in academic literature is interesting to mine.
Inspired by the idea of molecular markers tracing in the field of biochemistry,
three named entities, namely, methods, datasets and metrics are used as AI
markers for AI literature. These entities can be used to trace the research
process described in the bodies of papers, which opens up new perspectives for
seeking and mining more valuable academic information. Firstly, the entity
extraction model is used in this study to extract AI markers from large-scale
AI literature. Secondly, original papers are traced for AI markers. Statistical
and propagation analysis are performed based on tracing results. Finally, the
co-occurrences of AI markers are used to achieve clustering. The evolution
within method clusters and the influencing relationships amongst different
research scene clusters are explored. The above-mentioned mining based on AI
markers yields many meaningful discoveries. For example, the propagation of
effective methods on the datasets is rapidly increasing with the development of
time; effective methods proposed by China in recent years have increasing
influence on other countries, whilst France is the opposite. Saliency
detection, a classic computer vision research scene, is the least likely to be
affected by other research scenes.
</summary>
    <author>
      <name>Rujing Yao</name>
    </author>
    <author>
      <name>Yingchun Ye</name>
    </author>
    <author>
      <name>Ji Zhang</name>
    </author>
    <author>
      <name>Shuxiao Li</name>
    </author>
    <author>
      <name>Ou Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2011.00518v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.00518v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01134v3</id>
    <updated>2019-12-20T20:03:56Z</updated>
    <published>2017-11-03T12:54:51Z</published>
    <title>Accountability of AI Under the Law: The Role of Explanation</title>
    <summary>  The ubiquity of systems using artificial intelligence or "AI" has brought
increasing attention to how those systems should be regulated. The choice of
how to regulate AI systems will require care. AI systems have the potential to
synthesize large amounts of data, allowing for greater levels of
personalization and precision than ever before---applications range from
clinical decision support to autonomous driving and predictive policing. That
said, there exist legitimate concerns about the intentional and unintentional
negative consequences of AI systems. There are many ways to hold AI systems
accountable. In this work, we focus on one: explanation. Questions about a
legal right to explanation from AI systems was recently debated in the EU
General Data Protection Regulation, and thus thinking carefully about when and
how explanation from AI systems might improve accountability is timely. In this
work, we review contexts in which explanation is currently required under the
law, and then list the technical considerations that must be considered if we
desired AI systems that could provide kinds of explanations that are currently
required of humans.
</summary>
    <author>
      <name>Finale Doshi-Velez</name>
    </author>
    <author>
      <name>Mason Kortz</name>
    </author>
    <author>
      <name>Ryan Budish</name>
    </author>
    <author>
      <name>Chris Bavitz</name>
    </author>
    <author>
      <name>Sam Gershman</name>
    </author>
    <author>
      <name>David O'Brien</name>
    </author>
    <author>
      <name>Kate Scott</name>
    </author>
    <author>
      <name>Stuart Schieber</name>
    </author>
    <author>
      <name>James Waldo</name>
    </author>
    <author>
      <name>David Weinberger</name>
    </author>
    <author>
      <name>Adrian Weller</name>
    </author>
    <author>
      <name>Alexandra Wood</name>
    </author>
    <link href="http://arxiv.org/abs/1711.01134v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01134v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.04797v1</id>
    <updated>2018-09-13T06:46:34Z</updated>
    <published>2018-09-13T06:46:34Z</published>
    <title>Focus Group on Artificial Intelligence for Health</title>
    <summary>  Artificial Intelligence (AI) - the phenomenon of machines being able to solve
problems that require human intelligence - has in the past decade seen an
enormous rise of interest due to significant advances in effectiveness and use.
The health sector, one of the most important sectors for societies and
economies worldwide, is particularly interesting for AI applications, given the
ongoing digitalisation of all types of health information. The potential for AI
assistance in the health domain is immense, because AI can support medical
decision making at reduced costs, everywhere. However, due to the complexity of
AI algorithms, it is difficult to distinguish good from bad AI-based solutions
and to understand their strengths and weaknesses, which is crucial for
clarifying responsibilities and for building trust. For this reason, the
International Telecommunication Union (ITU) has established a new Focus Group
on "Artificial Intelligence for Health" (FG-AI4H) in partnership with the World
Health Organization (WHO). Health and care services are usually the
responsibility of a government - even when provided through private insurance
systems - and thus under the responsibility of WHO/ITU member states. FG-AI4H
will identify opportunities for international standardization, which will
foster the application of AI to health issues on a global scale. In particular,
it will establish a standardized assessment framework with open benchmarks for
the evaluation of AI-based methods for health, such as AI-based diagnosis,
triage or treatment decisions.
</summary>
    <author>
      <name>Marcel Salathé</name>
    </author>
    <author>
      <name>Thomas Wiegand</name>
    </author>
    <author>
      <name>Markus Wenzel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Whitepaper on ITU Focus Group AI4H for 1st workshop at WHO</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.04797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.04797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.12847v2</id>
    <updated>2018-11-01T12:34:32Z</updated>
    <published>2018-10-30T16:41:22Z</published>
    <title>AI for the Common Good?! Pitfalls, challenges, and Ethics Pen-Testing</title>
    <summary>  Recently, many AI researchers and practitioners have embarked on research
visions that involve doing AI for "Good". This is part of a general drive
towards infusing AI research and practice with ethical thinking. One frequent
theme in current ethical guidelines is the requirement that AI be good for all,
or: contribute to the Common Good. But what is the Common Good, and is it
enough to want to be good? Via four lead questions, I will illustrate
challenges and pitfalls when determining, from an AI point of view, what the
Common Good is and how it can be enhanced by AI. The questions are: What is the
problem / What is a problem?, Who defines the problem?, What is the role of
knowledge?, and What are important side effects and dynamics? The illustration
will use an example from the domain of "AI for Social Good", more specifically
"Data Science for Social Good". Even if the importance of these questions may
be known at an abstract level, they do not get asked sufficiently in practice,
as shown by an exploratory study of 99 contributions to recent conferences in
the field. Turning these challenges and pitfalls into a positive
recommendation, as a conclusion I will draw on another characteristic of
computer-science thinking and practice to make these impediments visible and
attenuate them: "attacks" as a method for improving design. This results in the
proposal of ethics pen-testing as a method for helping AI designs to better
contribute to the Common Good.
</summary>
    <author>
      <name>Bettina Berendt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in Paladyn. Journal of Behavioral Robotics; accepted on
  27-10-2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.12847v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.12847v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.08186v2</id>
    <updated>2019-03-22T17:31:24Z</updated>
    <published>2018-11-20T11:26:36Z</published>
    <title>Analysing Results from AI Benchmarks: Key Indicators and How to Obtain
  Them</title>
    <summary>  Item response theory (IRT) can be applied to the analysis of the evaluation
of results from AI benchmarks. The two-parameter IRT model provides two
indicators (difficulty and discrimination) on the side of the item (or AI
problem) while only one indicator (ability) on the side of the respondent (or
AI agent). In this paper we analyse how to make this set of indicators dual, by
adding a fourth indicator, generality, on the side of the respondent.
Generality is meant to be dual to discrimination, and it is based on
difficulty. Namely, generality is defined as a new metric that evaluates
whether an agent is consistently good at easy problems and bad at difficult
ones. With the addition of generality, we see that this set of four key
indicators can give us more insight on the results of AI benchmarks. In
particular, we explore two popular benchmarks in AI, the Arcade Learning
Environment (Atari 2600 games) and the General Video Game AI competition. We
provide some guidelines to estimate and interpret these indicators for other AI
benchmarks and competitions.
</summary>
    <author>
      <name>Fernando Martínez-Plumed</name>
    </author>
    <author>
      <name>José Hernández-Orallo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TG.2018.2883773</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TG.2018.2883773" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This report is a preliminary version of a related paper with title
  "Dual Indicators to Analyse AI Benchmarks: Difficulty, Discrimination,
  Ability and Generality", accepted for publication at IEEE Transactions on
  Games. Please refer to and cite the journal paper
  (https://doi.org/10.1109/TG.2018.2883773)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Games, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.08186v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.08186v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.08579v2</id>
    <updated>2019-07-16T16:23:00Z</updated>
    <published>2019-01-24T18:53:07Z</published>
    <title>Forecasting Transformative AI: An Expert Survey</title>
    <summary>  Transformative AI technologies have the potential to reshape critical aspects
of society in the near future. However, in order to properly prepare policy
initiatives for the arrival of such technologies accurate forecasts and
timelines are necessary. A survey was administered to attendees of three AI
conferences during the summer of 2018 (ICML, IJCAI and the HLAI conference).
The survey included questions for estimating AI capabilities over the next
decade, questions for forecasting five scenarios of transformative AI and
questions concerning the impact of computational resources in AI research.
Respondents indicated a median of 21.5% of human tasks (i.e., all tasks that
humans are currently paid to do) can be feasibly automated now, and that this
figure would rise to 40% in 5 years and 60% in 10 years. Median forecasts
indicated a 50% probability of AI systems being capable of automating 90% of
current human tasks in 25 years and 99% of current human tasks in 50 years. The
conference of attendance was found to have a statistically significant impact
on all forecasts, with attendees of HLAI providing more optimistic timelines
with less uncertainty. These findings suggest that AI experts expect major
advances in AI technology to continue over the next decade to a degree that
will likely have profound transformative impacts on society.
</summary>
    <author>
      <name>Ross Gruetzemacher</name>
    </author>
    <author>
      <name>David Paradice</name>
    </author>
    <author>
      <name>Kang Bok Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.08579v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.08579v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.11174v1</id>
    <updated>2020-02-25T21:00:52Z</updated>
    <published>2020-02-25T21:00:52Z</published>
    <title>TanksWorld: A Multi-Agent Environment for AI Safety Research</title>
    <summary>  The ability to create artificial intelligence (AI) capable of performing
complex tasks is rapidly outpacing our ability to ensure the safe and assured
operation of AI-enabled systems. Fortunately, a landscape of AI safety research
is emerging in response to this asymmetry and yet there is a long way to go. In
particular, recent simulation environments created to illustrate AI safety
risks are relatively simple or narrowly-focused on a particular issue. Hence,
we see a critical need for AI safety research environments that abstract
essential aspects of complex real-world applications. In this work, we
introduce the AI safety TanksWorld as an environment for AI safety research
with three essential aspects: competing performance objectives, human-machine
teaming, and multi-agent competition. The AI safety TanksWorld aims to
accelerate the advancement of safe multi-agent decision-making algorithms by
providing a software framework to support competitions with both system
performance and safety objectives. As a work in progress, this paper introduces
our research objectives and learning environment with reference code and
baseline performance metrics to follow in a future work.
</summary>
    <author>
      <name>Corban G. Rivera</name>
    </author>
    <author>
      <name>Olivia Lyons</name>
    </author>
    <author>
      <name>Arielle Summitt</name>
    </author>
    <author>
      <name>Ayman Fatima</name>
    </author>
    <author>
      <name>Ji Pak</name>
    </author>
    <author>
      <name>William Shao</name>
    </author>
    <author>
      <name>Robert Chalmers</name>
    </author>
    <author>
      <name>Aryeh Englander</name>
    </author>
    <author>
      <name>Edward W. Staley</name>
    </author>
    <author>
      <name>I-Jeng Wang</name>
    </author>
    <author>
      <name>Ashley J. Llorens</name>
    </author>
    <link href="http://arxiv.org/abs/2002.11174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.13563v1</id>
    <updated>2020-04-26T13:05:29Z</updated>
    <published>2020-04-26T13:05:29Z</published>
    <title>Towards Ubiquitous AI in 6G with Federated Learning</title>
    <summary>  With 5G cellular systems being actively deployed worldwide, the research
community has started to explore novel technological advances for the
subsequent generation, i.e., 6G. It is commonly believed that 6G will be built
on a new vision of ubiquitous AI, an hyper-flexible architecture that brings
human-like intelligence into every aspect of networking systems. Despite its
great promise, there are several novel challenges expected to arise in
ubiquitous AI-based 6G. Although numerous attempts have been made to apply AI
to wireless networks, these attempts have not yet seen any large-scale
implementation in practical systems. One of the key challenges is the
difficulty to implement distributed AI across a massive number of heterogeneous
devices. Federated learning (FL) is an emerging distributed AI solution that
enables data-driven AI solutions in heterogeneous and potentially massive-scale
networks. Although it still in an early stage of development, FL-inspired
architecture has been recognized as one of the most promising solutions to
fulfill ubiquitous AI in 6G. In this article, we identify the requirements that
will drive convergence between 6G and AI. We propose an FL-based network
architecture and discuss its potential for addressing some of the novel
challenges expected in 6G. Future trends and key research problems for
FL-enabled 6G are also discussed.
</summary>
    <author>
      <name>Yong Xiao</name>
    </author>
    <author>
      <name>Guangming Shi</name>
    </author>
    <author>
      <name>Marwan Krunz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Communication Magazine</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.13563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.13563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.02202v2</id>
    <updated>2021-03-11T06:48:43Z</updated>
    <published>2020-07-04T22:48:15Z</published>
    <title>A Survey on Applications of Artificial Intelligence in Fighting Against
  COVID-19</title>
    <summary>  The COVID-19 pandemic caused by the SARS-CoV-2 virus has spread rapidly
worldwide, leading to a global outbreak. Most governments, enterprises, and
scientific research institutions are participating in the COVID-19 struggle to
curb the spread of the pandemic. As a powerful tool against COVID-19,
artificial intelligence (AI) technologies are widely used in combating this
pandemic. In this survey, we investigate the main scope and contributions of AI
in combating COVID-19 from the aspects of disease detection and diagnosis,
virology and pathogenesis, drug and vaccine development, and epidemic and
transmission prediction. In addition, we summarize the available data and
resources that can be used for AI-based COVID-19 research. Finally, the main
challenges and potential directions of AI in fighting against COVID-19 are
discussed. Currently, AI mainly focuses on medical image inspection, genomics,
drug development, and transmission prediction, and thus AI still has great
potential in this field. This survey presents medical and AI researchers with a
comprehensive view of the existing and potential applications of AI technology
in combating COVID-19 with the goal of inspiring researchers to continue to
maximize the advantages of AI and big data to fight COVID-19.
</summary>
    <author>
      <name>Jianguo Chen</name>
    </author>
    <author>
      <name>Kenli Li</name>
    </author>
    <author>
      <name>Zhaolei Zhang</name>
    </author>
    <author>
      <name>Keqin Li</name>
    </author>
    <author>
      <name>Philip S. Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This manuscript was submitted to ACM Computing Surveys</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.02202v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.02202v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.15911v2</id>
    <updated>2021-01-05T08:32:38Z</updated>
    <published>2020-07-31T09:08:27Z</published>
    <title>The role of explainability in creating trustworthy artificial
  intelligence for health care: a comprehensive survey of the terminology,
  design choices, and evaluation strategies</title>
    <summary>  Artificial intelligence (AI) has huge potential to improve the health and
well-being of people, but adoption in clinical practice is still limited. Lack
of transparency is identified as one of the main barriers to implementation, as
clinicians should be confident the AI system can be trusted. Explainable AI has
the potential to overcome this issue and can be a step towards trustworthy AI.
In this paper we review the recent literature to provide guidance to
researchers and practitioners on the design of explainable AI systems for the
health-care domain and contribute to formalization of the field of explainable
AI. We argue the reason to demand explainability determines what should be
explained as this determines the relative importance of the properties of
explainability (i.e. interpretability and fidelity). Based on this, we propose
a framework to guide the choice between classes of explainable AI methods
(explainable modelling versus post-hoc explanation; model-based,
attribution-based, or example-based explanations; global and local
explanations). Furthermore, we find that quantitative evaluation metrics, which
are important for objective standardized evaluation, are still lacking for some
properties (e.g. clarity) and types of explanations (e.g. example-based
methods). We conclude that explainable modelling can contribute to trustworthy
AI, but the benefits of explainability still need to be proven in practice and
complementary measures might be needed to create trustworthy AI in health care
(e.g. reporting data quality, performing extensive (external) validation, and
regulation).
</summary>
    <author>
      <name>Aniek F. Markus</name>
    </author>
    <author>
      <name>Jan A. Kors</name>
    </author>
    <author>
      <name>Peter R. Rijnbeek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jbi.2020.103655</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jbi.2020.103655" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Biomedical Informatics, 113 (2021), 103655</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2007.15911v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.15911v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.07141v7</id>
    <updated>2021-03-15T02:25:55Z</updated>
    <published>2020-08-17T08:06:43Z</published>
    <title>AIPerf: Automated machine learning as an AI-HPC benchmark</title>
    <summary>  The plethora of complex artificial intelligence (AI) algorithms and available
high performance computing (HPC) power stimulates the expeditious development
of AI components with heterogeneous designs. Consequently, the need for
cross-stack performance benchmarking of AI-HPC systems emerges rapidly. The de
facto HPC benchmark LINPACK can not reflect AI computing power and I/O
performance without representative workload. The current popular AI benchmarks
like MLPerf have fixed problem size therefore limited scalability. To address
these issues, we propose an end-to-end benchmark suite utilizing automated
machine learning (AutoML), which not only represents real AI scenarios, but
also is auto-adaptively scalable to various scales of machines. We implement
the algorithms in a highly parallel and flexible way to ensure the efficiency
and optimization potential on diverse systems with customizable configurations.
We utilize operations per second (OPS), which is measured in an analytical and
systematic approach, as the major metric to quantify the AI performance. We
perform evaluations on various systems to ensure the benchmark's stability and
scalability, from 4 nodes with 32 NVIDIA Tesla T4 (56.1 Tera-OPS measured), up
to 512 nodes with 4096 Huawei Ascend 910 (194.53 Peta-OPS measured), and the
results show near-linear weak scalability. With flexible workload and single
metric, our benchmark can scale and rank AI-HPC easily.
</summary>
    <author>
      <name>Zhixiang Ren</name>
    </author>
    <author>
      <name>Yongheng Liu</name>
    </author>
    <author>
      <name>Tianhui Shi</name>
    </author>
    <author>
      <name>Lei Xie</name>
    </author>
    <author>
      <name>Yue Zhou</name>
    </author>
    <author>
      <name>Jidong Zhai</name>
    </author>
    <author>
      <name>Youhui Zhang</name>
    </author>
    <author>
      <name>Yunquan Zhang</name>
    </author>
    <author>
      <name>Wenguang Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2008.07141v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.07141v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.09079v3</id>
    <updated>2021-02-28T16:30:22Z</updated>
    <published>2020-09-02T11:33:07Z</published>
    <title>Problems in AI research and how the SP System may help to solve them</title>
    <summary>  This paper describes problems in AI research and how the SP System (described
in an appendix) may help to solve them. Most of the problems are described by
leading researchers in AI in interviews with science writer Martin Ford, and
reported by him in his book {\em Architects of Intelligence}. These problems
are: the need to bridge the divide between symbolic and non-symbolic kinds of
knowledge and processing; the tendency of deep neural networks (DNNs) to make
large and unexpected errors in recognition; the need to strengthen the
representation and processing of natural languages; the challenges of
unsupervised learning; the need for a coherent account of generalisation; how
to learn usable knowledge from a single exposure; how to achieve transfer
learning; how to increase the efficiency of AI processing; the need for
transparency in AI structures and processes; how to achieve varieties of
probabilistic reasoning; the need for more emphasis on top-down strategies; how
to minimise the risk of accidents with self-driving vehicles; the need for
strong compositionality in AI knowledge; the challenges of commonsense
reasoning and commonsense knowledge; establishing the importance of information
compression in AI research; establishing the importance of a biological
perspective in AI research; establishing whether knowledge in the brain is
represented in `distributed' or `localist' form; how to bypassing the limited
scope for adaptation in deep neural networks; the need to develop `broad AI';
and how to eliminate the problem of catastrophic forgetting.
</summary>
    <author>
      <name>J Gerard Wolff</name>
    </author>
    <link href="http://arxiv.org/abs/2009.09079v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09079v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.13830v4</id>
    <updated>2020-12-14T19:15:24Z</updated>
    <published>2020-10-26T18:32:24Z</published>
    <title>Proceedings of the AI-HRI Symposium at AAAI-FSS 2020</title>
    <summary>  The Artificial Intelligence (AI) for Human-Robot Interaction (HRI) Symposium
has been a successful venue of discussion and collaboration since 2014. In that
time, the related topic of trust in robotics has been rapidly growing, with
major research efforts at universities and laboratories across the world.
Indeed, many of the past participants in AI-HRI have been or are now involved
with research into trust in HRI. While trust has no consensus definition, it is
regularly associated with predictability, reliability, inciting confidence, and
meeting expectations. Furthermore, it is generally believed that trust is
crucial for adoption of both AI and robotics, particularly when transitioning
technologies from the lab to industrial, social, and consumer applications.
However, how does trust apply to the specific situations we encounter in the
AI-HRI sphere? Is the notion of trust in AI the same as that in HRI? We see a
growing need for research that lives directly at the intersection of AI and HRI
that is serviced by this symposium. Over the course of the two-day meeting, we
propose to create a collaborative forum for discussion of current efforts in
trust for AI-HRI, with a sub-session focused on the related topic of
explainable AI (XAI) for HRI.
</summary>
    <author>
      <name>Shelly Bagchi</name>
    </author>
    <author>
      <name>Jason R. Wilson</name>
    </author>
    <author>
      <name>Muneeb I. Ahmad</name>
    </author>
    <author>
      <name>Christian Dondrup</name>
    </author>
    <author>
      <name>Zhao Han</name>
    </author>
    <author>
      <name>Justin W. Hart</name>
    </author>
    <author>
      <name>Matteo Leonetti</name>
    </author>
    <author>
      <name>Katrin Lohan</name>
    </author>
    <author>
      <name>Ross Mead</name>
    </author>
    <author>
      <name>Emmanuel Senft</name>
    </author>
    <author>
      <name>Jivko Sinapov</name>
    </author>
    <author>
      <name>Megan L. Zimmerman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Symposium proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.13830v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.13830v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.05303v4</id>
    <updated>2021-10-05T18:00:06Z</updated>
    <published>2021-01-13T19:01:32Z</published>
    <title>Understanding the Effect of Out-of-distribution Examples and Interactive
  Explanations on Human-AI Decision Making</title>
    <summary>  Although AI holds promise for improving human decision making in societally
critical domains, it remains an open question how human-AI teams can reliably
outperform AI alone and human alone in challenging prediction tasks (also known
as complementary performance). We explore two directions to understand the gaps
in achieving complementary performance. First, we argue that the typical
experimental setup limits the potential of human-AI teams. To account for lower
AI performance out-of-distribution than in-distribution because of distribution
shift, we design experiments with different distribution types and investigate
human performance for both in-distribution and out-of-distribution examples.
Second, we develop novel interfaces to support interactive explanations so that
humans can actively engage with AI assistance. Using virtual pilot studies and
large-scale randomized experiments across three tasks, we demonstrate a clear
difference between in-distribution and out-of-distribution, and observe mixed
results for interactive explanations: while interactive explanations improve
human perception of AI assistance's usefulness, they may reinforce human biases
and lead to limited performance improvement. Overall, our work points out
critical challenges and future directions towards enhancing human performance
with AI assistance.
</summary>
    <author>
      <name>Han Liu</name>
    </author>
    <author>
      <name>Vivian Lai</name>
    </author>
    <author>
      <name>Chenhao Tan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3479552</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3479552" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">45 pages, 24 figures, accepted to CSCW 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.05303v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.05303v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.00233v1</id>
    <updated>2021-01-30T14:46:23Z</updated>
    <published>2021-01-30T14:46:23Z</published>
    <title>An evolutionary view on the emergence of Artificial Intelligence</title>
    <summary>  This paper draws upon the evolutionary concepts of technological relatedness
and knowledge complexity to enhance our understanding of the long-term
evolution of Artificial Intelligence (AI). We reveal corresponding patterns in
the emergence of AI - globally and in the context of specific geographies of
the US, Japan, South Korea, and China. We argue that AI emergence is associated
with increasing related variety due to knowledge commonalities as well as
increasing complexity. We use patent-based indicators for the period between
1974-2018 to analyse the evolution of AI's global technological space, to
identify its technological core as well as changes to its overall relatedness
and knowledge complexity. At the national level, we also measure countries'
overall specialisations against AI-specific ones. At the global level, we find
increasing overall relatedness and complexity of AI. However, for the
technological core of AI, which has been stable over time, we find decreasing
related variety and increasing complexity. This evidence points out that AI
innovations related to core technologies are becoming increasingly distinct
from each other. At the country level, we find that the US and Japan have been
increasing the overall relatedness of their innovations. The opposite is the
case for China and South Korea, which we associate with the fact that these
countries are overall less technologically developed than the US and Japan.
Finally, we observe a stable increasing overall complexity for all countries
apart from China, which we explain by the focus of this country in technologies
not strongly linked to AI.
</summary>
    <author>
      <name>Matheus E. Leusin</name>
    </author>
    <author>
      <name>Bjoern Jindra</name>
    </author>
    <author>
      <name>Daniel S. Hain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: Artificial Intelligence; technological space; evolutionary
  economic geography; technological relatedness; knowledge complexity</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.00233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.00233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.04527v1</id>
    <updated>2021-02-08T20:53:42Z</updated>
    <published>2021-02-08T20:53:42Z</published>
    <title>Playing the Blame Game with Robots</title>
    <summary>  Recent research shows -- somewhat astonishingly -- that people are willing to
ascribe moral blame to AI-driven systems when they cause harm [1]-[4]. In this
paper, we explore the moral-psychological underpinnings of these findings. Our
hypothesis was that the reason why people ascribe moral blame to AI systems is
that they consider them capable of entertaining inculpating mental states (what
is called mens rea in the law). To explore this hypothesis, we created a
scenario in which an AI system runs a risk of poisoning people by using a novel
type of fertilizer. Manipulating the computational (or quasi-cognitive)
abilities of the AI system in a between-subjects design, we tested whether
people's willingness to ascribe knowledge of a substantial risk of harm (i.e.,
recklessness) and blame to the AI system. Furthermore, we investigated whether
the ascription of recklessness and blame to the AI system would influence the
perceived blameworthiness of the system's user (or owner). In an experiment
with 347 participants, we found (i) that people are willing to ascribe blame to
AI systems in contexts of recklessness, (ii) that blame ascriptions depend
strongly on the willingness to attribute recklessness and (iii) that the
latter, in turn, depends on the perceived "cognitive" capacities of the system.
Furthermore, our results suggest (iv) that the higher the computational
sophistication of the AI system, the more blame is shifted from the human user
to the AI system.
</summary>
    <author>
      <name>Markus Kneer</name>
    </author>
    <author>
      <name>Michael T. Stuart</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3434074.3447202</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3434074.3447202" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, 2 tables, HRI'21</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.04527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.04527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.09051v1</id>
    <updated>2021-03-04T21:48:11Z</updated>
    <published>2021-03-04T21:48:11Z</published>
    <title>Exploring the Assessment List for Trustworthy AI in the Context of
  Advanced Driver-Assistance Systems</title>
    <summary>  Artificial Intelligence (AI) is increasingly used in critical applications.
Thus, the need for dependable AI systems is rapidly growing. In 2018, the
European Commission appointed experts to a High-Level Expert Group on AI
(AI-HLEG). AI-HLEG defined Trustworthy AI as 1) lawful, 2) ethical, and 3)
robust and specified seven corresponding key requirements. To help development
organizations, AI-HLEG recently published the Assessment List for Trustworthy
AI (ALTAI). We present an illustrative case study from applying ALTAI to an
ongoing development project of an Advanced Driver-Assistance System (ADAS) that
relies on Machine Learning (ML). Our experience shows that ALTAI is largely
applicable to ADAS development, but specific parts related to human agency and
transparency can be disregarded. Moreover, bigger questions related to societal
and environmental impact cannot be tackled by an ADAS supplier in isolation. We
present how we plan to develop the ADAS to ensure ALTAI-compliance. Finally, we
provide three recommendations for the next revision of ALTAI, i.e., life-cycle
variants, domain-specific adaptations, and removed redundancy.
</summary>
    <author>
      <name>Markus Borg</name>
    </author>
    <author>
      <name>Joshua Bronson</name>
    </author>
    <author>
      <name>Linus Christensson</name>
    </author>
    <author>
      <name>Fredrik Olsson</name>
    </author>
    <author>
      <name>Olof Lennartsson</name>
    </author>
    <author>
      <name>Elias Sonnsjö</name>
    </author>
    <author>
      <name>Hamid Ebabi</name>
    </author>
    <author>
      <name>Martin Karsberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in the Proc. of the 2nd Workshop on Ethics
  in Software Engineering Research and Practice</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.09051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.09051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.06910v1</id>
    <updated>2021-04-14T15:00:39Z</updated>
    <published>2021-04-14T15:00:39Z</published>
    <title>Towards a framework for evaluating the safety, acceptability and
  efficacy of AI systems for health: an initial synthesis</title>
    <summary>  The potential presented by Artificial Intelligence (AI) for healthcare has
long been recognised by the technical community. More recently, this potential
has been recognised by policymakers, resulting in considerable public and
private investment in the development of AI for healthcare across the globe.
Despite this, excepting limited success stories, real-world implementation of
AI systems into front-line healthcare has been limited. There are numerous
reasons for this, but a main contributory factor is the lack of internationally
accepted, or formalised, regulatory standards to assess AI safety and impact
and effectiveness. This is a well-recognised problem with numerous ongoing
research and policy projects to overcome it. Our intention here is to
contribute to this problem-solving effort by seeking to set out a minimally
viable framework for evaluating the safety, acceptability and efficacy of AI
systems for healthcare. We do this by conducting a systematic search across
Scopus, PubMed and Google Scholar to identify all the relevant literature
published between January 1970 and November 2020 related to the evaluation of:
output performance; efficacy; and real-world use of AI systems, and
synthesising the key themes according to the stages of evaluation: pre-clinical
(theoretical phase); exploratory phase; definitive phase; and post-market
surveillance phase (monitoring). The result is a framework to guide AI system
developers, policymakers, and regulators through a sufficient evaluation of an
AI system designed for use in healthcare.
</summary>
    <author>
      <name>Jessica Morley</name>
    </author>
    <author>
      <name>Caroline Morton</name>
    </author>
    <author>
      <name>Kassandra Karpathakis</name>
    </author>
    <author>
      <name>Mariarosaria Taddeo</name>
    </author>
    <author>
      <name>Luciano Floridi</name>
    </author>
    <link href="http://arxiv.org/abs/2104.06910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.06910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.14506v1</id>
    <updated>2021-04-25T23:39:14Z</updated>
    <published>2021-04-25T23:39:14Z</published>
    <title>Explainable AI For COVID-19 CT Classifiers: An Initial Comparison Study</title>
    <summary>  Artificial Intelligence (AI) has made leapfrogs in development across all the
industrial sectors especially when deep learning has been introduced. Deep
learning helps to learn the behaviour of an entity through methods of
recognising and interpreting patterns. Despite its limitless potential, the
mystery is how deep learning algorithms make a decision in the first place.
Explainable AI (XAI) is the key to unlocking AI and the black-box for deep
learning. XAI is an AI model that is programmed to explain its goals, logic,
and decision making so that the end users can understand. The end users can be
domain experts, regulatory agencies, managers and executive board members, data
scientists, users that use AI, with or without awareness, or someone who is
affected by the decisions of an AI model. Chest CT has emerged as a valuable
tool for the clinical diagnostic and treatment management of the lung diseases
associated with COVID-19. AI can support rapid evaluation of CT scans to
differentiate COVID-19 findings from other lung diseases. However, how these AI
tools or deep learning algorithms reach such a decision and which are the most
influential features derived from these neural networks with typically deep
layers are not clear. The aim of this study is to propose and develop XAI
strategies for COVID-19 classification models with an investigation of
comparison. The results demonstrate promising quantification and qualitative
visualisations that can further enhance the clinician's understanding and
decision making with more granular information from the results given by the
learned XAI models.
</summary>
    <author>
      <name>Qinghao Ye</name>
    </author>
    <author>
      <name>Jun Xia</name>
    </author>
    <author>
      <name>Guang Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, IEEE CBMS 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.14506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.14506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.01774v2</id>
    <updated>2021-06-18T15:21:10Z</updated>
    <published>2021-05-04T21:40:04Z</published>
    <title>Envisioning Communities: A Participatory Approach Towards AI for Social
  Good</title>
    <summary>  Research in artificial intelligence (AI) for social good presupposes some
definition of social good, but potential definitions have been seldom suggested
and never agreed upon. The normative question of what AI for social good
research should be "for" is not thoughtfully elaborated, or is frequently
addressed with a utilitarian outlook that prioritizes the needs of the majority
over those who have been historically marginalized, brushing aside realities of
injustice and inequity. We argue that AI for social good ought to be assessed
by the communities that the AI system will impact, using as a guide the
capabilities approach, a framework to measure the ability of different policies
to improve human welfare equity. Furthermore, we lay out how AI research has
the potential to catalyze social progress by expanding and equalizing
capabilities. We show how the capabilities approach aligns with a participatory
approach for the design and implementation of AI for social good research in a
framework we introduce called PACT, in which community members affected should
be brought in as partners and their input prioritized throughout the project.
We conclude by providing an incomplete set of guiding questions for carrying
out such participatory AI research in a way that elicits and respects a
community's own definition of social good.
</summary>
    <author>
      <name>Elizabeth Bondi</name>
    </author>
    <author>
      <name>Lily Xu</name>
    </author>
    <author>
      <name>Diana Acosta-Navas</name>
    </author>
    <author>
      <name>Jackson A. Killian</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3461702.3462612</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3461702.3462612" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Bondi and Xu Equal contribution. 12 pages, 5 figures. Accepted at the
  Fourth AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society
  (AIES-21)</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.01774v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.01774v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.05568v2</id>
    <updated>2022-10-24T11:20:06Z</updated>
    <published>2021-06-10T07:47:33Z</published>
    <title>Explainable AI, but explainable to whom?</title>
    <summary>  Advances in AI technologies have resulted in superior levels of AI-based
model performance. However, this has also led to a greater degree of model
complexity, resulting in 'black box' models. In response to the AI black box
problem, the field of explainable AI (xAI) has emerged with the aim of
providing explanations catered to human understanding, trust, and transparency.
Yet, we still have a limited understanding of how xAI addresses the need for
explainable AI in the context of healthcare. Our research explores the
differing explanation needs amongst stakeholders during the development of an
AI-system for classifying COVID-19 patients for the ICU. We demonstrate that
there is a constellation of stakeholders who have different explanation needs,
not just the 'user'. Further, the findings demonstrate how the need for xAI
emerges through concerns associated with specific stakeholder groups i.e., the
development team, subject matter experts, decision makers, and the audience.
Our findings contribute to the expansion of xAI by highlighting that different
stakeholders have different explanation needs. From a practical perspective,
the study provides insights on how AI systems can be adjusted to support
different stakeholders needs, ensuring better implementation and operation in a
healthcare context.
</summary>
    <author>
      <name>Julie Gerlings</name>
    </author>
    <author>
      <name>Millie Søndergaard Jensen</name>
    </author>
    <author>
      <name>Arisa Shollo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-83620-7_7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-83620-7_7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Book chapter for AI in Healthcare</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.05568v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.05568v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.12486v1</id>
    <updated>2021-07-26T21:19:03Z</updated>
    <published>2021-07-26T21:19:03Z</published>
    <title>AI Multi-Tenancy on Edge: Concurrent Deep Learning Model Executions and
  Dynamic Model Placements on Edge Devices</title>
    <summary>  Many real-world applications are widely adopting the edge computing paradigm
due to its low latency and better privacy protection. With notable success in
AI and deep learning (DL), edge devices and AI accelerators play a crucial role
in deploying DL inference services at the edge of the Internet. While prior
works quantified various edge devices' efficiency, most studies focused on the
performance of edge devices with single DL tasks. Therefore, there is an urgent
need to investigate AI multi-tenancy on edge devices, required by many advanced
DL applications for edge computing.
  This work investigates two techniques - concurrent model executions and
dynamic model placements - for AI multi-tenancy on edge devices. With image
classification as an example scenario, we empirically evaluate AI multi-tenancy
on various edge devices, AI accelerators, and DL frameworks to identify its
benefits and limitations. Our results show that multi-tenancy significantly
improves DL inference throughput by up to 3.3x -- 3.8x on Jetson TX2. These AI
multi-tenancy techniques also open up new opportunities for flexible deployment
of multiple DL services on edge devices and AI accelerators.
</summary>
    <author>
      <name>Piyush Subedi</name>
    </author>
    <author>
      <name>Jianwei Hao</name>
    </author>
    <author>
      <name>In Kee Kim</name>
    </author>
    <author>
      <name>Lakshmish Ramaswamy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, To appear in 2021 IEEE International Conference on Cloud
  Computing, September, 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.12486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.12486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.02202v1</id>
    <updated>2021-09-06T01:39:48Z</updated>
    <published>2021-09-06T01:39:48Z</published>
    <title>Fairness via AI: Bias Reduction in Medical Information</title>
    <summary>  Most Fairness in AI research focuses on exposing biases in AI systems. A
broader lens on fairness reveals that AI can serve a greater aspiration:
rooting out societal inequities from their source. Specifically, we focus on
inequities in health information, and aim to reduce bias in that domain using
AI. The AI algorithms under the hood of search engines and social media, many
of which are based on recommender systems, have an outsized impact on the
quality of medical and health information online. Therefore, embedding bias
detection and reduction into these recommender systems serving up medical and
health content online could have an outsized positive impact on patient
outcomes and wellbeing.
  In this position paper, we offer the following contributions: (1) we propose
a novel framework of Fairness via AI, inspired by insights from medical
education, sociology and antiracism; (2) we define a new term, bisinformation,
which is related to, but distinct from, misinformation, and encourage
researchers to study it; (3) we propose using AI to study, detect and mitigate
biased, harmful, and/or false health information that disproportionately hurts
minority groups in society; and (4) we suggest several pillars and pose several
open problems in order to seed inquiry in this new space. While part (3) of
this work specifically focuses on the health domain, the fundamental computer
science advances and contributions stemming from research efforts in bias
reduction and Fairness via AI have broad implications in all areas of society.
</summary>
    <author>
      <name>Shiri Dori-Hacohen</name>
    </author>
    <author>
      <name>Roberto Montenegro</name>
    </author>
    <author>
      <name>Fabricio Murai</name>
    </author>
    <author>
      <name>Scott A. Hale</name>
    </author>
    <author>
      <name>Keen Sung</name>
    </author>
    <author>
      <name>Michela Blain</name>
    </author>
    <author>
      <name>Jennifer Edwards-Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in: The 4th FAccTRec Workshop on Responsible Recommendation
  at RecSys 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.02202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.02202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01231v1</id>
    <updated>2021-11-10T00:44:53Z</updated>
    <published>2021-11-10T00:44:53Z</published>
    <title>Internationalizing AI: Evolution and Impact of Distance Factors</title>
    <summary>  International collaboration has become imperative in the field of AI.
However, few studies exist concerning how distance factors have affected the
international collaboration in AI research. In this study, we investigate this
problem by using 1,294,644 AI related collaborative papers harvested from the
Microsoft Academic Graph (MAG) dataset. A framework including 13 indicators to
quantify the distance factors between countries from 5 perspectives (i.e.,
geographic distance, economic distance, cultural distance, academic distance,
and industrial distance) is proposed. The relationships were conducted by the
methods of descriptive analysis and regression analysis. The results show that
international collaboration in the field of AI today is not prevalent (only
15.7%). All the separations in international collaborations have increased over
years, except for the cultural distance in masculinity/felinity dimension and
the industrial distance. The geographic distance, economic distance and
academic distances have shown significantly negative relationships with the
degree of international collaborations in the field of AI. The industrial
distance has a significant positive relationship with the degree of
international collaboration in the field of AI. Also, the results demonstrate
that the participation of the United States and China have promoted the
international collaboration in the field of AI. This study provides a
comprehensive understanding of internationalizing AI research in geographic,
economic, cultural, academic, and industrial aspects.
</summary>
    <author>
      <name>Xuli Tang</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Feicheng Ma</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scientometrics 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2112.01231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.07446v1</id>
    <updated>2022-02-04T15:29:57Z</updated>
    <published>2022-02-04T15:29:57Z</published>
    <title>Relational Artificial Intelligence</title>
    <summary>  The impact of Artificial Intelligence does not depend only on fundamental
research and technological developments, but for a large part on how these
systems are introduced into society and used in everyday situations. Even
though AI is traditionally associated with rational decision making,
understanding and shaping the societal impact of AI in all its facets requires
a relational perspective. A rational approach to AI, where computational
algorithms drive decision making independent of human intervention, insights
and emotions, has shown to result in bias and exclusion, laying bare societal
vulnerabilities and insecurities. A relational approach, that focus on the
relational nature of things, is needed to deal with the ethical, legal,
societal, cultural, and environmental implications of AI. A relational approach
to AI recognises that objective and rational reasoning cannot does not always
result in the 'right' way to proceed because what is 'right' depends on the
dynamics of the situation in which the decision is taken, and that rather than
solving ethical problems the focus of design and use of AI must be on asking
the ethical question. In this position paper, I start with a general discussion
of current conceptualisations of AI followed by an overview of existing
approaches to governance and responsible development and use of AI. Then, I
reflect over what should be the bases of a social paradigm for AI and how this
should be embedded in relational, feminist and non-Western philosophies, in
particular the Ubuntu philosophy.
</summary>
    <author>
      <name>Virginia Dignum</name>
    </author>
    <link href="http://arxiv.org/abs/2202.07446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.07446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.08979v1</id>
    <updated>2022-02-16T22:02:09Z</updated>
    <published>2022-02-16T22:02:09Z</published>
    <title>The Response Shift Paradigm to Quantify Human Trust in AI
  Recommendations</title>
    <summary>  Explainability, interpretability and how much they affect human trust in AI
systems are ultimately problems of human cognition as much as machine learning,
yet the effectiveness of AI recommendations and the trust afforded by end-users
are typically not evaluated quantitatively. We developed and validated a
general purpose Human-AI interaction paradigm which quantifies the impact of AI
recommendations on human decisions. In our paradigm we confronted human users
with quantitative prediction tasks: asking them for a first response, before
confronting them with an AI's recommendations (and explanation), and then
asking the human user to provide an updated final response. The difference
between final and first responses constitutes the shift or sway in the human
decision which we use as metric of the AI's recommendation impact on the human,
representing the trust they place on the AI. We evaluated this paradigm on
hundreds of users through Amazon Mechanical Turk using a multi-branched
experiment confronting users with good/poor AI systems that had good, poor or
no explainability. Our proof-of-principle paradigm allows one to quantitatively
compare the rapidly growing set of XAI/IAI approaches in terms of their effect
on the end-user and opens up the possibility of (machine) learning trust.
</summary>
    <author>
      <name>Ali Shafti</name>
    </author>
    <author>
      <name>Victoria Derks</name>
    </author>
    <author>
      <name>Hannah Kay</name>
    </author>
    <author>
      <name>A. Aldo Faisal</name>
    </author>
    <link href="http://arxiv.org/abs/2202.08979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.08979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.05151v1</id>
    <updated>2022-04-11T14:36:39Z</updated>
    <published>2022-04-11T14:36:39Z</published>
    <title>Metaethical Perspectives on 'Benchmarking' AI Ethics</title>
    <summary>  Benchmarks are seen as the cornerstone for measuring technical progress in
Artificial Intelligence (AI) research and have been developed for a variety of
tasks ranging from question answering to facial recognition. An increasingly
prominent research area in AI is ethics, which currently has no set of
benchmarks nor commonly accepted way for measuring the 'ethicality' of an AI
system. In this paper, drawing upon research in moral philosophy and
metaethics, we argue that it is impossible to develop such a benchmark. As
such, alternative mechanisms are necessary for evaluating whether an AI system
is 'ethical'. This is especially pressing in light of the prevalence of
applied, industrial AI research. We argue that it makes more sense to talk
about 'values' (and 'value alignment') rather than 'ethics' when considering
the possible actions of present and future AI systems. We further highlight
that, because values are unambiguously relative, focusing on values forces us
to consider explicitly what the values are and whose values they are. Shifting
the emphasis from ethics to values therefore gives rise to several new ways of
understanding how researchers might advance research programmes for robustly
safe or beneficial AI. We conclude by highlighting a number of possible ways
forward for the field as a whole, and we advocate for different approaches
towards more value-aligned AI research.
</summary>
    <author>
      <name>Travis LaCroix</name>
    </author>
    <author>
      <name>Alexandra Sasha Luccioni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.05151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.05151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.01066v2</id>
    <updated>2022-05-03T08:17:32Z</updated>
    <published>2022-04-24T14:19:47Z</published>
    <title>Quantifying Health Inequalities Induced by Data and AI Models</title>
    <summary>  AI technologies are being increasingly tested and applied in critical
environments including healthcare. Without an effective way to detect and
mitigate AI induced inequalities, AI might do more harm than good, potentially
leading to the widening of underlying inequalities. This paper proposes a
generic allocation-deterioration framework for detecting and quantifying AI
induced inequality. Specifically, AI induced inequalities are quantified as the
area between two allocation-deterioration curves. To assess the framework's
performance, experiments were conducted on ten synthetic datasets (N&gt;33,000)
generated from HiRID - a real-world Intensive Care Unit (ICU) dataset, showing
its ability to accurately detect and quantify inequality proportionally to
controlled inequalities. Extensive analyses were carried out to quantify health
inequalities (a) embedded in two real-world ICU datasets; (b) induced by AI
models trained for two resource allocation scenarios. Results showed that
compared to men, women had up to 33% poorer deterioration in markers of
prognosis when admitted to HiRID ICUs. All four AI models assessed were shown
to induce significant inequalities (2.45% to 43.2%) for non-White compared to
White patients. The models exacerbated data embedded inequalities significantly
in 3 out of 8 assessments, one of which was &gt;9 times worse.
  The codebase is at https://github.com/knowlab/DAindex-Framework.
</summary>
    <author>
      <name>Honghan Wu</name>
    </author>
    <author>
      <name>Minhong Wang</name>
    </author>
    <author>
      <name>Aneeta Sylolypavan</name>
    </author>
    <author>
      <name>Sarah Wild</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IJCAI-ECAI 2022 AI for Good track</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.01066v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.01066v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.07722v2</id>
    <updated>2022-11-15T10:08:35Z</updated>
    <published>2022-05-16T14:39:37Z</published>
    <title>How Different Groups Prioritize Ethical Values for Responsible AI</title>
    <summary>  Private companies, public sector organizations, and academic groups have
outlined ethical values they consider important for responsible artificial
intelligence technologies. While their recommendations converge on a set of
central values, little is known about the values a more representative public
would find important for the AI technologies they interact with and might be
affected by. We conducted a survey examining how individuals perceive and
prioritize responsible AI values across three groups: a representative sample
of the US population (N=743), a sample of crowdworkers (N=755), and a sample of
AI practitioners (N=175). Our results empirically confirm a common concern: AI
practitioners' value priorities differ from those of the general public.
Compared to the US-representative sample, AI practitioners appear to consider
responsible AI values as less important and emphasize a different set of
values. In contrast, self-identified women and black respondents found
responsible AI values more important than other groups. Surprisingly, more
liberal-leaning participants, rather than participants reporting experiences
with discrimination, were more likely to prioritize fairness than other groups.
Our findings highlight the importance of paying attention to who gets to define
responsible AI.
</summary>
    <author>
      <name>Maurice Jakesch</name>
    </author>
    <author>
      <name>Zana Buçinca</name>
    </author>
    <author>
      <name>Saleema Amershi</name>
    </author>
    <author>
      <name>Alexandra Olteanu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3531146.3533097</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3531146.3533097" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2022 ACM Conference on Fairness, Accountability, and Transparency
  (FAccT '22), June 21-24, 2022, Seoul, Republic of Korea</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2205.07722v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.07722v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.12749v2</id>
    <updated>2022-07-01T15:57:09Z</updated>
    <published>2022-05-25T12:59:13Z</published>
    <title>A Human-Centric Assessment Framework for AI</title>
    <summary>  With the rise of AI systems in real-world applications comes the need for
reliable and trustworthy AI. An essential aspect of this are explainable AI
systems. However, there is no agreed standard on how explainable AI systems
should be assessed. Inspired by the Turing test, we introduce a human-centric
assessment framework where a leading domain expert accepts or rejects the
solutions of an AI system and another domain expert. By comparing the
acceptance rates of provided solutions, we can assess how the AI system
performs compared to the domain expert, and whether the AI system's
explanations (if provided) are human-understandable. This setup -- comparable
to the Turing test -- can serve as a framework for a wide range of
human-centric AI system assessments. We demonstrate this by presenting two
instantiations: (1) an assessment that measures the classification accuracy of
a system with the option to incorporate label uncertainties; (2) an assessment
where the usefulness of provided explanations is determined in a human-centric
manner.
</summary>
    <author>
      <name>Sascha Saralajew</name>
    </author>
    <author>
      <name>Ammar Shaker</name>
    </author>
    <author>
      <name>Zhao Xu</name>
    </author>
    <author>
      <name>Kiril Gashteovski</name>
    </author>
    <author>
      <name>Bhushan Kotnis</name>
    </author>
    <author>
      <name>Wiem Ben Rim</name>
    </author>
    <author>
      <name>Jürgen Quittek</name>
    </author>
    <author>
      <name>Carolin Lawrence</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as submission to ICML 2022 Workshop on Human-Machine
  Collaboration and Teaming</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.12749v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.12749v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.13131v1</id>
    <updated>2022-05-26T03:41:12Z</updated>
    <published>2022-05-26T03:41:12Z</published>
    <title>On the Evolution of A.I. and Machine Learning: Towards Measuring and
  Understanding Impact, Influence, and Leadership at Premier A.I. Conferences</title>
    <summary>  Artificial Intelligence is now recognized as a general-purpose technology
with ample impact on human life. In this work, we aim to understand the
evolution of AI and Machine learning over the years by analyzing researchers'
impact, influence, and leadership over the last decades. This work also intends
to shed new light on the history and evolution of AI by exploring the dynamics
involved in the field's evolution through the lenses of the papers published on
AI conferences since the first International Joint Conference on Artificial
Intelligence (IJCAI) in 1969. AI development and evolution have led to
increasing research output, reflected in the number of articles published over
the last sixty years. We construct comprehensive citation-collaboration and
paper-author datasets and compute corresponding centrality measures to carry
out our analyses. These analyses allow a better understanding of how AI has
reached its current state of affairs in research. Throughout the process, we
correlate these datasets with the work of the ACM Turing Award winners and the
so-called two AI winters the field has gone through. We also look at
self-citation trends and new authors' behaviors. Finally, we present a novel
way to infer the country of affiliation of a paper from its organization.
Therefore, this work provides a deep analysis of Artificial Intelligence
history from information gathered and analyzed from large technical venues
datasets and suggests novel insights that can contribute to understanding and
measuring AI's evolution.
</summary>
    <author>
      <name>Rafael B. Audibert</name>
    </author>
    <author>
      <name>Henrique Lemos</name>
    </author>
    <author>
      <name>Pedro Avelar</name>
    </author>
    <author>
      <name>Anderson R. Tavares</name>
    </author>
    <author>
      <name>Luís C. Lamb</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">87 pages, 57 figures, 11 tables. Draft</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.13131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.13131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; I.2.6; K.1; K.2; K.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.05862v7</id>
    <updated>2022-09-20T16:49:56Z</updated>
    <published>2022-06-13T00:22:50Z</published>
    <title>X-Risk Analysis for AI Research</title>
    <summary>  Artificial intelligence (AI) has the potential to greatly improve society,
but as with any powerful technology, it comes with heightened risks and
responsibilities. Current AI research lacks a systematic discussion of how to
manage long-tail risks from AI systems, including speculative long-term risks.
Keeping in mind the potential benefits of AI, there is some concern that
building ever more intelligent and powerful AI systems could eventually result
in systems that are more powerful than us; some say this is like playing with
fire and speculate that this could create existential risks (x-risks). To add
precision and ground these discussions, we provide a guide for how to analyze
AI x-risk, which consists of three parts: First, we review how systems can be
made safer today, drawing on time-tested concepts from hazard analysis and
systems safety that have been designed to steer large processes in safer
directions. Next, we discuss strategies for having long-term impacts on the
safety of future systems. Finally, we discuss a crucial concept in making AI
systems safer by improving the balance between safety and general capabilities.
We hope this document and the presented concepts and tools serve as a useful
guide for understanding how to analyze AI x-risk.
</summary>
    <author>
      <name>Dan Hendrycks</name>
    </author>
    <author>
      <name>Mantas Mazeika</name>
    </author>
    <link href="http://arxiv.org/abs/2206.05862v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.05862v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.11068v1</id>
    <updated>2022-06-22T13:33:11Z</updated>
    <published>2022-06-22T13:33:11Z</published>
    <title>AI Challenges for Society and Ethics</title>
    <summary>  Artificial intelligence is already being applied in and impacting many
important sectors in society, including healthcare, finance, and policing.
These applications will increase as AI capabilities continue to progress, which
has the potential to be highly beneficial for society, or to cause serious
harm. The role of AI governance is ultimately to take practical steps to
mitigate this risk of harm while enabling the benefits of innovation in AI.
This requires answering challenging empirical questions about current and
potential risks and benefits of AI: assessing impacts that are often widely
distributed and indirect, and making predictions about a highly uncertain
future. It also requires thinking through the normative question of what
beneficial use of AI in society looks like, which is equally challenging.
Though different groups may agree on high-level principles that uses of AI
should respect (e.g., privacy, fairness, and autonomy), challenges arise when
putting these principles into practice. For example, it is straightforward to
say that AI systems must protect individual privacy, but there is presumably
some amount or type of privacy that most people would be willing to give up to
develop life-saving medical treatments. Despite these challenges, research can
and has made progress on these questions. The aim of this chapter will be to
give readers an understanding of this progress, and of the challenges that
remain.
</summary>
    <author>
      <name>Jess Whittlestone</name>
    </author>
    <author>
      <name>Sam Clarke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/oxfordhb/9780197579329.013.3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/oxfordhb/9780197579329.013.3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, published in Oxford Handbook of AI Governance</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.11068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.11068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.01352v1</id>
    <updated>2022-08-02T10:46:50Z</updated>
    <published>2022-08-02T10:46:50Z</published>
    <title>Interplay between Distributed AI Workflow and URLLC</title>
    <summary>  Distributed artificial intelligence (AI) has recently accomplished tremendous
breakthroughs in various communication services, ranging from fault-tolerant
factory automation to smart cities. When distributed learning is run over a set
of wireless connected devices, random channel fluctuations, and the incumbent
services simultaneously running on the same network affect the performance of
distributed learning. In this paper, we investigate the interplay between
distributed AI workflow and ultra-reliable low latency communication (URLLC)
services running concurrently over a network. Using 3GPP compliant simulations
in a factory automation use case, we show the impact of various distributed AI
settings (e.g., model size and the number of participating devices) on the
convergence time of distributed AI and the application layer performance of
URLLC. Unless we leverage the existing 5G-NR quality of service handling
mechanisms to separate the traffic from the two services, our simulation
results show that the impact of distributed AI on the availability of the URLLC
devices is significant. Moreover, with proper setting of distributed AI (e.g.,
proper user selection), we can substantially reduce network resource
utilization, leading to lower latency for distributed AI and higher
availability for the URLLC users. Our results provide important insights for
future 6G and AI standardization.
</summary>
    <author>
      <name>Milad Ganjalizadeh</name>
    </author>
    <author>
      <name>Hossein S. Ghadikolaei</name>
    </author>
    <author>
      <name>Johan Haraldson</name>
    </author>
    <author>
      <name>Marina Petrova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in 2022 IEEE Global Communications Conference (GLOBECOM)</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.01352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.01352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.14451v3</id>
    <updated>2022-09-12T12:16:29Z</updated>
    <published>2022-08-30T14:55:51Z</published>
    <title>Foreseeing the Impact of the Proposed AI Act on the Sustainability and
  Safety of Critical Infrastructures</title>
    <summary>  The AI Act has been recently proposed by the European Commission to regulate
the use of AI in the EU, especially on high-risk applications, i.e. systems
intended to be used as safety components in the management and operation of
road traffic and the supply of water, gas, heating and electricity. On the
other hand, IEC 61508, one of the most adopted international standards for
safety-critical electronic components, seem to mostly forbid the use of AI in
such systems. Given this conflict between IEC 61508 and the proposed AI Act,
also stressed by the fact that IEC 61508 is not an harmonised European
standard, with the present paper we study and analyse what is going to happen
to industry after the entry into force of the AI Act. In particular, we focus
on how the proposed AI Act might positively impact on the sustainability of
critical infrastructures by allowing the use of AI on an industry where it was
previously forbidden. To do so, we provide several examples of AI-based
solutions falling under the umbrella of IEC 61508 that might have a positive
impact on sustainability in alignment with the current long-term goals of the
EU and the Sustainable Development Goals of the United Nations, i.e.,
affordable and clean energy, sustainable cities and communities.
</summary>
    <author>
      <name>Francesco Sovrano</name>
    </author>
    <author>
      <name>Giulio Masetti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3560107.3560253</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3560107.3560253" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.14451v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.14451v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.09780v2</id>
    <updated>2022-09-27T15:20:26Z</updated>
    <published>2022-09-20T15:05:55Z</published>
    <title>Dislocated Accountabilities in the AI Supply Chain: Modularity and
  Developers' Notions of Responsibility</title>
    <summary>  Responsible AI guidelines often ask engineers to consider how their systems
might harm. However, contemporary AI systems are built by composing many
preexisting software modules that pass through many hands before becoming a
finished product or service. How does this shape responsible AI practice? In
interviews with 27 AI engineers across industry, open source, and academia, our
participants often did not see the questions posed in responsible AI guidelines
to be within their agency, capability, or responsibility to address. We use
Lucy Suchman's notion of located accountability to show how responsible AI
labor is currently organized, and to explore how it could be done differently.
We identify cross-cutting social logics, like modularizability, scale,
reputation, and customer orientation, that organize which responsible AI
actions do take place, and which are relegated to low status staff or believed
to be the work of the next or previous person in the chain. We argue that
current responsible AI interventions, like ethics checklists and guidelines
that assume panoptical knowledge and control over systems, could improve by
taking a located accountability approach, where relations and obligations
intertwine and incrementally add value in the process. This would constitute a
shift from "supply chain' thinking to "value chain" thinking.
</summary>
    <author>
      <name>David Gray Widder</name>
    </author>
    <author>
      <name>Dawn Nafus</name>
    </author>
    <link href="http://arxiv.org/abs/2209.09780v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.09780v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.10604v1</id>
    <updated>2022-09-21T18:56:14Z</updated>
    <published>2022-09-21T18:56:14Z</published>
    <title>Current and Near-Term AI as a Potential Existential Risk Factor</title>
    <summary>  There is a substantial and ever-growing corpus of evidence and literature
exploring the impacts of Artificial intelligence (AI) technologies on society,
politics, and humanity as a whole. A separate, parallel body of work has
explored existential risks to humanity, including but not limited to that
stemming from unaligned Artificial General Intelligence (AGI). In this paper,
we problematise the notion that current and near-term artificial intelligence
technologies have the potential to contribute to existential risk by acting as
intermediate risk factors, and that this potential is not limited to the
unaligned AGI scenario. We propose the hypothesis that certain
already-documented effects of AI can act as existential risk factors,
magnifying the likelihood of previously identified sources of existential risk.
Moreover, future developments in the coming decade hold the potential to
significantly exacerbate these risk factors, even in the absence of artificial
general intelligence. Our main contribution is a (non-exhaustive) exposition of
potential AI risk factors and the causal relationships between them, focusing
on how AI can affect power dynamics and information security. This exposition
demonstrates that there exist causal pathways from AI systems to existential
risks that do not presuppose hypothetical future AI capabilities.
</summary>
    <author>
      <name>Benjamin S. Bucknall</name>
    </author>
    <author>
      <name>Shiri Dori-Hacohen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3514094.3534146</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3514094.3534146" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AIES '22: Proceedings of the 2022 AAAI/ACM Conference on AI,
  Ethics, and Society</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2209.10604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.10604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.11812v2</id>
    <updated>2023-02-09T11:20:11Z</updated>
    <published>2022-09-23T19:10:59Z</published>
    <title>On Explanations, Fairness, and Appropriate Reliance in Human-AI
  Decision-Making</title>
    <summary>  Proponents of explainable AI have often argued that it constitutes an
essential path towards algorithmic fairness. Prior works examining these claims
have primarily evaluated explanations based on their effects on humans'
perceptions, but there is scant research on the relationship between
explanations and distributive fairness of AI-assisted decisions. In this paper,
we conduct an empirical study to examine the relationship between feature-based
explanations and distributive fairness, mediated by human perceptions and
reliance on AI recommendations. Our findings show that explanations influence
fairness perceptions, which, in turn, relate to humans' tendency to adhere to
AI recommendations. However, our findings suggest that such explanations do not
enable humans to discern correct and wrong AI recommendations. Instead, we show
that they may affect reliance irrespective of the correctness of AI
recommendations. Depending on which features an explanation highlights, this
can foster or hinder distributive fairness: when explanations highlight
features that are task-irrelevant and evidently associated with the sensitive
attribute, this prompts overrides that counter stereotype-aligned AI
recommendations. Meanwhile, if explanations appear task-relevant, this induces
reliance behavior that reinforces stereotype-aligned errors. These results show
that feature-based explanations are not a reliable mechanism to improve
distributive fairness, as their ability to do so relies on a human-in-the-loop
operationalization of the flawed notion of "fairness through unawareness".
Finally, our study design provides a blueprint to evaluate the suitability of
other explanations as pathways towards improved distributive fairness of
AI-assisted decisions.
</summary>
    <author>
      <name>Jakob Schoeffer</name>
    </author>
    <author>
      <name>Maria De-Arteaga</name>
    </author>
    <author>
      <name>Niklas Kuehl</name>
    </author>
    <link href="http://arxiv.org/abs/2209.11812v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.11812v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.14292v3</id>
    <updated>2022-11-29T02:19:10Z</updated>
    <published>2022-09-28T17:55:46Z</published>
    <title>Proceedings of the AI-HRI Symposium at AAAI-FSS 2022</title>
    <summary>  The Artificial Intelligence (AI) for Human-Robot Interaction (HRI) Symposium
has been a successful venue of discussion and collaboration on AI theory and
methods aimed at HRI since 2014. This year, after a review of the achievements
of the AI-HRI community over the last decade in 2021, we are focusing on a
visionary theme: exploring the future of AI-HRI. Accordingly, we added a Blue
Sky Ideas track to foster a forward-thinking discussion on future research at
the intersection of AI and HRI. As always, we appreciate all contributions
related to any topic on AI/HRI and welcome new researchers who wish to take
part in this growing community.
  With the success of past symposia, AI-HRI impacts a variety of communities
and problems, and has pioneered the discussions in recent trends and interests.
This year's AI-HRI Fall Symposium aims to bring together researchers and
practitioners from around the globe, representing a number of university,
government, and industry laboratories. In doing so, we hope to accelerate
research in the field, support technology transition and user adoption, and
determine future directions for our group and our research.
</summary>
    <author>
      <name>Zhao Han</name>
    </author>
    <author>
      <name>Emmanuel Senft</name>
    </author>
    <author>
      <name>Muneeb I. Ahmad</name>
    </author>
    <author>
      <name>Shelly Bagchi</name>
    </author>
    <author>
      <name>Amir Yazdani</name>
    </author>
    <author>
      <name>Jason R. Wilson</name>
    </author>
    <author>
      <name>Boyoung Kim</name>
    </author>
    <author>
      <name>Ruchen Wen</name>
    </author>
    <author>
      <name>Justin W. Hart</name>
    </author>
    <author>
      <name>Daniel Hernández García</name>
    </author>
    <author>
      <name>Matteo Leonetti</name>
    </author>
    <author>
      <name>Ross Mead</name>
    </author>
    <author>
      <name>Reuth Mirsky</name>
    </author>
    <author>
      <name>Ahalya Prabhakar</name>
    </author>
    <author>
      <name>Megan L. Zimmerman</name>
    </author>
    <link href="http://arxiv.org/abs/2209.14292v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.14292v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.03527v1</id>
    <updated>2022-10-07T13:11:28Z</updated>
    <published>2022-10-07T13:11:28Z</published>
    <title>Do We Need Explainable AI in Companies? Investigation of Challenges,
  Expectations, and Chances from Employees' Perspective</title>
    <summary>  By using AI, companies want to improve their business success and innovation
chances. However, in doing so, they (companies and their employees) are faced
with new requirements. In particular, legal regulations call for transparency
and comprehensibility of AI systems. The field of XAI deals with these issues.
Currently, the results are mostly obtained in lab studies, while the transfer
to real-world applications is lacking. This includes considering employees'
needs and attributes, which may differ from end-users in the lab. Therefore,
this project report paper provides initial insights into employees' specific
needs and attitudes towards (X)AI. For this, the results of a project's online
survey are reported that investigate two employees' perspectives (i.e., company
level and employee level) on (X)AI to create a holistic view of challenges,
risks, and needs of employees. Our findings suggest that AI and XAI are
well-known terms perceived as important for employees. This is a first step for
XAI to be a potential driver to foster the successful usage of AI by providing
transparent and comprehensible insights into AI technologies. To benefit from
(X)AI technologies, supportive employees on the management level are valuable
catalysts. This work contributes to the ongoing demand for XAI research to
develop human-centered and domain-specific XAI designs.
</summary>
    <author>
      <name>Katharina Weitz</name>
    </author>
    <author>
      <name>Chi Tai Dang</name>
    </author>
    <author>
      <name>Elisabeth André</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project report</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.03527v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.03527v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.3; J.4; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.09010v1</id>
    <updated>2022-10-08T08:17:30Z</updated>
    <published>2022-10-08T08:17:30Z</published>
    <title>Good AI for Good: How AI Strategies of the Nordic Countries Address the
  Sustainable Development Goals</title>
    <summary>  Developed and used responsibly Artificial Intelligence (AI) is a force for
global sustainable development. Given this opportunity, we expect that the many
of the existing guidelines and recommendations for trustworthy or responsible
AI will provide explicit guidance on how AI can contribute to the achievement
of United Nations' Sustainable Development Goals (SDGs). This would in
particular be the case for the AI strategies of the Nordic countries, at least
given their high ranking and overall political focus when it comes to the
achievement of the SDGs. In this paper, we present an analysis of existing AI
recommendations from 10 different countries or organisations based on topic
modelling techniques to identify how much these strategy documents refer to the
SDGs. The analysis shows no significant difference on how much these documents
refer to SDGs. Moreover, the Nordic countries are not different from the others
albeit their long-term commitment to SDGs. More importantly, references to
\textit{gender equality} (SDG 5) and \textit{inequality} (SDG 10), as well as
references to environmental impact of AI development and use, and in particular
the consequences for life on earth, are notably missing from the guidelines.
</summary>
    <author>
      <name>Andreas Theodorou</name>
    </author>
    <author>
      <name>Juan Carlos Nieves</name>
    </author>
    <author>
      <name>Virginia Dignum</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI-AIofAI 2022 : 2nd Workshop on Adverse Impacts and Collateral
  Effects of AI Technologies</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.09010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.09010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.15552v1</id>
    <updated>2022-11-28T16:55:32Z</updated>
    <published>2022-11-28T16:55:32Z</published>
    <title>AI Enabled Maneuver Identification via the Maneuver Identification
  Challenge</title>
    <summary>  Artificial intelligence (AI) has enormous potential to improve Air Force
pilot training by providing actionable feedback to pilot trainees on the
quality of their maneuvers and enabling instructor-less flying familiarization
for early-stage trainees in low-cost simulators. Historically, AI challenges
consisting of data, problem descriptions, and example code have been critical
to fueling AI breakthroughs. The Department of the Air Force-Massachusetts
Institute of Technology AI Accelerator (DAF-MIT AI Accelerator) developed such
an AI challenge using real-world Air Force flight simulator data. The Maneuver
ID challenge assembled thousands of virtual reality simulator flight recordings
collected by actual Air Force student pilots at Pilot Training Next (PTN). This
dataset has been publicly released at Maneuver-ID.mit.edu and represents the
first of its kind public release of USAF flight training data. Using this
dataset, we have applied a variety of AI methods to separate "good" vs "bad"
simulator data and categorize and characterize maneuvers. These data,
algorithms, and software are being released as baselines of model performance
for others to build upon to enable the AI ecosystem for flight simulator
training.
</summary>
    <author>
      <name>Kaira Samuel</name>
    </author>
    <author>
      <name>Matthew LaRosa</name>
    </author>
    <author>
      <name>Kyle McAlpin</name>
    </author>
    <author>
      <name>Morgan Schaefer</name>
    </author>
    <author>
      <name>Brandon Swenson</name>
    </author>
    <author>
      <name>Devin Wasilefsky</name>
    </author>
    <author>
      <name>Yan Wu</name>
    </author>
    <author>
      <name>Dan Zhao</name>
    </author>
    <author>
      <name>Jeremy Kepner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures, 4 tables, accepted to and presented at I/ITSEC</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.15552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.15552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.00740v1</id>
    <updated>2022-11-30T18:45:55Z</updated>
    <published>2022-11-30T18:45:55Z</published>
    <title>Prioritizing Policies for Furthering Responsible Artificial Intelligence
  in the United States</title>
    <summary>  Several policy options exist, or have been proposed, to further responsible
artificial intelligence (AI) development and deployment. Institutions,
including U.S. government agencies, states, professional societies, and private
and public sector businesses, are well positioned to implement these policies.
However, given limited resources, not all policies can or should be equally
prioritized. We define and review nine suggested policies for furthering
responsible AI, rank each policy on potential use and impact, and recommend
prioritization relative to each institution type. We find that pre-deployment
audits and assessments and post-deployment accountability are likely to have
the highest impact but also the highest barriers to adoption. We recommend that
U.S. government agencies and companies highly prioritize development of
pre-deployment audits and assessments, while the U.S. national legislature
should highly prioritize post-deployment accountability. We suggest that U.S.
government agencies and professional societies should highly prioritize
policies that support responsible AI research and that states should highly
prioritize support of responsible AI education. We propose that companies can
highly prioritize involving community stakeholders in development efforts and
supporting diversity in AI development. We advise lower levels of
prioritization across institutions for AI ethics statements and databases of AI
technologies or incidents. We recognize that no one policy will lead to
responsible AI and instead advocate for strategic policy implementation across
institutions.
</summary>
    <author>
      <name>Emily Hadley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in conference proceedings for the 1st International
  Workshop on Responsible AI and Data Ethics (RAIDE 2022) in Conjunction with
  the 2022 IEEE International Conference on Big Data (IEEE BigData 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.00740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.00740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.02159v1</id>
    <updated>2022-12-05T10:59:05Z</updated>
    <published>2022-12-05T10:59:05Z</published>
    <title>WAIR-D: Wireless AI Research Dataset</title>
    <summary>  It is a common sense that datasets with high-quality data samples play an
important role in artificial intelligence (AI), machine learning (ML) and
related studies. However, although AI/ML has been introduced in wireless
researches long time ago, few datasets are commonly used in the research
community. Without a common dataset, AI-based methods proposed for wireless
systems are hard to compare with both the traditional baselines and even each
other. The existing wireless AI researches usually rely on datasets generated
based on statistical models or ray-tracing simulations with limited
environments. The statistical data hinder the trained AI models from further
fine-tuning for a specific scenario, and ray-tracing data with limited
environments lower down the generalization capability of the trained AI models.
In this paper, we present the Wireless AI Research Dataset (WAIR-D)1, which
consists of two scenarios. Scenario 1 contains 10,000 environments with
sparsely dropped user equipments (UEs), and Scenario 2 contains 100
environments with densely dropped UEs. The environments are randomly picked up
from more than 40 cities in the real world map. The large volume of the data
guarantees that the trained AI models enjoy good generalization capability,
while fine-tuning can be easily carried out on a specific chosen environment.
Moreover, both the wireless channels and the corresponding environmental
information are provided in WAIR-D, so that extra-information-aided
communication mechanism can be designed and evaluated. WAIR-D provides the
researchers benchmarks to compare their different designs or reproduce results
of others. In this paper, we show the detailed construction of this dataset and
examples of using it.
</summary>
    <author>
      <name>Yourui Huangfu</name>
    </author>
    <author>
      <name>Jian Wang</name>
    </author>
    <author>
      <name>Shengchen Dai</name>
    </author>
    <author>
      <name>Rong Li</name>
    </author>
    <author>
      <name>Jun Wang</name>
    </author>
    <author>
      <name>Chongwen Huang</name>
    </author>
    <author>
      <name>Zhaoyang Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.02159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.02159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.06823v2</id>
    <updated>2023-01-26T21:49:57Z</updated>
    <published>2022-12-13T18:59:31Z</published>
    <title>Explanations Can Reduce Overreliance on AI Systems During
  Decision-Making</title>
    <summary>  Prior work has identified a resilient phenomenon that threatens the
performance of human-AI decision-making teams: overreliance, when people agree
with an AI, even when it is incorrect. Surprisingly, overreliance does not
reduce when the AI produces explanations for its predictions, compared to only
providing predictions. Some have argued that overreliance results from
cognitive biases or uncalibrated trust, attributing overreliance to an
inevitability of human cognition. By contrast, our paper argues that people
strategically choose whether or not to engage with an AI explanation,
demonstrating empirically that there are scenarios where AI explanations reduce
overreliance. To achieve this, we formalize this strategic choice in a
cost-benefit framework, where the costs and benefits of engaging with the task
are weighed against the costs and benefits of relying on the AI. We manipulate
the costs and benefits in a maze task, where participants collaborate with a
simulated AI to find the exit of a maze. Through 5 studies (N = 731), we find
that costs such as task difficulty (Study 1), explanation difficulty (Study 2,
3), and benefits such as monetary compensation (Study 4) affect overreliance.
Finally, Study 5 adapts the Cognitive Effort Discounting paradigm to quantify
the utility of different explanations, providing further support for our
framework. Our results suggest that some of the null effects found in
literature could be due in part to the explanation not sufficiently reducing
the costs of verifying the AI's prediction.
</summary>
    <author>
      <name>Helena Vasconcelos</name>
    </author>
    <author>
      <name>Matthew Jörke</name>
    </author>
    <author>
      <name>Madeleine Grunde-McLaughlin</name>
    </author>
    <author>
      <name>Tobias Gerstenberg</name>
    </author>
    <author>
      <name>Michael Bernstein</name>
    </author>
    <author>
      <name>Ranjay Krishna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CSCW 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.06823v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.06823v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.06676v1</id>
    <updated>2023-01-17T03:17:07Z</updated>
    <published>2023-01-17T03:17:07Z</published>
    <title>Explainable, Interpretable &amp; Trustworthy AI for Intelligent Digital
  Twin: Case Study on Remaining Useful Life</title>
    <summary>  Machine learning (ML) and Artificial Intelligence (AI) are increasingly used
in energy and engineering systems, but these models must be fair, unbiased, and
explainable. It is critical to have confidence in AI's trustworthiness. ML
techniques have been useful in predicting important parameters and improving
model performance. However, for these AI techniques to be useful for making
decisions, they need to be audited, accounted for, and easy to understand.
Therefore, the use of Explainable AI (XAI) and interpretable machine learning
(IML) is crucial for the accurate prediction of prognostics, such as remaining
useful life (RUL) in a digital twin system to make it intelligent while
ensuring that the AI model is transparent in its decision-making processes and
that the predictions it generates can be understood and trusted by users. By
using AI that is explainable, interpretable, and trustworthy, intelligent
digital twin systems can make more accurate predictions of RUL, leading to
better maintenance and repair planning and, ultimately, improved system
performance. The objective of this paper is to understand the idea of XAI and
IML and justify the important role of ML/AI in the Digital Twin framework and
components, which requires XAI to understand the prediction better. This paper
explains the importance of XAI and IML in both local and global aspects to
ensure the use of trustworthy ML/AI applications for RUL prediction. This paper
used the RUL prediction for the XAI and IML studies and leveraged the
integrated python toolbox for interpretable machine learning (PiML).
</summary>
    <author>
      <name>Kazuma Kobayashi</name>
    </author>
    <author>
      <name>Bader Almutairi</name>
    </author>
    <author>
      <name>Md Nazmus Sakib</name>
    </author>
    <author>
      <name>Souvik Chakraborty</name>
    </author>
    <author>
      <name>Syed B. Alam</name>
    </author>
    <link href="http://arxiv.org/abs/2301.06676v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.06676v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.11616v2</id>
    <updated>2023-01-30T08:58:00Z</updated>
    <published>2023-01-27T09:34:01Z</published>
    <title>A Survey on AI Risk Assessment Frameworks</title>
    <summary>  The rapid development of artificial intelligence (AI) has led to increasing
concerns about the capability of AI systems to make decisions and behave
responsibly. Responsible AI (RAI) refers to the development and use of AI
systems that benefit humans, society, and the environment while minimising the
risk of negative consequences. To ensure responsible AI, the risks associated
with AI systems' development and use must be identified, assessed and
mitigated. Various AI risk assessment frameworks have been released recently by
governments, organisations, and companies. However, it can be challenging for
AI stakeholders to have a clear picture of the available frameworks and
determine the most suitable ones for a specific context. Additionally, there is
a need to identify areas that require further research or development of new
frameworks. To fill the gap, we present a survey of 16 existing RAI risk
assessment frameworks from the industry, governments, and non-government
organizations (NGOs). We identify key characteristics of each framework and
analyse them in terms of RAI principles, stakeholders, system lifecycle stages,
geographical locations, targeted domains, and assessment methods. Our study
provides a comprehensive analysis of the current state of the frameworks and
highlights areas of convergence and divergence among them. We also identify the
deficiencies in existing frameworks and outlines the essential characteristics
a concrete framework should possess. Our findings and insights can help
relevant stakeholders choose suitable RAI risk assessment frameworks and guide
the design of future frameworks towards concreteness.
</summary>
    <author>
      <name>Boming Xia</name>
    </author>
    <author>
      <name>Qinghua Lu</name>
    </author>
    <author>
      <name>Harsha Perera</name>
    </author>
    <author>
      <name>Liming Zhu</name>
    </author>
    <author>
      <name>Zhenchang Xing</name>
    </author>
    <author>
      <name>Yue Liu</name>
    </author>
    <author>
      <name>Jon Whittle</name>
    </author>
    <link href="http://arxiv.org/abs/2301.11616v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.11616v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.12149v1</id>
    <updated>2023-02-23T16:33:40Z</updated>
    <published>2023-02-23T16:33:40Z</published>
    <title>Beyond Bias and Compliance: Towards Individual Agency and Plurality of
  Ethics in AI</title>
    <summary>  AI ethics is an emerging field with multiple, competing narratives about how
to best solve the problem of building human values into machines. Two major
approaches are focused on bias and compliance, respectively. But neither of
these ideas fully encompasses ethics: using moral principles to decide how to
act in a particular situation. Our method posits that the way data is labeled
plays an essential role in the way AI behaves, and therefore in the ethics of
machines themselves. The argument combines a fundamental insight from ethics
(i.e. that ethics is about values) with our practical experience building and
scaling machine learning systems. We want to build AI that is actually ethical
by first addressing foundational concerns: how to build good systems, how to
define what is good in relation to system architecture, and who should provide
that definition.
  Building ethical AI creates a foundation of trust between a company and the
users of that platform. But this trust is unjustified unless users experience
the direct value of ethical AI. Until users have real control over how
algorithms behave, something is missing in current AI solutions. This causes
massive distrust in AI, and apathy towards AI ethics solutions. The scope of
this paper is to propose an alternative path that allows for the plurality of
values and the freedom of individual expression. Both are essential for
realizing true moral character.
</summary>
    <author>
      <name>Thomas Krendl Gilbert</name>
    </author>
    <author>
      <name>Megan Welle Brozek</name>
    </author>
    <author>
      <name>Andrew Brozek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages total, 1 table, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.12149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.12149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.2649v1</id>
    <updated>2009-05-16T02:34:32Z</updated>
    <published>2009-05-16T02:34:32Z</published>
    <title>An Immune System Inspired Approach to Automated Program Verification</title>
    <summary>  An immune system inspired Artificial Immune System (AIS) algorithm is
presented, and is used for the purposes of automated program verification.
Relevant immunological concepts are discussed and the field of AIS is briefly
reviewed. It is proposed to use this AIS algorithm for a specific automated
program verification task: that of predicting shape of program invariants. It
is shown that the algorithm correctly predicts program invariant shape for a
variety of benchmarked programs.
</summary>
    <author>
      <name>Soumya Banerjee</name>
    </author>
    <link href="http://arxiv.org/abs/0905.2649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.2649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.8571v1</id>
    <updated>2014-12-30T07:05:42Z</updated>
    <published>2014-12-30T07:05:42Z</published>
    <title>A matrix equation $X^n = aI$</title>
    <summary>  In this paper, we study a matrix equation $X^n = aI$. We factorize $X^n - aI$
based upon the factorization of $x^n - a$ and then give a necessary and
sufficient condition for one of the factors to be the zero matrix.
</summary>
    <author>
      <name>Taehyeok Heo</name>
    </author>
    <author>
      <name>Jihoon Choi</name>
    </author>
    <author>
      <name>Suh-Ryung Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.8571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.8571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.RA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.RA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="15A24" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.12353v1</id>
    <updated>2020-03-25T02:17:08Z</updated>
    <published>2020-03-25T02:17:08Z</published>
    <title>Planning with Brain-inspired AI</title>
    <summary>  This article surveys engineering and neuroscientific models of planning as a
cognitive function, which is regarded as a typical function of fluid
intelligence in the discussion of general intelligence. It aims to present
existing planning models as references for realizing the planning function in
brain-inspired AI or artificial general intelligence (AGI). It also proposes
themes for the research and development of brain-inspired AI from the viewpoint
of tasks and architecture.
</summary>
    <author>
      <name>Naoya Arakawa</name>
    </author>
    <link href="http://arxiv.org/abs/2003.12353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.12353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.10363v1</id>
    <updated>2020-11-20T12:05:50Z</updated>
    <published>2020-11-20T12:05:50Z</published>
    <title>SophiaPop: Experiments in Human-AI Collaboration on Popular Music</title>
    <summary>  A diverse team of engineers, artists, and algorithms, collaborated to create
songs for SophiaPop, via various neural networks, robotics technologies, and
artistic tools, and animated the results on Sophia the Robot, a robotic
celebrity and animated character. Sophia is a platform for arts, research, and
other uses. To advance the art and technology of Sophia, we combine various AI
with a fictional narrative of her burgeoning career as a popstar. Her actual
AI-generated pop lyrics, music, and paintings, and animated conversations
wherein she interacts with humans real-time in narratives that discuss her
experiences. To compose the music, SophiaPop team built corpora from human and
AI-generated Sophia character personality content, along with pop music song
forms, to train and provide seeds for a number of AI algorithms including
expert models, and custom-trained transformer neural networks, which then
generated original pop-song lyrics and melodies. Our musicians including
Frankie Storm, Adam Pickrell, and Tiger Darrow, then performed interpretations
of the AI-generated musical content, including singing and instrumentation. The
human-performed singing data then was processed by a neural-network-based
Sophia voice, which was custom-trained from human performances by Cereproc.
This AI then generated the unique Sophia voice singing of the songs. Then we
animated Sophia to sing the songs in music videos, using a variety of animation
generators and human-generated animations. Being algorithms and humans, working
together, SophiaPop represents a human-AI collaboration, aspiring toward human
AI symbiosis. We believe that such a creative convergence of multiple
disciplines with humans and AI working together, can make AI relevant to human
culture in new and exciting ways, and lead to a hopeful vision for the future
of human-AI relations.
</summary>
    <author>
      <name>David Hanson</name>
    </author>
    <author>
      <name>Frankie Storm</name>
    </author>
    <author>
      <name>Wenwei Huang</name>
    </author>
    <author>
      <name>Vytas Krisciunas</name>
    </author>
    <author>
      <name>Tiger Darrow</name>
    </author>
    <author>
      <name>Audrey Brown</name>
    </author>
    <author>
      <name>Mengna Lei</name>
    </author>
    <author>
      <name>Matthew Aylett</name>
    </author>
    <author>
      <name>Adam Pickrell</name>
    </author>
    <author>
      <name>Sophia the Robot</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.10363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.10363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.10840v1</id>
    <updated>2018-11-27T06:58:59Z</updated>
    <published>2018-11-27T06:58:59Z</published>
    <title>Robust Artificial Intelligence and Robust Human Organizations</title>
    <summary>  Every AI system is deployed by a human organization. In high risk
applications, the combined human plus AI system must function as a
high-reliability organization in order to avoid catastrophic errors. This short
note reviews the properties of high-reliability organizations and draws
implications for the development of AI technology and the safe application of
that technology.
</summary>
    <author>
      <name>Thomas G. Dietterich</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear as a Perspective in Frontiers in Computer Science</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.10840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.10840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.02637v1</id>
    <updated>2019-10-21T12:30:17Z</updated>
    <published>2019-10-21T12:30:17Z</published>
    <title>Homo Cyberneticus: The Era of Human-AI Integration</title>
    <summary>  This article is submitted and accepted as ACM UIST 2019 Visions. UIST Visions
is a venue for forward thinking ideas to inspire the community. The goal is not
to report research but to project and propose new research directions. This
article, entitled "Homo Cyberneticus: The Era of Human-AI Integration",
proposes HCI research directions, namely human-augmentation and
human-AI-integration.
</summary>
    <author>
      <name>Jun Rekimoto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1235</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1235" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM UIST 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.02637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.02637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00078v1</id>
    <updated>2019-12-11T19:21:54Z</updated>
    <published>2019-12-11T19:21:54Z</published>
    <title>Regulatory Markets for AI Safety</title>
    <summary>  We propose a new model for regulation to achieve AI safety: global regulatory
markets. We first sketch the model in general terms and provide an overview of
the costs and benefits of this approach. We then demonstrate how the model
might work in practice: responding to the risk of adversarial attacks on AI
models employed in commercial drones.
</summary>
    <author>
      <name>Jack Clark</name>
    </author>
    <author>
      <name>Gillian K. Hadfield</name>
    </author>
    <link href="http://arxiv.org/abs/2001.00078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.07558v1</id>
    <updated>2020-06-13T04:31:42Z</updated>
    <published>2020-06-13T04:31:42Z</published>
    <title>Ethical Considerations for AI Researchers</title>
    <summary>  Use of artificial intelligence is growing and expanding into applications
that impact people's lives. People trust their technology without really
understanding it or its limitations. There is the potential for harm and we are
already seeing examples of that in the world. AI researchers have an obligation
to consider the impact of intelligent applications they work on. While the
ethics of AI is not clear-cut, there are guidelines we can consider to minimize
the harm we might introduce.
</summary>
    <author>
      <name>Kyle Dent</name>
    </author>
    <link href="http://arxiv.org/abs/2006.07558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.07558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.16879v1</id>
    <updated>2020-06-18T19:14:00Z</updated>
    <published>2020-06-18T19:14:00Z</published>
    <title>Combating Anti-Blackness in the AI Community</title>
    <summary>  In response to a national and international awakening on the issues of
anti-Blackness and systemic discrimination, we have penned this piece to serve
as a resource for allies in the AI community who are wondering how they can
more effectively engage with dismantling racist systems. This work aims to help
elucidate areas where the AI community actively and passively contributes to
anti-Blackness and offers actionable items on ways to reduce harm.
</summary>
    <author>
      <name>Devin Guillory</name>
    </author>
    <link href="http://arxiv.org/abs/2006.16879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.16879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.02039v1</id>
    <updated>2020-12-28T18:51:05Z</updated>
    <published>2020-12-28T18:51:05Z</published>
    <title>A survey of the European Union's artificial intelligence ecosystem</title>
    <summary>  Compared to other global powers, the European Union (EU) is rarely considered
a leading player in the development of artificial intelligence (AI). Why is
this, and does this in fact accurately reflect the activities of the EU? What
would it take for the EU to take a more leading role in AI? This report surveys
core components of the current AI ecosystem of the EU, providing the crucial
background context for answering these questions.
</summary>
    <author>
      <name>Charlotte Stix</name>
    </author>
    <link href="http://arxiv.org/abs/2101.02039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.02039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.08165v1</id>
    <updated>2021-04-16T15:24:50Z</updated>
    <published>2021-04-16T15:24:50Z</published>
    <title>The Cuntz semigroup of unital commutative AI-algebras</title>
    <summary>  We provide an abstract characterization for the Cuntz semigroup of unital
commutative AI-algebras, as well as a characterization for abstract Cuntz
semigroups of the form $\text{Lsc} (X,\overline{\mathbb{N}})$ for some
$T_1$-space $X$. In our investigations, we also uncover new properties that the
Cuntz semigroup of all AI-algebras satisfies.
</summary>
    <author>
      <name>Eduard Vilalta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.08165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="46L05, 46L85" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.09059v1</id>
    <updated>2021-05-19T10:59:17Z</updated>
    <published>2021-05-19T10:59:17Z</published>
    <title>The State of AI Ethics Report (January 2021)</title>
    <summary>  The 3rd edition of the Montreal AI Ethics Institute's The State of AI Ethics
captures the most relevant developments in AI Ethics since October 2020. It
aims to help anyone, from machine learning experts to human rights activists
and policymakers, quickly digest and understand the field's ever-changing
developments. Through research and article summaries, as well as expert
commentary, this report distills the research and reporting surrounding various
domains related to the ethics of AI, including: algorithmic injustice,
discrimination, ethical AI, labor impacts, misinformation, privacy, risk and
security, social media, and more.
  In addition, The State of AI Ethics includes exclusive content written by
world-class AI Ethics experts from universities, research institutes,
consulting firms, and governments. Unique to this report is "The Abuse and
Misogynoir Playbook," written by Dr. Katlyn Tuner (Research Scientist, Space
Enabled Research Group, MIT), Dr. Danielle Wood (Assistant Professor, Program
in Media Arts and Sciences; Assistant Professor, Aeronautics and Astronautics;
Lead, Space Enabled Research Group, MIT) and Dr. Catherine D'Ignazio (Assistant
Professor, Urban Science and Planning; Director, Data + Feminism Lab, MIT). The
piece (and accompanying infographic), is a deep-dive into the historical and
systematic silencing, erasure, and revision of Black women's contributions to
knowledge and scholarship in the United Stations, and globally. Exposing and
countering this Playbook has become increasingly important following the firing
of AI Ethics expert Dr. Timnit Gebru (and several of her supporters) at Google.
  This report should be used not only as a point of reference and insight on
the latest thinking in the field of AI Ethics, but should also be used as a
tool for introspection as we aim to foster a more nuanced conversation
regarding the impacts of AI on the world.
</summary>
    <author>
      <name>Abhishek Gupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Alexandrine Royer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Connor Wright</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Falaah Arif Khan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Victoria Heath</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Erick Galinkin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Khurana</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Marianna Bergamaschi Ganapini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Muriam Fancy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Masa Sweidan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Mo Akif</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <author>
      <name>Renjie Butalid</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">188 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.09059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.09059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.13966v1</id>
    <updated>2021-07-23T03:51:10Z</updated>
    <published>2021-07-23T03:51:10Z</published>
    <title>Artificial Intelligence in Achieving Sustainable Development Goals</title>
    <summary>  This perspective illustrates some of the AI applications that can accelerate
the achievement of SDGs and also highlights some of the considerations that
could hinder the efforts towards them. This emphasizes the importance of
establishing standard AI guidelines and regulations for the beneficial
applications of AI.
</summary>
    <author>
      <name>Hoe-Han Goh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure, under evaluation as a Perspective in Science</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.13966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.13966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.10744v1</id>
    <updated>2021-08-24T13:56:15Z</updated>
    <published>2021-08-24T13:56:15Z</published>
    <title>Interpretable deep-learning models to help achieve the Sustainable
  Development Goals</title>
    <summary>  We discuss our insights into interpretable artificial-intelligence (AI)
models, and how they are essential in the context of developing ethical AI
systems, as well as data-driven solutions compliant with the Sustainable
Development Goals (SDGs). We highlight the potential of extracting
truly-interpretable models from deep-learning methods, for instance via
symbolic models obtained through inductive biases, to ensure a sustainable
development of AI.
</summary>
    <author>
      <name>Ricardo Vinuesa</name>
    </author>
    <author>
      <name>Beril Sirmacek</name>
    </author>
    <link href="http://arxiv.org/abs/2108.10744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.10744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.01726v1</id>
    <updated>2021-11-02T16:46:46Z</updated>
    <published>2021-11-02T16:46:46Z</published>
    <title>Instructive artificial intelligence (AI) for human training, assistance,
  and explainability</title>
    <summary>  We propose a novel approach to explainable AI (XAI) based on the concept of
"instruction" from neural networks. In this case study, we demonstrate how a
superhuman neural network might instruct human trainees as an alternative to
traditional approaches to XAI. Specifically, an AI examines human actions and
calculates variations on the human strategy that lead to better performance.
Experiments with a JHU/APL-developed AI player for the cooperative card game
Hanabi suggest this technique makes unique contributions to explainability
while improving human performance. One area of focus for Instructive AI is in
the significant discrepancies that can arise between a human's actual strategy
and the strategy they profess to use. This inaccurate self-assessment presents
a barrier for XAI, since explanations of an AI's strategy may not be properly
understood or implemented by human recipients. We have developed and are
testing a novel, Instructive AI approach that estimates human strategy by
observing human actions. With neural networks, this allows a direct calculation
of the changes in weights needed to improve the human strategy to better
emulate a more successful AI. Subjected to constraints (e.g. sparsity) these
weight changes can be interpreted as recommended changes to human strategy
(e.g. "value A more, and value B less"). Instruction from AI such as this
functions both to help humans perform better at tasks, but also to better
understand, anticipate, and correct the actions of an AI. Results will be
presented on AI instruction's ability to improve human decision-making and
human-AI teaming in Hanabi.
</summary>
    <author>
      <name>Nicholas Kantack</name>
    </author>
    <author>
      <name>Nina Cohen</name>
    </author>
    <author>
      <name>Nathan Bos</name>
    </author>
    <author>
      <name>Corey Lowman</name>
    </author>
    <author>
      <name>James Everett</name>
    </author>
    <author>
      <name>Timothy Endres</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures, to be published in SPIE Defense &amp; Commercial
  Sensing (Artificial Intelligence and Machine Learning for Multi-Domain
  Operations Applications IV) proceedings (April 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.01726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.01726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.11471v1</id>
    <updated>2021-12-21T19:00:02Z</updated>
    <published>2021-12-21T19:00:02Z</published>
    <title>Towards a Science of Human-AI Decision Making: A Survey of Empirical
  Studies</title>
    <summary>  As AI systems demonstrate increasingly strong predictive performance, their
adoption has grown in numerous domains. However, in high-stakes domains such as
criminal justice and healthcare, full automation is often not desirable due to
safety, ethical, and legal concerns, yet fully manual approaches can be
inaccurate and time consuming. As a result, there is growing interest in the
research community to augment human decision making with AI assistance. Besides
developing AI technologies for this purpose, the emerging field of human-AI
decision making must embrace empirical approaches to form a foundational
understanding of how humans interact and work with AI to make decisions. To
invite and help structure research efforts towards a science of understanding
and improving human-AI decision making, we survey recent literature of
empirical human-subject studies on this topic. We summarize the study design
choices made in over 100 papers in three important aspects: (1) decision tasks,
(2) AI models and AI assistance elements, and (3) evaluation metrics. For each
aspect, we summarize current trends, discuss gaps in current practices of the
field, and make a list of recommendations for future research. Our survey
highlights the need to develop common frameworks to account for the design and
research spaces of human-AI decision making, so that researchers can make
rigorous choices in study design, and the research community can build on each
other's work and produce generalizable scientific knowledge. We also hope this
survey will serve as a bridge for HCI and AI communities to work together to
mutually shape the empirical science and computational technologies for
human-AI decision making.
</summary>
    <author>
      <name>Vivian Lai</name>
    </author>
    <author>
      <name>Chacha Chen</name>
    </author>
    <author>
      <name>Q. Vera Liao</name>
    </author>
    <author>
      <name>Alison Smith-Renner</name>
    </author>
    <author>
      <name>Chenhao Tan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, 2 figures, see https://haidecisionmaking.github.io for
  website</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.11471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.11471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.01081v1</id>
    <updated>2022-04-03T14:28:16Z</updated>
    <published>2022-04-03T14:28:16Z</published>
    <title>Faces: AI Blitz XIII Solutions</title>
    <summary>  AI Blitz XIII Faces challenge hosted on www.aicrowd.com platform consisted of
five problems: Sentiment Classification, Age Prediction, Mask Prediction, Face
Recognition, and Face De-Blurring. Our team GLaDOS took second place. Here we
present our solutions and results. Code implementation:
https://github.com/ndrwmlnk/ai-blitz-xiii
</summary>
    <author>
      <name>Andrew Melnik</name>
    </author>
    <author>
      <name>Eren Akbulut</name>
    </author>
    <author>
      <name>Jannik Sheikh</name>
    </author>
    <author>
      <name>Kira Loos</name>
    </author>
    <author>
      <name>Michael Buettner</name>
    </author>
    <author>
      <name>Tobias Lenze</name>
    </author>
    <link href="http://arxiv.org/abs/2204.01081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.01081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.06327v1</id>
    <updated>2022-08-12T15:33:42Z</updated>
    <published>2022-08-12T15:33:42Z</published>
    <title>Developing moral AI to support antimicrobial decision making</title>
    <summary>  Artificial intelligence (AI) assisting with antimicrobial prescribing raises
significant moral questions. Utilising ethical frameworks alongside AI-driven
systems, while considering infection specific complexities, can support moral
decision making to tackle antimicrobial resistance.
</summary>
    <author>
      <name>William J Bolton</name>
    </author>
    <author>
      <name>Cosmin Badea</name>
    </author>
    <author>
      <name>Pantelis Georgiou</name>
    </author>
    <author>
      <name>Alison Holmes</name>
    </author>
    <author>
      <name>Timothy M Rawson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s42256-022-00558-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s42256-022-00558-5" rel="related"/>
    <link href="http://arxiv.org/abs/2208.06327v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.06327v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.11234v1</id>
    <updated>2022-09-15T04:21:07Z</updated>
    <published>2022-09-15T04:21:07Z</published>
    <title>Artificial Intelligence in Material Engineering: A review on
  applications of AI in Material Engineering</title>
    <summary>  Recently, there has been extensive use of artificial Intelligence (AI) in the
field of material engineering. This can be attributed to the development of
high performance computing and thereby feasibility to test deep learning models
with large parameters. In this article we tried to review some of the latest
developments in the applications of AI in material engineering.
</summary>
    <author>
      <name>Lipichanda Goswami</name>
    </author>
    <author>
      <name>Manoj Deka</name>
    </author>
    <author>
      <name>Mohendra Roy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">V1</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.11234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.11234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.17218v1</id>
    <updated>2022-10-31T10:57:20Z</updated>
    <published>2022-10-31T10:57:20Z</published>
    <title>Artificial intelligence in government: Concepts, standards, and a
  unified framework</title>
    <summary>  Recent advances in artificial intelligence (AI) and machine learning (ML)
hold the promise of improving government. Given the advanced capabilities of AI
applications, it is critical that these are embedded using standard operational
procedures, clear epistemic criteria, and behave in alignment with the
normative expectations of society. Scholars in multiple domains have
subsequently begun to conceptualize the different forms that AI systems may
take, highlighting both their potential benefits and pitfalls. However, the
literature remains fragmented, with researchers in social science disciplines
like public administration and political science, and the fast-moving fields of
AI, ML, and robotics, all developing concepts in relative isolation. Although
there are calls to formalize the emerging study of AI in government, a balanced
account that captures the full breadth of theoretical perspectives needed to
understand the consequences of embedding AI into a public sector context is
lacking. Here, we unify efforts across social and technical disciplines by
using concept mapping to identify 107 different terms used in the
multidisciplinary study of AI. We inductively sort these into three distinct
semantic groups, which we label the (a) operational, (b) epistemic, and (c)
normative domains. We then build on the results of this mapping exercise by
proposing three new multifaceted concepts to study AI-based systems for
government (AI-GOV) in an integrated, forward-looking way, which we call (1)
operational fitness, (2) epistemic completeness, and (3) normative salience.
Finally, we put these concepts to work by using them as dimensions in a
conceptual typology of AI-GOV and connecting each with emerging AI technical
measurement standards to encourage operationalization, foster
cross-disciplinary dialogue, and stimulate debate among those aiming to reshape
public administration with AI.
</summary>
    <author>
      <name>Vincent J. Straub</name>
    </author>
    <author>
      <name>Deborah Morgan</name>
    </author>
    <author>
      <name>Jonathan Bright</name>
    </author>
    <author>
      <name>Helen Margetts</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages with references and appendix, 3 tables, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.17218v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.17218v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.03656v1</id>
    <updated>2023-01-09T19:56:47Z</updated>
    <published>2023-01-09T19:56:47Z</published>
    <title>Towards Multifaceted Human-Centered AI</title>
    <summary>  Human-centered AI workflows involve stakeholders with multiple roles
interacting with each other and automated agents to accomplish diverse tasks.
In this paper, we call for a holistic view when designing support mechanisms,
such as interaction paradigms, interfaces, and systems, for these multifaceted
workflows.
</summary>
    <author>
      <name>Sajjadur Rahman</name>
    </author>
    <author>
      <name>Hannah Kim</name>
    </author>
    <author>
      <name>Dan Zhang</name>
    </author>
    <author>
      <name>Estevam Hruschka</name>
    </author>
    <author>
      <name>Eser Kandogan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop on Human-Centered AI at NeurIPS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.03656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.03656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/math/0311317v1</id>
    <updated>2003-11-18T19:55:24Z</updated>
    <published>2003-11-18T19:55:24Z</published>
    <title>Chains of Unusual Excellent Local Rings</title>
    <summary>  Let (T,M) be a complete local domain containing the integers. Let p1
\subseteq p2 \subseteq ... \subseteq pn be a chain of nonmaximal prime ideals T
such that T_pn is a regular local ring. We construct a chain of excellent local
domains An \subseteq A1 such that for each i, the completion of Ai is T, the
generic formal fiber of Ai is local with maximal ideal pi, and if I is a
nonzero ideal of Ai then Ai/I is complete. Consequently, if in addition T is a
UFD, then we can construct a chain of excellent local UFDs satisfying the same
conditions.
</summary>
    <author>
      <name>Kai Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/math/0311317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/math/0311317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.AC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="13J05, 13J10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.5601v2</id>
    <updated>2010-07-31T19:35:00Z</updated>
    <published>2009-06-30T18:28:48Z</published>
    <title>The curious moduli spaces of unmarked Kleinian surface groups</title>
    <summary>  Fixing a closed hyperbolic surface S, we define a moduli space AI(S) of
unmarked hyperbolic 3-manifolds homotopy equivalent to S. This 3-dimensional
analogue of the moduli space M(S) of unmarked hyperbolic surfaces homeomorphic
to S has bizarre local topology, possessing many points that are not closed.
There is, however, a natural embedding of M(S) into AI(S) and a
compactification of AI(S) such that this embedding extends to an embedding of
the Deligne-Mumford compactification of M(S) into the compactification of
AI(S).
</summary>
    <author>
      <name>Richard Canary</name>
    </author>
    <author>
      <name>Peter Storm</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.5601v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.5601v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="57M50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.0255v2</id>
    <updated>2010-10-12T03:16:43Z</updated>
    <published>2010-01-01T20:16:29Z</published>
    <title>Potentials of homotopy cyclic $\AI$-algebras</title>
    <summary>  For a cyclic $\AI$-algebra, a potential recording the structure constants can
be defined. We define an analogous potential for a homotopy cyclic
$\AI$-algebra and prove its properties. On the other hand, we find another
different potential for a homotopy cyclic $\AI$-algebra, which is related to
the algebraic analogue of generalized holonomy map of Abbaspour, Tradler and
Zeinalian.
</summary>
    <author>
      <name>Cheol-Hyun Cho</name>
    </author>
    <author>
      <name>Sangwook Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Homology, Homotopy, Appl. 14 (2012), no.1, 203-220</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.0255v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0255v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.QA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.QA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="46CXX, 53DXX" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.2025v1</id>
    <updated>2011-10-10T12:39:22Z</updated>
    <published>2011-10-10T12:39:22Z</published>
    <title>Polynomials Associated with the Higher Derivatives of the Airy Functions
  Ai(z) and Ai'(z)</title>
    <summary>  The Airy function Ai(z) and its derivative Ai'(z) occur in a large number of
applications in Chemistry and Physics. As a result, there is a continuing
interest in the properties of these functions. Recently, there has been
interest in obtaining general expressions for the higher derivatives of these
functions. In this work, general expressions for the polynomials which are
contained in these derivatives are given in terms of the partial Bell
polynomials.
</summary>
    <author>
      <name>Bernard J. Laurenzi</name>
    </author>
    <link href="http://arxiv.org/abs/1110.2025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.2025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5713v2</id>
    <updated>2015-03-31T10:26:48Z</updated>
    <published>2013-12-19T19:28:18Z</published>
    <title>Giving the AI definition a form suitable for the engineer</title>
    <summary>  Artificial Intelligence - what is this? That is the question! In earlier
papers we already gave a formal definition for AI, but if one desires to build
an actual AI implementation, the following issues require attention and are
treated here: the data format to be used, the idea of Undef and Nothing
symbols, various ways for defining the "meaning of life", and finally, a new
notion of "incorrect move". These questions are of minor importance in the
theoretical discussion, but we already know the answer of the question "Does AI
exist?" Now we want to make the next step and to create this program.
</summary>
    <author>
      <name>Dimiter Dobrev</name>
    </author>
    <link href="http://arxiv.org/abs/1312.5713v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5713v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.05373v1</id>
    <updated>2015-05-20T13:34:34Z</updated>
    <published>2015-05-20T13:34:34Z</published>
    <title>Towards a Simulation-Based Programming Paradigm for AI applications</title>
    <summary>  We present initial ideas for a programming paradigm based on simulation that
is targeted towards applications of artificial intelligence (AI). The approach
aims at integrating techniques from different areas of AI and is based on the
idea that simulated entities may freely exchange data and behavioural patterns.
We define basic notions of a simulation-based programming paradigm and show how
it can be used for implementing AI applications.
</summary>
    <author>
      <name>Jörg Pührer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Workshop on Reactive Concepts in Knowledge
  Representation (ReactKnow 2014), co-located with the 21st European Conference
  on Artificial Intelligence (ECAI 2014). Proceedings of the International
  Workshop on Reactive Concepts in Knowledge Representation (ReactKnow 2014),
  pages 55-61, technical report, ISSN 1430-3701, Leipzig University, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.05373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.05373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.03506v1</id>
    <updated>2016-02-10T20:29:25Z</updated>
    <published>2016-02-10T20:29:25Z</published>
    <title>Research Priorities for Robust and Beneficial Artificial Intelligence</title>
    <summary>  Success in the quest for artificial intelligence has the potential to bring
unprecedented benefits to humanity, and it is therefore worthwhile to
investigate how to maximize these benefits while avoiding potential pitfalls.
This article gives numerous examples (which should by no means be construed as
an exhaustive list) of such worthwhile research aimed at ensuring that AI
remains robust and beneficial.
</summary>
    <author>
      <name>Stuart Russell</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Berkeley</arxiv:affiliation>
    </author>
    <author>
      <name>Daniel Dewey</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">FHI</arxiv:affiliation>
    </author>
    <author>
      <name>Max Tegmark</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MIT</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article gives examples of the type of research advocated by the
  open letter for robust &amp; beneficial AI at
  http://futureoflife.org/ai-open-letter</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AI Magazine 36:4 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.03506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.03506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.04019v1</id>
    <updated>2016-02-12T11:32:59Z</updated>
    <published>2016-02-12T11:32:59Z</published>
    <title>Energetics of the brain and AI</title>
    <summary>  Does the energy requirements for the human brain give energy constraints that
give reason to doubt the feasibility of artificial intelligence? This report
will review some relevant estimates of brain bioenergetics and analyze some of
the methods of estimating brain emulation energy requirements. Turning to AI,
there are reasons to believe the energy requirements for de novo AI to have
little correlation with brain (emulation) energy requirements since cost could
depend merely of the cost of processing higher-level representations rather
than billions of neural firings. Unless one thinks the human way of thinking is
the most optimal or most easily implementable way of achieving software
intelligence, we should expect de novo AI to make use of different, potentially
very compressed and fast, processes.
</summary>
    <author>
      <name>Anders Sandberg</name>
    </author>
    <link href="http://arxiv.org/abs/1602.04019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.04019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08514v4</id>
    <updated>2020-07-23T17:33:59Z</updated>
    <published>2016-06-27T23:51:04Z</published>
    <title>Towards Verified Artificial Intelligence</title>
    <summary>  Verified artificial intelligence (AI) is the goal of designing AI-based
systems that that have strong, ideally provable, assurances of correctness with
respect to mathematically-specified requirements. This paper considers Verified
AI from a formal methods perspective. We describe five challenges for achieving
Verified AI, and five corresponding principles for addressing these challenges.
</summary>
    <author>
      <name>Sanjit A. Seshia</name>
    </author>
    <author>
      <name>Dorsa Sadigh</name>
    </author>
    <author>
      <name>S. Shankar Sastry</name>
    </author>
    <link href="http://arxiv.org/abs/1606.08514v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08514v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.12808v1</id>
    <updated>2020-03-28T15:09:13Z</updated>
    <published>2020-03-28T15:09:13Z</published>
    <title>Towards Automating the AI Operations Lifecycle</title>
    <summary>  Today's AI deployments often require significant human involvement and skill
in the operational stages of the model lifecycle, including pre-release
testing, monitoring, problem diagnosis and model improvements. We present a set
of enabling technologies that can be used to increase the level of automation
in AI operations, thus lowering the human effort required. Since a common
source of human involvement is the need to assess the performance of deployed
models, we focus on technologies for performance prediction and KPI analysis
and show how they can be used to improve automation in the key stages of a
typical AI operations pipeline.
</summary>
    <author>
      <name>Matthew Arnold</name>
    </author>
    <author>
      <name>Jeffrey Boston</name>
    </author>
    <author>
      <name>Michael Desmond</name>
    </author>
    <author>
      <name>Evelyn Duesterwald</name>
    </author>
    <author>
      <name>Benjamin Elder</name>
    </author>
    <author>
      <name>Anupama Murthi</name>
    </author>
    <author>
      <name>Jiri Navratil</name>
    </author>
    <author>
      <name>Darrell Reimer</name>
    </author>
    <link href="http://arxiv.org/abs/2003.12808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.12808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.2003v1</id>
    <updated>2010-09-10T12:58:30Z</updated>
    <published>2010-09-10T12:58:30Z</published>
    <title>AI 3D Cybug Gaming</title>
    <summary>  In this short paper I briefly discuss 3D war Game based on artificial
intelligence concepts called AI WAR. Going in to the details, I present the
importance of CAICL language and how this language is used in AI WAR. Moreover
I also present a designed and implemented 3D War Cybug for AI WAR using CAICL
and discus the implemented strategy to defeat its enemies during the game life.
</summary>
    <author>
      <name>Zeeshan Ahmed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In the proceedings of 9th National Research Conference on Management
  and Computer Sciences, SZABIST Institute of Science and Technology, Pakistan</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.2003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.2003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.04534v1</id>
    <updated>2019-07-10T06:51:04Z</updated>
    <published>2019-07-10T06:51:04Z</published>
    <title>The Role of Cooperation in Responsible AI Development</title>
    <summary>  In this paper, we argue that competitive pressures could incentivize AI
companies to underinvest in ensuring their systems are safe, secure, and have a
positive social impact. Ensuring that AI systems are developed responsibly may
therefore require preventing and solving collective action problems between
companies. We note that there are several key factors that improve the
prospects for cooperation in collective action problems. We use this to
identify strategies to improve the prospects for industry cooperation on the
responsible development of AI.
</summary>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Miles Brundage</name>
    </author>
    <author>
      <name>Gillian Hadfield</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.04534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.1; K.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04315v3</id>
    <updated>2017-02-22T20:02:46Z</updated>
    <published>2016-04-14T22:43:30Z</published>
    <title>Moving Beyond the Turing Test with the Allen AI Science Challenge</title>
    <summary>  Given recent successes in AI (e.g., AlphaGo's victory against Lee Sedol in
the game of GO), it's become increasingly important to assess: how close are AI
systems to human-level intelligence? This paper describes the Allen AI Science
Challenge---an approach towards that goal which led to a unique Kaggle
Competition, its results, the lessons learned, and our next steps.
</summary>
    <author>
      <name>Carissa Schoenick</name>
    </author>
    <author>
      <name>Peter Clark</name>
    </author>
    <author>
      <name>Oyvind Tafjord</name>
    </author>
    <author>
      <name>Peter Turney</name>
    </author>
    <author>
      <name>Oren Etzioni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04315v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04315v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.02918v3</id>
    <updated>2019-03-23T06:17:08Z</updated>
    <published>2019-01-09T20:16:44Z</published>
    <title>Making AI meaningful again</title>
    <summary>  Artificial intelligence (AI) research enjoyed an initial period of enthusiasm
in the 1970s and 80s. But this enthusiasm was tempered by a long interlude of
frustration when genuinely useful AI applications failed to be forthcoming.
Today, we are experiencing once again a period of enthusiasm, fired above all
by the successes of the technology of deep neural networks or deep machine
learning. In this paper we draw attention to what we take to be serious
problems underlying current views of artificial intelligence encouraged by
these successes, especially in the domain of language processing. We then show
an alternative approach to language-centric AI, in which we identify a role for
philosophy.
</summary>
    <author>
      <name>Jobst Landgrebe</name>
    </author>
    <author>
      <name>Barry Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 1 Table</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.02918v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.02918v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.09413v1</id>
    <updated>2019-01-27T18:12:03Z</updated>
    <published>2019-01-27T18:12:03Z</published>
    <title>An Information-Theoretic Explanation for the Adversarial Fragility of AI
  Classifiers</title>
    <summary>  We present a simple hypothesis about a compression property of artificial
intelligence (AI) classifiers and present theoretical arguments to show that
this hypothesis successfully accounts for the observed fragility of AI
classifiers to small adversarial perturbations. We also propose a new method
for detecting when small input perturbations cause classifier errors, and show
theoretical guarantees for the performance of this detection method. We present
experimental results with a voice recognition system to demonstrate this
method. The ideas in this paper are motivated by a simple analogy between AI
classifiers and the standard Shannon model of a communication system.
</summary>
    <author>
      <name>Hui Xie</name>
    </author>
    <author>
      <name>Jirong Yi</name>
    </author>
    <author>
      <name>Weiyu Xu</name>
    </author>
    <author>
      <name>Raghu Mudumbai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.09413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.09413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.10345v1</id>
    <updated>2019-08-27T17:36:27Z</updated>
    <published>2019-08-27T17:36:27Z</published>
    <title>Artificial Intelligence Approaches</title>
    <summary>  Artificial Intelligence (AI) has received tremendous attention from academia,
industry, and the general public in recent years. The integration of geography
and AI, or GeoAI, provides novel approaches for addressing a variety of
problems in the natural environment and our human society. This entry briefly
reviews the recent development of AI with a focus on machine learning and deep
learning approaches. We discuss the integration of AI with geography and
particularly geographic information science, and present a number of GeoAI
applications and possible future directions.
</summary>
    <author>
      <name>Yingjie Hu</name>
    </author>
    <author>
      <name>Wenwen Li</name>
    </author>
    <author>
      <name>Dawn Wright</name>
    </author>
    <author>
      <name>Orhun Aydin</name>
    </author>
    <author>
      <name>Daniel Wilson</name>
    </author>
    <author>
      <name>Omar Maher</name>
    </author>
    <author>
      <name>Mansour Raad</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.22224/gistbok/2019.3.4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.22224/gistbok/2019.3.4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Intelligence Approaches. The Geographic Information
  Science &amp; Technology Body of Knowledge (3rd Quarter 2019 Edition), John P.
  Wilson (ed.)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1908.10345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.01172v2</id>
    <updated>2020-01-07T21:15:45Z</updated>
    <published>2020-01-05T05:47:48Z</published>
    <title>The Human Visual System and Adversarial AI</title>
    <summary>  This paper applies theories about the Human Visual System to make Adversarial
AI more effective. To date, Adversarial AI has modeled perceptual distances
between clean and adversarial examples of images using Lp norms. These norms
have the benefit of simple mathematical description and reasonable
effectiveness in approximating perceptual distance. However, in prior decades,
other areas of image processing have moved beyond simpler models like Mean
Squared Error (MSE) towards more complex models that better approximate the
Human Visual System (HVS). We demonstrate a proof of concept of incorporating
HVS models into Adversarial AI.
</summary>
    <author>
      <name>Yaoshiang Ho</name>
    </author>
    <author>
      <name>Samuel Wookey</name>
    </author>
    <link href="http://arxiv.org/abs/2001.01172v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01172v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.01014v1</id>
    <updated>2020-02-03T21:03:20Z</updated>
    <published>2020-02-03T21:03:20Z</published>
    <title>Four Principles of Explainable AI as Applied to Biometrics and Facial
  Forensic Algorithms</title>
    <summary>  Traditionally, researchers in automatic face recognition and biometric
technologies have focused on developing accurate algorithms. With this
technology being integrated into operational systems, engineers and scientists
are being asked, do these systems meet societal norms? The origin of this line
of inquiry is `trust' of artificial intelligence (AI) systems. In this paper,
we concentrate on adapting explainable AI to face recognition and biometrics,
and we present four principles of explainable AI to face recognition and
biometrics. The principles are illustrated by $\it{four}$ case studies, which
show the challenges and issues in developing algorithms that can produce
explanations.
</summary>
    <author>
      <name>P. Jonathon Phillips</name>
    </author>
    <author>
      <name>Mark Przybocki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.01014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.01014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.10437v1</id>
    <updated>2020-06-18T11:33:02Z</updated>
    <published>2020-06-18T11:33:02Z</published>
    <title>"EHLO WORLD" -- Checking If Your Conversational AI Knows Right from
  Wrong</title>
    <summary>  In this paper we discuss approaches to evaluating and validating the ethical
claims of a Conversational AI system. We outline considerations around both a
top-down regulatory approach and bottom-up processes. We describe the ethical
basis for each approach and propose a hybrid which we demonstrate by taking the
case of a customer service chatbot as an example. We speculate on the kinds of
top-down and bottom-up processes that would need to exist for a hybrid
framework to successfully function as both an enabler as well as a shepherd
among multiple use-cases and multiple competing AI solutions.
</summary>
    <author>
      <name>Elayne Ruane</name>
    </author>
    <author>
      <name>Vivek Nallur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, SoCAI 2020 : AISB Symposium on Conversational AI</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.10437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.10437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2, J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.05260v1</id>
    <updated>2020-09-11T07:31:23Z</updated>
    <published>2020-09-11T07:31:23Z</published>
    <title>The AIQ Meta-Testbed: Pragmatically Bridging Academic AI Testing and
  Industrial Q Needs</title>
    <summary>  AI solutions seem to appear in any and all application domains. As AI becomes
more pervasive, the importance of quality assurance increases. Unfortunately,
there is no consensus on what artificial intelligence means and interpretations
range from simple statistical analysis to sentient humanoid robots. On top of
that, quality is a notoriously hard concept to pinpoint. What does this mean
for AI quality? In this paper, we share our working definition and a pragmatic
approach to address the corresponding quality assurance with a focus on
testing. Finally, we present our ongoing work on establishing the AIQ
Meta-Testbed.
</summary>
    <author>
      <name>Markus Borg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in the Proc. of the Software Quality Days
  2021, Vienna, Austria</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.05260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.05260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.08922v2</id>
    <updated>2020-09-25T08:40:57Z</updated>
    <published>2020-09-18T16:39:54Z</published>
    <title>AI and Wargaming</title>
    <summary>  Recent progress in Game AI has demonstrated that given enough data from human
gameplay, or experience gained via simulations, machines can rival or surpass
the most skilled human players in classic games such as Go, or commercial
computer games such as Starcraft. We review the current state-of-the-art
through the lens of wargaming, and ask firstly what features of wargames
distinguish them from the usual AI testbeds, and secondly which recent AI
advances are best suited to address these wargame-specific features.
</summary>
    <author>
      <name>James Goodman</name>
    </author>
    <author>
      <name>Sebastian Risi</name>
    </author>
    <author>
      <name>Simon Lucas</name>
    </author>
    <link href="http://arxiv.org/abs/2009.08922v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.08922v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.00048v1</id>
    <updated>2020-09-30T18:28:01Z</updated>
    <published>2020-09-30T18:28:01Z</published>
    <title>Creative Captioning: An AI Grand Challenge Based on the Dixit Board Game</title>
    <summary>  We propose a new class of "grand challenge" AI problems that we call creative
captioning---generating clever, interesting, or abstract captions for images,
as well as understanding such captions. Creative captioning draws on core AI
research areas of vision, natural language processing, narrative reasoning, and
social reasoning, and across all these areas, it requires sophisticated uses of
common sense and cultural knowledge. In this paper, we analyze several specific
research problems that fall under creative captioning, using the popular board
game Dixit as both inspiration and proposed testing ground. We expect that
Dixit could serve as an engaging and motivating benchmark for creative
captioning across numerous AI research communities for the coming 1-2 decades.
</summary>
    <author>
      <name>Maithilee Kunda</name>
    </author>
    <author>
      <name>Irina Rabkina</name>
    </author>
    <link href="http://arxiv.org/abs/2010.00048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.00048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01316v1</id>
    <updated>2020-10-03T10:05:58Z</updated>
    <published>2020-10-03T10:05:58Z</published>
    <title>Accounts, Accountability and Agency for Safe and Ethical AI</title>
    <summary>  We examine the problem of explainable AI (xAI) and explore what delivering
xAI means in practice, particularly in contexts that involve formal or informal
and ad-hoc collaboration where agency and accountability in decision-making are
achieved and sustained interactionally. We use an example from an earlier study
of collaborative decision-making in screening mammography and the difficulties
users faced when trying to interpret the behavior of an AI tool to illustrate
the challenges of delivering usable and effective xAI. We conclude by setting
out a study programme for future research to help advance our understanding of
xAI requirements for safe and ethical AI.
</summary>
    <author>
      <name>Rob Procter</name>
    </author>
    <author>
      <name>Mark Rouncefield</name>
    </author>
    <author>
      <name>Peter Tolmie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.02911v1</id>
    <updated>2020-10-06T17:47:53Z</updated>
    <published>2020-10-06T17:47:53Z</published>
    <title>Chess as a Testing Grounds for the Oracle Approach to AI Safety</title>
    <summary>  To reduce the danger of powerful super-intelligent AIs, we might make the
first such AIs oracles that can only send and receive messages. This paper
proposes a possibly practical means of using machine learning to create two
classes of narrow AI oracles that would provide chess advice: those aligned
with the player's interest, and those that want the player to lose and give
deceptively bad advice. The player would be uncertain which type of oracle it
was interacting with. As the oracles would be vastly more intelligent than the
player in the domain of chess, experience with these oracles might help us
prepare for future artificial general intelligence oracles.
</summary>
    <author>
      <name>James D. Miller</name>
    </author>
    <author>
      <name>Roman Yampolskiy</name>
    </author>
    <author>
      <name>Olle Haggstrom</name>
    </author>
    <author>
      <name>Stuart Armstrong</name>
    </author>
    <link href="http://arxiv.org/abs/2010.02911v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.02911v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.06686v1</id>
    <updated>2020-10-31T09:27:53Z</updated>
    <published>2020-10-31T09:27:53Z</published>
    <title>Computing Machinery and Knowledge</title>
    <summary>  The purpose of this paper is to discuss the possibilities for computing
machinery, or AI agents, to know and to possess knowledge. This is done mainly
from a virtue epistemology perspective and definition of knowledge. However,
this inquiry also shed light on the human condition, what it means for a human
to know, and to possess knowledge. The paper argues that it is possible for an
AI agent to know and examines this from both current state-of-the-art in
artificial intelligence as well as from the perspective of what the future AI
development might bring in terms of superintelligent AI agents.
</summary>
    <author>
      <name>Raymond Anneborg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.06686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.06686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.05957v1</id>
    <updated>2021-01-15T03:46:27Z</updated>
    <published>2021-01-15T03:46:27Z</published>
    <title>Descriptive AI Ethics: Collecting and Understanding the Public Opinion</title>
    <summary>  There is a growing need for data-driven research efforts on how the public
perceives the ethical, moral, and legal issues of autonomous AI systems. The
current debate on the responsibility gap posed by these systems is one such
example. This work proposes a mixed AI ethics model that allows normative and
descriptive research to complement each other, by aiding scholarly discussion
with data gathered from the public. We discuss its implications on bridging the
gap between optimistic and pessimistic views towards AI systems' deployment.
</summary>
    <author>
      <name>Gabriel Lima</name>
    </author>
    <author>
      <name>Meeyoung Cha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the Ethics in Design Workshop at ACM CSCW 2020
  (https://ethicsindesignworkshop.wordpress.com/). 5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.05957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.05957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.06166v1</id>
    <updated>2021-02-11T18:15:23Z</updated>
    <published>2021-02-11T18:15:23Z</published>
    <title>Testing Framework for Black-box AI Models</title>
    <summary>  With widespread adoption of AI models for important decision making, ensuring
reliability of such models remains an important challenge. In this paper, we
present an end-to-end generic framework for testing AI Models which performs
automated test generation for different modalities such as text, tabular, and
time-series data and across various properties such as accuracy, fairness, and
robustness. Our tool has been used for testing industrial AI models and was
very effective to uncover issues present in those models. Demo video link:
https://youtu.be/984UCU17YZI
</summary>
    <author>
      <name>Aniya Aggarwal</name>
    </author>
    <author>
      <name>Samiulla Shaikh</name>
    </author>
    <author>
      <name>Sandeep Hans</name>
    </author>
    <author>
      <name>Swastik Haldar</name>
    </author>
    <author>
      <name>Rema Ananthanarayanan</name>
    </author>
    <author>
      <name>Diptikalyan Saha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages Demonstrations track paper accepted at ICSE 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.06166v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.06166v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.10787v1</id>
    <updated>2021-02-22T05:49:02Z</updated>
    <published>2021-02-22T05:49:02Z</published>
    <title>Fair and Responsible AI: A Focus on the Ability to Contest</title>
    <summary>  As the use of artificial intelligence (AI) in high-stakes decision-making
increases, the ability to contest such decisions is being recognised in AI
ethics guidelines as an important safeguard for individuals. Yet, there is
little guidance on how AI systems can be designed to support contestation. In
this paper we explain that the design of a contestation process is important
due to its impact on perceptions of fairness and satisfaction. We also consider
design challenges, including a lack of transparency as well as the numerous
design options that decision-making entities will be faced with. We argue for a
human-centred approach to designing for contestability to ensure that the needs
of decision subjects, and the community, are met.
</summary>
    <author>
      <name>Henrietta Lyons</name>
    </author>
    <author>
      <name>Eduardo Velloso</name>
    </author>
    <author>
      <name>Tim Miller</name>
    </author>
    <link href="http://arxiv.org/abs/2102.10787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.10787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.09990v1</id>
    <updated>2021-03-18T02:39:28Z</updated>
    <published>2021-03-18T02:39:28Z</published>
    <title>Human-AI Symbiosis: A Survey of Current Approaches</title>
    <summary>  In this paper, we aim at providing a comprehensive outline of the different
threads of work in human-AI collaboration. By highlighting various aspects of
works on the human-AI team such as the flow of complementing, task horizon,
model representation, knowledge level, and teaming goal, we make a taxonomy of
recent works according to these dimensions. We hope that the survey will
provide a more clear connection between the works in the human-AI team and
guidance to new researchers in this area.
</summary>
    <author>
      <name>Zahra Zahedi</name>
    </author>
    <author>
      <name>Subbarao Kambhampati</name>
    </author>
    <link href="http://arxiv.org/abs/2103.09990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.09990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.14033v1</id>
    <updated>2021-03-22T18:27:51Z</updated>
    <published>2021-03-22T18:27:51Z</published>
    <title>A Novel Methodology For Crowdsourcing AI Models in an Enterprise</title>
    <summary>  The evolution of AI is advancing rapidly, creating both challenges and
opportunities for industry-community collaboration. In this work, we present a
novel methodology aiming to facilitate this collaboration through crowdsourcing
of AI models. Concretely, we have implemented a system and a process that any
organization can easily adopt to host AI competitions. The system allows them
to automatically harvest and evaluate the submitted models against in-house
proprietary data and also to incorporate them as reusable services in a
product.
</summary>
    <author>
      <name>Parthasarathy Suryanarayanan</name>
    </author>
    <author>
      <name>Sundar Saranathan</name>
    </author>
    <author>
      <name>Shilpa Mahatma</name>
    </author>
    <author>
      <name>Divya Pathak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Challenges in Machine Learning workshop 2020 (CiML2020)
  @NeurIPS (http://ciml.chalearn.org/)</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.14033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.14033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.07852v1</id>
    <updated>2021-05-04T22:56:04Z</updated>
    <published>2021-05-04T22:56:04Z</published>
    <title>Hard Choices and Hard Limits for Artificial Intelligence</title>
    <summary>  Artificial intelligence (AI) is supposed to help us make better choices. Some
of these choices are small, like what route to take to work, or what music to
listen to. Others are big, like what treatment to administer for a disease or
how long to sentence someone for a crime. If AI can assist with these big
decisions, we might think it can also help with hard choices, cases where
alternatives are neither better, worse nor equal but on a par. The aim of this
paper, however, is to show that this view is mistaken: the fact of parity shows
that there are hard limits on AI in decision making and choices that AI cannot,
and should not, resolve.
</summary>
    <author>
      <name>Bryce Goodman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3461702.3462539</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3461702.3462539" rel="related"/>
    <link href="http://arxiv.org/abs/2105.07852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.07852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.07921v1</id>
    <updated>2021-06-15T07:18:07Z</updated>
    <published>2021-06-15T07:18:07Z</published>
    <title>Diagnosing the Impact of AI on Radiology in China</title>
    <summary>  Artificial Intelligence will significantly impact the work environment of
radiologists. I suggest that up to 50% of a radiologists work in 2021 will be
performed by AI-models in 2025. However, it won't increase beyond that 50%
level, as radiologists remain key for human-centered aspects of their job. I
project that few to no radiologists will be laid off in China due to the
existing supply shortage of radiology services in 2021. The application of AI
in radiology could contribute 1.7 billion USD to China's GDP in 2025. It will
further allow radiologists to start productive work up to four years earlier.
AI in radiology will positively impact the health of patients and radiologists
themselves.
</summary>
    <author>
      <name>Niklas Muennighoff</name>
    </author>
    <link href="http://arxiv.org/abs/2106.07921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.07921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.12289v1</id>
    <updated>2021-08-19T05:34:12Z</updated>
    <published>2021-08-19T05:34:12Z</published>
    <title>Key Considerations for the Responsible Development and Fielding of
  Artificial Intelligence</title>
    <summary>  We review key considerations, practices, and areas for future work aimed at
the responsible development and fielding of AI technologies. We describe
critical challenges and make recommendations on topics that should be given
priority consideration, practices that should be implemented, and policies that
should be defined or updated to reflect developments with capabilities and uses
of AI technologies. The Key Considerations were developed with a lens for
adoption by U.S. government departments and agencies critical to national
security. However, they are relevant more generally for the design,
construction, and use of AI systems.
</summary>
    <author>
      <name>Eric Horvitz</name>
    </author>
    <author>
      <name>Jessica Young</name>
    </author>
    <author>
      <name>Rama G. Elluru</name>
    </author>
    <author>
      <name>Chuck Howell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Report of the National Security Commission on AI (NSCAI), 44 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.12289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.12289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.04646v1</id>
    <updated>2021-09-10T03:24:50Z</updated>
    <published>2021-09-10T03:24:50Z</published>
    <title>AI Agents in Emergency Response Applications</title>
    <summary>  Emergency personnel respond to various situations ranging from fire, medical,
hazardous materials, industrial accidents, to natural disasters. Situations
such as natural disasters or terrorist acts require a multifaceted response of
firefighters, paramedics, hazmat teams, and other agencies. Engineering AI
systems that aid emergency personnel proves to be a difficult system
engineering problem. Mission-critical "edge AI" situations require low-latency,
reliable analytics. To further add complexity, a high degree of model accuracy
is required when lives are at stake, creating a need for the deployment of
highly accurate, however computationally intensive models to
resource-constrained devices. To address all these issues, we propose an
agent-based architecture for deployment of AI agents via 5G service-based
architecture.
</summary>
    <author>
      <name>Aryan Naim</name>
    </author>
    <author>
      <name>Ryan Alimo</name>
    </author>
    <author>
      <name>Jay Braun</name>
    </author>
    <link href="http://arxiv.org/abs/2109.04646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.04646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.11320v1</id>
    <updated>2021-09-23T12:07:26Z</updated>
    <published>2021-09-23T12:07:26Z</published>
    <title>Nine Challenges in Artificial Intelligence and Wireless Communications
  for 6G</title>
    <summary>  In recent years, techniques developed in artificial intelligence (AI),
especially those in machine learning (ML), have been successfully applied in
various areas, leading to a widespread belief that AI will collectively play an
important role in future wireless communications. To accomplish the aspiration,
we present nine challenges to be addressed by the interdisciplinary areas of
AI/ML and wireless communications, with particular focus towards the sixth
generation (6G) wireless networks. Specifically, this article classifies the
nine challenges into computation in AI, distributed neural networks and
learning, and ML enabled semantic communications.
</summary>
    <author>
      <name>Wen Tong</name>
    </author>
    <author>
      <name>Geoffrey Ye Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.11320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.11320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.03320v1</id>
    <updated>2021-10-07T10:30:18Z</updated>
    <published>2021-10-07T10:30:18Z</published>
    <title>Automated Testing of AI Models</title>
    <summary>  The last decade has seen tremendous progress in AI technology and
applications. With such widespread adoption, ensuring the reliability of the AI
models is crucial. In past, we took the first step of creating a testing
framework called AITEST for metamorphic properties such as fairness, robustness
properties for tabular, time-series, and text classification models. In this
paper, we extend the capability of the AITEST tool to include the testing
techniques for Image and Speech-to-text models along with interpretability
testing for tabular models. These novel extensions make AITEST a comprehensive
framework for testing AI models.
</summary>
    <author>
      <name>Swagatam Haldar</name>
    </author>
    <author>
      <name>Deepak Vijaykeerthy</name>
    </author>
    <author>
      <name>Diptikalyan Saha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 Figures, 4 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.03320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.03320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.00585v3</id>
    <updated>2022-03-11T08:26:38Z</updated>
    <published>2021-10-31T20:18:45Z</published>
    <title>JEDAI: A System for Skill-Aligned Explainable Robot Planning</title>
    <summary>  This paper presents JEDAI, an AI system designed for outreach and educational
efforts aimed at non-AI experts. JEDAI features a novel synthesis of research
ideas from integrated task and motion planning and explainable AI. JEDAI helps
users create high-level, intuitive plans while ensuring that they will be
executable by the robot. It also provides users customized explanations about
errors and helps improve their understanding of AI planning as well as the
limits and capabilities of the underlying robot system.
</summary>
    <author>
      <name>Naman Shah</name>
    </author>
    <author>
      <name>Pulkit Verma</name>
    </author>
    <author>
      <name>Trevor Angle</name>
    </author>
    <author>
      <name>Siddharth Srivastava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAMS 2022 (Demonstration Track)</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.00585v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.00585v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.02001v1</id>
    <updated>2021-11-03T03:34:19Z</updated>
    <published>2021-11-03T03:34:19Z</published>
    <title>Certifiable Artificial Intelligence Through Data Fusion</title>
    <summary>  This paper reviews and proposes concerns in adopting, fielding, and
maintaining artificial intelligence (AI) systems. While the AI community has
made rapid progress, there are challenges in certifying AI systems. Using
procedures from design and operational test and evaluation, there are
opportunities towards determining performance bounds to manage expectations of
intended use. A notional use case is presented with image data fusion to
support AI object recognition certifiability considering precision versus
distance.
</summary>
    <author>
      <name>Erik Blasch</name>
    </author>
    <author>
      <name>Junchi Bin</name>
    </author>
    <author>
      <name>Zheng Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at AAAI FSS-21: Artificial Intelligence in Government and
  Public Sector, Washington, DC, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.02001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.02001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.04437v1</id>
    <updated>2021-10-22T21:43:15Z</updated>
    <published>2021-10-22T21:43:15Z</published>
    <title>Artistic Autonomy in AI Art</title>
    <summary>  The concept of art has transposed meaning and medium across time, with its
context being a deciding factor for its evolution. However, human beings'
innermost functionality remains the same, and art, to this day, serves as an
expression of the subconscious. Accelerated by the conception of GANs in 2014,
automation has become a central medium in Artificial Intelligence (AI) Art.
However, this raises concern over AI's influence on artistic autonomy within
the process of creativity. This paper introduces the ethical responsibility of
AI towards maintaining the artist's volition in exercising autonomy and
utilizes principles of self-determination theory alongside fundamental limits
of creativity to do so.
</summary>
    <author>
      <name>Alayt Issak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Machine Learning for Creativity and Design workshop at
  the 35th Conference on Neural Information Processing Systems (NeurIPS 2021),
  Sydney, Australia</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.04437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.04437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.08609v1</id>
    <updated>2021-11-16T16:43:07Z</updated>
    <published>2021-11-16T16:43:07Z</published>
    <title>Document AI: Benchmarks, Models and Applications</title>
    <summary>  Document AI, or Document Intelligence, is a relatively new research topic
that refers to the techniques for automatically reading, understanding, and
analyzing business documents. It is an important research direction for natural
language processing and computer vision. In recent years, the popularity of
deep learning technology has greatly advanced the development of Document AI,
such as document layout analysis, visual information extraction, document
visual question answering, document image classification, etc. This paper
briefly reviews some of the representative models, tasks, and benchmark
datasets. Furthermore, we also introduce early-stage heuristic rule-based
document analysis, statistical machine learning algorithms, and deep learning
approaches especially pre-training methods. Finally, we look into future
directions for Document AI research.
</summary>
    <author>
      <name>Lei Cui</name>
    </author>
    <author>
      <name>Yiheng Xu</name>
    </author>
    <author>
      <name>Tengchao Lv</name>
    </author>
    <author>
      <name>Furu Wei</name>
    </author>
    <link href="http://arxiv.org/abs/2111.08609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.08609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.10283v2</id>
    <updated>2021-11-22T12:44:49Z</updated>
    <published>2021-11-19T15:44:10Z</published>
    <title>The Joy of Neural Painting</title>
    <summary>  Neural Painters is a class of models that follows a GAN framework to generate
brushstrokes, which are then composed to create paintings. GANs are great
generative models for AI Art but they are known to be notoriously difficult to
train. To overcome GAN's limitations and to speed up the Neural Painter
training, we applied Transfer Learning to the process reducing it from days to
only hours, while achieving the same level of visual aesthetics in the final
paintings generated. We report our approach and results in this work.
</summary>
    <author>
      <name>Ernesto Diaz-Aviles</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Libre AI</arxiv:affiliation>
    </author>
    <author>
      <name>Claudia Orellana-Rodriguez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Libre AI</arxiv:affiliation>
    </author>
    <author>
      <name>Beth Jochim</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Libre AI</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2111.10283v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.10283v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; I.4.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.14062v1</id>
    <updated>2021-11-28T06:25:49Z</updated>
    <published>2021-11-28T06:25:49Z</published>
    <title>P4AI: Approaching AI Ethics through Principlism</title>
    <summary>  The field of computer vision is rapidly evolving, particularly in the context
of new methods of neural architecture design. These models contribute to (1)
the Climate Crisis - increased CO2 emissions and (2) the Privacy Crisis - data
leakage concerns. To address the often overlooked impact the Computer Vision
(CV) community has on these crises, we outline a novel ethical framework,
\textit{P4AI}: Principlism for AI, an augmented principlistic view of ethical
dilemmas within AI. We then suggest using P4AI to make concrete recommendations
to the community to mitigate the climate and privacy crises.
</summary>
    <author>
      <name>Andre Fu</name>
    </author>
    <author>
      <name>Elisa Ding</name>
    </author>
    <author>
      <name>Mahdi S. Hosseini</name>
    </author>
    <author>
      <name>Konstantinos N. Plataniotis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Human-Centered AI workshop at NeurIPS 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.14062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.14062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.08093v1</id>
    <updated>2021-12-15T13:09:22Z</updated>
    <published>2021-12-15T13:09:22Z</published>
    <title>Towards Controllable Agent in MOBA Games with Generative Modeling</title>
    <summary>  We propose novel methods to develop action controllable agent that behaves
like a human and has the ability to align with human players in Multiplayer
Online Battle Arena (MOBA) games. By modeling the control problem as an action
generation process, we devise a deep latent alignment neural network model for
training agent, and a corresponding sampling algorithm for controlling an
agent's action. Particularly, we propose deterministic and stochastic attention
implementations of the core latent alignment model. Both simulated and online
experiments in the game Honor of Kings demonstrate the efficacy of the proposed
methods.
</summary>
    <author>
      <name>Shubao Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Human-Compatible AI; Human-AI Cooperation; AI control; AI Alignment</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.08093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.08093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.10436v2</id>
    <updated>2022-05-11T18:34:11Z</updated>
    <published>2022-01-25T16:32:35Z</published>
    <title>Safe AI -- How is this Possible?</title>
    <summary>  Ttraditional safety engineering is coming to a turning point moving from
deterministic, non-evolving systems operating in well-defined contexts to
increasingly autonomous and learning-enabled AI systems which are acting in
largely unpredictable operating contexts. We outline some of underlying
challenges of safe AI and suggest a rigorous engineering framework for
minimizing uncertainty, thereby increasing confidence, up to tolerable levels,
in the safe behavior of AI systems.
</summary>
    <author>
      <name>Harald Rueß</name>
    </author>
    <author>
      <name>Simon Burton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.10436v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.10436v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.04787v1</id>
    <updated>2022-02-10T01:15:50Z</updated>
    <published>2022-02-10T01:15:50Z</published>
    <title>Proceedings of the Robust Artificial Intelligence System Assurance
  (RAISA) Workshop 2022</title>
    <summary>  The Robust Artificial Intelligence System Assurance (RAISA) workshop will
focus on research, development and application of robust artificial
intelligence (AI) and machine learning (ML) systems. Rather than studying
robustness with respect to particular ML algorithms, our approach will be to
explore robustness assurance at the system architecture level, during both
development and deployment, and within the human-machine teaming context. While
the research community is converging on robust solutions for individual AI
models in specific scenarios, the problem of evaluating and assuring the
robustness of an AI system across its entire life cycle is much more complex.
Moreover, the operational context in which AI systems are deployed necessitates
consideration of robustness and its relation to principles of fairness,
privacy, and explainability.
</summary>
    <author>
      <name>Olivia Brown</name>
    </author>
    <author>
      <name>Brad Dillman</name>
    </author>
    <link href="http://arxiv.org/abs/2202.04787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.04787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.00419v1</id>
    <updated>2022-02-25T23:28:31Z</updated>
    <published>2022-02-25T23:28:31Z</published>
    <title>Artificial Intelligence and work: a critical review of recent research
  from the social sciences</title>
    <summary>  This review seeks to present a comprehensive picture of recent discussions in
the social sciences of the anticipated impact of AI on the world of work.
Issues covered include technological unemployment, algorithmic management,
platform work an the politics of AI work. The review identifies the major
disciplinary and methodological perspectives on AI's impact on work, and the
obstacles they face in making predictions. Two parameters influencing the
development and deployment of AI in the economy are highlighted, the capitalist
imperative and nationalistic pressures.
</summary>
    <author>
      <name>Jean-Philippe Deranty</name>
    </author>
    <author>
      <name>Thomas Corbin</name>
    </author>
    <link href="http://arxiv.org/abs/2204.00419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.00419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.04211v1</id>
    <updated>2022-04-07T17:09:07Z</updated>
    <published>2022-04-07T17:09:07Z</published>
    <title>Measuring AI Systems Beyond Accuracy</title>
    <summary>  Current test and evaluation (T&amp;E) methods for assessing machine learning (ML)
system performance often rely on incomplete metrics. Testing is additionally
often siloed from the other phases of the ML system lifecycle. Research
investigating cross-domain approaches to ML T&amp;E is needed to drive the state of
the art forward and to build an Artificial Intelligence (AI) engineering
discipline. This paper advocates for a robust, integrated approach to testing
by outlining six key questions for guiding a holistic T&amp;E strategy.
</summary>
    <author>
      <name>Violet Turri</name>
    </author>
    <author>
      <name>Rachel Dzombak</name>
    </author>
    <author>
      <name>Eric Heim</name>
    </author>
    <author>
      <name>Nathan VanHoudnos</name>
    </author>
    <author>
      <name>Jay Palat</name>
    </author>
    <author>
      <name>Anusha Sinha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, Presented at 2022 AAAI Spring Symposium Series Workshop on
  AI Engineering: Creating Scalable, Human-Centered and Robust AI Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.04211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.04211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.06919v1</id>
    <updated>2022-05-13T23:01:57Z</updated>
    <published>2022-05-13T23:01:57Z</published>
    <title>Grounding Explainability Within the Context of Global South in XAI</title>
    <summary>  In this position paper, we propose building a broader and deeper
understanding around Explainability in AI by 'grounding' it in social contexts,
the socio-technical systems operate in. We situate our understanding of
grounded explainability in the 'Global South' in general and India in
particular and express the need for more research within the global south
context when it comes to explainability and AI.
</summary>
    <author>
      <name>Deepa Singh</name>
    </author>
    <author>
      <name>Michal Slupczynski</name>
    </author>
    <author>
      <name>Ajit G. Pillai</name>
    </author>
    <author>
      <name>Vinoth Pandian Sermuga Pandian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Presented at CHI 2022 Workshop on Human-Centered Explainable
  AI (HCXAI): Beyond Opening the Black-Box of AI</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.06919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.06919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.13471v1</id>
    <updated>2022-05-26T16:30:30Z</updated>
    <published>2022-05-26T16:30:30Z</published>
    <title>Characterising Research Areas in the field of AI</title>
    <summary>  Interest in Artificial Intelligence (AI) continues to grow rapidly, hence it
is crucial to support researchers and organisations in understanding where AI
research is heading. In this study, we conducted a bibliometric analysis on
257K articles in AI, retrieved from OpenAlex. We identified the main conceptual
themes by performing clustering analysis on the co-occurrence network of
topics. Finally, we observed how such themes evolved over time. The results
highlight the growing academic interest in research themes like deep learning,
machine learning, and internet of things.
</summary>
    <author>
      <name>Alessandra Belfiore</name>
    </author>
    <author>
      <name>Angelo Salatino</name>
    </author>
    <author>
      <name>Francesco Osborne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">paper presented at SIS2022 - 51ST SCIENTIFIC MEETING OF THE ITALIAN
  STATISTICAL SOCIETY</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.13471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.13471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.12052v1</id>
    <updated>2022-07-25T10:42:50Z</updated>
    <published>2022-07-25T10:42:50Z</published>
    <title>Designing an AI-Driven Talent Intelligence Solution: Exploring Big Data
  to extend the TOE Framework</title>
    <summary>  AI has the potential to improve approaches to talent management enabling
dynamic provisions through implementing advanced automation. This study aims to
identify the new requirements for developing AI-oriented artifacts to address
talent management issues. Focusing on enhancing interactions between
professional assessment and planning attributes, the design artifact is an
intelligent employment automation solution for career guidance that is largely
dependent on a talent intelligent module and an individuals growth needs. A
design science method is adopted for conducting the experimental study with
structured machine learning techniques which is the primary element of a
comprehensive AI solution framework informed through a proposed moderation of
the technology-organization-environment theory.
</summary>
    <author>
      <name>Ali Faqihi</name>
    </author>
    <author>
      <name>Shah J Miah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Working paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.12052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.12052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.15327v2</id>
    <updated>2022-10-31T09:32:32Z</updated>
    <published>2022-10-27T11:11:08Z</published>
    <title>Towards Language-driven Scientific AI</title>
    <summary>  Inspired by recent and revolutionary developments in AI, particularly in
language understanding and generation, we set about designing AI systems that
are able to address complex scientific tasks that challenge human capabilities
to make new discoveries. Central to our approach is the notion of natural
language as core representation, reasoning, and exchange format between
scientific AI and human scientists. In this paper, we identify and discuss some
of the main research challenges to accomplish such vision.
</summary>
    <author>
      <name>José Manuel Gómez-Pérez</name>
    </author>
    <link href="http://arxiv.org/abs/2210.15327v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.15327v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.02759v1</id>
    <updated>2022-11-04T21:49:52Z</updated>
    <published>2022-11-04T21:49:52Z</published>
    <title>Diversity-based Deep Reinforcement Learning Towards Multidimensional
  Difficulty for Fighting Game AI</title>
    <summary>  In fighting games, individual players of the same skill level often exhibit
distinct strategies from one another through their gameplay. Despite this, the
majority of AI agents for fighting games have only a single strategy for each
"level" of difficulty. To make AI opponents more human-like, we'd ideally like
to see multiple different strategies at each level of difficulty, a concept we
refer to as "multidimensional" difficulty. In this paper, we introduce a
diversity-based deep reinforcement learning approach for generating a set of
agents of similar difficulty that utilize diverse strategies. We find this
approach outperforms a baseline trained with specialized, human-authored reward
functions in both diversity and performance.
</summary>
    <author>
      <name>Emily Halina</name>
    </author>
    <author>
      <name>Matthew Guzdial</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, Experimental AI in Games 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.02759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.02759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.06346v1</id>
    <updated>2022-11-02T15:30:40Z</updated>
    <published>2022-11-02T15:30:40Z</published>
    <title>AI Ethics in Smart Healthcare</title>
    <summary>  This article reviews the landscape of ethical challenges of integrating
artificial intelligence (AI) into smart healthcare products, including medical
electronic devices. Differences between traditional ethics in the medical
domain and emerging ethical challenges with AI-driven healthcare are presented,
particularly as they relate to transparency, bias, privacy, safety,
responsibility, justice, and autonomy. Open challenges and recommendations are
outlined to enable the integration of ethical principles into the design,
validation, clinical trials, deployment, monitoring, repair, and retirement of
AI-based smart healthcare products.
</summary>
    <author>
      <name>Sudeep Pasricha</name>
    </author>
    <link href="http://arxiv.org/abs/2211.06346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.06346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.15271v2</id>
    <updated>2022-11-29T11:22:38Z</updated>
    <published>2022-11-28T12:54:34Z</published>
    <title>The Myth of Culturally Agnostic AI Models</title>
    <summary>  The paper discusses the potential of large vision-language models as objects
of interest for empirical cultural studies. Focusing on the comparative
analysis of outputs from two popular text-to-image synthesis models, DALL-E 2
and Stable Diffusion, the paper tries to tackle the pros and cons of striving
towards culturally agnostic vs. culturally specific AI models. The paper
discusses several examples of memorization and bias in generated outputs which
showcase the trade-off between risk mitigation and cultural specificity, as
well as the overall impossibility of developing culturally agnostic models.
</summary>
    <author>
      <name>Eva Cetinic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for "Cultures in AI/AI in Culture" NeurIPS 2022 Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.15271v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.15271v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.00018v1</id>
    <updated>2022-11-30T09:59:55Z</updated>
    <published>2022-11-30T09:59:55Z</published>
    <title>ESG In Corporate Filings: An AI Perspective</title>
    <summary>  Our main contribution is that we are using AI to discern the key drivers of
variation of ESG mentions in the corporate filings. With AI, we are able to
separate "dimensions" along which the corporate management presents their ESG
policies to the world. These dimensions are 1) diversity, 2) hazardous
materials, and 3) greenhouse gasses. We are also able to identify separate
"background" dimensions of unofficial ESG activity in the firms, which provide
more color into the firms and their shareholders' thinking about their ESG
processes. We then measure investors' response to the ESG activity "factors".
The AI techniques presented can assist in building better, more reliable and
useful ESG ratings systems.
</summary>
    <author>
      <name>Irene Aldridge</name>
    </author>
    <author>
      <name>Payton Martin</name>
    </author>
    <link href="http://arxiv.org/abs/2212.00018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.00018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.05397v1</id>
    <updated>2022-12-07T14:46:38Z</updated>
    <published>2022-12-07T14:46:38Z</published>
    <title>The problem with AI consciousness: A neurogenetic case against synthetic
  sentience</title>
    <summary>  Ever since the creation of the first artificial intelligence (AI) machinery
built on machine learning (ML), public society has entertained the idea that
eventually computers could become sentient and develop a consciousness of their
own. As these models now get increasingly better and convincingly more
anthropomorphic, even some engineers have started to believe that AI might
become conscious, which would result in serious social consequences. The
present paper argues against the plausibility of sentient AI primarily based on
the theory of neurogenetic structuralism, which claims that the physiology of
biological neurons and their structural organization into complex brains are
necessary prerequisites for true consciousness to emerge.
</summary>
    <author>
      <name>Yoshija Walter</name>
    </author>
    <author>
      <name>Lukas Zbinden</name>
    </author>
    <link href="http://arxiv.org/abs/2301.05397v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.05397v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.04175v1</id>
    <updated>2019-05-10T13:56:52Z</updated>
    <published>2019-05-10T13:56:52Z</published>
    <title>AI in the media and creative industries</title>
    <summary>  Thanks to the Big Data revolution and increasing computing capacities,
Artificial Intelligence (AI) has made an impressive revival over the past few
years and is now omnipresent in both research and industry. The creative
sectors have always been early adopters of AI technologies and this continues
to be the case. As a matter of fact, recent technological developments keep
pushing the boundaries of intelligent systems in creative applications: the
critically acclaimed movie "Sunspring", released in 2016, was entirely written
by AI technology, and the first-ever Music Album, called "Hello World",
produced using AI has been released this year. Simultaneously, the exploratory
nature of the creative process is raising important technical challenges for AI
such as the ability for AI-powered techniques to be accurate under limited data
resources, as opposed to the conventional "Big Data" approach, or the ability
to process, analyse and match data from multiple modalities (text, sound,
images, etc.) at the same time. The purpose of this white paper is to
understand future technological advances in AI and their growing impact on
creative industries. This paper addresses the following questions: Where does
AI operate in creative Industries? What is its operative role? How will AI
transform creative industries in the next ten years? This white paper aims to
provide a realistic perspective of the scope of AI actions in creative
industries, proposes a vision of how this technology could contribute to
research and development works in such context, and identifies research and
development challenges.
</summary>
    <author>
      <name>Giuseppe Amato</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CNR PISA</arxiv:affiliation>
    </author>
    <author>
      <name>Malte Behrmann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <author>
      <name>Frédéric Bimbot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <author>
      <name>Baptiste Caramiaux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LRI, EX-SITU</arxiv:affiliation>
    </author>
    <author>
      <name>Fabrizio Falchi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CNR PISA</arxiv:affiliation>
    </author>
    <author>
      <name>Ander Garcia</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Joost Geurts</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Jaume Gibert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LinkMedia</arxiv:affiliation>
    </author>
    <author>
      <name>Guillaume Gravier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LinkMedia</arxiv:affiliation>
    </author>
    <author>
      <name>Hadmut Holken</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">HKU</arxiv:affiliation>
    </author>
    <author>
      <name>Hartmut Koenitz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">HKU</arxiv:affiliation>
    </author>
    <author>
      <name>Sylvain Lefebvre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MFX</arxiv:affiliation>
    </author>
    <author>
      <name>Antoine Liutkus</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA, ZENITH</arxiv:affiliation>
    </author>
    <author>
      <name>Fabien Lotte</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Potioc, LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Perkis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NTNU</arxiv:affiliation>
    </author>
    <author>
      <name>Rafael Redondo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">FEP</arxiv:affiliation>
    </author>
    <author>
      <name>Enrico Turrin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">FEP</arxiv:affiliation>
    </author>
    <author>
      <name>Thierry Vieville</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Mnemosyne</arxiv:affiliation>
    </author>
    <author>
      <name>Emmanuel Vincent</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MULTISPEECH</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1905.04175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.04175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.05509v3</id>
    <updated>2021-05-28T15:28:07Z</updated>
    <published>2020-06-09T21:06:46Z</published>
    <title>Can artificial intelligence (AI) be used to accurately detect
  tuberculosis (TB) from chest X-rays? An evaluation of five AI products for TB
  screening and triaging in a high TB burden setting</title>
    <summary>  Artificial intelligence (AI) products can be trained to recognize
tuberculosis (TB)-related abnormalities on chest radiographs. Various AI
products are available commercially, yet there is lack of evidence on how their
performance compared with each other and with radiologists. We evaluated five
AI software products for screening and triaging TB using a large dataset that
had not been used to train any commercial AI products. Individuals (&gt;=15 years
old) presenting to three TB screening centers in Dhaka, Bangladesh, were
recruited consecutively. All CXR were read independently by a group of three
Bangladeshi registered radiologists and five commercial AI products: CAD4TB
(v7), InferReadDR (v2), Lunit INSIGHT CXR (v4.9.0), JF CXR-1 (v2), and qXR
(v3). All five AI products significantly outperformed the Bangladeshi
radiologists. The areas under the receiver operating characteristic curve are
qXR: 90.81% (95% CI:90.33-91.29%), CAD4TB: 90.34% (95% CI:89.81-90.87), Lunit
INSIGHT CXR: 88.61% (95% CI:88.03%-89.20%), InferReadDR: 84.90% (95% CI:
84.27-85.54%) and JF CXR-1: 84.89% (95% CI:84.26-85.53%). Only qXR met the TPP
with 74.3% specificity at 90% sensitivity. Five AI algorithms can reduce the
number of Xpert tests required by 50%, while maintaining a sensitivity above
90%. All AI algorithms performed worse among the older age and people with
prior TB history. AI products can be highly accurate and useful screening and
triage tools for TB detection in high burden regions and outperform human
readers.
</summary>
    <author>
      <name>Zhi Zhen Qin</name>
    </author>
    <author>
      <name>Shahriar Ahmed</name>
    </author>
    <author>
      <name>Mohammad Shahnewaz Sarker</name>
    </author>
    <author>
      <name>Kishor Paul</name>
    </author>
    <author>
      <name>Ahammad Shafiq Sikder Adel</name>
    </author>
    <author>
      <name>Tasneem Naheyan</name>
    </author>
    <author>
      <name>Rachael Barrett</name>
    </author>
    <author>
      <name>Sayera Banu</name>
    </author>
    <author>
      <name>Jacob Creswell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 3 Tables 3 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.05509v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.05509v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.12898v2</id>
    <updated>2020-12-04T21:26:25Z</updated>
    <published>2020-09-27T17:00:02Z</published>
    <title>An Artificial Intelligence-Driven Agent for Real-Time Head-and-Neck IMRT
  Plan Generation using Conditional Generative Adversarial Network (cGAN)</title>
    <summary>  Purpose: To develop an Artificial Intelligence (AI) agent for fully-automated
rapid head and neck (H&amp;N) IMRT plan generation without time-consuming inverse
planning.$$$$
  Methods: This AI agent was trained using a conditional Generative Adversarial
Network architecture. The generator, PyraNet, is a novel Deep Learning network
that implements 28 classic ResNet blocks in pyramid-like concatenations. The
discriminator is a customized 4-layer DenseNet. The AI agent first generates
customized 2D projections at 9 template beam angles from 3D CT volume and
structures of a patient. These projections are then stacked as 4D inputs of
PyraNet, from which 9 radiation fluence maps are generated simultaneously.
Finally, the predicted fluence maps are imported into a commercial treatment
planning system (TPS) for plan integrity checks. The AI agent was built and
tested upon 231 oropharyngeal plans from a TPS plan library. Only the primary
plans in the sequential boost regime were studied. A customized Harr wavelet
loss was adopted for fluence map comparison. Isodose distributions in test AI
plans and TPS plans were qualitatively evaluated. Key dosimetric metrics were
statistically compared.$$$$
  Results: All test AI plans were successfully generated. Isodose gradients
outside of PTV in AI plans were comparable with TPS plans. After PTV coverage
normalization, $D_{mean}$ of parotids and oral cavity in AI plans and TPS plans
were comparable without statistical significance. AI plans achieved comparable
$D_{max}$ at 0.01cc of brainstem and cord+5mm without clinically relevant
differences, but body $D_{max}$ was higher than the TPS plan results. The AI
agent needs ~3s per case to predict fluence maps.$$$$
  Conclusions: The developed AI agent can generate H&amp;N IMRT plans with
satisfying dosimetry quality. With rapid and fully automated implementation, it
holds great potential for clinical applications.
</summary>
    <author>
      <name>Xinyi Li</name>
    </author>
    <author>
      <name>Yang Sheng</name>
    </author>
    <author>
      <name>Jiahan Zhang</name>
    </author>
    <author>
      <name>Wentao Wang</name>
    </author>
    <author>
      <name>Fang-Fang Yin</name>
    </author>
    <author>
      <name>Qiuwen Wu</name>
    </author>
    <author>
      <name>Yaorong Ge</name>
    </author>
    <author>
      <name>Q. Jackie Wu</name>
    </author>
    <author>
      <name>Chunhao Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/mp.14770</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/mp.14770" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Withdraw for internal review</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.12898v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.12898v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.04918v8</id>
    <updated>2022-01-05T07:53:02Z</updated>
    <published>2021-03-08T17:31:19Z</published>
    <title>A Survey of Embodied AI: From Simulators to Research Tasks</title>
    <summary>  There has been an emerging paradigm shift from the era of "internet AI" to
"embodied AI", where AI algorithms and agents no longer learn from datasets of
images, videos or text curated primarily from the internet. Instead, they learn
through interactions with their environments from an egocentric perception
similar to humans. Consequently, there has been substantial growth in the
demand for embodied AI simulators to support various embodied AI research
tasks. This growing interest in embodied AI is beneficial to the greater
pursuit of Artificial General Intelligence (AGI), but there has not been a
contemporary and comprehensive survey of this field. This paper aims to provide
an encyclopedic survey for the field of embodied AI, from its simulators to its
research. By evaluating nine current embodied AI simulators with our proposed
seven features, this paper aims to understand the simulators in their provision
for use in embodied AI research and their limitations. Lastly, this paper
surveys the three main research tasks in embodied AI -- visual exploration,
visual navigation and embodied question answering (QA), covering the
state-of-the-art approaches, evaluation metrics and datasets. Finally, with the
new insights revealed through surveying the field, the paper will provide
suggestions for simulator-for-task selections and recommendations for the
future directions of the field.
</summary>
    <author>
      <name>Jiafei Duan</name>
    </author>
    <author>
      <name>Samson Yu</name>
    </author>
    <author>
      <name>Hui Li Tan</name>
    </author>
    <author>
      <name>Hongyuan Zhu</name>
    </author>
    <author>
      <name>Cheston Tan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been accepted by IEEE Transactions on Emerging Topics
  in Computational Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.04918v8" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.04918v8" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.13426v2</id>
    <updated>2021-11-15T11:01:21Z</updated>
    <published>2021-07-28T15:16:37Z</published>
    <title>On the properties of the asymptotic incompatibility measure in
  multiparameter quantum estimation</title>
    <summary>  We address the use of asymptotic incompatibility (AI) to assess the
quantumness of a multiparameter quantum statistical model. AI is a recently
introduced measure which quantifies the difference between the Holevo and the
SLD scalar bounds, and can be evaluated using only the symmetric logarithmic
derivative (SLD) operators of the model. At first, we evaluate analytically the
AI of the most general quantum statistical models involving two-level (qubit)
and single-mode Gaussian continuous-variable quantum systems, and prove that AI
is a simple monotonous function of the state purity. Then, we numerically
investigate the same problem for qudits ($d$-dimensional quantum systems, with
$2 &lt; d \leq 4$), showing that, while in general AI is not in general a function
of purity, we have enough numerical evidence to conclude that the maximum
amount of AI is attainable only for quantum statistical models characterized by
a purity larger than $\mu_{\sf min} = 1/(d-1)$. In addition, by parametrizing
qudit states as thermal (Gibbs) states, numerical results suggest that, once
the spectrum of the Hamiltonian is fixed, the AI measure is in one-to-one
correspondence with the fictitious temperature parameter $\beta$ characterizing
the family of density operators. Finally, by studying in detail the definition
and properties of the AI measure we find that: i) given a quantum statistical
model, one can readily identify the maximum number of asymptotically
compatibile parameters; ii) the AI of a quantum statistical model bounds from
above the AI of any sub-model that can be defined by fixing one or more of the
original unknown parameters (or functions thereof), leading to possibly useful
bounds on the AI of models involving noisy quantum dynamics.
</summary>
    <author>
      <name>Alessandro Candeloro</name>
    </author>
    <author>
      <name>Matteo G. A. Paris</name>
    </author>
    <author>
      <name>Marco G. Genoni</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1751-8121/ac331e</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1751-8121/ac331e" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Phys. A 54, 485301 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2107.13426v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.13426v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.13756v1</id>
    <updated>2021-11-23T17:46:28Z</updated>
    <published>2021-11-23T17:46:28Z</published>
    <title>Demystifying Ten Big Ideas and Rules Every Fire Scientist &amp; Engineer
  Should Know About Blackbox, Whitebox &amp; Causal Artificial Intelligence</title>
    <summary>  Artificial intelligence (AI) is paving the way towards the fourth industrial
revolution with the fire domain (Fire 4.0). As a matter of fact, the next few
years will be elemental to how this technology will shape our academia,
practice, and entrepreneurship. Despite the growing interest between fire
research groups, AI remains absent of our curriculum, and we continue to lack a
methodical framework to adopt, apply and create AI solutions suitable for our
problems. The above is also true for parallel engineering domains (i.e.,
civil/mechanical engineering), and in order to negate the notion of history
repeats itself (e.g., look at the continued debate with regard to modernizing
standardized fire testing, etc.), it is the motivation behind this letter to
the Editor to demystify some of the big ideas behind AI to jump-start prolific
and strategic discussions on the front of AI &amp; Fire. In addition, this letter
intends to explain some of the most fundamental concepts and clear common
misconceptions specific to the adoption of AI in fire engineering. This short
letter is a companion to the Smart Systems in Fire Engineering special issue
sponsored by Fire Technology. An in-depth review of AI algorithms [1] and
success stories to the proper implementations of such algorithms can be found
in the aforenoted special issue and collection of papers. This letter comprises
two sections. The first section outlines big ideas pertaining to AI, and
answers some of the burning questions with regard to the merit of adopting AI
in our domain. The second section presents a set of rules or technical
recommendations an AI user may deem helpful to practice whenever AI is used as
an investigation methodology. The presented set of rules are complementary to
the big ideas.
</summary>
    <author>
      <name>M. Z. Naser</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10694-021-01210-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10694-021-01210-1" rel="related"/>
    <link href="http://arxiv.org/abs/2111.13756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.13756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01298v2</id>
    <updated>2022-05-19T15:28:01Z</updated>
    <published>2021-11-25T11:05:37Z</published>
    <title>Meaningful human control: actionable properties for AI system
  development</title>
    <summary>  How can humans remain in control of artificial intelligence (AI)-based
systems designed to perform tasks autonomously? Such systems are increasingly
ubiquitous, creating benefits - but also undesirable situations where moral
responsibility for their actions cannot be properly attributed to any
particular person or group. The concept of meaningful human control has been
proposed to address responsibility gaps and mitigate them by establishing
conditions that enable a proper attribution of responsibility for humans;
however, clear requirements for researchers, designers, and engineers are yet
inexistent, making the development of AI-based systems that remain under
meaningful human control challenging. In this paper, we address the gap between
philosophical theory and engineering practice by identifying, through an
iterative process of abductive thinking, four actionable properties for
AI-based systems under meaningful human control, which we discuss making use of
two applications scenarios: automated vehicles and AI-based hiring. First, a
system in which humans and AI algorithms interact should have an explicitly
defined domain of morally loaded situations within which the system ought to
operate. Second, humans and AI agents within the system should have appropriate
and mutually compatible representations. Third, responsibility attributed to a
human should be commensurate with that human's ability and authority to control
the system. Fourth, there should be explicit links between the actions of the
AI agents and actions of humans who are aware of their moral responsibility. We
argue that these four properties will support practically-minded professionals
to take concrete steps toward designing and engineering for AI systems that
facilitate meaningful human control.
</summary>
    <author>
      <name>Luciano Cavalcante Siebert</name>
    </author>
    <author>
      <name>Maria Luce Lupetti</name>
    </author>
    <author>
      <name>Evgeni Aizenberg</name>
    </author>
    <author>
      <name>Niek Beckers</name>
    </author>
    <author>
      <name>Arkady Zgonnikov</name>
    </author>
    <author>
      <name>Herman Veluwenkamp</name>
    </author>
    <author>
      <name>David Abbink</name>
    </author>
    <author>
      <name>Elisa Giaccardi</name>
    </author>
    <author>
      <name>Geert-Jan Houben</name>
    </author>
    <author>
      <name>Catholijn M. Jonker</name>
    </author>
    <author>
      <name>Jeroen van den Hoven</name>
    </author>
    <author>
      <name>Deborah Forster</name>
    </author>
    <author>
      <name>Reginald L. Lagendijk</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s43681-022-00167-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s43681-022-00167-3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. Published AI and Ethics (2022):
  https://doi.org/10.1007/s43681-022-00167-3</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AI Ethics (2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2112.01298v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01298v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.15144v1</id>
    <updated>2022-03-28T23:37:08Z</updated>
    <published>2022-03-28T23:37:08Z</published>
    <title>Human-AI Collaboration Enables More Empathic Conversations in Text-based
  Peer-to-Peer Mental Health Support</title>
    <summary>  Advances in artificial intelligence (AI) are enabling systems that augment
and collaborate with humans to perform simple, mechanistic tasks like
scheduling meetings and grammar-checking text. However, such Human-AI
collaboration poses challenges for more complex, creative tasks, such as
carrying out empathic conversations, due to difficulties of AI systems in
understanding complex human emotions and the open-ended nature of these tasks.
Here, we focus on peer-to-peer mental health support, a setting in which
empathy is critical for success, and examine how AI can collaborate with humans
to facilitate peer empathy during textual, online supportive conversations. We
develop Hailey, an AI-in-the-loop agent that provides just-in-time feedback to
help participants who provide support (peer supporters) respond more
empathically to those seeking help (support seekers). We evaluate Hailey in a
non-clinical randomized controlled trial with real-world peer supporters on
TalkLife (N=300), a large online peer-to-peer support platform. We show that
our Human-AI collaboration approach leads to a 19.60% increase in
conversational empathy between peers overall. Furthermore, we find a larger
38.88% increase in empathy within the subsample of peer supporters who
self-identify as experiencing difficulty providing support. We systematically
analyze the Human-AI collaboration patterns and find that peer supporters are
able to use the AI feedback both directly and indirectly without becoming
overly reliant on AI while reporting improved self-efficacy post-feedback. Our
findings demonstrate the potential of feedback-driven, AI-in-the-loop writing
systems to empower humans in open-ended, social, creative tasks such as
empathic conversations.
</summary>
    <author>
      <name>Ashish Sharma</name>
    </author>
    <author>
      <name>Inna W. Lin</name>
    </author>
    <author>
      <name>Adam S. Miner</name>
    </author>
    <author>
      <name>David C. Atkins</name>
    </author>
    <author>
      <name>Tim Althoff</name>
    </author>
    <link href="http://arxiv.org/abs/2203.15144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.15144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.04793v1</id>
    <updated>2022-05-30T14:54:00Z</updated>
    <published>2022-05-30T14:54:00Z</published>
    <title>Securing AI-based Healthcare Systems using Blockchain Technology: A
  State-of-the-Art Systematic Literature Review and Future Research Directions</title>
    <summary>  Healthcare systems are increasingly incorporating Artificial Intelligence
into their systems, but it is not a solution for all difficulties. AI's
extraordinary potential is being held back by challenges such as a lack of
medical datasets for training AI models, adversarial attacks, and a lack of
trust due to its black box working style. We explored how blockchain technology
can improve the reliability and trustworthiness of AI-based healthcare. This
paper has conducted a Systematic Literature Review to explore the
state-of-the-art research studies conducted in healthcare applications
developed with different AI techniques and Blockchain Technology. This
systematic literature review proceeds with three different paths as natural
language processing-based healthcare systems, computer vision-based healthcare
systems and acoustic AI-based healthcare systems. We found that 1) Defence
techniques for adversarial attacks on AI are available for specific kind of
attacks and even adversarial training is AI based technique which in further
prone to different attacks. 2) Blockchain can address security and privacy
issues in healthcare fraternity. 3) Medical data verification and user
provenance can be enabled with Blockchain. 4) Blockchain can protect
distributed learning on heterogeneous medical data. 5) The issues like single
point of failure, non-transparency in healthcare systems can be resolved with
Blockchain. Nevertheless, it has been identified that research is at the
initial stage. As a result, we have synthesized a conceptual framework using
Blockchain Technology for AI-based healthcare applications that considers the
needs of each NLP, Computer Vision, and Acoustic AI application. A global
solution for all sort of adversarial attacks on AI based healthcare. However,
this technique has significant limits and challenges that need to be addressed
in future studies.
</summary>
    <author>
      <name>Rucha Shinde</name>
    </author>
    <author>
      <name>Shruti Patil</name>
    </author>
    <author>
      <name>Ketan Kotecha</name>
    </author>
    <author>
      <name>Vidyasagar Potdar</name>
    </author>
    <author>
      <name>Ganeshsree Selvachandran</name>
    </author>
    <author>
      <name>Ajith Abraham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">44 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.04793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.04793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.08287v3</id>
    <updated>2022-11-23T13:06:57Z</updated>
    <published>2022-06-16T16:41:23Z</published>
    <title>Definition drives design: Disability models and mechanisms of bias in AI
  technologies</title>
    <summary>  The increasing deployment of artificial intelligence (AI) tools to inform
decision making across diverse areas including healthcare, employment, social
benefits, and government policy, presents a serious risk for disabled people,
who have been shown to face bias in AI implementations. While there has been
significant work on analysing and mitigating algorithmic bias, the broader
mechanisms of how bias emerges in AI applications are not well understood,
hampering efforts to address bias where it begins. In this article, we
illustrate how bias in AI-assisted decision making can arise from a range of
specific design decisions, each of which may seem self-contained and
non-biasing when considered separately. These design decisions include basic
problem formulation, the data chosen for analysis, the use the AI technology is
put to, and operational design elements in addition to the core algorithmic
design. We draw on three historical models of disability common to different
decision-making settings to demonstrate how differences in the definition of
disability can lead to highly distinct decisions on each of these aspects of
design, leading in turn to AI technologies with a variety of biases and
downstream effects. We further show that the potential harms arising from
inappropriate definitions of disability in fundamental design stages are
further amplified by a lack of transparency and disabled participation
throughout the AI design process. Our analysis provides a framework for
critically examining AI technologies in decision-making contexts and guiding
the development of a design praxis for disability-related AI analytics. We put
forth this article to provide key questions to facilitate disability-led design
and participatory development to produce more fair and equitable AI
technologies in disability-related contexts.
</summary>
    <author>
      <name>Denis Newman-Griffis</name>
    </author>
    <author>
      <name>Jessica Sage Rauchberg</name>
    </author>
    <author>
      <name>Rahaf Alharbi</name>
    </author>
    <author>
      <name>Louise Hickman</name>
    </author>
    <author>
      <name>Harry Hochheiser</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 1 figure, 2 tables. Keywords: artificial intelligence;
  critical disability studies; information and communication technologies; data
  analytics; data science; fairness, accountability, transparency, and ethics</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.08287v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.08287v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.02944v1</id>
    <updated>2023-02-06T17:22:18Z</updated>
    <published>2023-02-06T17:22:18Z</published>
    <title>Learning Complementary Policies for Human-AI Teams</title>
    <summary>  Human-AI complementarity is important when neither the algorithm nor the
human yields dominant performance across all instances in a given context.
Recent work that explored human-AI collaboration has considered decisions that
correspond to classification tasks. However, in many important contexts where
humans can benefit from AI complementarity, humans undertake course of action.
In this paper, we propose a framework for a novel human-AI collaboration for
selecting advantageous course of action, which we refer to as Learning
Complementary Policy for Human-AI teams (\textsc{lcp-hai}). Our solution aims
to exploit the human-AI complementarity to maximize decision rewards by
learning both an algorithmic policy that aims to complement humans by a routing
model that defers decisions to either a human or the AI to leverage the
resulting complementarity. We then extend our approach to leverage
opportunities and mitigate risks that arise in important contexts in practice:
1) when a team is composed of multiple humans with differential and potentially
complementary abilities, 2) when the observational data includes consistent
deterministic actions, and 3) when the covariate distribution of future
decisions differ from that in the historical data. We demonstrate the
effectiveness of our proposed methods using data on real human responses and
semi-synthetic, and find that our methods offer reliable and advantageous
performance across setting, and that it is superior to when either the
algorithm or the AI make decisions on their own. We also find that the
extensions we propose effectively improve the robustness of the human-AI
collaboration performance in the presence of different challenging settings.
</summary>
    <author>
      <name>Ruijiang Gao</name>
    </author>
    <author>
      <name>Maytal Saar-Tsechansky</name>
    </author>
    <author>
      <name>Maria De-Arteaga</name>
    </author>
    <author>
      <name>Ligong Han</name>
    </author>
    <author>
      <name>Wei Sun</name>
    </author>
    <author>
      <name>Min Kyung Lee</name>
    </author>
    <author>
      <name>Matthew Lease</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Previous name: Robust Human-AI Collaboration with Bandit Feedback;
  Best student paper award at Conference on Information Systems and Technology
  (CIST), 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.02944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.02944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/quant-ph/0401124v2</id>
    <updated>2011-06-04T20:51:11Z</updated>
    <published>2004-01-20T19:21:56Z</published>
    <title>Quantum Artificial Intelligence</title>
    <summary>  This report introduces researchers in AI to some of the concepts in quantum
heurisitics and quantum AI.
</summary>
    <author>
      <name>B. Aoun</name>
    </author>
    <author>
      <name>M. Tarifi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is withdrawn because it was incomplete at the time of
  posting</arxiv:comment>
    <link href="http://arxiv.org/abs/quant-ph/0401124v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/quant-ph/0401124v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06048v1</id>
    <updated>2016-05-19T16:45:12Z</updated>
    <published>2016-05-19T16:45:12Z</published>
    <title>Philosophy in the Face of Artificial Intelligence</title>
    <summary>  In this article, I discuss how the AI community views concerns about the
emergence of superintelligent AI and related philosophical issues.
</summary>
    <author>
      <name>Vincent Conitzer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Prospect, May 4, 2016.
  http://www.prospectmagazine.co.uk/science-and-technology/artificial-intelligence-wheres-the-philosophical-scrutiny</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09032v1</id>
    <updated>2017-07-27T20:16:32Z</updated>
    <published>2017-07-27T20:16:32Z</published>
    <title>Evolution towards Smart Optical Networking: Where Artificial
  Intelligence (AI) meets the World of Photonics</title>
    <summary>  Smart optical networks are the next evolution of programmable networking and
programmable automation of optical networks, with human-in-the-loop network
control and management. The paper discusses this evolution and the role of
Artificial Intelligence (AI).
</summary>
    <author>
      <name>Admela Jukan</name>
    </author>
    <author>
      <name>Mohit Chamania</name>
    </author>
    <link href="http://arxiv.org/abs/1707.09032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00162v1</id>
    <updated>2016-11-01T09:04:35Z</updated>
    <published>2016-11-01T09:04:35Z</published>
    <title>The Flow of Gauge Transformations on Riemannian Surface with Boundary</title>
    <summary>  We consider the gauge transformations of a metric $G$-bundle over a compact
Riemannian surface with boundary. By employing the heat flow method, the local
existence and the long time existence of generalized solution are proved.
</summary>
    <author>
      <name>Wanjun Ai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s40304-017-0112-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s40304-017-0112-y" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30pages, 3figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.00162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="58J35 (Primary), 58E15 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00163v1</id>
    <updated>2016-11-01T09:18:05Z</updated>
    <published>2016-11-01T09:18:05Z</published>
    <title>Neck Analysis of Extrinsic Polyharmonic Maps</title>
    <summary>  We prove the energy identity and the no neck property for a sequence of
smooth extrinsic polyharmonic maps with bounded total energy.
</summary>
    <author>
      <name>Wanjun Ai</name>
    </author>
    <author>
      <name>Hao Yin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10455-017-9551-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10455-017-9551-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28pages, all comments are welcome!</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.00163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="35J60 (Primary), 35B99 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08512v1</id>
    <updated>2018-08-26T06:52:25Z</updated>
    <published>2018-08-26T06:52:25Z</published>
    <title>Data Motifs: A Lens Towards Fully Understanding Big Data and AI
  Workloads</title>
    <summary>  The complexity and diversity of big data and AI workloads make understanding
them difficult and challenging. This paper proposes a new approach to modelling
and characterizing big data and AI workloads. We consider each big data and AI
workload as a pipeline of one or more classes of units of computation performed
on different initial or intermediate data inputs. Each class of unit of
computation captures the common requirements while being reasonably divorced
from individual implementations, and hence we call it a data motif. For the
first time, among a wide variety of big data and AI workloads, we identify
eight data motifs that take up most of the run time of those workloads,
including Matrix, Sampling, Logic, Transform, Set, Graph, Sort and Statistic.
We implement the eight data motifs on different software stacks as the micro
benchmarks of an open-source big data and AI benchmark suite ---BigDataBench
4.0 (publicly available from http://prof.ict.ac.cn/BigDataBench), and perform
comprehensive characterization of those data motifs from perspective of data
sizes, types, sources, and patterns as a lens towards fully understanding big
data and AI workloads. We believe the eight data motifs are promising
abstractions and tools for not only big data and AI benchmarking, but also
domain-specific hardware and software co-design.
</summary>
    <author>
      <name>Wanling Gao</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Chunjie Luo</name>
    </author>
    <author>
      <name>Daoyi Zheng</name>
    </author>
    <author>
      <name>Fei Tang</name>
    </author>
    <author>
      <name>Biwei Xie</name>
    </author>
    <author>
      <name>Chen Zheng</name>
    </author>
    <author>
      <name>Xu Wen</name>
    </author>
    <author>
      <name>Xiwen He</name>
    </author>
    <author>
      <name>Hainan Ye</name>
    </author>
    <author>
      <name>Rui Ren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper will be published on The 27th International Conference on
  Parallel Architectures and Compilation Techniques (PACT18)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.08512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.00136v3</id>
    <updated>2018-12-20T06:59:22Z</updated>
    <published>2018-12-01T04:01:03Z</published>
    <title>Theory of Cognitive Relativity: A Promising Paradigm for True AI</title>
    <summary>  The rise of deep learning has brought artificial intelligence (AI) to the
forefront. The ultimate goal of AI is to realize machines with human mind and
consciousness, but existing achievements mainly simulate intelligent behavior
on computer platforms. These achievements all belong to weak AI rather than
strong AI. How to achieve strong AI is not known yet in the field of
intelligence science. Currently, this field is calling for a new paradigm,
especially Theory of Cognitive Relativity (TCR). The TCR aims to summarize a
simple and elegant set of first principles about the nature of intelligence, at
least including the Principle of World's Relativity and the Principle of
Symbol's Relativity. The Principle of World's Relativity states that the
subjective world an intelligent agent can observe is strongly constrained by
the way it perceives the objective world. The Principle of Symbol's Relativity
states that an intelligent agent can use any physical symbol system to express
what it observes in its subjective world. The two principles are derived from
scientific facts and life experience. Thought experiments show that they are
important to understand high-level intelligence and necessary to establish a
scientific theory of mind and consciousness. Rather than brain-like
intelligence, the TCR indeed advocates a promising change in direction to
realize true AI, i.e. artificial general intelligence or artificial
consciousness, particularly different from humans' and animals'. Furthermore, a
TCR creed has been presented and extended to reveal the secrets of
consciousness and to guide realization of conscious machines. In the sense that
true AI could be diversely implemented in a brain-different way, the TCR would
probably drive an intelligence revolution in combination with some additional
first principles.
</summary>
    <author>
      <name>Yujian Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages (double spaced), 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.00136v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.00136v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.01184v1</id>
    <updated>2018-12-04T02:59:44Z</updated>
    <published>2018-12-04T02:59:44Z</published>
    <title>Making BREAD: Biomimetic strategies for Artificial Intelligence Now and
  in the Future</title>
    <summary>  The Artificial Intelligence (AI) revolution foretold of during the 1960s is
well underway in the second decade of the 21st century. Its period of
phenomenal growth likely lies ahead. Still, we believe, there are crucial
lessons that biology can offer that will enable a prosperous future for AI. For
machines in general, and for AI's especially, operating over extended periods
or in extreme environments will require energy usage orders of magnitudes more
efficient than exists today. In many operational environments, energy sources
will be constrained. Any plans for AI devices operating in a challenging
environment must begin with the question of how they are powered, where fuel is
located, how energy is stored and made available to the machine, and how long
the machine can operate on specific energy units. Hence, the materials and
technologies that provide the needed energy represent a critical challenge
towards future use-scenarios of AI and should be integrated into their design.
Here we make four recommendations for stakeholders and especially decision
makers to facilitate a successful trajectory for this technology. First, that
scientific societies and governments coordinate Biomimetic Research for
Energy-efficient, AI Designs (BREAD); a multinational initiative and a funding
strategy for investments in the future integrated design of energetics into AI.
Second, that biomimetic energetic solutions be central to design consideration
for future AI. Third, that a pre-competitive space be organized between
stakeholder partners and fourth, that a trainee pipeline be established to
ensure the human capital required for success in this area.
</summary>
    <author>
      <name>Jeffrey L. Krichmar</name>
    </author>
    <author>
      <name>William Severa</name>
    </author>
    <author>
      <name>Salar M. Khan</name>
    </author>
    <author>
      <name>James L. Olds</name>
    </author>
    <link href="http://arxiv.org/abs/1812.01184v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.01184v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.03245v2</id>
    <updated>2019-11-01T18:00:00Z</updated>
    <published>2019-02-08T19:00:02Z</published>
    <title>Ask Not What AI Can Do, But What AI Should Do: Towards a Framework of
  Task Delegability</title>
    <summary>  While artificial intelligence (AI) holds promise for addressing societal
challenges, issues of exactly which tasks to automate and to what extent to do
so remain understudied. We approach this problem of task delegability from a
human-centered perspective by developing a framework on human perception of
task delegation to AI. We consider four high-level factors that can contribute
to a delegation decision: motivation, difficulty, risk, and trust. To obtain an
empirical understanding of human preferences in different tasks, we build a
dataset of 100 tasks from academic papers, popular media portrayal of AI, and
everyday life, and administer a survey based on our proposed framework. We find
little preference for full AI control and a strong preference for
machine-in-the-loop designs, in which humans play the leading role. Among the
four factors, trust is the most correlated with human preferences of optimal
human-machine delegation. This framework represents a first step towards
characterizing human preferences of AI automation across tasks. We hope this
work encourages future efforts towards understanding such individual attitudes;
our goal is to inform the public and the AI research community rather than
dictating any direction in technology development.
</summary>
    <author>
      <name>Brian Lubars</name>
    </author>
    <author>
      <name>Chenhao Tan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 3 figures, 5 tables, NeurIPS 2019, dataset available at
  https://delegability.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.03245v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03245v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.02503v1</id>
    <updated>2019-03-06T17:24:11Z</updated>
    <published>2019-03-06T17:24:11Z</published>
    <title>The AI Driving Olympics at NeurIPS 2018</title>
    <summary>  Despite recent breakthroughs, the ability of deep learning and reinforcement
learning to outperform traditional approaches to control physically embodied
robotic agents remains largely unproven. To help bridge this gap, we created
the 'AI Driving Olympics' (AI-DO), a competition with the objective of
evaluating the state of the art in machine learning and artificial intelligence
for mobile robotics. Based on the simple and well specified autonomous driving
and navigation environment called 'Duckietown', AI-DO includes a series of
tasks of increasing complexity -- from simple lane-following to fleet
management. For each task, we provide tools for competitors to use in the form
of simulators, logs, code templates, baseline implementations and low-cost
access to robotic hardware. We evaluate submissions in simulation online, on
standardized hardware environments, and finally at the competition event. The
first AI-DO, AI-DO 1, occurred at the Neural Information Processing Systems
(NeurIPS) conference in December 2018. The results of AI-DO 1 highlight the
need for better benchmarks, which are lacking in robotics, as well as improved
mechanisms to bridge the gap between simulation and reality.
</summary>
    <author>
      <name>Julian Zilly</name>
    </author>
    <author>
      <name>Jacopo Tani</name>
    </author>
    <author>
      <name>Breandan Considine</name>
    </author>
    <author>
      <name>Bhairav Mehta</name>
    </author>
    <author>
      <name>Andrea F. Daniele</name>
    </author>
    <author>
      <name>Manfred Diaz</name>
    </author>
    <author>
      <name>Gianmarco Bernasconi</name>
    </author>
    <author>
      <name>Claudio Ruch</name>
    </author>
    <author>
      <name>Jan Hakenberg</name>
    </author>
    <author>
      <name>Florian Golemo</name>
    </author>
    <author>
      <name>A. Kirsten Bowser</name>
    </author>
    <author>
      <name>Matthew R. Walter</name>
    </author>
    <author>
      <name>Ruslan Hristov</name>
    </author>
    <author>
      <name>Sunil Mallya</name>
    </author>
    <author>
      <name>Emilio Frazzoli</name>
    </author>
    <author>
      <name>Andrea Censi</name>
    </author>
    <author>
      <name>Liam Paull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Competition, robotics, safety-critical AI, self-driving cars,
  autonomous mobility on demand, Duckietown</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.02503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.02503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.01023v1</id>
    <updated>2019-05-02T11:21:45Z</updated>
    <published>2019-05-02T11:21:45Z</published>
    <title>Physicist's Journeys Through the AI World - A Topical Review. There is
  no royal road to unsupervised learning</title>
    <summary>  Artificial Intelligence (AI), defined in its most simple form, is a
technological tool that makes machines intelligent. Since learning is at the
core of intelligence, machine learning poses itself as a core sub-field of AI.
Then there comes a subclass of machine learning, known as deep learning, to
address the limitations of their predecessors. AI has generally acquired its
prominence over the past few years due to its considerable progress in various
fields. AI has vastly invaded the realm of research. This has led physicists to
attentively direct their research towards implementing AI tools. Their central
aim has been to gain better understanding and enrich their intuition. This
review article is meant to supplement the previously presented efforts to
bridge the gap between AI and physics, and take a serious step forward to
filter out the "Babelian" clashes brought about from such gabs. This
necessitates first to have fundamental knowledge about common AI tools. To this
end, the review's primary focus shall be on deep learning models called
artificial neural networks. They are deep learning models which train
themselves through different learning processes. It discusses also the concept
of Markov decision processes. Finally, shortcut to the main goal, the review
thoroughly examines how these neural networks are capable to construct a
physical theory describing some observations without applying any previous
physical knowledge.
</summary>
    <author>
      <name>Imad Alhousseini</name>
    </author>
    <author>
      <name>Wissam Chemissany</name>
    </author>
    <author>
      <name>Fatima Kleit</name>
    </author>
    <author>
      <name>Aly Nasrallah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 10 figures, 2 appendices, 5 algorithms</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.01023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.05505v1</id>
    <updated>2019-07-11T22:02:18Z</updated>
    <published>2019-07-11T22:02:18Z</published>
    <title>Artificial Intelligence as a Services (AI-aaS) on Software-Defined
  Infrastructure</title>
    <summary>  This paper investigates a paradigm for offering artificial intelligence as a
service (AI-aaS) on software-defined infrastructures (SDIs). The increasing
complexity of networking and computing infrastructures is already driving the
introduction of automation in networking and cloud computing management
systems. Here we consider how these automation mechanisms can be leveraged to
offer AI-aaS. Use cases for AI-aaS are easily found in addressing smart
applications in sectors such as transportation, manufacturing, energy, water,
air quality, and emissions. We propose an architectural scheme based on SDIs
where each AI-aaS application is comprised of a monitoring, analysis, policy,
execution plus knowledge (MAPE-K) loop (MKL). Each application is composed as
one or more specific service chains embedded in SDI, some of which will include
a Machine Learning (ML) pipeline. Our model includes a new training plane and
an AI-aaS plane to deal with the model-development and operational phases of AI
applications. We also consider the role of an ML/MKL sandbox in ensuring
coherency and consistency in the operation of multiple parallel MKL loops. We
present experimental measurement results for three AI-aaS applications deployed
on the SAVI testbed: 1. Compressing monitored data in SDI using autoencoders;
2. Traffic monitoring to allocate CPUs resources to VNFs; and 3. Highway
segment classification in smart transportation.
</summary>
    <author>
      <name>Saeedeh Parsaeefard</name>
    </author>
    <author>
      <name>Iman Tabrizian</name>
    </author>
    <author>
      <name>Alberto Leon-Garcia</name>
    </author>
    <link href="http://arxiv.org/abs/1907.05505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.05505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.10332v1</id>
    <updated>2017-12-21T19:03:14Z</updated>
    <published>2017-12-21T19:03:14Z</published>
    <title>Integrals containing the logarithm of the Airy Function Ai'(x)</title>
    <summary>  Integrals occurring in Thomas-Fermi theory which contains the logarithm of
the Airy function Ai'(x) have been obtained in terms of analytical expressions.
</summary>
    <author>
      <name>Bernard J. Laurenzi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.10332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.10332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.11872v3</id>
    <updated>2020-06-20T18:17:48Z</updated>
    <published>2019-11-26T22:47:34Z</published>
    <title>Artificial Intelligence-Based Image Classification for Diagnosis of Skin
  Cancer: Challenges and Opportunities</title>
    <summary>  Recently, there has been great interest in developing Artificial Intelligence
(AI) enabled computer-aided diagnostics solutions for the diagnosis of skin
cancer. With the increasing incidence of skin cancers, low awareness among a
growing population, and a lack of adequate clinical expertise and services,
there is an immediate need for AI systems to assist clinicians in this domain.
A large number of skin lesion datasets are available publicly, and researchers
have developed AI-based image classification solutions, particularly deep
learning algorithms, to distinguish malignant skin lesions from benign lesions
in different image modalities such as dermoscopic, clinical, and histopathology
images. Despite the various claims of AI systems achieving higher accuracy than
dermatologists in the classification of different skin lesions, these AI
systems are still in the very early stages of clinical application in terms of
being ready to aid clinicians in the diagnosis of skin cancers. In this review,
we discuss advancements in the digital image-based AI solutions for the
diagnosis of skin cancer, along with some challenges and future opportunities
to improve these AI systems to support dermatologists and enhance their ability
to diagnose skin cancer.
</summary>
    <author>
      <name>Manu Goyal</name>
    </author>
    <author>
      <name>Thomas Knackstedt</name>
    </author>
    <author>
      <name>Shaofeng Yan</name>
    </author>
    <author>
      <name>Saeed Hassanpour</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AI Skin Cancer</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.11872v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.11872v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.11595v2</id>
    <updated>2020-01-24T18:43:43Z</updated>
    <published>2019-12-25T05:30:40Z</published>
    <title>The Windfall Clause: Distributing the Benefits of AI for the Common Good</title>
    <summary>  As the transformative potential of AI has become increasingly salient as a
matter of public and political interest, there has been growing discussion
about the need to ensure that AI broadly benefits humanity. This in turn has
spurred debate on the social responsibilities of large technology companies to
serve the interests of society at large. In response, ethical principles and
codes of conduct have been proposed to meet the escalating demand for this
responsibility to be taken seriously. As yet, however, few institutional
innovations have been suggested to translate this responsibility into legal
commitments which apply to companies positioned to reap large financial gains
from the development and use of AI. This paper offers one potentially
attractive tool for addressing such issues: the Windfall Clause, which is an ex
ante commitment by AI firms to donate a significant amount of any eventual
extremely large profits. By this we mean an early commitment that profits that
a firm could not earn without achieving fundamental, economically
transformative breakthroughs in AI capabilities will be donated to benefit
humanity broadly, with particular attention towards mitigating any downsides
from deployment of windfall-generating AI.
</summary>
    <author>
      <name>Cullen O'Keefe</name>
    </author>
    <author>
      <name>Peter Cihon</name>
    </author>
    <author>
      <name>Ben Garfinkel</name>
    </author>
    <author>
      <name>Carrick Flynn</name>
    </author>
    <author>
      <name>Jade Leung</name>
    </author>
    <author>
      <name>Allan Dafoe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Short version to be published in proceedings of AIES</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.11595v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11595v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.05671v2</id>
    <updated>2020-07-09T11:32:14Z</updated>
    <published>2020-02-12T11:26:44Z</published>
    <title>AI safety: state of the field through quantitative lens</title>
    <summary>  Last decade has seen major improvements in the performance of artificial
intelligence which has driven wide-spread applications. Unforeseen effects of
such mass-adoption has put the notion of AI safety into the public eye. AI
safety is a relatively new field of research focused on techniques for building
AI beneficial for humans. While there exist survey papers for the field of AI
safety, there is a lack of a quantitative look at the research being conducted.
The quantitative aspect gives a data-driven insight about the emerging trends,
knowledge gaps and potential areas for future research. In this paper,
bibliometric analysis of the literature finds significant increase in research
activity since 2015. Also, the field is so new that most of the technical
issues are open, including: explainability with its long-term utility, and
value alignment which we have identified as the most important long-term
research topic. Equally, there is a severe lack of research into concrete
policies regarding AI. As we expect AI to be the one of the main driving forces
of changes in society, AI safety is the field under which we need to decide the
direction of humanity's future.
</summary>
    <author>
      <name>Mislav Juric</name>
    </author>
    <author>
      <name>Agneza Sandic</name>
    </author>
    <author>
      <name>Mario Brcic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2020 43rd International Convention on Information and Communication
  Technology, Electronics and Microelectronics (MIPRO)</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.05671v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05671v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.12450v1</id>
    <updated>2020-02-27T21:23:27Z</updated>
    <published>2020-02-27T21:23:27Z</published>
    <title>Do ML Experts Discuss Explainability for AI Systems? A discussion case
  in the industry for a domain-specific solution</title>
    <summary>  The application of Artificial Intelligence (AI) tools in different domains
are becoming mandatory for all companies wishing to excel in their industries.
One major challenge for a successful application of AI is to combine the
machine learning (ML) expertise with the domain knowledge to have the best
results applying AI tools. Domain specialists have an understanding of the data
and how it can impact their decisions. ML experts have the ability to use
AI-based tools dealing with large amounts of data and generating insights for
domain experts. But without a deep understanding of the data, ML experts are
not able to tune their models to get optimal results for a specific domain.
Therefore, domain experts are key users for ML tools and the explainability of
those AI tools become an essential feature in that context. There are a lot of
efforts to research AI explainability for different contexts, users and goals.
In this position paper, we discuss interesting findings about how ML experts
can express concerns about AI explainability while defining features of an ML
tool to be developed for a specific domain. We analyze data from two brainstorm
sessions done to discuss the functionalities of an ML tool to support
geoscientists (domain experts) on analyzing seismic data (domain-specific data)
with ML resources.
</summary>
    <author>
      <name>Juliana Jansen Ferreira</name>
    </author>
    <author>
      <name>Mateus de Souza Monteiro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, IUI workshop on Explainable Smart Systems and Algorithmic
  Transparency in Emerging Technologies (ExSS-ATEC'20)</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.12450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.12450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.03459v4</id>
    <updated>2021-09-05T13:44:38Z</updated>
    <published>2020-05-06T01:24:25Z</published>
    <title>AIBench Scenario: Scenario-distilling AI Benchmarking</title>
    <summary>  Modern real-world application scenarios like Internet services consist of a
diversity of AI and non-AI modules with huge code sizes and long and
complicated execution paths, which raises serious benchmarking or evaluating
challenges. Using AI components or micro benchmarks alone can lead to
error-prone conclusions. This paper presents a methodology to attack the above
challenge. We formalize a real-world application scenario as a Directed Acyclic
Graph-based model and propose the rules to distill it into a permutation of
essential AI and non-AI tasks, which we call a scenario benchmark. Together
with seventeen industry partners, we extract nine typical scenario benchmarks.
We design and implement an extensible, configurable, and flexible benchmark
framework. We implement two Internet service AI scenario benchmarks based on
the framework as proxies to two real-world application scenarios. We consider
scenario, component, and micro benchmarks as three indispensable parts for
evaluating. Our evaluation shows the advantage of our methodology against using
component or micro AI benchmarks alone. The specifications, source code,
testbed, and results are publicly available from
\url{https://www.benchcouncil.org/aibench/scenario/}.
</summary>
    <author>
      <name>Wanling Gao</name>
    </author>
    <author>
      <name>Fei Tang</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <author>
      <name>Xu Wen</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Zheng Cao</name>
    </author>
    <author>
      <name>Chuanxin Lan</name>
    </author>
    <author>
      <name>Chunjie Luo</name>
    </author>
    <author>
      <name>Xiaoli Liu</name>
    </author>
    <author>
      <name>Zihan Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted by The 30th International Conference on
  Parallel Architectures and Compilation Techniques (PACT 2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.03459v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.03459v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.07211v1</id>
    <updated>2020-06-12T14:15:02Z</updated>
    <published>2020-06-12T14:15:02Z</published>
    <title>The Threats of Artificial Intelligence Scale (TAI). Development,
  Measurement and Test Over Three Application Domains</title>
    <summary>  In recent years Artificial Intelligence (AI) has gained much popularity, with
the scientific community as well as with the public. AI is often ascribed many
positive impacts for different social domains such as medicine and the economy.
On the other side, there is also growing concern about its precarious impact on
society and individuals. Several opinion polls frequently query the public fear
of autonomous robots and artificial intelligence (FARAI), a phenomenon coming
also into scholarly focus. As potential threat perceptions arguably vary with
regard to the reach and consequences of AI functionalities and the domain of
application, research still lacks necessary precision of a respective
measurement that allows for wide-spread research applicability. We propose a
fine-grained scale to measure threat perceptions of AI that accounts for four
functional classes of AI systems and is applicable to various domains of AI
applications. Using a standardized questionnaire in a survey study (N=891), we
evaluate the scale over three distinct AI domains (loan origination, job
recruitment and medical treatment). The data support the dimensional structure
of the proposed Threats of AI (TAI) scale as well as the internal consistency
and factoral validity of the indicators. Implications of the results and the
empirical application of the scale are discussed in detail. Recommendations for
further empirical use of the TAI scale are provided.
</summary>
    <author>
      <name>Kimon Kieslich</name>
    </author>
    <author>
      <name>Marco Lünich</name>
    </author>
    <author>
      <name>Frank Marcinkowski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s12369-020-00734-w</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s12369-020-00734-w" rel="related"/>
    <link href="http://arxiv.org/abs/2006.07211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.07211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.12683v6</id>
    <updated>2022-12-08T04:50:30Z</updated>
    <published>2020-06-23T01:02:15Z</published>
    <title>Improving Workflow Integration with xPath: Design and Evaluation of a
  Human-AI Diagnosis System in Pathology</title>
    <summary>  Recent developments in AI have provided assisting tools to support
pathologists' diagnoses. However, it remains challenging to incorporate such
tools into pathologists' practice; one main concern is AI's insufficient
workflow integration with medical decisions. We observed pathologists'
examination and discovered that the main hindering factor to integrate AI is
its incompatibility with pathologists' workflow. To bridge the gap between
pathologists and AI, we developed a human-AI collaborative diagnosis tool --
xPath -- that shares a similar examination process to that of pathologists,
which can improve AI's integration into their routine examination. The
viability of xPath is confirmed by a technical evaluation and work sessions
with twelve medical professionals in pathology. This work identifies and
addresses the challenge of incorporating AI models into pathology, which can
offer first-hand knowledge about how HCI researchers can work with medical
professionals side-by-side to bring technological advances to medical tasks
towards practical applications.
</summary>
    <author>
      <name>Hongyan Gu</name>
    </author>
    <author>
      <name>Yuan Liang</name>
    </author>
    <author>
      <name>Yifan Xu</name>
    </author>
    <author>
      <name>Christopher Kazu Williams</name>
    </author>
    <author>
      <name>Shino Magaki</name>
    </author>
    <author>
      <name>Negar Khanlou</name>
    </author>
    <author>
      <name>Harry Vinters</name>
    </author>
    <author>
      <name>Zesheng Chen</name>
    </author>
    <author>
      <name>Shuo Ni</name>
    </author>
    <author>
      <name>Chunxu Yang</name>
    </author>
    <author>
      <name>Wenzhong Yan</name>
    </author>
    <author>
      <name>Xinhai Robert Zhang</name>
    </author>
    <author>
      <name>Yang Li</name>
    </author>
    <author>
      <name>Mohammad Haeri</name>
    </author>
    <author>
      <name>Xiang 'Anthony' Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 13 figures. Accepted ACM Transactions on Computer-Human
  Interaction</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.12683v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.12683v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.12391v6</id>
    <updated>2021-07-02T11:35:08Z</updated>
    <published>2020-07-24T07:29:52Z</published>
    <title>Artificial Intelligence in the Creative Industries: A Review</title>
    <summary>  This paper reviews the current state of the art in Artificial Intelligence
(AI) technologies and applications in the context of the creative industries. A
brief background of AI, and specifically Machine Learning (ML) algorithms, is
provided including Convolutional Neural Network (CNNs), Generative Adversarial
Networks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement
Learning (DRL). We categorise creative applications into five groups related to
how AI technologies are used: i) content creation, ii) information analysis,
iii) content enhancement and post production workflows, iv) information
extraction and enhancement, and v) data compression. We critically examine the
successes and limitations of this rapidly advancing technology in each of these
areas. We further differentiate between the use of AI as a creative tool and
its potential as a creator in its own right. We foresee that, in the near
future, machine learning-based AI will be adopted widely as a tool or
collaborative assistant for creativity. In contrast, we observe that the
successes of machine learning in domains with fewer constraints, where AI is
the `creator', remain modest. The potential of AI (or its developers) to win
awards for its original creations in competition with human creatives is also
limited, based on contemporary technologies. We therefore conclude that, in the
context of creative industries, maximum benefit from AI will be derived where
its focus is human centric -- where it is designed to augment, rather than
replace, human creativity.
</summary>
    <author>
      <name>Nantheera Anantrasirichai</name>
    </author>
    <author>
      <name>David Bull</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10462-021-10039-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10462-021-10039-7" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Artif Intell Rev (2021) 1-68</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2007.12391v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.12391v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.07326v1</id>
    <updated>2020-08-11T09:46:00Z</updated>
    <published>2020-08-11T09:46:00Z</published>
    <title>Progressing Towards Responsible AI</title>
    <summary>  The field of Artificial Intelligence (AI) and, in particular, the Machine
Learning area, counts on a wide range of performance metrics and benchmark data
sets to assess the problem-solving effectiveness of its solutions. However, the
appearance of research centres, projects or institutions addressing AI
solutions from a multidisciplinary and multi-stakeholder perspective suggests a
new approach to assessment comprising ethical guidelines, reports or tools and
frameworks to help both academia and business to move towards a responsible
conceptualisation of AI. They all highlight the relevance of three key aspects:
(i) enhancing cooperation among the different stakeholders involved in the
design, deployment and use of AI; (ii) promoting multidisciplinary dialogue,
including different domains of expertise in this process; and (iii) fostering
public engagement to maximise a trusted relation with new technologies and
practitioners. In this paper, we introduce the Observatory on Society and
Artificial Intelligence (OSAI), an initiative grew out of the project AI4EU
aimed at stimulating reflection on a broad spectrum of issues of AI (ethical,
legal, social, economic and cultural). In particular, we describe our work in
progress around OSAI and suggest how this and similar initiatives can promote a
wider appraisal of progress in AI. This will give us the opportunity to present
our vision and our modus operandi to enhance the implementation of these three
fundamental dimensions.
</summary>
    <author>
      <name>Teresa Scantamburlo</name>
    </author>
    <author>
      <name>Atia Cortés</name>
    </author>
    <author>
      <name>Marie Schacht</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">1st International Workshop on Evaluating Progress in AI (EPAI)
  held in conjunction with ECAI 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2008.07326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.07326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.13996v1</id>
    <updated>2020-09-29T13:29:44Z</updated>
    <published>2020-09-29T13:29:44Z</published>
    <title>Explainable AI without Interpretable Model</title>
    <summary>  Explainability has been a challenge in AI for as long as AI has existed. With
the recently increased use of AI in society, it has become more important than
ever that AI systems would be able to explain the reasoning behind their
results also to end-users in situations such as being eliminated from a
recruitment process or having a bank loan application refused by an AI system.
Especially if the AI system has been trained using Machine Learning, it tends
to contain too many parameters for them to be analysed and understood, which
has caused them to be called `black-box' systems. Most Explainable AI (XAI)
methods are based on extracting an interpretable model that can be used for
producing explanations. However, the interpretable model does not necessarily
map accurately to the original black-box model. Furthermore, the
understandability of interpretable models for an end-user remains questionable.
The notions of Contextual Importance and Utility (CIU) presented in this paper
make it possible to produce human-like explanations of black-box outcomes
directly, without creating an interpretable model. Therefore, CIU explanations
map accurately to the black-box model itself. CIU is completely model-agnostic
and can be used with any black-box system. In addition to feature importance,
the utility concept that is well-known in Decision Theory provides a new
dimension to explanations compared to most existing XAI methods. Finally, CIU
can produce explanations at any level of abstraction and using different
vocabularies and other means of interaction, which makes it possible to adjust
explanations and interaction according to the context and to the target users.
</summary>
    <author>
      <name>Kary Främling</name>
    </author>
    <link href="http://arxiv.org/abs/2009.13996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.13996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.04827v1</id>
    <updated>2020-10-09T22:12:22Z</updated>
    <published>2020-10-09T22:12:22Z</published>
    <title>Towards Self-Regulating AI: Challenges and Opportunities of AI Model
  Governance in Financial Services</title>
    <summary>  AI systems have found a wide range of application areas in financial
services. Their involvement in broader and increasingly critical decisions has
escalated the need for compliance and effective model governance. Current
governance practices have evolved from more traditional financial applications
and modeling frameworks. They often struggle with the fundamental differences
in AI characteristics such as uncertainty in the assumptions, and the lack of
explicit programming. AI model governance frequently involves complex review
flows and relies heavily on manual steps. As a result, it faces serious
challenges in effectiveness, cost, complexity, and speed. Furthermore, the
unprecedented rate of growth in the AI model complexity raises questions on the
sustainability of the current practices. This paper focuses on the challenges
of AI model governance in the financial services industry. As a part of the
outlook, we present a system-level framework towards increased self-regulation
for robustness and compliance. This approach aims to enable potential solution
opportunities through increased automation and the integration of monitoring,
management, and mitigation capabilities. The proposed framework also provides
model governance and risk management improved capabilities to manage model risk
during deployment.
</summary>
    <author>
      <name>Eren Kurshan</name>
    </author>
    <author>
      <name>Hongda Shen</name>
    </author>
    <author>
      <name>Jiahao Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3383455.3422564</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3383455.3422564" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 1st International Conference on AI in Finance
  (ICAIF '20), October 15-16, 2020, New York</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.04827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.04827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01, 91-02" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.15581v1</id>
    <updated>2020-10-22T15:11:14Z</updated>
    <published>2020-10-22T15:11:14Z</published>
    <title>The De-democratization of AI: Deep Learning and the Compute Divide in
  Artificial Intelligence Research</title>
    <summary>  Increasingly, modern Artificial Intelligence (AI) research has become more
computationally intensive. However, a growing concern is that due to unequal
access to computing power, only certain firms and elite universities have
advantages in modern AI research. Using a novel dataset of 171394 papers from
57 prestigious computer science conferences, we document that firms, in
particular, large technology firms and elite universities have increased
participation in major AI conferences since deep learning's unanticipated rise
in 2012. The effect is concentrated among elite universities, which are ranked
1-50 in the QS World University Rankings. Further, we find two strategies
through which firms increased their presence in AI research: first, they have
increased firm-only publications; and second, firms are collaborating primarily
with elite universities. Consequently, this increased presence of firms and
elite universities in AI research has crowded out mid-tier (QS ranked 201-300)
and lower-tier (QS ranked 301-500) universities. To provide causal evidence
that deep learning's unanticipated rise resulted in this divergence, we
leverage the generalized synthetic control method, a data-driven counterfactual
estimator. Using machine learning based text analysis methods, we provide
additional evidence that the divergence between these two groups - large firms
and non-elite universities - is driven by access to computing power or compute,
which we term as the "compute divide". This compute divide between large firms
and non-elite universities increases concerns around bias and fairness within
AI technology, and presents an obstacle towards "democratizing" AI. These
results suggest that a lack of access to specialized equipment such as compute
can de-democratize knowledge production.
</summary>
    <author>
      <name>Nur Ahmed</name>
    </author>
    <author>
      <name>Muntasir Wahed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">52 pages,13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.15581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.15581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.00250v1</id>
    <updated>2021-01-01T15:11:28Z</updated>
    <published>2021-01-01T15:11:28Z</published>
    <title>Interplay between RIS and AI in Wireless Communications: Fundamentals,
  Architectures, Applications, and Open Research Problems</title>
    <summary>  Future wireless communication networks are expected to fulfill the
unprecedented performance requirements to support our highly digitized and
globally data-driven society. Various technological challenges must be overcome
to achieve our goal. Among many potential technologies, reconfigurable
intelligent surface (RIS) and artificial intelligence (AI) have attracted
extensive attention, thereby leading to a proliferation of studies for
utilizing them in wireless communication systems. The RIS-based wireless
communication frameworks and AI-enabled technologies, two of the promising
technologies for the sixth-generation networks, interact and promote with each
other, striving to collaboratively create a controllable, intelligent,
reconfigurable, and programmable wireless propagation environment. This paper
explores the road to implementing the combination of RIS and AI; specifically,
integrating AI-enabled technologies into RIS-based frameworks for maximizing
the practicality of RIS to facilitate the realization of smart radio
propagation environments, elaborated from shallow to deep insights. We begin
with the basic concept and fundamental characteristics of RIS, followed by the
overview of the research status of RIS. Then, we analyze the inevitable trend
of RIS to be combined with AI. In particular, we focus on recent research about
RIS-based architectures embedded with AI, elucidating from the intelligent
structures and systems of metamaterials to the AI-embedded RIS-assisted
wireless communication systems. Finally, the challenges and potential of the
topic are discussed.
</summary>
    <author>
      <name>Jinghe Wang</name>
    </author>
    <author>
      <name>Wankai Tang</name>
    </author>
    <author>
      <name>Yu Han</name>
    </author>
    <author>
      <name>Shi Jin</name>
    </author>
    <author>
      <name>Xiao Li</name>
    </author>
    <author>
      <name>Chao-Kai Wen</name>
    </author>
    <author>
      <name>Qiang Cheng</name>
    </author>
    <author>
      <name>Tie Jun Cui</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JSAC.2021.3087259</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JSAC.2021.3087259" rel="related"/>
    <link href="http://arxiv.org/abs/2101.00250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.02008v1</id>
    <updated>2020-12-27T07:41:26Z</updated>
    <published>2020-12-27T07:41:26Z</published>
    <title>An Ecosystem Approach to Ethical AI and Data Use: Experimental
  Reflections</title>
    <summary>  While we have witnessed a rapid growth of ethics documents meant to guide AI
development, the promotion of AI ethics has nonetheless proceeded with little
input from AI practitioners themselves. Given the proliferation of AI for
Social Good initiatives, this is an emerging gap that needs to be addressed in
order to develop more meaningful ethical approaches to AI use and development.
This paper offers a methodology, a shared fairness approach, aimed at
identifying the needs of AI practitioners when it comes to confronting and
resolving ethical challenges and to find a third space where their operational
language can be married with that of the more abstract principles that
presently remain at the periphery of their work experiences. We offer a
grassroots approach to operational ethics based on dialog and mutualised
responsibility. This methodology is centred around conversations intended to
elicit practitioners perceived ethical attribution and distribution over key
value laden operational decisions, to identify when these decisions arise and
what ethical challenges they confront, and to engage in a language of ethics
and responsibility which enables practitioners to internalise ethical
responsibility. The methodology bridges responsibility imbalances that rest in
structural decision making power and elite technical knowledge, by commencing
with personal, facilitated conversations, returning the ethical discourse to
those meant to give it meaning at the sharp end of the ecosystem. Our primary
contribution is to add to the recent literature seeking to bring AI
practitioners' experiences to the fore by offering a methodology for
understanding how ethics manifests as a relational and interdependent
sociotechnical practice in their work.
</summary>
    <author>
      <name>Mark Findlay</name>
    </author>
    <author>
      <name>Josephine Seah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the 2020 IEEE / ITU International Conference on
  Artificial Intelligence for Good</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.02008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.02008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.07570v1</id>
    <updated>2021-01-19T11:26:19Z</updated>
    <published>2021-01-19T11:26:19Z</published>
    <title>Creation and Evaluation of a Pre-tertiary Artificial Intelligence (AI)
  Curriculum</title>
    <summary>  Contributions: The Chinese University of Hong Kong (CUHK)-Jockey Club AI for
the Future Project (AI4Future) co-created an AI curriculum for pre-tertiary
education and evaluated its efficacy. While AI is conventionally taught in
tertiary level education, our co-creation process successfully developed the
curriculum that has been used in secondary school teaching in Hong Kong and
received positive feedback. Background: AI4Future is a cross-sector project
that engages five major partners - CUHK Faculty of Engineering and Faculty of
Education, Hong Kong secondary schools, the government and the AI industry. A
team of 14 professors with expertise in engineering and education collaborated
with 17 principals and teachers from 6 secondary schools to co-create the
curriculum. This team formation bridges the gap between researchers in
engineering and education, together with practitioners in education context.
Research Questions: What are the main features of the curriculum content
developed through the co-creation process? Would the curriculum significantly
improve the students perceived competence in, as well as attitude and
motivation towards AI? What are the teachers perceptions of the co-creation
process that aims to accommodate and foster teacher autonomy? Methodology: This
study adopted a mix of quantitative and qualitative methods and involved 335
student participants. Findings: 1) two main features of learning resources, 2)
the students perceived greater competence, and developed more positive attitude
to learn AI, and 3) the co-creation process generated a variety of resources
which enhanced the teachers knowledge in AI, as well as fostered teachers
autonomy in bringing the subject matter into their classrooms.
</summary>
    <author>
      <name>Thomas K. F. Chiu</name>
    </author>
    <author>
      <name>Helen Meng</name>
    </author>
    <author>
      <name>Ching-Sing Chai</name>
    </author>
    <author>
      <name>Irwin King</name>
    </author>
    <author>
      <name>Savio Wong</name>
    </author>
    <author>
      <name>Yeung Yam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.07570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.07570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.01740v1</id>
    <updated>2021-02-02T20:25:23Z</updated>
    <published>2021-02-02T20:25:23Z</published>
    <title>Reliability Analysis of Artificial Intelligence Systems Using Recurrent
  Events Data from Autonomous Vehicles</title>
    <summary>  Artificial intelligence (AI) systems have become increasingly common and the
trend will continue. Examples of AI systems include autonomous vehicles (AV),
computer vision, natural language processing, and AI medical experts. To allow
for safe and effective deployment of AI systems, the reliability of such
systems needs to be assessed. Traditionally, reliability assessment is based on
reliability test data and the subsequent statistical modeling and analysis. The
availability of reliability data for AI systems, however, is limited because
such data are typically sensitive and proprietary. The California Department of
Motor Vehicles (DMV) oversees and regulates an AV testing program, in which
many AV manufacturers are conducting AV road tests. Manufacturers participating
in the program are required to report recurrent disengagement events to
California DMV. This information is being made available to the public. In this
paper, we use recurrent disengagement events as a representation of the
reliability of the AI system in AV, and propose a statistical framework for
modeling and analyzing the recurrent events data from AV driving tests. We use
traditional parametric models in software reliability and propose a new
nonparametric model based on monotonic splines to describe the event process.
We develop inference procedures for selecting the best models, quantifying
uncertainty, and testing heterogeneity in the event process. We then analyze
the recurrent events data from four AV manufacturers, and make inferences on
the reliability of the AI systems in AV. We also describe how the proposed
analysis can be applied to assess the reliability of other AI systems.
</summary>
    <author>
      <name>Yili Hong</name>
    </author>
    <author>
      <name>Jie Min</name>
    </author>
    <author>
      <name>Caleb B. King</name>
    </author>
    <author>
      <name>William Q. Meeker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.01740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.01740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.09783v1</id>
    <updated>2021-03-17T17:11:43Z</updated>
    <published>2021-03-17T17:11:43Z</published>
    <title>Characterizing Technical Debt and Antipatterns in AI-Based Systems: A
  Systematic Mapping Study</title>
    <summary>  Background: With the rising popularity of Artificial Intelligence (AI), there
is a growing need to build large and complex AI-based systems in a
cost-effective and manageable way. Like with traditional software, Technical
Debt (TD) will emerge naturally over time in these systems, therefore leading
to challenges and risks if not managed appropriately. The influence of data
science and the stochastic nature of AI-based systems may also lead to new
types of TD or antipatterns, which are not yet fully understood by researchers
and practitioners. Objective: The goal of our study is to provide a clear
overview and characterization of the types of TD (both established and new
ones) that appear in AI-based systems, as well as the antipatterns and related
solutions that have been proposed. Method: Following the process of a
systematic mapping study, 21 primary studies are identified and analyzed.
Results: Our results show that (i) established TD types, variations of them,
and four new TD types (data, model, configuration, and ethics debt) are present
in AI-based systems, (ii) 72 antipatterns are discussed in the literature, the
majority related to data and model deficiencies, and (iii) 46 solutions have
been proposed, either to address specific TD types, antipatterns, or TD in
general. Conclusions: Our results can support AI professionals with reasoning
about and communicating aspects of TD present in their systems. Additionally,
they can serve as a foundation for future research to further our understanding
of TD in AI-based systems.
</summary>
    <author>
      <name>Justus Bogner</name>
    </author>
    <author>
      <name>Roberto Verdecchia</name>
    </author>
    <author>
      <name>Ilias Gerostathopoulos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TechDebt52882.2021.00016</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TechDebt52882.2021.00016" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the 4th International Conference on Technical Debt
  (TechDebt 2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.09783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.09783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.15750v2</id>
    <updated>2021-03-30T15:04:40Z</updated>
    <published>2021-03-25T16:29:55Z</published>
    <title>Enabling Design Methodologies and Future Trends for Edge AI:
  Specialization and Co-design</title>
    <summary>  Artificial intelligence (AI) technologies have dramatically advanced in
recent years, resulting in revolutionary changes in people's lives. Empowered
by edge computing, AI workloads are migrating from centralized cloud
architectures to distributed edge systems, introducing a new paradigm called
edge AI. While edge AI has the promise of bringing significant increases in
autonomy and intelligence into everyday lives through common edge devices, it
also raises new challenges, especially for the development of its algorithms
and the deployment of its services, which call for novel design methodologies
catered to these unique challenges. In this paper, we provide a comprehensive
survey of the latest enabling design methodologies that span the entire edge AI
development stack. We suggest that the key methodologies for effective edge AI
development are single-layer specialization and cross-layer co-design. We
discuss representative methodologies in each category in detail, including
on-device training methods, specialized software design, dedicated hardware
design, benchmarking and design automation, software/hardware co-design,
software/compiler co-design, and compiler/hardware co-design. Moreover, we
attempt to reveal hidden cross-layer design opportunities that can further
boost the solution quality of future edge AI and provide insights into future
directions and emerging areas that require increased research focus.
</summary>
    <author>
      <name>Cong Hao</name>
    </author>
    <author>
      <name>Jordan Dotzel</name>
    </author>
    <author>
      <name>Jinjun Xiong</name>
    </author>
    <author>
      <name>Luca Benini</name>
    </author>
    <author>
      <name>Zhiru Zhang</name>
    </author>
    <author>
      <name>Deming Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE Design &amp; Test (D&amp;T)</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.15750v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15750v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.07237v2</id>
    <updated>2021-05-05T15:36:44Z</updated>
    <published>2021-04-15T05:07:11Z</published>
    <title>Skilled and Mobile: Survey Evidence of AI Researchers' Immigration
  Preferences</title>
    <summary>  Countries, companies, and universities are increasingly competing over
top-tier artificial intelligence (AI) researchers. Where are these researchers
likely to immigrate and what affects their immigration decisions? We conducted
a survey $(n = 524)$ of the immigration preferences and motivations of
researchers that had papers accepted at one of two prestigious AI conferences:
the Conference on Neural Information Processing Systems (NeurIPS) and the
International Conference on Machine Learning (ICML). We find that the U.S. is
the most popular destination for AI researchers, followed by the U.K., Canada,
Switzerland, and France. A country's professional opportunities stood out as
the most common factor that influences immigration decisions of AI researchers,
followed by lifestyle and culture, the political climate, and personal
relations. The destination country's immigration policies were important to
just under half of the researchers surveyed, while around a quarter noted
current immigration difficulties to be a deciding factor. Visa and immigration
difficulties were perceived to be a particular impediment to conducting AI
research in the U.S., the U.K., and Canada. Implications of the findings for
the future of AI talent policies and governance are discussed.
</summary>
    <author>
      <name>Remco Zwetsloot</name>
    </author>
    <author>
      <name>Baobao Zhang</name>
    </author>
    <author>
      <name>Noemi Dreksler</name>
    </author>
    <author>
      <name>Lauren Kahn</name>
    </author>
    <author>
      <name>Markus Anderljung</name>
    </author>
    <author>
      <name>Allan Dafoe</name>
    </author>
    <author>
      <name>Michael C. Horowitz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3461702.3462617</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3461702.3462617" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for poster presentation at the 2021 AAAI/ACM Conference on
  AI, Ethics, and Society</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.07237v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.07237v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.7.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.11067v1</id>
    <updated>2021-04-21T09:30:16Z</updated>
    <published>2021-04-21T09:30:16Z</published>
    <title>Künstliche Intelligenz, quo vadis?</title>
    <summary>  This paper outlines the state of the art in AI. It then describes basic
machine learning and knowledge processing techniques. Based on this, some
possibilities and limitations of future AI developments are discussed.
</summary>
    <author>
      <name>Ulrike Barthelmeß</name>
    </author>
    <author>
      <name>Ulrich Furbach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in German</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.11067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.11067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.00060v1</id>
    <updated>2021-04-30T19:46:31Z</updated>
    <published>2021-04-30T19:46:31Z</published>
    <title>Ethical Implementation of Artificial Intelligence to Select Embryos in
  In Vitro Fertilization</title>
    <summary>  AI has the potential to revolutionize many areas of healthcare. Radiology,
dermatology, and ophthalmology are some of the areas most likely to be impacted
in the near future, and they have received significant attention from the
broader research community. But AI techniques are now also starting to be used
in in vitro fertilization (IVF), in particular for selecting which embryos to
transfer to the woman. The contribution of AI to IVF is potentially
significant, but must be done carefully and transparently, as the ethical
issues are significant, in part because this field involves creating new
people. We first give a brief introduction to IVF and review the use of AI for
embryo selection. We discuss concerns with the interpretation of the reported
results from scientific and practical perspectives. We then consider the
broader ethical issues involved. We discuss in detail the problems that result
from the use of black-box methods in this context and advocate strongly for the
use of interpretable models. Importantly, there have been no published trials
of clinical effectiveness, a problem in both the AI and IVF communities, and we
therefore argue that clinical implementation at this point would be premature.
Finally, we discuss ways for the broader AI community to become involved to
ensure scientifically sound and ethically responsible development of AI in IVF.
</summary>
    <author>
      <name>Michael Anis Mihdi Afnan</name>
    </author>
    <author>
      <name>Cynthia Rudin</name>
    </author>
    <author>
      <name>Vincent Conitzer</name>
    </author>
    <author>
      <name>Julian Savulescu</name>
    </author>
    <author>
      <name>Abhishek Mishra</name>
    </author>
    <author>
      <name>Yanhe Liu</name>
    </author>
    <author>
      <name>Masoud Afnan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3461702.3462589</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3461702.3462589" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AIES 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2105.00060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.00060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.05012v1</id>
    <updated>2021-05-11T13:19:06Z</updated>
    <published>2021-05-11T13:19:06Z</published>
    <title>Robotic Assistant Agent for Student and Machine Co-Learning on AI-FML
  Practice with AIoT Application</title>
    <summary>  In this paper, the Robotic Assistant Agent for student and machine
co-learning on AI-FML practice with AIoT application is presented. The
structure of AI-FML contains three parts, including fuzzy logic, neural
network, and evolutionary computation. Besides, the Robotic Assistant Agent
(RAA) can assist students and machines in co-learning English and AI-FML
practice based on the robot Kebbi Air and AIoT-FML learning tool. Since Sept.
2019, we have introduced an Intelligent Speaking English Assistant (ISEA) App
and AI-FML platform to English and computer science learning classes at two
elementary schools in Taiwan. We use the collected English-learning data to
train a predictive regression model based on students' monthly examination
scores. In Jan. 2021, we further combined the developed AI-FML platform with a
novel AIoT-FML learning tool to enhance students' interests in learning English
and AI-FML with basic hands-on practice. The proposed RAA is responsible for
reasoning students' learning performance and showing the results on the
AIoT-FML learning tool after communicating with the AI-FML platform. The
experimental results and the collection of students' feedback show that this
kind of learning model is popular with elementary-school and high-school
students, and the learning performance of elementary-school students is
improved.
</summary>
    <author>
      <name>Chang-Shing Lee</name>
    </author>
    <author>
      <name>Mei-Hui Wang</name>
    </author>
    <author>
      <name>Zong-Han Ciou</name>
    </author>
    <author>
      <name>Rin-Pin Chang</name>
    </author>
    <author>
      <name>Chun-Hao Tsai</name>
    </author>
    <author>
      <name>Shen-Chien Chen</name>
    </author>
    <author>
      <name>Tzong-Xiang Huang</name>
    </author>
    <author>
      <name>Eri Sato-Shimokawara</name>
    </author>
    <author>
      <name>Toru Yamaguchi</name>
    </author>
    <link href="http://arxiv.org/abs/2105.05012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.05012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.07844v1</id>
    <updated>2021-04-30T17:23:07Z</updated>
    <published>2021-04-30T17:23:07Z</published>
    <title>Does "AI" stand for augmenting inequality in the era of covid-19
  healthcare?</title>
    <summary>  Among the most damaging characteristics of the covid-19 pandemic has been its
disproportionate effect on disadvantaged communities. As the outbreak has
spread globally, factors such as systemic racism, marginalisation, and
structural inequality have created path dependencies that have led to poor
health outcomes. These social determinants of infectious disease and
vulnerability to disaster have converged to affect already disadvantaged
communities with higher levels of economic instability, disease exposure,
infection severity, and death. Artificial intelligence (AI) technologies are an
important part of the health informatics toolkit used to fight contagious
disease. AI is well known, however, to be susceptible to algorithmic biases
that can entrench and augment existing inequality. Uncritically deploying AI in
the fight against covid-19 thus risks amplifying the pandemic's adverse effects
on vulnerable groups, exacerbating health inequity. In this paper, we claim
that AI systems can introduce or reflect bias and discrimination in three ways:
in patterns of health discrimination that become entrenched in datasets, in
data representativeness, and in human choices made during the design,
development, and deployment of these systems. We highlight how the use of AI
technologies threaten to exacerbate the disparate effect of covid-19 on
marginalised, under-represented, and vulnerable groups, particularly black,
Asian, and other minoritised ethnic people, older populations, and those of
lower socioeconomic status. We conclude that, to mitigate the compounding
effects of AI on inequalities associated with covid-19, decision makers,
technology developers, and health officials must account for the potential
biases and inequities at all stages of the AI process.
</summary>
    <author>
      <name>David Leslie</name>
    </author>
    <author>
      <name>Anjali Mazumder</name>
    </author>
    <author>
      <name>Aidan Peppin</name>
    </author>
    <author>
      <name>Maria Wolters</name>
    </author>
    <author>
      <name>Alexa Hagerty</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1136/bmj.n304</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1136/bmj.n304" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">bmj, 372 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2105.07844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.07844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.09140v1</id>
    <updated>2021-06-16T21:35:56Z</updated>
    <published>2021-06-16T21:35:56Z</published>
    <title>Human-AI Interactions Through A Gricean Lens</title>
    <summary>  Grice's Cooperative Principle (1975) describes the implicit maxims that guide
conversation between humans. As humans begin to interact with non-human
dialogue systems more frequently and in a broader scope, an important question
emerges: what principles govern those interactions? The present study addresses
this question by evaluating human-AI interactions using Grice's four maxims; we
demonstrate that humans do, indeed, apply these maxims to interactions with AI,
even making explicit references to the AI's performance through a Gricean lens.
Twenty-three participants interacted with an American English-speaking Alexa
and rated and discussed their experience with an in-lab researcher. Researchers
then reviewed each exchange, identifying those that might relate to Grice's
maxims: Quantity, Quality, Manner, and Relevance. Many instances of explicit
user frustration stemmed from violations of Grice's maxims. Quantity violations
were noted for too little but not too much information, while Quality
violations were rare, indicating trust in Alexa's responses. Manner violations
focused on speed and humanness. Relevance violations were the most frequent,
and they appear to be the most frustrating. While the maxims help describe many
of the issues participants encountered, other issues do not fit neatly into
Grice's framework. Participants were particularly averse to Alexa initiating
exchanges or making unsolicited suggestions. To address this gap, we propose
the addition of human Priority to describe human-AI interaction. Humans and AIs
are not conversational equals, and human initiative takes priority. We suggest
that the application of Grice's Cooperative Principles to human-AI interactions
is beneficial both from an AI development perspective and as a tool for
describing an emerging form of interaction.
</summary>
    <author>
      <name>Laura Panfili</name>
    </author>
    <author>
      <name>Steve Duman</name>
    </author>
    <author>
      <name>Andrew Nave</name>
    </author>
    <author>
      <name>Katherine Phelps Ridgeway</name>
    </author>
    <author>
      <name>Nathan Eversole</name>
    </author>
    <author>
      <name>Ruhi Sarikaya</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3765/plsa.v6i1.4971</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3765/plsa.v6i1.4971" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Linguistic Society of America 6 (2021) 288-302</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.09140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.09140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.01033v1</id>
    <updated>2021-08-02T16:45:00Z</updated>
    <published>2021-08-02T16:45:00Z</published>
    <title>Bringing AI pipelines onto cloud-HPC: setting a baseline for accuracy of
  COVID-19 AI diagnosis</title>
    <summary>  HPC is an enabling platform for AI. The introduction of AI workloads in the
HPC applications basket has non-trivial consequences both on the way of
designing AI applications and on the way of providing HPC computing. This is
the leitmotif of the convergence between HPC and AI. The formalized definition
of AI pipelines is one of the milestones of HPC-AI convergence. If well
conducted, it allows, on the one hand, to obtain portable and scalable
applications. On the other hand, it is crucial for the reproducibility of
scientific pipelines. In this work, we advocate the StreamFlow Workflow
Management System as a crucial ingredient to define a parametric pipeline,
called "CLAIRE COVID-19 Universal Pipeline," which is able to explore the
optimization space of methods to classify COVID-19 lung lesions from CT scans,
compare them for accuracy, and therefore set a performance baseline. The
universal pipeline automatizes the training of many different Deep Neural
Networks (DNNs) and many different hyperparameters. It, therefore, requires a
massive computing power, which is found in traditional HPC infrastructure
thanks to the portability-by-design of pipelines designed with StreamFlow.
Using the universal pipeline, we identified a DNN reaching over 90% accuracy in
detecting COVID-19 lesions in CT scans.
</summary>
    <author>
      <name>Iacopo Colonnelli</name>
    </author>
    <author>
      <name>Barbara Cantalupo</name>
    </author>
    <author>
      <name>Concetto Spampinato</name>
    </author>
    <author>
      <name>Matteo Pennisi</name>
    </author>
    <author>
      <name>Marco Aldinucci</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.5151511</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.5151511" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In F. Iannone editor, ENEA CRESCO in the fight against COVID-19,
  pages 66-73. ISBN: 978-88-8286-415-6. June 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2108.01033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.01033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.3; D.3.2; C.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.03383v1</id>
    <updated>2021-08-07T07:14:36Z</updated>
    <published>2021-08-07T07:14:36Z</published>
    <title>Artificial Intelligence-Driven Customized Manufacturing Factory: Key
  Technologies, Applications, and Challenges</title>
    <summary>  The traditional production paradigm of large batch production does not offer
flexibility towards satisfying the requirements of individual customers. A new
generation of smart factories is expected to support new multi-variety and
small-batch customized production modes. For that, Artificial Intelligence (AI)
is enabling higher value-added manufacturing by accelerating the integration of
manufacturing and information communication technologies, including computing,
communication, and control. The characteristics of a customized smart factory
are to include self-perception, operations optimization, dynamic
reconfiguration, and intelligent decision-making. The AI technologies will
allow manufacturing systems to perceive the environment, adapt to the external
needs, and extract the process knowledge, including business models, such as
intelligent production, networked collaboration, and extended service models.
  This paper focuses on the implementation of AI in customized manufacturing
(CM). The architecture of an AI-driven customized smart factory is presented.
Details of intelligent manufacturing devices, intelligent information
interaction, and construction of a flexible manufacturing line are showcased.
The state-of-the-art AI technologies of potential use in CM, i.e., machine
learning, multi-agent systems, Internet of Things, big data, and cloud-edge
computing are surveyed. The AI-enabled technologies in a customized smart
factory are validated with a case study of customized packaging. The
experimental results have demonstrated that the AI-assisted CM offers the
possibility of higher production flexibility and efficiency. Challenges and
solutions related to AI in CM are also discussed.
</summary>
    <author>
      <name>Jiafu Wan</name>
    </author>
    <author>
      <name>Xiaomin Li</name>
    </author>
    <author>
      <name>Hong-Ning Dai</name>
    </author>
    <author>
      <name>Andrew Kusiak</name>
    </author>
    <author>
      <name>Miguel Martínez-García</name>
    </author>
    <author>
      <name>Di Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JPROC.2020.3034808</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JPROC.2020.3034808" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 12 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the IEEE, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2108.03383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.03383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T40, 68T42, 68T05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1; I.2.11; I.2.9" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.06874v1</id>
    <updated>2021-09-14T11:39:37Z</updated>
    <published>2021-09-14T11:39:37Z</published>
    <title>Agile, Antifragile, Artificial-Intelligence-Enabled, Command and Control</title>
    <summary>  Artificial Intelligence (AI) is rapidly becoming integrated into military
Command and Control (C2) systems as a strategic priority for many defence
forces. The successful implementation of AI is promising to herald a
significant leap in C2 agility through automation. However, realistic
expectations need to be set on what AI can achieve in the foreseeable future.
This paper will argue that AI could lead to a fragility trap, whereby the
delegation of C2 functions to an AI could increase the fragility of C2,
resulting in catastrophic strategic failures. This calls for a new framework
for AI in C2 to avoid this trap. We will argue that antifragility along with
agility should form the core design principles for AI-enabled C2 systems. This
duality is termed Agile, Antifragile, AI-Enabled Command and Control (A3IC2).
An A3IC2 system continuously improves its capacity to perform in the face of
shocks and surprises through overcompensation from feedback during the C2
decision-making cycle. An A3IC2 system will not only be able to survive within
a complex operational environment, it will also thrive, benefiting from the
inevitable shocks and volatility of war.
</summary>
    <author>
      <name>Jacob Simpson</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of New South Wales Canberra</arxiv:affiliation>
    </author>
    <author>
      <name>Rudolph Oosthuizen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Pretoria</arxiv:affiliation>
    </author>
    <author>
      <name>Sondoss El Sawah</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of New South Wales Canberra</arxiv:affiliation>
    </author>
    <author>
      <name>Hussein Abbass</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of New South Wales Canberra</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures, included in the 26th International Command and
  Control Research and Technology Symposium (ICCRTS)</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.06874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.06874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.04452v1</id>
    <updated>2021-10-09T04:35:23Z</updated>
    <published>2021-10-09T04:35:23Z</published>
    <title>Towards AI Logic for Social Reasoning</title>
    <summary>  Artificial Intelligence (AI) logic formalizes the reasoning of intelligent
agents. In this paper, we discuss how an argumentation-based AI logic could be
used also to formalize important aspects of social reasoning. Besides reasoning
about the knowledge and actions of individual agents, social AI logic can
reason also about social dependencies among agents using the rights,
obligations and permissions of the agents. We discuss four aspects of social AI
logic. First, we discuss how rights represent relations between the obligations
and permissions of intelligent agents. Second, we discuss how to argue about
the right-to-know, a central issue in the recent discussion of privacy and
ethics. Third, we discuss how a wide variety of conflicts among intelligent
agents can be identified and (sometimes) resolved by comparing formal
arguments. Importantly, to cover a wide range of arguments occurring in daily
life, also fallacious arguments can be represented and reasoned about. Fourth,
we discuss how to argue about the freedom to act for intelligent agents.
Examples from social, legal and ethical reasoning highlight the challenges in
developing social AI logic. The discussion of the four challenges leads to a
research program for argumentation-based social AI logic, contributing towards
the future development of AI logic.
</summary>
    <author>
      <name>Huimin Dong</name>
    </author>
    <author>
      <name>Réka Markovich</name>
    </author>
    <author>
      <name>Leendert van der Torre</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Zhejiang University, Vol. 5, No. 50 (2020): 31-50</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.04452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.04452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.08222v1</id>
    <updated>2021-11-16T04:35:34Z</updated>
    <published>2021-11-16T04:35:34Z</published>
    <title>Will We Trust What We Don't Understand? Impact of Model Interpretability
  and Outcome Feedback on Trust in AI</title>
    <summary>  Despite AI's superhuman performance in a variety of domains, humans are often
unwilling to adopt AI systems. The lack of interpretability inherent in many
modern AI techniques is believed to be hurting their adoption, as users may not
trust systems whose decision processes they do not understand. We investigate
this proposition with a novel experiment in which we use an interactive
prediction task to analyze the impact of interpretability and outcome feedback
on trust in AI and on human performance in AI-assisted prediction tasks. We
find that interpretability led to no robust improvements in trust, while
outcome feedback had a significantly greater and more reliable effect. However,
both factors had modest effects on participants' task performance. Our findings
suggest that (1) factors receiving significant attention, such as
interpretability, may be less effective at increasing trust than factors like
outcome feedback, and (2) augmenting human performance via AI systems may not
be a simple matter of increasing trust in AI, as increased trust is not always
associated with equally sizable improvements in performance. These findings
invite the research community to focus not only on methods for generating
interpretations but also on techniques for ensuring that interpretations impact
trust and performance in practice.
</summary>
    <author>
      <name>Daehwan Ahn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The Wharton School, University of Pennsylvania</arxiv:affiliation>
    </author>
    <author>
      <name>Abdullah Almaatouq</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Sloan School of Management, Massachusetts Institute of Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Monisha Gulabani</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The Wharton School, University of Pennsylvania</arxiv:affiliation>
    </author>
    <author>
      <name>Kartik Hosanagar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">The Wharton School, University of Pennsylvania</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2111.08222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.08222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.07773v1</id>
    <updated>2021-12-14T22:45:28Z</updated>
    <published>2021-12-14T22:45:28Z</published>
    <title>Filling gaps in trustworthy development of AI</title>
    <summary>  The range of application of artificial intelligence (AI) is vast, as is the
potential for harm. Growing awareness of potential risks from AI systems has
spurred action to address those risks, while eroding confidence in AI systems
and the organizations that develop them. A 2019 study found over 80
organizations that published and adopted "AI ethics principles'', and more have
joined since. But the principles often leave a gap between the "what" and the
"how" of trustworthy AI development. Such gaps have enabled questionable or
ethically dubious behavior, which casts doubts on the trustworthiness of
specific organizations, and the field more broadly. There is thus an urgent
need for concrete methods that both enable AI developers to prevent harm and
allow them to demonstrate their trustworthiness through verifiable behavior.
Below, we explore mechanisms (drawn from arXiv:2004.07213) for creating an
ecosystem where AI developers can earn trust - if they are trustworthy. Better
assessment of developer trustworthiness could inform user choice, employee
actions, investment decisions, legal recourse, and emerging governance regimes.
</summary>
    <author>
      <name>Shahar Avin</name>
    </author>
    <author>
      <name>Haydn Belfield</name>
    </author>
    <author>
      <name>Miles Brundage</name>
    </author>
    <author>
      <name>Gretchen Krueger</name>
    </author>
    <author>
      <name>Jasmine Wang</name>
    </author>
    <author>
      <name>Adrian Weller</name>
    </author>
    <author>
      <name>Markus Anderljung</name>
    </author>
    <author>
      <name>Igor Krawczuk</name>
    </author>
    <author>
      <name>David Krueger</name>
    </author>
    <author>
      <name>Jonathan Lebensold</name>
    </author>
    <author>
      <name>Tegan Maharaj</name>
    </author>
    <author>
      <name>Noa Zilberman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1126/science.abi7176</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1126/science.abi7176" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Science (2021) Vol 374, Issue 6573, pp. 1327-1329</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2112.07773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.07773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.08453v1</id>
    <updated>2021-12-15T19:57:38Z</updated>
    <published>2021-12-15T19:57:38Z</published>
    <title>The Need for Ethical, Responsible, and Trustworthy Artificial
  Intelligence for Environmental Sciences</title>
    <summary>  Given the growing use of Artificial Intelligence (AI) and machine learning
(ML) methods across all aspects of environmental sciences, it is imperative
that we initiate a discussion about the ethical and responsible use of AI. In
fact, much can be learned from other domains where AI was introduced, often
with the best of intentions, yet often led to unintended societal consequences,
such as hard coding racial bias in the criminal justice system or increasing
economic inequality through the financial system. A common misconception is
that the environmental sciences are immune to such unintended consequences when
AI is being used, as most data come from observations, and AI algorithms are
based on mathematical formulas, which are often seen as objective. In this
article, we argue the opposite can be the case. Using specific examples, we
demonstrate many ways in which the use of AI can introduce similar consequences
in the environmental sciences. This article will stimulate discussion and
research efforts in this direction. As a community, we should avoid repeating
any foreseeable mistakes made in other domains through the introduction of AI.
In fact, with proper precautions, AI can be a great tool to help {\it reduce}
climate and environmental injustice. We primarily focus on weather and climate
examples but the conclusions apply broadly across the environmental sciences.
</summary>
    <author>
      <name>Amy McGovern</name>
    </author>
    <author>
      <name>Imme Ebert-Uphoff</name>
    </author>
    <author>
      <name>David John Gagne II</name>
    </author>
    <author>
      <name>Ann Bostrom</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1017/eds.2022.5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1017/eds.2022.5" rel="related"/>
    <link href="http://arxiv.org/abs/2112.08453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.08453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.0; I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.02734v1</id>
    <updated>2022-01-02T01:43:24Z</updated>
    <published>2022-01-02T01:43:24Z</published>
    <title>Building Human-like Communicative Intelligence: A Grounded Perspective</title>
    <summary>  Modern Artificial Intelligence (AI) systems excel at diverse tasks, from
image classification to strategy games, even outperforming humans in many of
these domains. After making astounding progress in language learning in the
recent decade, AI systems, however, seem to approach the ceiling that does not
reflect important aspects of human communicative capacities. Unlike human
learners, communicative AI systems often fail to systematically generalize to
new data, suffer from sample inefficiency, fail to capture common-sense
semantic knowledge, and do not translate to real-world communicative
situations. Cognitive Science offers several insights on how AI could move
forward from this point. This paper aims to: (1) suggest that the dominant
cognitively-inspired AI directions, based on nativist and symbolic paradigms,
lack necessary substantiation and concreteness to guide progress in modern AI,
and (2) articulate an alternative, "grounded", perspective on AI advancement,
inspired by Embodied, Embedded, Extended, and Enactive Cognition (4E) research.
I review results on 4E research lines in Cognitive Science to distinguish the
main aspects of naturalistic learning conditions that play causal roles for
human language development. I then use this analysis to propose a list of
concrete, implementable components for building "grounded" linguistic
intelligence. These components include embodying machines in a
perception-action cycle, equipping agents with active exploration mechanisms so
they can build their own curriculum, allowing agents to gradually develop motor
abilities to promote piecemeal language development, and endowing the agents
with adaptive feedback from their physical and social environment. I hope that
these ideas can direct AI research towards building machines that develop
human-like language abilities through their experiences with the world.
</summary>
    <author>
      <name>Marina Dubova</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cogsys.2021.12.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cogsys.2021.12.002" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Cognitive Systems Research, 72, 63-79 (2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2201.02734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.02734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.07040v2</id>
    <updated>2022-05-12T13:25:37Z</updated>
    <published>2022-01-18T15:05:28Z</published>
    <title>Benchmark datasets driving artificial intelligence development fail to
  capture the needs of medical professionals</title>
    <summary>  Publicly accessible benchmarks that allow for assessing and comparing model
performances are important drivers of progress in artificial intelligence (AI).
While recent advances in AI capabilities hold the potential to transform
medical practice by assisting and augmenting the cognitive processes of
healthcare professionals, the coverage of clinically relevant tasks by AI
benchmarks is largely unclear. Furthermore, there is a lack of systematized
meta-information that allows clinical AI researchers to quickly determine
accessibility, scope, content and other characteristics of datasets and
benchmark datasets relevant to the clinical domain.
  To address these issues, we curated and released a comprehensive catalogue of
datasets and benchmarks pertaining to the broad domain of clinical and
biomedical natural language processing (NLP), based on a systematic review of
literature and online resources. A total of 450 NLP datasets were manually
systematized and annotated with rich metadata, such as targeted tasks, clinical
applicability, data types, performance metrics, accessibility and licensing
information, and availability of data splits. We then compared tasks covered by
AI benchmark datasets with relevant tasks that medical practitioners reported
as highly desirable targets for automation in a previous empirical study.
  Our analysis indicates that AI benchmarks of direct clinical relevance are
scarce and fail to cover most work activities that clinicians want to see
addressed. In particular, tasks associated with routine documentation and
patient data administration workflows are not represented despite significant
associated workloads. Thus, currently available AI benchmarks are improperly
aligned with desired targets for AI automation in clinical settings, and novel
benchmarks should be created to fill these gaps.
</summary>
    <author>
      <name>Kathrin Blagec</name>
    </author>
    <author>
      <name>Jakob Kraiger</name>
    </author>
    <author>
      <name>Wolfgang Frühwirt</name>
    </author>
    <author>
      <name>Matthias Samwald</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jbi.2022.104274</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jbi.2022.104274" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(this version extends the literature references)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Bioinformatics, January 2023</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2201.07040v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.07040v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.09292v1</id>
    <updated>2022-02-18T16:37:54Z</updated>
    <published>2022-02-18T16:37:54Z</published>
    <title>System Safety and Artificial Intelligence</title>
    <summary>  This chapter formulates seven lessons for preventing harm in artificial
intelligence (AI) systems based on insights from the field of system safety for
software-based automation in safety-critical domains. New applications of AI
across societal domains and public organizations and infrastructures come with
new hazards, which lead to new forms of harm, both grave and pernicious. The
text addresses the lack of consensus for diagnosing and eliminating new AI
system hazards. For decades, the field of system safety has dealt with
accidents and harm in safety-critical systems governed by varying degrees of
software-based automation and decision-making. This field embraces the core
assumption of systems and control that AI systems cannot be safeguarded by
technical design choices on the model or algorithm alone, instead requiring an
end-to-end hazard analysis and design frame that includes the context of use,
impacted stakeholders and the formal and informal institutional environment in
which the system operates. Safety and other values are then inherently
socio-technical and emergent system properties that require design and control
measures to instantiate these across the technical, social and institutional
components of a system. This chapter honors system safety pioneer Nancy
Leveson, by situating her core lessons for today's AI system safety challenges.
For every lesson, concrete tools are offered for rethinking and reorganizing
the safety management of AI systems, both in design and governance. This
history tells us that effective AI safety management requires transdisciplinary
approaches and a shared language that allows involvement of all levels of
society.
</summary>
    <author>
      <name>Roel I. J. Dobbe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in: Oxford Handbook on AI Governance (Oxford University
  Press, 2022 forthcoming)</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.09292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.09292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.03712v2</id>
    <updated>2022-03-14T14:30:20Z</updated>
    <published>2022-02-16T11:56:41Z</published>
    <title>Trusted Data Forever: Is AI the Answer?</title>
    <summary>  Archival institutions and programs worldwide work to ensure that the records
of governments, organizations, communities, and individuals are preserved for
future generations as cultural heritage, as sources of rights, and as vehicles
for holding the past accountable and to inform the future. This commitment is
guaranteed through the adoption of strategic and technical measures for the
long-term preservation of digital assets in any medium and form - textual,
visual, or aural. Public and private archives are the largest providers of data
big and small in the world and collectively host yottabytes of trusted data, to
be preserved forever. Several aspects of retention and preservation,
arrangement and description, management and administrations, and access and use
are still open to improvement. In particular, recent advances in Artificial
Intelligence (AI) open the discussion as to whether AI can support the ongoing
availability and accessibility of trustworthy public records. This paper
presents preliminary results of the InterPARES Trust AI (I Trust AI)
international research partnership, which aims to (1) identify and develop
specific AI technologies to address critical records and archives challenges;
(2) determine the benefits and risks of employing AI technologies on records
and archives; (3) ensure that archival concepts and principles inform the
development of responsible AI; and (4) validate outcomes through a conglomerate
of case studies and demonstrations.
</summary>
    <author>
      <name>Emanuele Frontoni</name>
    </author>
    <author>
      <name>Marina Paolanti</name>
    </author>
    <author>
      <name>Tracey P. Lauriault</name>
    </author>
    <author>
      <name>Michael Stiber</name>
    </author>
    <author>
      <name>Luciana Duranti</name>
    </author>
    <author>
      <name>Abdul-Mageed Muhammad</name>
    </author>
    <link href="http://arxiv.org/abs/2203.03712v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.03712v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.01160v1</id>
    <updated>2022-04-03T21:00:51Z</updated>
    <published>2022-04-03T21:00:51Z</published>
    <title>Best-Response Bayesian Reinforcement Learning with Bayes-adaptive POMDPs
  for Centaurs</title>
    <summary>  Centaurs are half-human, half-AI decision-makers where the AI's goal is to
complement the human. To do so, the AI must be able to recognize the goals and
constraints of the human and have the means to help them. We present a novel
formulation of the interaction between the human and the AI as a sequential
game where the agents are modelled using Bayesian best-response models. We show
that in this case the AI's problem of helping bounded-rational humans make
better decisions reduces to a Bayes-adaptive POMDP. In our simulated
experiments, we consider an instantiation of our framework for humans who are
subjectively optimistic about the AI's future behaviour. Our results show that
when equipped with a model of the human, the AI can infer the human's bounds
and nudge them towards better decisions. We discuss ways in which the machine
can learn to improve upon its own limitations as well with the help of the
human. We identify a novel trade-off for centaurs in partially observable
tasks: for the AI's actions to be acceptable to the human, the machine must
make sure their beliefs are sufficiently aligned, but aligning beliefs might be
costly. We present a preliminary theoretical analysis of this trade-off and its
dependence on task structure.
</summary>
    <author>
      <name>Mustafa Mert Çelikok</name>
    </author>
    <author>
      <name>Frans A. Oliehoek</name>
    </author>
    <author>
      <name>Samuel Kaski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is presented in part at the International Conference on
  Autonomous Agents and Multi-Agent Systems (AAMAS) 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.01160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.01160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.02766v2</id>
    <updated>2022-04-07T08:21:14Z</updated>
    <published>2022-04-06T12:22:43Z</published>
    <title>Data-Centric Green AI: An Exploratory Empirical Study</title>
    <summary>  With the growing availability of large-scale datasets, and the popularization
of affordable storage and computational capabilities, the energy consumed by AI
is becoming a growing concern. To address this issue, in recent years, studies
have focused on demonstrating how AI energy efficiency can be improved by
tuning the model training strategy. Nevertheless, how modifications applied to
datasets can impact the energy consumption of AI is still an open question. To
fill this gap, in this exploratory study, we evaluate if data-centric
approaches can be utilized to improve AI energy efficiency. To achieve our
goal, we conduct an empirical experiment, executed by considering 6 different
AI algorithms, a dataset comprising 5,574 data points, and two dataset
modifications (number of data points and number of features). Our results show
evidence that, by exclusively conducting modifications on datasets, energy
consumption can be drastically reduced (up to 92.16%), often at the cost of a
negligible or even absent accuracy decline. As additional introductory results,
we demonstrate how, by exclusively changing the algorithm used, energy savings
up to two orders of magnitude can be achieved. In conclusion, this exploratory
investigation empirically demonstrates the importance of applying data-centric
techniques to improve AI energy efficiency. Our results call for a research
agenda that focuses on data-centric techniques, to further enable and
democratize Green AI.
</summary>
    <author>
      <name>Roberto Verdecchia</name>
    </author>
    <author>
      <name>Luís Cruz</name>
    </author>
    <author>
      <name>June Sallou</name>
    </author>
    <author>
      <name>Michelle Lin</name>
    </author>
    <author>
      <name>James Wickenden</name>
    </author>
    <author>
      <name>Estelle Hotellier</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICT4S55073.2022.00015</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICT4S55073.2022.00015" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, 2 tables. Accepted at the 8th ICT for
  Sustainability Conference (ICT4S) 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.02766v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.02766v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.04460v1</id>
    <updated>2022-05-06T14:27:57Z</updated>
    <published>2022-05-06T14:27:57Z</published>
    <title>Rethinking Fairness: An Interdisciplinary Survey of Critiques of
  Hegemonic ML Fairness Approaches</title>
    <summary>  This survey article assesses and compares existing critiques of current
fairness-enhancing technical interventions into machine learning (ML) that draw
from a range of non-computing disciplines, including philosophy, feminist
studies, critical race and ethnic studies, legal studies, anthropology, and
science and technology studies. It bridges epistemic divides in order to offer
an interdisciplinary understanding of the possibilities and limits of hegemonic
computational approaches to ML fairness for producing just outcomes for
society's most marginalized. The article is organized according to nine major
themes of critique wherein these different fields intersect: 1) how "fairness"
in AI fairness research gets defined; 2) how problems for AI systems to address
get formulated; 3) the impacts of abstraction on how AI tools function and its
propensity to lead to technological solutionism; 4) how racial classification
operates within AI fairness research; 5) the use of AI fairness measures to
avoid regulation and engage in ethics washing; 6) an absence of participatory
design and democratic deliberation in AI fairness considerations; 7) data
collection practices that entrench "bias," are non-consensual, and lack
transparency; 8) the predatory inclusion of marginalized groups into AI
systems; and 9) a lack of engagement with AI's long-term social and ethical
outcomes. Drawing from these critiques, the article concludes by imagining
future ML fairness research directions that actively disrupt entrenched power
dynamics and structural injustices in society.
</summary>
    <author>
      <name>Lindsay Weinberg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1613/jair.1.13196</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1613/jair.1.13196" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Artificial Intelligence Research 74 (2022): 75-109</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2205.04460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.04460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.07555v1</id>
    <updated>2022-06-15T14:15:03Z</updated>
    <published>2022-06-15T14:15:03Z</published>
    <title>Respect as a Lens for the Design of AI Systems</title>
    <summary>  Critical examinations of AI systems often apply principles such as fairness,
justice, accountability, and safety, which is reflected in AI regulations such
as the EU AI Act. Are such principles sufficient to promote the design of
systems that support human flourishing? Even if a system is in some sense fair,
just, or 'safe', it can nonetheless be exploitative, coercive, inconvenient, or
otherwise conflict with cultural, individual, or social values. This paper
proposes a dimension of interactional ethics thus far overlooked: the ways AI
systems should treat human beings. For this purpose, we explore the
philosophical concept of respect: if respect is something everyone needs and
deserves, shouldn't technology aim to be respectful? Despite its intuitive
simplicity, respect in philosophy is a complex concept with many disparate
senses. Like fairness or justice, respect can characterise how people deserve
to be treated; but rather than relating primarily to the distribution of
benefits or punishments, respect relates to how people regard one another, and
how this translates to perception, treatment, and behaviour. We explore respect
broadly across several literatures, synthesising perspectives on respect from
Kantian, post-Kantian, dramaturgical, and agential realist design perspectives
with a goal of drawing together a view of what respect could mean for AI. In so
doing, we identify ways that respect may guide us towards more sociable
artefacts that ethically and inclusively honour and recognise humans using the
rich social language that we have evolved to interact with one another every
day.
</summary>
    <author>
      <name>William Seymour</name>
    </author>
    <author>
      <name>Max Van Kleek</name>
    </author>
    <author>
      <name>Reuben Binns</name>
    </author>
    <author>
      <name>Dave Murray-Rust</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3514094.3534186</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3514094.3534186" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proceedings of the 2022 AAAI/ACM Conference on AI,
  Ethics, and Society (AIES '22)</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.07555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.07555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.09374v1</id>
    <updated>2022-07-19T16:20:37Z</updated>
    <published>2022-07-19T16:20:37Z</published>
    <title>Alterfactual Explanations -- The Relevance of Irrelevance for Explaining
  AI Systems</title>
    <summary>  Explanation mechanisms from the field of Counterfactual Thinking are a
widely-used paradigm for Explainable Artificial Intelligence (XAI), as they
follow a natural way of reasoning that humans are familiar with. However, all
common approaches from this field are based on communicating information about
features or characteristics that are especially important for an AI's decision.
We argue that in order to fully understand a decision, not only knowledge about
relevant features is needed, but that the awareness of irrelevant information
also highly contributes to the creation of a user's mental model of an AI
system. Therefore, we introduce a new way of explaining AI systems. Our
approach, which we call Alterfactual Explanations, is based on showing an
alternative reality where irrelevant features of an AI's input are altered. By
doing so, the user directly sees which characteristics of the input data can
change arbitrarily without influencing the AI's decision. We evaluate our
approach in an extensive user study, revealing that it is able to significantly
contribute to the participants' understanding of an AI. We show that
alterfactual explanations are suited to convey an understanding of different
aspects of the AI's reasoning than established counterfactual explanation
methods.
</summary>
    <author>
      <name>Silvan Mertes</name>
    </author>
    <author>
      <name>Christina Karle</name>
    </author>
    <author>
      <name>Tobias Huber</name>
    </author>
    <author>
      <name>Katharina Weitz</name>
    </author>
    <author>
      <name>Ruben Schlagowski</name>
    </author>
    <author>
      <name>Elisabeth André</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at IJCAI 2022 Workshop on XAI</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.09374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.09374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.00761v1</id>
    <updated>2022-08-01T11:41:04Z</updated>
    <published>2022-08-01T11:41:04Z</published>
    <title>AI Augmented Edge and Fog Computing: Trends and Challenges</title>
    <summary>  In recent years, the landscape of computing paradigms has witnessed a gradual
yet remarkable shift from monolithic computing to distributed and decentralized
paradigms such as Internet of Things (IoT), Edge, Fog, Cloud, and Serverless.
The frontiers of these computing technologies have been boosted by shift from
manually encoded algorithms to Artificial Intelligence (AI)-driven autonomous
systems for optimum and reliable management of distributed computing resources.
Prior work focuses on improving existing systems using AI across a wide range
of domains, such as efficient resource provisioning, application deployment,
task placement, and service management. This survey reviews the evolution of
data-driven AI-augmented technologies and their impact on computing systems. We
demystify new techniques and draw key insights in Edge, Fog and Cloud resource
management-related uses of AI methods and also look at how AI can innovate
traditional applications for enhanced Quality of Service (QoS) in the presence
of a continuum of resources. We present the latest trends and impact areas such
as optimizing AI models that are deployed on or for computing systems. We
layout a roadmap for future research directions in areas such as resource
management for QoS optimization and service reliability. Finally, we discuss
blue-sky ideas and envision this work as an anchor point for future research on
AI-driven computing systems.
</summary>
    <author>
      <name>Shreshth Tuli</name>
    </author>
    <author>
      <name>Fatemeh Mirhakimi</name>
    </author>
    <author>
      <name>Samodha Pallewatta</name>
    </author>
    <author>
      <name>Syed Zawad</name>
    </author>
    <author>
      <name>Giuliano Casale</name>
    </author>
    <author>
      <name>Bahman Javadi</name>
    </author>
    <author>
      <name>Feng Yan</name>
    </author>
    <author>
      <name>Rajkumar Buyya</name>
    </author>
    <author>
      <name>Nicholas R. Jennings</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint submitted to Elsevier Journal of Network and Computer
  Applications</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.00761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.00761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.10544v1</id>
    <updated>2022-08-22T18:45:53Z</updated>
    <published>2022-08-22T18:45:53Z</published>
    <title>The Value of AI Guidance in Human Examination of Synthetically-Generated
  Faces</title>
    <summary>  Face image synthesis has progressed beyond the point at which humans can
effectively distinguish authentic faces from synthetically generated ones.
Recently developed synthetic face image detectors boast "better-than-human"
discriminative ability, especially those guided by human perceptual
intelligence during the model's training process. In this paper, we investigate
whether these human-guided synthetic face detectors can assist non-expert human
operators in the task of synthetic image detection when compared to models
trained without human-guidance. We conducted a large-scale experiment with more
than 1,560 subjects classifying whether an image shows an authentic or
synthetically-generated face, and annotate regions that supported their
decisions. In total, 56,015 annotations across 3,780 unique face images were
collected. All subjects first examined samples without any AI support, followed
by samples given (a) the AI's decision ("synthetic" or "authentic"), (b) class
activation maps illustrating where the model deems salient for its decision, or
(c) both the AI's decision and AI's saliency map. Synthetic faces were
generated with six modern Generative Adversarial Networks. Interesting
observations from this experiment include: (1) models trained with
human-guidance offer better support to human examination of face images when
compared to models trained traditionally using cross-entropy loss, (2) binary
decisions presented to humans offers better support than saliency maps, (3)
understanding the AI's accuracy helps humans to increase trust in a given model
and thus increase their overall accuracy. This work demonstrates that although
humans supported by machines achieve better-than-random accuracy of synthetic
face detection, the ways of supplying humans with AI support and of building
trust are key factors determining high effectiveness of the human-AI tandem.
</summary>
    <author>
      <name>Aidan Boyd</name>
    </author>
    <author>
      <name>Patrick Tinsley</name>
    </author>
    <author>
      <name>Kevin Bowyer</name>
    </author>
    <author>
      <name>Adam Czajka</name>
    </author>
    <link href="http://arxiv.org/abs/2208.10544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.10544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.03735v2</id>
    <updated>2023-02-16T20:14:24Z</updated>
    <published>2022-10-02T20:17:11Z</published>
    <title>"Help Me Help the AI": Understanding How Explainability Can Support
  Human-AI Interaction</title>
    <summary>  Despite the proliferation of explainable AI (XAI) methods, little is
understood about end-users' explainability needs and behaviors around XAI
explanations. To address this gap and contribute to understanding how
explainability can support human-AI interaction, we conducted a mixed-methods
study with 20 end-users of a real-world AI application, the Merlin bird
identification app, and inquired about their XAI needs, uses, and perceptions.
We found that participants desire practically useful information that can
improve their collaboration with the AI, more so than technical system details.
Relatedly, participants intended to use XAI explanations for various purposes
beyond understanding the AI's outputs: calibrating trust, improving their task
skills, changing their behavior to supply better inputs to the AI, and giving
constructive feedback to developers. Finally, among existing XAI approaches,
participants preferred part-based explanations that resemble human reasoning
and explanations. We discuss the implications of our findings and provide
recommendations for future XAI design.
</summary>
    <author>
      <name>Sunnie S. Y. Kim</name>
    </author>
    <author>
      <name>Elizabeth Anne Watkins</name>
    </author>
    <author>
      <name>Olga Russakovsky</name>
    </author>
    <author>
      <name>Ruth Fong</name>
    </author>
    <author>
      <name>Andrés Monroy-Hernández</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3544548.3581001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3544548.3581001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CHI 2023</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2023 CHI Conference on Human Factors in
  Computing Systems (CHI '23), April 23-28, 2023, Hamburg, Germany. ACM, New
  York, NY, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2210.03735v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.03735v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.12346v1</id>
    <updated>2022-10-22T04:22:16Z</updated>
    <published>2022-10-22T04:22:16Z</published>
    <title>AI-based Arabic Language and Speech Tutor</title>
    <summary>  In the past decade, we have observed a growing interest in using technologies
such as artificial intelligence (AI), machine learning, and chatbots to provide
assistance to language learners, especially in second language learning. By
using AI and natural language processing (NLP) and chatbots, we can create an
intelligent self-learning environment that goes beyond multiple-choice
questions and/or fill in the blank exercises. In addition, NLP allows for
learning to be adaptive in that it offers more than an indication that an error
has occurred. It also provides a description of the error, uses linguistic
analysis to isolate the source of the error, and then suggests additional
drills to achieve optimal individualized learning outcomes. In this paper, we
present our approach for developing an Artificial Intelligence-based Arabic
Language and Speech Tutor (AI-ALST) for teaching the Moroccan Arabic dialect.
The AI-ALST system is an intelligent tutor that provides analysis and
assessment of students learning the Moroccan dialect at University of Arizona
(UA). The AI-ALST provides a self-learned environment to practice each lesson
for pronunciation training. In this paper, we present our initial experimental
evaluation of the AI-ALST that is based on MFCC (Mel frequency cepstrum
coefficient) feature extraction, bidirectional LSTM (Long Short-Term Memory),
attention mechanism, and a cost-based strategy for dealing with class-imbalance
learning. We evaluated our tutor on the word pronunciation of lesson 1 of the
Moroccan Arabic dialect class. The experimental results show that the AI-ALST
can effectively and successfully detect pronunciation errors and evaluate its
performance by using F_1-score, accuracy, precision, and recall.
</summary>
    <author>
      <name>Sicong Shao</name>
    </author>
    <author>
      <name>Saleem Alharir</name>
    </author>
    <author>
      <name>Salim Hariri</name>
    </author>
    <author>
      <name>Pratik Satam</name>
    </author>
    <author>
      <name>Sonia Shiri</name>
    </author>
    <author>
      <name>Abdessamad Mbarki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 19th IEEE/ACS International Conference on Computer
  Systems and Applications</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.12346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.12346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.15149v3</id>
    <updated>2022-11-06T08:28:39Z</updated>
    <published>2022-10-27T03:24:52Z</published>
    <title>Fully Automated Deep Learning-enabled Detection for Hepatic Steatosis on
  Computed Tomography: A Multicenter International Validation Study</title>
    <summary>  Despite high global prevalence of hepatic steatosis, no automated diagnostics
demonstrated generalizability in detecting steatosis on multiple international
datasets. Traditionally, hepatic steatosis detection relies on clinicians
selecting the region of interest (ROI) on computed tomography (CT) to measure
liver attenuation. ROI selection demands time and expertise, and therefore is
not routinely performed in populations. To automate the process, we validated
an existing artificial intelligence (AI) system for 3D liver segmentation and
used it to purpose a novel method: AI-ROI, which could automatically select the
ROI for attenuation measurements. AI segmentation and AI-ROI method were
evaluated on 1,014 non-contrast enhanced chest CT images from eight
international datasets: LIDC-IDRI, NSCLC-Lung1, RIDER, VESSEL12, RICORD-1A,
RICORD-1B, COVID-19-Italy, and COVID-19-China. AI segmentation achieved a mean
dice coefficient of 0.957. Attenuations measured by AI-ROI showed no
significant differences (p = 0.545) and a reduction of 71% time compared to
expert measurements. The area under the curve (AUC) of the steatosis
classification of AI-ROI is 0.921 (95% CI: 0.883 - 0.959). If performed as a
routine screening method, our AI protocol could potentially allow early
non-invasive, non-pharmacological preventative interventions for hepatic
steatosis. 1,014 expert-annotated liver segmentations of patients with hepatic
steatosis annotations can be downloaded here:
https://drive.google.com/drive/folders/1-g_zJeAaZXYXGqL1OeF6pUjr6KB0igJX.
</summary>
    <author>
      <name>Zhongyi Zhang</name>
    </author>
    <author>
      <name>Guixia Li</name>
    </author>
    <author>
      <name>Ziqiang Wang</name>
    </author>
    <author>
      <name>Feng Xia</name>
    </author>
    <author>
      <name>Ning Zhao</name>
    </author>
    <author>
      <name>Huibin Nie</name>
    </author>
    <author>
      <name>Zezhong Ye</name>
    </author>
    <author>
      <name>Joshua Lin</name>
    </author>
    <author>
      <name>Yiyi Hui</name>
    </author>
    <author>
      <name>Xiangchun Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2210.15149v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.15149v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.03157v2</id>
    <updated>2022-11-10T17:14:28Z</updated>
    <published>2022-11-06T15:46:02Z</published>
    <title>Examining the Differential Risk from High-level Artificial Intelligence
  and the Question of Control</title>
    <summary>  Artificial Intelligence (AI) is one of the most transformative technologies
of the 21st century. The extent and scope of future AI capabilities remain a
key uncertainty, with widespread disagreement on timelines and potential
impacts. As nations and technology companies race toward greater complexity and
autonomy in AI systems, there are concerns over the extent of integration and
oversight of opaque AI decision processes. This is especially true in the
subfield of machine learning (ML), where systems learn to optimize objectives
without human assistance. Objectives can be imperfectly specified or executed
in an unexpected or potentially harmful way. This becomes more concerning as
systems increase in power and autonomy, where an abrupt capability jump could
result in unexpected shifts in power dynamics or even catastrophic failures.
This study presents a hierarchical complex systems framework to model AI risk
and provide a template for alternative futures analysis. Survey data were
collected from domain experts in the public and private sectors to classify AI
impact and likelihood. The results show increased uncertainty over the powerful
AI agent scenario, confidence in multiagent environments, and increased concern
over AI alignment failures and influence-seeking behavior.
</summary>
    <author>
      <name>Kyle A. Kilian</name>
    </author>
    <author>
      <name>Christopher J. Ventura</name>
    </author>
    <author>
      <name>Mark M. Bailey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">62 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.03157v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.03157v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.07860v1</id>
    <updated>2022-11-15T02:54:23Z</updated>
    <published>2022-11-15T02:54:23Z</published>
    <title>Enabling AI Quality Control via Feature Hierarchical Edge Inference</title>
    <summary>  With the rise of edge computing, various AI services are expected to be
available at a mobile side through the inference based on deep neural network
(DNN) operated at the network edge, called edge inference (EI). On the other
hand, the resulting AI quality (e.g., mean average precision in objective
detection) has been regarded as a given factor, and AI quality control has yet
to be explored despite its importance in addressing the diverse demands of
different users. This work aims at tackling the issue by proposing a feature
hierarchical EI (FHEI), comprising feature network and inference network
deployed at an edge server and corresponding mobile, respectively.
Specifically, feature network is designed based on feature hierarchy, a
one-directional feature dependency with a different scale. A higher scale
feature requires more computation and communication loads while it provides a
better AI quality. The tradeoff enables FHEI to control AI quality gradually
w.r.t. communication and computation loads, leading to deriving a
near-to-optimal solution to maximize multi-user AI quality under the
constraints of uplink \&amp; downlink transmissions and edge server and mobile
computation capabilities. It is verified by extensive simulations that the
proposed joint communication-and-computation control on FHEI architecture
always outperforms several benchmarks by differentiating each user's AI quality
depending on the communication and computation conditions.
</summary>
    <author>
      <name>Jinhyuk Choi</name>
    </author>
    <author>
      <name>Seong-Lyun Kim</name>
    </author>
    <author>
      <name>Seung-Woo Ko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures, Conference Version</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.07860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.07860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.09360v1</id>
    <updated>2022-12-19T10:54:51Z</updated>
    <published>2022-12-19T10:54:51Z</published>
    <title>AI Security for Geoscience and Remote Sensing: Challenges and Future
  Trends</title>
    <summary>  Recent advances in artificial intelligence (AI) have significantly
intensified research in the geoscience and remote sensing (RS) field. AI
algorithms, especially deep learning-based ones, have been developed and
applied widely to RS data analysis. The successful application of AI covers
almost all aspects of Earth observation (EO) missions, from low-level vision
tasks like super-resolution, denoising, and inpainting, to high-level vision
tasks like scene classification, object detection, and semantic segmentation.
While AI techniques enable researchers to observe and understand the Earth more
accurately, the vulnerability and uncertainty of AI models deserve further
attention, considering that many geoscience and RS tasks are highly
safety-critical. This paper reviews the current development of AI security in
the geoscience and RS field, covering the following five important aspects:
adversarial attack, backdoor attack, federated learning, uncertainty, and
explainability. Moreover, the potential opportunities and trends are discussed
to provide insights for future research. To the best of the authors' knowledge,
this paper is the first attempt to provide a systematic review of AI
security-related research in the geoscience and RS community. Available code
and datasets are also listed in the paper to move this vibrant field of
research forward.
</summary>
    <author>
      <name>Yonghao Xu</name>
    </author>
    <author>
      <name>Tao Bai</name>
    </author>
    <author>
      <name>Weikang Yu</name>
    </author>
    <author>
      <name>Shizhen Chang</name>
    </author>
    <author>
      <name>Peter M. Atkinson</name>
    </author>
    <author>
      <name>Pedram Ghamisi</name>
    </author>
    <link href="http://arxiv.org/abs/2212.09360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.09360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.13866v1</id>
    <updated>2022-12-28T15:33:28Z</updated>
    <published>2022-12-28T15:33:28Z</published>
    <title>Architecture Decisions in AI-based Systems Development: An Empirical
  Study</title>
    <summary>  Artificial Intelligence (AI) technologies have been developed rapidly, and
AI-based systems have been widely used in various application domains with
opportunities and challenges. However, little is known about the architecture
decisions made in AI-based systems development, which has a substantial impact
on the success and sustainability of these systems. To this end, we conducted
an empirical study by collecting and analyzing the data from Stack Overflow
(SO) and GitHub. More specifically, we searched on SO with six sets of keywords
and explored 32 AI-based projects on GitHub, and finally we collected 174 posts
and 128 GitHub issues related to architecture decisions. The results show that
in AI-based systems development (1) architecture decisions are expressed in six
linguistic patterns, among which Solution Proposal and Information Giving are
most frequently used, (2) Technology Decision, Component Decision, and Data
Decision are the main types of architecture decisions made, (3) Game is the
most common application domain among the eighteen application domains
identified, (4) the dominant quality attribute considered in architecture
decision-making is Performance, and (5) the main limitations and challenges
encountered by practitioners in making architecture decisions are Design Issues
and Data Issues. Our results suggest that the limitations and challenges when
making architecture decisions in AI-based systems development are highly
specific to the characteristics of AI-based systems and are mainly of technical
nature, which need to be properly confronted.
</summary>
    <author>
      <name>Beiqi Zhang</name>
    </author>
    <author>
      <name>Tianyang Liu</name>
    </author>
    <author>
      <name>Peng Liang</name>
    </author>
    <author>
      <name>Chong Wang</name>
    </author>
    <author>
      <name>Mojtaba Shahin</name>
    </author>
    <author>
      <name>Jiaxin Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 30th IEEE International Conference on Software Analysis,
  Evolution, and Reengineering (SANER)</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.13866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.13866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.05347v1</id>
    <updated>2023-01-13T01:08:49Z</updated>
    <published>2023-01-13T01:08:49Z</published>
    <title>Towards Reconciling Usability and Usefulness of Explainable AI
  Methodologies</title>
    <summary>  Interactive Artificial Intelligence (AI) agents are becoming increasingly
prevalent in society. However, application of such systems without
understanding them can be problematic. Black-box AI systems can lead to
liability and accountability issues when they produce an incorrect decision.
Explainable AI (XAI) seeks to bridge the knowledge gap, between developers and
end-users, by offering insights into how an AI algorithm functions. Many modern
algorithms focus on making the AI model "transparent", i.e. unveil the inherent
functionality of the agent in a simpler format. However, these approaches do
not cater to end-users of these systems, as users may not possess the requisite
knowledge to understand these explanations in a reasonable amount of time.
Therefore, to be able to develop suitable XAI methods, we need to understand
the factors which influence subjective perception and objective usability. In
this paper, we present a novel user-study which studies four differing XAI
modalities commonly employed in prior work for explaining AI behavior, i.e.
Decision Trees, Text, Programs. We study these XAI modalities in the context of
explaining the actions of a self-driving car on a highway, as driving is an
easily understandable real-world task and self-driving cars is a keen area of
interest within the AI community. Our findings highlight internal consistency
issues wherein participants perceived language explanations to be significantly
more usable, however participants were better able to objectively understand
the decision making process of the car through a decision tree explanation. Our
work also provides further evidence of importance of integrating user-specific
and situational criteria into the design of XAI systems. Our findings show that
factors such as computer science experience, and watching the car succeed or
fail can impact the perception and usefulness of the explanation.
</summary>
    <author>
      <name>Pradyumna Tambwekar</name>
    </author>
    <author>
      <name>Matthew Gombolay</name>
    </author>
    <link href="http://arxiv.org/abs/2301.05347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.05347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.06421v1</id>
    <updated>2023-01-16T13:19:53Z</updated>
    <published>2023-01-16T13:19:53Z</published>
    <title>AI Alignment Dialogues: An Interactive Approach to AI Alignment in
  Support Agents</title>
    <summary>  AI alignment is about ensuring AI systems only pursue goals and activities
that are beneficial to humans. Most of the current approach to AI alignment is
to learn what humans value from their behavioural data. This paper proposes a
different way of looking at the notion of alignment, namely by introducing AI
Alignment Dialogues: dialogues with which users and agents try to achieve and
maintain alignment via interaction. We argue that alignment dialogues have a
number of advantages in comparison to data-driven approaches, especially for
behaviour support agents, which aim to support users in achieving their desired
future behaviours rather than their current behaviours. The advantages of
alignment dialogues include allowing the users to directly convey higher-level
concepts to the agent, and making the agent more transparent and trustworthy.
In this paper we outline the concept and high-level structure of alignment
dialogues. Moreover, we conducted a qualitative focus group user study from
which we developed a model that describes how alignment dialogues affect users,
and created design suggestions for AI alignment dialogues. Through this we
establish foundations for AI alignment dialogues and shed light on what
requires further development and research.
</summary>
    <author>
      <name>Pei-Yu Chen</name>
    </author>
    <author>
      <name>Myrthe L. Tielman</name>
    </author>
    <author>
      <name>Dirk K. J. Heylen</name>
    </author>
    <author>
      <name>Catholijn M. Jonker</name>
    </author>
    <author>
      <name>M. Birna van Riemsdijk</name>
    </author>
    <link href="http://arxiv.org/abs/2301.06421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.06421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.06890v1</id>
    <updated>2023-01-10T11:08:39Z</updated>
    <published>2023-01-10T11:08:39Z</published>
    <title>AI-Based Affective Music Generation Systems: A Review of Methods, and
  Challenges</title>
    <summary>  Music is a powerful medium for altering the emotional state of the listener.
In recent years, with significant advancement in computing capabilities,
artificial intelligence-based (AI-based) approaches have become popular for
creating affective music generation (AMG) systems that are empowered with the
ability to generate affective music. Entertainment, healthcare, and
sensor-integrated interactive system design are a few of the areas in which
AI-based affective music generation (AI-AMG) systems may have a significant
impact. Given the surge of interest in this topic, this article aims to provide
a comprehensive review of AI-AMG systems. The main building blocks of an AI-AMG
system are discussed, and existing systems are formally categorized based on
the core algorithm used for music generation. In addition, this article
discusses the main musical features employed to compose affective music, along
with the respective AI-based approaches used for tailoring them. Lastly, the
main challenges and open questions in this field, as well as their potential
solutions, are presented to guide future research. We hope that this review
will be useful for readers seeking to understand the state-of-the-art in AI-AMG
systems, and gain an overview of the methods used for developing them, thereby
helping them explore this field in the future.
</summary>
    <author>
      <name>Adyasha Dash</name>
    </author>
    <author>
      <name>Kat R. Agres</name>
    </author>
    <link href="http://arxiv.org/abs/2301.06890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.06890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.09487v2</id>
    <updated>2023-02-22T05:18:08Z</updated>
    <published>2023-02-19T05:59:06Z</published>
    <title>Understanding how the use of AI decision support tools affect critical
  thinking and over-reliance on technology by drug dispensers in Tanzania</title>
    <summary>  The use of AI in healthcare is designed to improve care delivery and augment
the decisions of providers to enhance patient outcomes. When deployed in
clinical settings, the interaction between providers and AI is a critical
component for measuring and understanding the effectiveness of these digital
tools on broader health outcomes. Even in cases where AI algorithms have high
diagnostic accuracy, healthcare providers often still rely on their experience
and sometimes gut feeling to make a final decision. Other times, providers rely
unquestioningly on the outputs of the AI models, which leads to a concern about
over-reliance on the technology. The purpose of this research was to understand
how reliant drug shop dispensers were on AI-powered technologies when
determining a differential diagnosis for a presented clinical case vignette. We
explored how the drug dispensers responded to technology that is framed as
always correct in an attempt to measure whether they begin to rely on it
without any critical thought of their own. We found that dispensers relied on
the decision made by the AI 25 percent of the time, even when the AI provided
no explanation for its decision.
</summary>
    <author>
      <name>Ally Salim Jr</name>
    </author>
    <author>
      <name>Megan Allen</name>
    </author>
    <author>
      <name>Kelvin Mariki</name>
    </author>
    <author>
      <name>Kevin James Masoy</name>
    </author>
    <author>
      <name>Jafary Liana</name>
    </author>
    <link href="http://arxiv.org/abs/2302.09487v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.09487v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/math/0511132v2</id>
    <updated>2006-05-05T17:57:02Z</updated>
    <published>2005-11-05T11:00:25Z</published>
    <title>Strictly flat cyclic Fréchet modules and approximate identities</title>
    <summary>  Let A be a locally m-convex Fr\'echet algebra. We give a necessary and
sufficient condition for a cyclic Fr\'echet A-module X=A_+/I to be strictly
flat, generalizing thereby a criterion of Helemskii and Sheinberg. To this end,
we introduce a notion of locally bounded approximate identity (a.i.), and we
show that X is strictly flat if and only if the ideal I has a right locally
bounded a.i. An example is given of a commutative locally m-convex Fr\'echet
algebra that has a locally bounded a.i., but does not have a bounded a.i. On
the other hand, we show that a quasinormable locally m-convex Fr\'echet algebra
has a locally bounded a.i. if and only if it has a bounded a.i. Some
applications to amenable Fr\'echet algebras are also given.
</summary>
    <author>
      <name>A. Yu. Pirkovskii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages; version 2: a mistake in Theorem 3 is corrected</arxiv:comment>
    <link href="http://arxiv.org/abs/math/0511132v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/math/0511132v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 46M18, 46M10, 46H25; Secondary 46A45, 16D40, 18G50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.4311v1</id>
    <updated>2007-08-31T11:12:26Z</updated>
    <published>2007-08-31T11:12:26Z</published>
    <title>2006: Celebrating 75 years of AI - History and Outlook: the Next 25
  Years</title>
    <summary>  When Kurt Goedel layed the foundations of theoretical computer science in
1931, he also introduced essential concepts of the theory of Artificial
Intelligence (AI). Although much of subsequent AI research has focused on
heuristics, which still play a major role in many practical AI applications, in
the new millennium AI theory has finally become a full-fledged formal science,
with important optimality results for embodied agents living in unknown
environments, obtained through a combination of theory a la Goedel and
probability theory. Here we look back at important milestones of AI history,
mention essential recent results, and speculate about what we may expect from
the next 25 years, emphasizing the significance of the ongoing dramatic
hardware speedups, and discussing Goedel-inspired, self-referential,
self-improving universal problem solvers.
</summary>
    <author>
      <name>Juergen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages; preprint of invited contribution to the Proceedings of the
  ``50th Anniversary Summit of Artificial Intelligence'' at Monte Verita,
  Ascona, Switzerland, 9-14 July 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.4311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.4311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.4314v3</id>
    <updated>2008-05-16T10:46:24Z</updated>
    <published>2008-01-28T15:32:05Z</published>
    <title>Artificial Immune Systems (AIS) - A New Paradigm for Heuristic Decision
  Making</title>
    <summary>  Over the last few years, more and more heuristic decision making techniques
have been inspired by nature, e.g. evolutionary algorithms, ant colony
optimisation and simulated annealing. More recently, a novel computational
intelligence technique inspired by immunology has emerged, called Artificial
Immune Systems (AIS). This immune system inspired technique has already been
useful in solving some computational problems. In this keynote, we will very
briefly describe the immune system metaphors that are relevant to AIS. We will
then give some illustrative real-world problems suitable for AIS use and show a
step-by-step algorithm walkthrough. A comparison of AIS to other well-known
algorithms and areas for future work will round this keynote off. It should be
noted that as AIS is still a young and evolving field, there is not yet a fixed
algorithm template and hence actual implementations might differ somewhat from
the examples given here.
</summary>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Invited Keynote Talk, Annual Operational Research Conference 46,
  York, UK, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.4314v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.4314v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.0887v1</id>
    <updated>2012-10-02T19:28:42Z</updated>
    <published>2012-10-02T19:28:42Z</published>
    <title>The Definition of AI in Terms of Multi Agent Systems</title>
    <summary>  The questions which we will consider here are "What is AI?" and "How can we
make AI?". Here we will present the definition of AI in terms of multi-agent
systems. This means that here you will not find a new answer to the question
"What is AI?", but an old answer in a new form.
  This new form of the definition of AI is of interest for the theory of
multi-agent systems because it gives us better understanding of this theory.
More important is that this work will help us answer the second question. We
want to make a program which is capable of constructing a model of its
environment. Every multi-agent model is equivalent to a single-agent model but
multi-agent models are more natural and accordingly more easily discoverable.
</summary>
    <author>
      <name>Dimiter Dobrev</name>
    </author>
    <link href="http://arxiv.org/abs/1210.0887v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.0887v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.1521v1</id>
    <updated>2014-03-06T18:26:49Z</updated>
    <published>2014-03-06T18:26:49Z</published>
    <title>Approximation Models of Combat in StarCraft 2</title>
    <summary>  Real-time strategy (RTS) games make heavy use of artificial intelligence
(AI), especially in the design of computerized opponents. Because of the
computational complexity involved in managing all aspects of these games, many
AI opponents are designed to optimize only a few areas of playing style. In
games like StarCraft 2, a very popular and recently released RTS, most AI
strategies revolve around economic and building efficiency: AI opponents try to
gather and spend all resources as quickly and effectively as possible while
ensuring that no units are idle. The aim of this work was to help address the
need for AI combat strategies that are not computationally intensive. Our goal
was to produce a computationally efficient model that is accurate at predicting
the results of complex battles between diverse armies, including which army
will win and how many units will remain. Our results suggest it may be possible
to develop a relatively simple approximation model of combat that can
accurately predict many battles that do not involve micromanagement. Future
designs of AI opponents may be able to incorporate such an approximation model
into their decision and planning systems to provide a challenge that is
strategically balanced across all aspects of play.
</summary>
    <author>
      <name>Ian Helmke</name>
    </author>
    <author>
      <name>Daniel Kreymer</name>
    </author>
    <author>
      <name>Karl Wiegand</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.1521v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.1521v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06565v2</id>
    <updated>2016-07-25T17:23:29Z</updated>
    <published>2016-06-21T13:37:05Z</published>
    <title>Concrete Problems in AI Safety</title>
    <summary>  Rapid progress in machine learning and artificial intelligence (AI) has
brought increasing attention to the potential impacts of AI technologies on
society. In this paper we discuss one such potential impact: the problem of
accidents in machine learning systems, defined as unintended and harmful
behavior that may emerge from poor design of real-world AI systems. We present
a list of five practical research problems related to accident risk,
categorized according to whether the problem originates from having the wrong
objective function ("avoiding side effects" and "avoiding reward hacking"), an
objective function that is too expensive to evaluate frequently ("scalable
supervision"), or undesirable behavior during the learning process ("safe
exploration" and "distributional shift"). We review previous work in these
areas as well as suggesting research directions with a focus on relevance to
cutting-edge AI systems. Finally, we consider the high-level question of how to
think most productively about the safety of forward-looking applications of AI.
</summary>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Chris Olah</name>
    </author>
    <author>
      <name>Jacob Steinhardt</name>
    </author>
    <author>
      <name>Paul Christiano</name>
    </author>
    <author>
      <name>John Schulman</name>
    </author>
    <author>
      <name>Dan Mané</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06565v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06565v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02461v1</id>
    <updated>2017-02-06T14:10:28Z</updated>
    <published>2017-02-06T14:10:28Z</published>
    <title>Who Will Win Practical Artificial Intelligence? AI Engineerings in China</title>
    <summary>  Currently, Artificial Intelligence (AI) has won unprecedented attention and
is becoming the increasingly popular focus in China. This change can be judged
by the impressive record of academic publications, the amount of state-level
investment and the presence of nation-wide participation and devotion. In this
paper, we place emphasis on discussing the progress of artificial intelligence
engineerings in China. We first introduce the focus on AI in Chinese academia,
including the supercomputing brain system, Cambrian Period supercomputer of
neural networks, and biometric systems. Then, the development of AI in
industrial circles and the latest layout of AI products in companies, such as
Baidu, Tencent, and Alibaba, are introduced. Last, we bring in the opinions and
arguments of the main intelligentsia of China about the future development of
AI, including how to examine the relationship between humanity on one side and
science and technology on the other.
</summary>
    <author>
      <name>Huai-Yu Wu</name>
    </author>
    <author>
      <name>Feiyue Wang</name>
    </author>
    <author>
      <name>Chunhong Pan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Affiliations: 1 National Laboratory of Pattern Recognition, Institute
  of Automation, Chinese Academy of Sciences, Beijing, 100190, P.R. China. 2
  The State Key Laboratory of Management and Control for Complex Systems,
  Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, P.R.
  China</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.02461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09095v2</id>
    <updated>2017-10-18T06:40:51Z</updated>
    <published>2017-07-28T03:28:16Z</published>
    <title>Toward the Starting Line: A Systems Engineering Approach to Strong AI</title>
    <summary>  Artificial General Intelligence (AGI) or Strong AI aims to create machines
with human-like or human-level intelligence, which is still a very ambitious
goal when compared to the existing computing and AI systems. After many hype
cycles and lessons from AI history, it is clear that a big conceptual leap is
needed for crossing the starting line to kick-start mainstream AGI research.
This position paper aims to make a small conceptual contribution toward
reaching that starting line. After a broad analysis of the AGI problem from
different perspectives, a system-theoretic and engineering-based research
approach is introduced, which builds upon the existing mainstream AI and
systems foundations. Several promising cross-fertilization opportunities
between systems disciplines and AI research are identified. Specific potential
research directions are discussed.
</summary>
    <author>
      <name>Tansu Alpcan</name>
    </author>
    <author>
      <name>Sarah M. Erfani</name>
    </author>
    <author>
      <name>Christopher Leckie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09095v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09095v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.00260v1</id>
    <updated>2020-02-29T14:05:28Z</updated>
    <published>2020-02-29T14:05:28Z</published>
    <title>On Safety Assessment of Artificial Intelligence</title>
    <summary>  In this paper we discuss how systems with Artificial Intelligence (AI) can
undergo safety assessment. This is relevant, if AI is used in safety related
applications. Taking a deeper look into AI models, we show, that many models of
artificial intelligence, in particular machine learning, are statistical
models. Safety assessment would then have t o concentrate on the model that is
used in AI, besides the normal assessment procedure. Part of the budget of
dangerous random failures for the relevant safety integrity level needs to be
used for the probabilistic faulty behavior of the AI system. We demonstrate our
thoughts with a simple example and propose a research challenge that may be
decisive for the use of AI in safety related systems.
</summary>
    <author>
      <name>Jens Braband</name>
    </author>
    <author>
      <name>Hendrik Schäbe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.21683/1729-2646-2020-20-4-25-34</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.21683/1729-2646-2020-20-4-25-34" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dependability, vol. 20 no. 4, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.00260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.00260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.00749v1</id>
    <updated>2020-03-02T10:32:21Z</updated>
    <published>2020-03-02T10:32:21Z</published>
    <title>A general framework for scientifically inspired explanations in AI</title>
    <summary>  Explainability in AI is gaining attention in the computer science community
in response to the increasing success of deep learning and the important need
of justifying how such systems make predictions in life-critical applications.
The focus of explainability in AI has predominantly been on trying to gain
insights into how machine learning systems function by exploring relationships
between input data and predicted outcomes or by extracting simpler
interpretable models. Through literature surveys of philosophy and social
science, authors have highlighted the sharp difference between these generated
explanations and human-made explanations and claimed that current explanations
in AI do not take into account the complexity of human interaction to allow for
effective information passing to not-expert users. In this paper we instantiate
the concept of structure of scientific explanation as the theoretical
underpinning for a general framework in which explanations for AI systems can
be implemented. This framework aims to provide the tools to build a
"mental-model" of any AI system so that the interaction with the user can
provide information on demand and be closer to the nature of human-made
explanations. We illustrate how we can utilize this framework through two very
different examples: an artificial neural network and a Prolog solver and we
provide a possible implementation for both examples.
</summary>
    <author>
      <name>David Tuckey</name>
    </author>
    <author>
      <name>Alessandra Russo</name>
    </author>
    <author>
      <name>Krysia Broda</name>
    </author>
    <link href="http://arxiv.org/abs/2003.00749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.00749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.03576v1</id>
    <updated>2020-03-07T13:05:03Z</updated>
    <published>2020-03-07T13:05:03Z</published>
    <title>A machine learning environment for evaluating autonomous driving
  software</title>
    <summary>  Autonomous vehicles need safe development and testing environments. Many
traffic scenarios are such that they cannot be tested in the real world. We see
hybrid photorealistic simulation as a viable tool for developing AI (artificial
intelligence) software for autonomous driving. We present a machine learning
environment for detecting autonomous vehicle corner case behavior. Our
environment is based on connecting the CARLA simulation software to TensorFlow
machine learning framework and custom AI client software. The AI client
software receives data from a simulated world via virtual sensors and
transforms the data into information using machine learning models. The AI
clients control vehicles in the simulated world. Our environment monitors the
state assumed by the vehicle AIs to the ground truth state derived from the
simulation model. Our system can search for corner cases where the vehicle AI
is unable to correctly understand the situation. In our paper, we present the
overall hybrid simulator architecture and compare different configurations. We
present performance measurements from real setups, and outline the main
parameters affecting the hybrid simulator performance.
</summary>
    <author>
      <name>Jussi Hanhirova</name>
    </author>
    <author>
      <name>Anton Debner</name>
    </author>
    <author>
      <name>Matias Hyyppä</name>
    </author>
    <author>
      <name>Vesa Hirvisalo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 13 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Embedded World Conference 2019 Proceedings</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.03576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.03576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.07678v3</id>
    <updated>2020-03-26T21:03:35Z</updated>
    <published>2020-03-02T21:35:13Z</published>
    <title>An Overview and Case Study of the Clinical AI Model Development Life
  Cycle for Healthcare Systems</title>
    <summary>  Healthcare is one of the most promising areas for machine learning models to
make a positive impact. However, successful adoption of AI-based systems in
healthcare depends on engaging and educating stakeholders from diverse
backgrounds about the development process of AI models. We present a broadly
accessible overview of the development life cycle of clinical AI models that is
general enough to be adapted to most machine learning projects, and then give
an in-depth case study of the development process of a deep learning based
system to detect aortic aneurysms in Computed Tomography (CT) exams. We hope
other healthcare institutions and clinical practitioners find the insights we
share about the development process useful in informing their own model
development efforts and to increase the likelihood of successful deployment and
integration of AI in healthcare.
</summary>
    <author>
      <name>Charles Lu</name>
    </author>
    <author>
      <name>Julia Strout</name>
    </author>
    <author>
      <name>Romane Gauriau</name>
    </author>
    <author>
      <name>Brad Wright</name>
    </author>
    <author>
      <name>Fabiola Bezerra De Carvalho Marcruz</name>
    </author>
    <author>
      <name>Varun Buch</name>
    </author>
    <author>
      <name>Katherine Andriole</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for oral presentation at ICLR 2020, AI for Affordable
  Healthcare workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.07678v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.07678v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.11658v1</id>
    <updated>2020-03-25T21:53:59Z</updated>
    <published>2020-03-25T21:53:59Z</published>
    <title>Artificial Intelligence in Quantitative Ultrasound Imaging: A Review</title>
    <summary>  Quantitative ultrasound (QUS) imaging is a reliable, fast and inexpensive
technique to extract physically descriptive parameters for assessing
pathologies. Despite its safety and efficacy, QUS suffers from several major
drawbacks: poor imaging quality, inter- and intra-observer variability which
hampers the reproducibility of measurements. Therefore, it is in great need to
develop automatic method to improve the imaging quality and aid in measurements
in QUS. In recent years, there has been an increasing interest in artificial
intelligence (AI) applications in ultrasound imaging. However, no research has
been found that surveyed the AI use in QUS. The purpose of this paper is to
review recent research into the AI applications in QUS. This review first
introduces the AI workflow, and then discusses the various AI applications in
QUS. Finally, challenges and future potential AI applications in QUS are
discussed.
</summary>
    <author>
      <name>Boran Zhou</name>
    </author>
    <author>
      <name>Xiaofeng Yang</name>
    </author>
    <author>
      <name>Tian Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.11658v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.11658v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.2874v1</id>
    <updated>2009-10-15T13:47:02Z</updated>
    <published>2009-10-15T13:47:02Z</published>
    <title>An Agent Based Classification Model</title>
    <summary>  The major function of this model is to access the UCI Wisconsin Breast Can-
cer data-set[1] and classify the data items into two categories, which are
normal and anomalous. This kind of classifi cation can be referred as anomaly
detection, which discriminates anomalous behaviour from normal behaviour in
computer systems. One popular solution for anomaly detection is Artifi cial
Immune Sys- tems (AIS). AIS are adaptive systems inspired by theoretical
immunology and observed immune functions, principles and models which are
applied to prob- lem solving. The Dendritic Cell Algorithm (DCA)[2] is an AIS
algorithm that is developed specifi cally for anomaly detection. It has been
successfully applied to intrusion detection in computer security. It is
believed that agent-based mod- elling is an ideal approach for implementing
AIS, as intelligent agents could be the perfect representations of immune
entities in AIS. This model evaluates the feasibility of re-implementing the
DCA in an agent-based simulation environ- ment called AnyLogic, where the
immune entities in the DCA are represented by intelligent agents. If this model
can be successfully implemented, it makes it possible to implement more
complicated and adaptive AIS models in the agent-based simulation environment.
</summary>
    <author>
      <name>Feng Gu</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Julie Greensmith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures, 9th European Agent Systems Summer School, Durham,
  UK</arxiv:comment>
    <link href="http://arxiv.org/abs/0910.2874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.2874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.00977v1</id>
    <updated>2015-12-03T07:45:32Z</updated>
    <published>2015-12-03T07:45:32Z</published>
    <title>A Study on Artificial Intelligence IQ and Standard Intelligent Model</title>
    <summary>  Currently, potential threats of artificial intelligence (AI) to human have
triggered a large controversy in society, behind which, the nature of the issue
is whether the artificial intelligence (AI) system can be evaluated
quantitatively. This article analyzes and evaluates the challenges that the AI
development level is facing, and proposes that the evaluation methods for the
human intelligence test and the AI system are not uniform; and the key reason
for which is that none of the models can uniformly describe the AI system and
the beings like human. Aiming at this problem, a standard intelligent system
model is established in this study to describe the AI system and the beings
like human uniformly. Based on the model, the article makes an abstract
mathematical description, and builds the standard intelligent machine
mathematical model; expands the Von Neumann architecture and proposes the
Liufeng - Shiyong architecture; gives the definition of the artificial
intelligence IQ, and establishes the artificial intelligence scale and the
evaluation method; conduct the test on 50 search engines and three human
subjects at different ages across the world, and finally obtains the ranking of
the absolute IQ and deviation IQ ranking for artificial intelligence IQ 2014.
</summary>
    <author>
      <name>Feng Liu</name>
    </author>
    <author>
      <name>Yong Shi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.00977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.00977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03306v4</id>
    <updated>2016-10-13T06:56:55Z</updated>
    <published>2016-07-12T10:49:59Z</published>
    <title>An Automatic Identification System (AIS) Database for Maritime
  Trajectory Prediction and Data Mining</title>
    <summary>  In recent years, maritime safety and efficiency become more and more
important across the world. Automatic Identification System (AIS) tracks vessel
movement by onboard transceiver and terrestrial and/or satellite base station.
The data collected by AIS contains broadcast kinematic information and static
information. Both of them are useful for anomaly detection and route prediction
which are key techniques in intelligent maritime research area. This paper is
devoted to construct a standard AIS database for maritime trajectory learning,
prediction and data mining. A path prediction algorithm is tested on this AIS
database and the testing results show this database can be used as a
standardized training resource for different trajectory prediction algorithms
and other AIS data mining algorithms.
</summary>
    <author>
      <name>Shangbo Mao</name>
    </author>
    <author>
      <name>Enmei Tu</name>
    </author>
    <author>
      <name>Guanghao Zhang</name>
    </author>
    <author>
      <name>Lily Rachmawati</name>
    </author>
    <author>
      <name>Eshan Rajabally</name>
    </author>
    <author>
      <name>Guang-Bin Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in ELM2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.03306v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03306v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.04608v2</id>
    <updated>2019-02-01T09:31:08Z</updated>
    <published>2018-12-11T18:50:02Z</published>
    <title>Metrics for Explainable AI: Challenges and Prospects</title>
    <summary>  The question addressed in this paper is: If we present to a user an AI system
that explains how it works, how do we know whether the explanation works and
the user has achieved a pragmatic understanding of the AI? In other words, how
do we know that an explanainable AI system (XAI) is any good? Our focus is on
the key concepts of measurement. We discuss specific methods for evaluating:
(1) the goodness of explanations, (2) whether users are satisfied by
explanations, (3) how well users understand the AI systems, (4) how curiosity
motivates the search for explanations, (5) whether the user's trust and
reliance on the AI are appropriate, and finally, (6) how the human-XAI work
system performs. The recommendations we present derive from our integration of
extensive research literatures and our own psychometric evaluations.
</summary>
    <author>
      <name>Robert R. Hoffman</name>
    </author>
    <author>
      <name>Shane T. Mueller</name>
    </author>
    <author>
      <name>Gary Klein</name>
    </author>
    <author>
      <name>Jordan Litman</name>
    </author>
    <link href="http://arxiv.org/abs/1812.04608v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.04608v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.11519v1</id>
    <updated>2019-05-27T21:42:56Z</updated>
    <published>2019-05-27T21:42:56Z</published>
    <title>Open Platforms for Artificial Intelligence for Social Good: Common
  Patterns as a Pathway to True Impact</title>
    <summary>  The AI for social good movement has now reached a state in which a large
number of one-off demonstrations have illustrated that partnerships of AI
practitioners and social change organizations are possible and can address
problems faced in sustainable development. In this paper, we discuss how moving
from demonstrations to true impact on humanity will require a different course
of action, namely open platforms containing foundational AI capabilities to
support common needs of multiple organizations working in similar topical
areas. We lend credence to this proposal by describing three example patterns
of social good problems and their AI-based solutions: natural language
processing for making sense of international development reports, causal
inference for providing guidance to vulnerable individuals, and
discrimination-aware classification for supporting unbiased allocation
decisions. We argue that the development of such platforms will be possible
through convenings of social change organizations, AI companies, and
grantmaking foundations.
</summary>
    <author>
      <name>Kush R. Varshney</name>
    </author>
    <author>
      <name>Aleksandra Mojsilovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">appearing at the 2019 ICML AI for Social Good Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.11519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.11519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.08779v1</id>
    <updated>2020-11-17T17:14:28Z</updated>
    <published>2020-11-17T17:14:28Z</published>
    <title>Exploring Energy-Accuracy Tradeoffs in AI Hardware</title>
    <summary>  Artificial intelligence (AI) is playing an increasingly significant role in
our everyday lives. This trend is expected to continue, especially with recent
pushes to move more AI to the edge. However, one of the biggest challenges
associated with AI on edge devices (mobile phones, unmanned vehicles, sensors,
etc.) is their associated size, weight, and power constraints. In this work, we
consider the scenario where an AI system may need to operate at
less-than-maximum accuracy in order to meet application-dependent energy
requirements. We propose a simple function that divides the cost of using an AI
system into the cost of the decision making process and the cost of decision
execution. For simple binary decision problems with convolutional neural
networks, it is shown that minimizing the cost corresponds to using fewer than
the maximum number of resources (e.g. convolutional neural network layers and
filters). Finally, it is shown that the cost associated with energy can be
significantly reduced by leveraging high-confidence predictions made in
lower-level layers of the network.
</summary>
    <author>
      <name>Cory Merkel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in the proceedings of the 2020 International Green
  and Sustainable Computing Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.08779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.08779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.12863v1</id>
    <updated>2020-11-25T16:39:12Z</updated>
    <published>2020-11-25T16:39:12Z</published>
    <title>European Strategy on AI: Are we truly fostering social good?</title>
    <summary>  Artificial intelligence (AI) is already part of our daily lives and is
playing a key role in defining the economic and social shape of the future. In
2018, the European Commission introduced its AI strategy able to compete in the
next years with world powers such as China and US, but relying on the respect
of European values and fundamental rights. As a result, most of the Member
States have published their own National Strategy with the aim to work on a
coordinated plan for Europe. In this paper, we present an ongoing study on how
European countries are approaching the field of Artificial Intelligence, with
its promises and risks, through the lens of their national AI strategies. In
particular, we aim to investigate how European countries are investing in AI
and to what extent the stated plans can contribute to the benefit of the whole
society. This paper reports the main findings of a qualitative analysis of the
investment plans reported in 15 European National Strategies
</summary>
    <author>
      <name>Francesca Foffano</name>
    </author>
    <author>
      <name>Teresa Scantamburlo</name>
    </author>
    <author>
      <name>Atia Cortés</name>
    </author>
    <author>
      <name>Chiara Bissolo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figures, submitted at IJCAI 2020 Workshop on AI for Social
  Good</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.12863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.12863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00961v1</id>
    <updated>2017-04-04T11:29:02Z</updated>
    <published>2017-04-04T11:29:02Z</published>
    <title>Adaptive Motion Gaming AI for Health Promotion</title>
    <summary>  This paper presents a design of a non-player character (AI) for promoting
balancedness in use of body segments when engaging in full-body motion gaming.
In our experiment, we settle a battle between the proposed AI and a player by
using FightingICE, a fighting game platform for AI development. A middleware
called UKI is used to allow the player to control the game by using body motion
instead of the keyboard and mouse. During gameplay, the proposed AI analyze
health states of the player; it determines its next action by predicting how
each candidate action, recommended by a Monte-Carlo tree search algorithm, will
induce the player to move, and how the player's health tends to be affected.
Our result demonstrates successful improvement in balancedness in use of body
segments on 4 out of 5 subjects.
</summary>
    <author>
      <name>Pujana Paliyawan</name>
    </author>
    <author>
      <name>Takahiro Kusano</name>
    </author>
    <author>
      <name>Yuto Nakagawa</name>
    </author>
    <author>
      <name>Tomohiro Harada</name>
    </author>
    <author>
      <name>Ruck Thawonmas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A revised version of our paper for 2017 AAAI Spring Symposium Series
  (Well-Being AI: From Machine Learning to Subjective Oriented Computing), San
  Francisco,USA, Mar. 27-29, 2017. Revised contents, due to our correction of
  (8), are highlighted in red. Many apologies, but the effectiveness of the
  proposed method/approach in the paper still holds</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1; K.8.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03021v1</id>
    <updated>2017-05-16T20:57:36Z</updated>
    <published>2017-05-16T20:57:36Z</published>
    <title>Ethical Artificial Intelligence - An Open Question</title>
    <summary>  Artificial Intelligence (AI) is an effective science which employs strong
enough approaches, methods, and techniques to solve unsolvable real world based
problems. Because of its unstoppable rise towards the future, there are also
some discussions about its ethics and safety. Shaping an AI friendly
environment for people and a people friendly environment for AI can be a
possible answer for finding a shared context of values for both humans and
robots. In this context, objective of this paper is to address the ethical
issues of AI and explore the moral dilemmas that arise from ethical algorithms,
from pre set or acquired values. In addition, the paper will also focus on the
subject of AI safety. As general, the paper will briefly analyze the concerns
and potential solutions to solving the ethical issues presented and increase
readers awareness on AI safety as another related research interest.
</summary>
    <author>
      <name>Alice Pavaloiu</name>
    </author>
    <author>
      <name>Utku Kose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Multidisciplinary Developments, 2(2), 2017, 15-27</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.03021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.10813v1</id>
    <updated>2018-03-28T19:11:24Z</updated>
    <published>2018-03-28T19:11:24Z</published>
    <title>Artificial Intelligence and Robotics</title>
    <summary>  The recent successes of AI have captured the wildest imagination of both the
scientific communities and the general public. Robotics and AI amplify human
potentials, increase productivity and are moving from simple reasoning towards
human-like cognitive abilities. Current AI technologies are used in a set area
of applications, ranging from healthcare, manufacturing, transport, energy, to
financial services, banking, advertising, management consulting and government
agencies. The global AI market is around 260 billion USD in 2016 and it is
estimated to exceed 3 trillion by 2024. To understand the impact of AI, it is
important to draw lessons from it's past successes and failures and this white
paper provides a comprehensive explanation of the evolution of AI, its current
status and future directions.
</summary>
    <author>
      <name>Javier Andreu-Perez</name>
    </author>
    <author>
      <name>Fani Deligianni</name>
    </author>
    <author>
      <name>Daniele Ravi</name>
    </author>
    <author>
      <name>Guang-Zhong Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1803.10813v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.10813v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.11244v1</id>
    <updated>2018-03-27T05:58:11Z</updated>
    <published>2018-03-27T05:58:11Z</published>
    <title>Phronesis of AI in radiology: Superhuman meets natural stupidity</title>
    <summary>  Advances in AI in the last decade have clearly made economists, politicians,
journalists, and citizenry in general believe that the machines are coming to
take human jobs. We review 'superhuman' AI performance claims in radiology and
then provide a self-reflection on our own work in the area in the form of a
critical review, a tribute of sorts to McDermotts 1976 paper, asking the field
for some self-discipline. Clearly there is an opportunity to replace humans,
but there are better opportunities, as we have discovered to fit cognitive
abilities of human and non-humans. We performed one of the first studies in
radiology to see how human and AI performance can complement and improve each
others performance for detecting pneumonia in chest X-rays. We question if
there is a practical wisdom or phronesis that we need to demonstrate in AI
today as well as in our field. Using this, we articulate what AI as a field has
already and probably can in the future learn from Psychology, Cognitive
Science, Sociology and Science and Technology Studies.
</summary>
    <author>
      <name>Judy W. Gichoya</name>
    </author>
    <author>
      <name>Siddhartha Nuthakki</name>
    </author>
    <author>
      <name>Pallavi G. Maity</name>
    </author>
    <author>
      <name>Saptarshi Purkayastha</name>
    </author>
    <link href="http://arxiv.org/abs/1803.11244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.11244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04268v1</id>
    <updated>2018-04-12T01:22:50Z</updated>
    <published>2018-04-12T01:22:50Z</published>
    <title>Incomplete Contracting and AI Alignment</title>
    <summary>  We suggest that the analysis of incomplete contracting developed by law and
economics researchers can provide a useful framework for understanding the AI
alignment problem and help to generate a systematic approach to finding
solutions. We first provide an overview of the incomplete contracting
literature and explore parallels between this work and the problem of AI
alignment. As we emphasize, misalignment between principal and agent is a core
focus of economic analysis. We highlight some technical results from the
economics literature on incomplete contracts that may provide insights for AI
alignment researchers. Our core contribution, however, is to bring to bear an
insight that economists have been urged to absorb from legal scholars and other
behavioral scientists: the fact that human contracting is supported by
substantial amounts of external structure, such as generally available
institutions (culture, law) that can supply implied terms to fill the gaps in
incomplete contracts. We propose a research agenda for AI alignment work that
focuses on the problem of how to build AI that can replicate the human
cognitive processes that connect individual incomplete contracts with this
supporting external structure.
</summary>
    <author>
      <name>Dylan Hadfield-Menell</name>
    </author>
    <author>
      <name>Gillian Hadfield</name>
    </author>
    <link href="http://arxiv.org/abs/1804.04268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00154v1</id>
    <updated>2018-06-30T11:28:17Z</updated>
    <published>2018-06-30T11:28:17Z</published>
    <title>AI in Education needs interpretable machine learning: Lessons from Open
  Learner Modelling</title>
    <summary>  Interpretability of the underlying AI representations is a key raison
d'\^{e}tre for Open Learner Modelling (OLM) -- a branch of Intelligent Tutoring
Systems (ITS) research. OLMs provide tools for 'opening' up the AI models of
learners' cognition and emotions for the purpose of supporting human learning
and teaching. Over thirty years of research in ITS (also known as AI in
Education) produced important work, which informs about how AI can be used in
Education to best effects and, through the OLM research, what are the necessary
considerations to make it interpretable and explainable for the benefit of
learning. We argue that this work can provide a valuable starting point for a
framework of interpretable AI, and as such is of relevance to the application
of both knowledge-based and machine learning systems in other high-stakes
contexts, beyond education.
</summary>
    <author>
      <name>Cristina Conati</name>
    </author>
    <author>
      <name>Kaska Porayska-Pomsta</name>
    </author>
    <author>
      <name>Manolis Mavrikis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">presented at 2018 ICML Workshop on Human Interpretability in Machine
  Learning (WHI 2018), Stockholm, Sweden</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.08328v1</id>
    <updated>2018-09-21T21:52:34Z</updated>
    <published>2018-09-21T21:52:34Z</published>
    <title>On Quantifying and Understanding the Role of Ethics in AI Research: A
  Historical Account of Flagship Conferences and Journals</title>
    <summary>  Recent developments in AI, Machine Learning and Robotics have raised concerns
about the ethical consequences of both academic and industrial AI research.
Leading academics, businessmen and politicians have voiced an increasing number
of questions about the consequences of AI not only over people, but also on the
large-scale consequences on the the future of work and employment, its social
consequences and the sustainability of the planet. In this work, we analyse the
use and the occurrence of ethics-related research in leading AI, machine
learning and robotics venues. In order to do so we perform long term,
historical corpus-based analyses on a large number of flagship conferences and
journals. Our experiments identify the prominence of ethics-related terms in
published papers and presents several statistics on related topics. Finally,
this research provides quantitative evidence on the pressing ethical concerns
of the AI community.
</summary>
    <author>
      <name>Marcelo Prates</name>
    </author>
    <author>
      <name>Pedro Avelar</name>
    </author>
    <author>
      <name>Luis C. Lamb</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.29007/74gj</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.29007/74gj" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures, 3 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">GCAI-2018. 4th Global Conference on Artificial Intelligence (2018)
  vol. 55 pages 188--201</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.08328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04053v1</id>
    <updated>2018-10-08T16:35:06Z</updated>
    <published>2018-10-08T16:35:06Z</published>
    <title>The 30-Year Cycle In The AI Debate</title>
    <summary>  In the last couple of years, the rise of Artificial Intelligence and the
successes of academic breakthroughs in the field have been inescapable. Vast
sums of money have been thrown at AI start-ups. Many existing tech companies --
including the giants like Google, Amazon, Facebook, and Microsoft -- have
opened new research labs. The rapid changes in these everyday work and
entertainment tools have fueled a rising interest in the underlying technology
itself; journalists write about AI tirelessly, and companies -- of tech nature
or not -- brand themselves with AI, Machine Learning or Deep Learning whenever
they get a chance. Confronting squarely this media coverage, several analysts
are starting to voice concerns about over-interpretation of AI's blazing
successes and the sometimes poor public reporting on the topic. This paper
reviews briefly the track-record in AI and Machine Learning and finds this
pattern of early dramatic successes, followed by philosophical critique and
unexpected difficulties, if not downright stagnation, returning almost to the
clock in 30-year cycles since 1958.
</summary>
    <author>
      <name>Jean-Marie Chauvet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.04053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.03493v2</id>
    <updated>2019-01-21T19:04:47Z</updated>
    <published>2018-11-07T01:38:24Z</published>
    <title>Integrative Biological Simulation, Neuropsychology, and AI Safety</title>
    <summary>  We describe a biologically-inspired research agenda with parallel tracks
aimed at AI and AI safety. The bottom-up component consists of building a
sequence of biophysically realistic simulations of simple organisms such as the
nematode $Caenorhabditis$ $elegans$, the fruit fly $Drosophila$ $melanogaster$,
and the zebrafish $Danio$ $rerio$ to serve as platforms for research into AI
algorithms and system architectures. The top-down component consists of an
approach to value alignment that grounds AI goal structures in neuropsychology,
broadly considered. Our belief is that parallel pursuit of these tracks will
inform the development of value-aligned AI systems that have been inspired by
embodied organisms with sensorimotor integration. An important set of side
benefits is that the research trajectories we describe here are grounded in
long-standing intellectual traditions within existing research communities and
funding structures. In addition, these research programs overlap with
significant contemporary themes in the biological and psychological sciences
such as data/model integration and reproducibility.
</summary>
    <author>
      <name>Gopal P. Sarma</name>
    </author>
    <author>
      <name>Adam Safron</name>
    </author>
    <author>
      <name>Nick J. Hay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the AAAI Workshop on Artificial Intelligence Safety
  2019 co-located with the Thirty-Third AAAI Conference on Artificial
  Intelligence 2019 (AAAI 2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.03493v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.03493v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.11686v2</id>
    <updated>2019-07-19T06:21:06Z</updated>
    <published>2019-04-26T06:27:27Z</published>
    <title>The Roadmap to 6G -- AI Empowered Wireless Networks</title>
    <summary>  The recent upsurge of diversified mobile applications, especially those
supported by Artificial Intelligence (AI), is spurring heated discussions on
the future evolution of wireless communications. While 5G is being deployed
around the world, efforts from industry and academia have started to look
beyond 5G and conceptualize 6G. We envision 6G to undergo an unprecedented
transformation that will make it substantially different from the previous
generations of wireless cellular systems. In particular, 6G will go beyond
mobile Internet and will be required to support ubiquitous AI services from the
core to the end devices of the network. Meanwhile, AI will play a critical role
in designing and optimizing 6G architectures, protocols, and operations. In
this article, we discuss potential technologies for 6G to enable mobile AI
applications, as well as AI-enabled methodologies for 6G network design and
optimization. Key trends in the evolution to 6G will also be discussed.
</summary>
    <author>
      <name>Khaled B. Letaief</name>
    </author>
    <author>
      <name>Wei Chen</name>
    </author>
    <author>
      <name>Yuanming Shi</name>
    </author>
    <author>
      <name>Jun Zhang</name>
    </author>
    <author>
      <name>Ying-Jun Angela Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.11686v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.11686v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.03106v1</id>
    <updated>2019-05-30T18:56:38Z</updated>
    <published>2019-05-30T18:56:38Z</published>
    <title>Smart Sustainable Agriculture (SSA) Solution Underpinned by Internet of
  Things (IoT) and Artificial Intelligence (AI)</title>
    <summary>  The Internet of Things (IoT) and Artificial Intelligence (AI) have been
employed in agriculture over a long period of time, alongside other advanced
computing technologies. However, increased attention is currently being paid to
the use of such smart technologies. Agriculture has provided an important
source of food for human beings over many thousands of years, including the
development of appropriate farming methods for different types of crops. The
emergence of new advanced IoT technologies has the potential to monitor the
agricultural environment to ensure high-quality products. However, there
remains a lack of research and development in relation to Smart Sustainable
Agriculture (SSA), accompanied by complex obstacles arising from the
fragmentation of agricultural processes, i.e. the control and operation of
IoT/AI machines; data sharing and management; interoperability; and large
amounts of data analysis and storage. This study firstly, explores existing
IoT/AI technologies adopted for SSA and secondly, identifies IoT/AI technical
architecture capable of underpinning the development of SSA platforms. As well
as contributing to the current body of knowledge, this research reviews
research and development within SSA and provides an IoT/AI architecture to
establish a Smart, Sustainable Agriculture platform as a solution.
</summary>
    <author>
      <name>Eissa Alreshidi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14569/IJACSA.2019.0100513</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14569/IJACSA.2019.0100513" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computer Science and
  Applications Vol. 10, No. 5, 2019. Pages 93-102</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.03106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.02619v1</id>
    <updated>2019-08-06T12:56:17Z</updated>
    <published>2019-08-06T12:56:17Z</published>
    <title>Experiential AI</title>
    <summary>  Experiential AI is proposed as a new research agenda in which artists and
scientists come together to dispel the mystery of algorithms and make their
mechanisms vividly apparent. It addresses the challenge of finding novel ways
of opening up the field of artificial intelligence to greater transparency and
collaboration between human and machine. The hypothesis is that art can mediate
between computer code and human comprehension to overcome the limitations of
explanations in and for AI systems. Artists can make the boundaries of systems
visible and offer novel ways to make the reasoning of AI transparent and
decipherable. Beyond this, artistic practice can explore new configurations of
humans and algorithms, mapping the terrain of inter-agencies between people and
machines. This helps to viscerally understand the complex causal chains in
environments with AI components, including questions about what data to collect
or who to collect it about, how the algorithms are chosen, commissioned and
configured or how humans are conditioned by their participation in algorithmic
processes.
</summary>
    <author>
      <name>Drew Hemment</name>
    </author>
    <author>
      <name>Ruth Aylett</name>
    </author>
    <author>
      <name>Vaishak Belle</name>
    </author>
    <author>
      <name>Dave Murray-Rust</name>
    </author>
    <author>
      <name>Ewa Luger</name>
    </author>
    <author>
      <name>Jane Hillston</name>
    </author>
    <author>
      <name>Michael Rovatsos</name>
    </author>
    <author>
      <name>Frank Broz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in AI Matters 5(1): 25-31 (2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.02619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.10414v2</id>
    <updated>2019-09-02T16:56:28Z</updated>
    <published>2019-08-27T19:13:55Z</published>
    <title>Artificial Intelligence Fairness in the Context of Accessibility
  Research on Intelligent Systems for People who are Deaf or Hard of Hearing</title>
    <summary>  We discuss issues of Artificial Intelligence (AI) fairness for people with
disabilities, with examples drawn from our research on human-computer
interaction (HCI) for AI-based systems for people who are Deaf or Hard of
Hearing (DHH). In particular, we discuss the need for inclusion of data from
people with disabilities in training sets, the lack of interpretability of AI
systems, ethical responsibilities of access technology researchers and
companies, the need for appropriate evaluation metrics for AI-based access
technologies (to determine if they are ready to be deployed and if they can be
trusted by users), and the ways in which AI systems influence human behavior
and influence the set of abilities needed by users to successfully interact
with computing systems.
</summary>
    <author>
      <name>Sushant Kafle</name>
    </author>
    <author>
      <name>Abraham Glasser</name>
    </author>
    <author>
      <name>Sedeeq Al-khazraji</name>
    </author>
    <author>
      <name>Larwan Berke</name>
    </author>
    <author>
      <name>Matthew Seita</name>
    </author>
    <author>
      <name>Matt Huenerfauth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, ACM ASSETS 2019 Workshop on AI Fairness for People with
  Disabilities</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.10414v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10414v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02330v2</id>
    <updated>2020-06-15T23:19:40Z</updated>
    <published>2019-10-05T21:04:27Z</published>
    <title>Towards Deployment of Robust AI Agents for Human-Machine Partnerships</title>
    <summary>  We study the problem of designing AI agents that can robustly cooperate with
people in human-machine partnerships. Our work is inspired by real-life
scenarios in which an AI agent, e.g., a virtual assistant, has to cooperate
with new users after its deployment. We model this problem via a parametric MDP
framework where the parameters correspond to a user's type and characterize her
behavior. In the test phase, the AI agent has to interact with a user of
unknown type. Our approach to designing a robust AI agent relies on observing
the user's actions to make inferences about the user's type and adapting its
policy to facilitate efficient cooperation. We show that without being
adaptive, an AI agent can end up performing arbitrarily bad in the test phase.
We develop two algorithms for computing policies that automatically adapt to
the user in the test phase. We demonstrate the effectiveness of our approach in
solving a two-agent collaborative task.
</summary>
    <author>
      <name>Ahana Ghosh</name>
    </author>
    <author>
      <name>Sebastian Tschiatschek</name>
    </author>
    <author>
      <name>Hamed Mahdavi</name>
    </author>
    <author>
      <name>Adish Singla</name>
    </author>
    <link href="http://arxiv.org/abs/1910.02330v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02330v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.03014v1</id>
    <updated>2019-10-07T19:04:02Z</updated>
    <published>2019-10-07T19:04:02Z</published>
    <title>Artificial Intelligence: Powering Human Exploration of the Moon and Mars</title>
    <summary>  Over the past decade, the NASA Autonomous Systems and Operations (ASO)
project has developed and demonstrated numerous autonomy enabling technologies
employing AI techniques. Our work has employed AI in three distinct ways to
enable autonomous mission operations capabilities. Crew Autonomy gives
astronauts tools to assist in the performance of each of these mission
operations functions. Vehicle System Management uses AI techniques to turn the
astronaut's spacecraft into a robot, allowing it to operate when astronauts are
not present, or to reduce astronaut workload. AI technology also enables
Autonomous Robots as crew assistants or proxies when the crew are not present.
We first describe human spaceflight mission operations capabilities. We then
describe the ASO project, and the development and demonstration performed by
ASO since 2011. We will describe the AI techniques behind each of these
demonstrations, which include a variety of symbolic automated reasoning and
machine learning based approaches. Finally, we conclude with an assessment of
future development needs for AI to enable NASA's future Exploration missions.
</summary>
    <author>
      <name>Jeremy D. Frank</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at AAAI FSS-19: Artificial Intelligence in Government and
  Public Sector, Arlington, Virginia, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.03014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.03014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03585v2</id>
    <updated>2020-05-01T20:50:59Z</updated>
    <published>2019-11-08T23:54:18Z</published>
    <title>When Machine Learning Meets Wireless Cellular Networks: Deployment,
  Challenges, and Applications</title>
    <summary>  Artificial intelligence (AI) powered wireless networks promise to
revolutionize the conventional operation and structure of current networks from
network design to infrastructure management, cost reduction, and user
performance improvement. Empowering future networks with AI functionalities
will enable a shift from reactive/incident driven operations to
proactive/data-driven operations. This paper provides an overview on the
integration of AI functionalities in 5G and beyond networks. Key factors for
successful AI integration such as data, security, and explainable AI are
highlighted. We also summarize the various types of network intelligence as
well as machine learning based air interface in future networks. Use case
examples for the application of AI to the wireless domain are then summarized.
We highlight on applications to the physical layer, mobility management,
wireless security, and localization.
</summary>
    <author>
      <name>Ursula Challita</name>
    </author>
    <author>
      <name>Henrik A. Ryden</name>
    </author>
    <author>
      <name>Hugo Tullberg</name>
    </author>
    <link href="http://arxiv.org/abs/1911.03585v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03585v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.02675v1</id>
    <updated>2019-12-05T16:02:02Z</updated>
    <published>2019-12-05T16:02:02Z</published>
    <title>In AI We Trust? Factors That Influence Trustworthiness of AI-infused
  Decision-Making Processes</title>
    <summary>  Many decision-making processes have begun to incorporate an AI element,
including prison sentence recommendations, college admissions, hiring, and
mortgage approval. In all of these cases, AI models are being trained to help
human decision makers reach accurate and fair judgments, but little is known
about what factors influence the extent to which people consider an AI-infused
decision-making process to be trustworthy. We aim to understand how different
factors about a decision-making process, and an AI model that supports that
process, influences peoples' perceptions of the trustworthiness of that
process. We report on our evaluation of how seven different factors -- decision
stakes, decision authority, model trainer, model interpretability, social
transparency, and model confidence -- influence ratings of trust in a
scenario-based study.
</summary>
    <author>
      <name>Maryam Ashoori</name>
    </author>
    <author>
      <name>Justin D. Weisz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.02675v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.02675v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.10005v1</id>
    <updated>2019-12-13T23:35:18Z</updated>
    <published>2019-12-13T23:35:18Z</published>
    <title>Does AlphaGo actually play Go? Concerning the State Space of Artificial
  Intelligence</title>
    <summary>  The overarching goal of this paper is to develop a general model of the state
space of AI. Given the breathtaking progress in AI research and technologies in
recent years, such conceptual work is of substantial theoretical interest. The
present AI hype is mainly driven by the triumph of deep learning neural
networks. As the distinguishing feature of such networks is the ability to
self-learn, self-learning is identified as one important dimension of the AI
state space. Another main dimension lies in the possibility to go over from
specific to more general types of problems. The third main dimension is
provided by semantic grounding. Since this is a philosophically complex and
controversial dimension, a larger part of the paper is devoted to it. We take a
fresh look at known foundational arguments in the philosophy of mind and
cognition that are gaining new relevance in view of the recent AI developments
including the blockhead objection, the Turing test, the symbol grounding
problem, the Chinese room argument, and general use-theoretic considerations of
meaning. Finally, the AI state space, spanned by the main dimensions
generalization, grounding and "selfx-ness", possessing self-x properties such
as self-learning, is outlined.
</summary>
    <author>
      <name>Holger Lyre</name>
    </author>
    <link href="http://arxiv.org/abs/1912.10005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.10005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.12835v1</id>
    <updated>2019-12-30T07:38:38Z</updated>
    <published>2019-12-30T07:38:38Z</published>
    <title>U.S. Public Opinion on the Governance of Artificial Intelligence</title>
    <summary>  Artificial intelligence (AI) has widespread societal implications, yet social
scientists are only beginning to study public attitudes toward the technology.
Existing studies find that the public's trust in institutions can play a major
role in shaping the regulation of emerging technologies. Using a large-scale
survey (N=2000), we examined Americans' perceptions of 13 AI governance
challenges as well as their trust in governmental, corporate, and
multistakeholder institutions to responsibly develop and manage AI. While
Americans perceive all of the AI governance issues to be important for tech
companies and governments to manage, they have only low to moderate trust in
these institutions to manage AI applications.
</summary>
    <author>
      <name>Baobao Zhang</name>
    </author>
    <author>
      <name>Allan Dafoe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages; 7 figures; 4 tables; accepted for oral presentation at the
  2020 AAAI/ACM Conference on AI, Ethics, and Society</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.12835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.12835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00006v1</id>
    <updated>2019-12-29T17:42:06Z</updated>
    <published>2019-12-29T17:42:06Z</published>
    <title>Learning from Learning Machines: Optimisation, Rules, and Social Norms</title>
    <summary>  There is an analogy between machine learning systems and economic entities in
that they are both adaptive, and their behaviour is specified in a more-or-less
explicit way. It appears that the area of AI that is most analogous to the
behaviour of economic entities is that of morally good decision-making, but it
is an open question as to how precisely moral behaviour can be achieved in an
AI system. This paper explores the analogy between these two complex systems,
and we suggest that a clearer understanding of this apparent analogy may help
us forward in both the socio-economic domain and the AI domain: known results
in economics may help inform feasible solutions in AI safety, but also known
results in AI may inform economic policy. If this claim is correct, then the
recent successes of deep learning for AI suggest that more implicit
specifications work better than explicit ones for solving such problems.
</summary>
    <author>
      <name>Travis LaCroix</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/2001.00006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.05149v2</id>
    <updated>2020-01-19T06:04:44Z</updated>
    <published>2020-01-15T06:44:52Z</published>
    <title>CheXplain: Enabling Physicians to Explore and UnderstandData-Driven,
  AI-Enabled Medical Imaging Analysis</title>
    <summary>  The recent development of data-driven AI promises to automate medical
diagnosis; however, most AI functions as 'black boxes' to physicians with
limited computational knowledge. Using medical imaging as a point of departure,
we conducted three iterations of design activities to formulate CheXplain---a
system that enables physicians to explore and understand AI-enabled chest X-ray
analysis: (1) a paired survey between referring physicians and radiologists
reveals whether, when, and what kinds of explanations are needed; (2) a
low-fidelity prototype co-designed with three physicians formulates eight key
features; and (3) a high-fidelity prototype evaluated by another six physicians
provides detailed summative insights on how each feature enables the
exploration and understanding of AI. We summarize by discussing recommendations
for future work to design and implement explainable medical AI systems that
encompass four recurring themes: motivation, constraint, explanation, and
justification.
</summary>
    <author>
      <name>Yao Xie</name>
    </author>
    <author>
      <name>Melody Chen</name>
    </author>
    <author>
      <name>David Kao</name>
    </author>
    <author>
      <name>Ge Gao</name>
    </author>
    <author>
      <name>Xiang 'Anthony' Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3313831.3376807</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3313831.3376807" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.05149v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05149v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.07038v4</id>
    <updated>2021-03-22T17:08:41Z</updated>
    <published>2020-01-20T10:09:50Z</published>
    <title>Measuring Diversity of Artificial Intelligence Conferences</title>
    <summary>  The lack of diversity of the Artificial Intelligence (AI) field is nowadays a
concern, and several initiatives such as funding schemes and mentoring programs
have been designed to overcome it. However, there is no indication on how these
initiatives actually impact AI diversity in the short and long term. This work
studies the concept of diversity in this particular context and proposes a
small set of diversity indicators (i.e. indexes) of AI scientific events. These
indicators are designed to quantify the diversity of the AI field and monitor
its evolution. We consider diversity in terms of gender, geographical location
and business (understood as the presence of academia versus industry). We
compute these indicators for the different communities of a conference:
authors, keynote speakers and organizing committee. From these components we
compute a summarized diversity indicator for each AI event. We evaluate the
proposed indexes for a set of recent major AI conferences and we discuss their
values and limitations.
</summary>
    <author>
      <name>Ana Freire</name>
    </author>
    <author>
      <name>Lorenzo Porcaro</name>
    </author>
    <author>
      <name>Emilia Gómez</name>
    </author>
    <link href="http://arxiv.org/abs/2001.07038v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.07038v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.07537v1</id>
    <updated>2020-01-21T13:51:36Z</updated>
    <published>2020-01-21T13:51:36Z</published>
    <title>AI Trust in business processes: The need for process-aware explanations</title>
    <summary>  Business processes underpin a large number of enterprise operations including
processing loan applications, managing invoices, and insurance claims. There is
a large opportunity for infusing AI to reduce cost or provide better customer
experience, and the business process management (BPM) literature is rich in
machine learning solutions including unsupervised learning to gain insights on
clusters of process traces, classification models to predict the outcomes,
duration, or paths of partial process traces, extracting business process from
documents, and models to recommend how to optimize a business process or
navigate decision points. More recently, deep learning models including those
from the NLP domain have been applied to process predictions.
  Unfortunately, very little of these innovations have been applied and adopted
by enterprise companies. We assert that a large reason for the lack of adoption
of AI models in BPM is that business users are risk-averse and do not
implicitly trust AI models. There has, unfortunately, been little attention
paid to explaining model predictions to business users with process context. We
challenge the BPM community to build on the AI interpretability literature, and
the AI Trust community to understand
</summary>
    <author>
      <name>Steve T. K. Jan</name>
    </author>
    <author>
      <name>Vatche Ishakian</name>
    </author>
    <author>
      <name>Vinod Muthusamy</name>
    </author>
    <link href="http://arxiv.org/abs/2001.07537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.07537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.07641v3</id>
    <updated>2021-12-02T13:11:55Z</updated>
    <published>2020-01-21T16:41:22Z</published>
    <title>Deceptive AI Explanations: Creation and Detection</title>
    <summary>  Artificial intelligence (AI) comes with great opportunities but can also pose
significant risks. Automatically generated explanations for decisions can
increase transparency and foster trust, especially for systems based on
automated predictions by AI models. However, given, e.g., economic incentives
to create dishonest AI, to what extent can we trust explanations? To address
this issue, our work investigates how AI models (i.e., deep learning, and
existing instruments to increase transparency regarding AI decisions) can be
used to create and detect deceptive explanations. As an empirical evaluation,
we focus on text classification and alter the explanations generated by
GradCAM, a well-established explanation technique in neural networks. Then, we
evaluate the effect of deceptive explanations on users in an experiment with
200 participants. Our findings confirm that deceptive explanations can indeed
fool humans. However, one can deploy machine learning (ML) methods to detect
seemingly minor deception attempts with accuracy exceeding 80% given sufficient
domain knowledge. Without domain knowledge, one can still infer inconsistencies
in the explanations in an unsupervised manner, given basic knowledge of the
predictive model under scrutiny.
</summary>
    <author>
      <name>Johannes Schneider</name>
    </author>
    <author>
      <name>Christian Meske</name>
    </author>
    <author>
      <name>Michalis Vlachos</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Agents and Artificial Intelligence
  (2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2001.07641v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.07641v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09388v1</id>
    <updated>2020-01-26T02:33:52Z</updated>
    <published>2020-01-26T02:33:52Z</published>
    <title>AI-Powered GUI Attack and Its Defensive Methods</title>
    <summary>  Since the first Graphical User Interface (GUI) prototype was invented in the
1970s, GUI systems have been deployed into various personal computer systems
and server platforms. Recently, with the development of artificial intelligence
(AI) technology, malicious malware powered by AI is emerging as a potential
threat to GUI systems. This type of AI-based cybersecurity attack, targeting at
GUI systems, is explored in this paper. It is twofold: (1) A malware is
designed to attack the existing GUI system by using AI-based object recognition
techniques. (2) Its defensive methods are discovered by generating adversarial
examples and other methods to alleviate the threats from the intelligent GUI
attack. The results have shown that a generic GUI attack can be implemented and
performed in a simple way based on current AI techniques and its
countermeasures are temporary but effective to mitigate the threats of GUI
attack so far.
</summary>
    <author>
      <name>Ning Yu</name>
    </author>
    <author>
      <name>Zachary Tuttle</name>
    </author>
    <author>
      <name>Carl Jake Thurnau</name>
    </author>
    <author>
      <name>Emmanuel Mireku</name>
    </author>
    <link href="http://arxiv.org/abs/2001.09388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09768v2</id>
    <updated>2020-10-05T12:03:19Z</updated>
    <published>2020-01-13T10:32:16Z</published>
    <title>Artificial Intelligence, Values and Alignment</title>
    <summary>  This paper looks at philosophical questions that arise in the context of AI
alignment. It defends three propositions. First, normative and technical
aspects of the AI alignment problem are interrelated, creating space for
productive engagement between people working in both domains. Second, it is
important to be clear about the goal of alignment. There are significant
differences between AI that aligns with instructions, intentions, revealed
preferences, ideal preferences, interests and values. A principle-based
approach to AI alignment, which combines these elements in a systematic way,
has considerable advantages in this context. Third, the central challenge for
theorists is not to identify 'true' moral principles for AI; rather, it is to
identify fair principles for alignment, that receive reflective endorsement
despite widespread variation in people's moral beliefs. The final part of the
paper explores three ways in which fair principles for AI alignment could
potentially be identified.
</summary>
    <author>
      <name>Iason Gabriel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11023-020-09539-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11023-020-09539-2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Minds and Machines 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2001.09768v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09768v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.01621v1</id>
    <updated>2020-02-05T03:31:48Z</updated>
    <published>2020-02-05T03:31:48Z</published>
    <title>Joint Optimization of AI Fairness and Utility: A Human-Centered Approach</title>
    <summary>  Today, AI is increasingly being used in many high-stakes decision-making
applications in which fairness is an important concern. Already, there are many
examples of AI being biased and making questionable and unfair decisions. The
AI research community has proposed many methods to measure and mitigate
unwanted biases, but few of them involve inputs from human policy makers. We
argue that because different fairness criteria sometimes cannot be
simultaneously satisfied, and because achieving fairness often requires
sacrificing other objectives such as model accuracy, it is key to acquire and
adhere to human policy makers' preferences on how to make the tradeoff among
these objectives. In this paper, we propose a framework and some exemplar
methods for eliciting such preferences and for optimizing an AI model according
to these preferences.
</summary>
    <author>
      <name>Yunfeng Zhang</name>
    </author>
    <author>
      <name>Rachel K. E. Bellamy</name>
    </author>
    <author>
      <name>Kush R. Varshney</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3375627.3375862</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3375627.3375862" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in AIES 2020 proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.01621v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.01621v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.02526v1</id>
    <updated>2020-01-11T17:15:58Z</updated>
    <published>2020-01-11T17:15:58Z</published>
    <title>How to Answer Why -- Evaluating the Explanations of AI Through Mental
  Model Analysis</title>
    <summary>  To achieve optimal human-system integration in the context of user-AI
interaction it is important that users develop a valid representation of how AI
works. In most of the everyday interaction with technical systems users
construct mental models (i.e., an abstraction of the anticipated mechanisms a
system uses to perform a given task). If no explicit explanations are provided
by a system (e.g. by a self-explaining AI) or other sources (e.g. an
instructor), the mental model is typically formed based on experiences, i.e.
the observations of the user during the interaction. The congruence of this
mental model and the actual systems functioning is vital, as it is used for
assumptions, predictions and consequently for decisions regarding system use. A
key question for human-centered AI research is therefore how to validly survey
users' mental models. The objective of the present research is to identify
suitable elicitation methods for mental model analysis. We evaluated whether
mental models are suitable as an empirical research method. Additionally,
methods of cognitive tutoring are integrated. We propose an exemplary method to
evaluate explainable AI approaches in a human-centered way.
</summary>
    <author>
      <name>Tim Schrills</name>
    </author>
    <author>
      <name>Thomas Franke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.02526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.02526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.03493v1</id>
    <updated>2020-02-10T01:32:24Z</updated>
    <published>2020-02-10T01:32:24Z</published>
    <title>AI-oriented Medical Workload Allocation for Hierarchical
  Cloud/Edge/Device Computing</title>
    <summary>  In a hierarchically-structured cloud/edge/device computing environment,
workload allocation can greatly affect the overall system performance. This
paper deals with AI-oriented medical workload generated in emergency rooms (ER)
or intensive care units (ICU) in metropolitan areas. The goal is to optimize
AI-workload allocation to cloud clusters, edge servers, and end devices so that
minimum response time can be achieved in life-saving emergency applications.
  In particular, we developed a new workload allocation method for the AI
workload in distributed cloud/edge/device computing systems. An efficient
scheduling and allocation strategy is developed in order to reduce the overall
response time to satisfy multi-patient demands. We apply several ICU AI
workloads from a comprehensive edge computing benchmark Edge AIBench. The
healthcare AI applications involved are short-of-breath alerts, patient
phenotype classification, and life-death threats. Our experimental results
demonstrate the high efficiency and effectiveness in real-life health-care and
emergency applications.
</summary>
    <author>
      <name>Tianshu Hao</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <author>
      <name>Kai Hwang</name>
    </author>
    <author>
      <name>Wanling Gao</name>
    </author>
    <author>
      <name>Xu Wen</name>
    </author>
    <link href="http://arxiv.org/abs/2002.03493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.08377v2</id>
    <updated>2020-11-09T16:09:11Z</updated>
    <published>2020-04-17T17:57:07Z</published>
    <title>ECCOLA -- a Method for Implementing Ethically Aligned AI Systems</title>
    <summary>  Various recent Artificial Intelligence (AI) system failures, some of which
have made the global headlines, have highlighted issues in these systems. These
failures have resulted in calls for more ethical AI systems that better take
into account their effects on various stakeholders. However, implementing AI
ethics into practice is still an on-going challenge. High-level guidelines for
doing so exist, devised by governments and private organizations alike, but
lack practicality for developers. To address this issue, in this paper, we
present a method for implementing AI ethics. The method, ECCOLA, has been
iteratively developed using a cyclical action design research approach. The
method aims at making the high-level AI ethics principles more practical,
making it possible for developers to more easily implement them in practice.
</summary>
    <author>
      <name>Ville Vakkuri</name>
    </author>
    <author>
      <name>Kai-Kristian Kemell</name>
    </author>
    <author>
      <name>Pekka Abrahamsson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SEAA51224.2020.00043</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SEAA51224.2020.00043" rel="related"/>
    <link href="http://arxiv.org/abs/2004.08377v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.08377v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.04707v1</id>
    <updated>2020-06-08T16:04:44Z</updated>
    <published>2020-06-08T16:04:44Z</published>
    <title>Principles to Practices for Responsible AI: Closing the Gap</title>
    <summary>  Companies have considered adoption of various high-level artificial
intelligence (AI) principles for responsible AI, but there is less clarity on
how to implement these principles as organizational practices. This paper
reviews the principles-to-practices gap. We outline five explanations for this
gap ranging from a disciplinary divide to an overabundance of tools. In turn,
we argue that an impact assessment framework which is broad, operationalizable,
flexible, iterative, guided, and participatory is a promising approach to close
the principles-to-practices gap. Finally, to help practitioners with applying
these recommendations, we review a case study of AI's use in forest ecosystem
restoration, demonstrating how an impact assessment framework can translate
into effective and responsible AI practices.
</summary>
    <author>
      <name>Daniel Schiff</name>
    </author>
    <author>
      <name>Bogdana Rakova</name>
    </author>
    <author>
      <name>Aladdin Ayesh</name>
    </author>
    <author>
      <name>Anat Fanti</name>
    </author>
    <author>
      <name>Michael Lennon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint draft. A version has been submitted to the 2020 European
  Conference on AI (ECAI) Workshop on "ADVANCING TOWARDS THE SDGs: ARTIFICIAL
  INTELLIGENCE FOR A FAIR, JUST AND EQUITABLE WORLD (AI4EQ)"</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.04707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.04707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; K.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.09529v1</id>
    <updated>2020-06-16T21:34:44Z</updated>
    <published>2020-06-16T21:34:44Z</published>
    <title>Quality Management of Machine Learning Systems</title>
    <summary>  In the past decade, Artificial Intelligence (AI) has become a part of our
daily lives due to major advances in Machine Learning (ML) techniques. In spite
of an explosive growth in the raw AI technology and in consumer facing
applications on the internet, its adoption in business applications has
conspicuously lagged behind. For business/mission-critical systems, serious
concerns about reliability and maintainability of AI applications remain. Due
to the statistical nature of the output, software 'defects' are not well
defined. Consequently, many traditional quality management techniques such as
program debugging, static code analysis, functional testing, etc. have to be
reevaluated. Beyond the correctness of an AI model, many other new quality
attributes, such as fairness, robustness, explainability, transparency, etc.
become important in delivering an AI system. The purpose of this paper is to
present a view of a holistic quality management framework for ML applications
based on the current advances and identify new areas of software engineering
research to achieve a more trustworthy AI.
</summary>
    <author>
      <name>P. Santhanam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AAAI-20 Workshop on Engineering Dependable and Secure Machine
  Learning Systems- February 7, 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.09529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.09529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.12358v4</id>
    <updated>2021-03-03T01:59:43Z</updated>
    <published>2020-06-22T15:57:30Z</published>
    <title>Where Responsible AI meets Reality: Practitioner Perspectives on
  Enablers for shifting Organizational Practices</title>
    <summary>  Large and ever-evolving technology companies continue to invest more time and
resources to incorporate responsible Artificial Intelligence (AI) into
production-ready systems to increase algorithmic accountability. This paper
examines and seeks to offer a framework for analyzing how organizational
culture and structure impact the effectiveness of responsible AI initiatives in
practice. We present the results of semi-structured qualitative interviews with
practitioners working in industry, investigating common challenges, ethical
tensions, and effective enablers for responsible AI initiatives. Focusing on
major companies developing or utilizing AI, we have mapped what organizational
structures currently support or hinder responsible AI initiatives, what
aspirational future processes and structures would best enable effective
initiatives, and what key elements comprise the transition from current work
practices to the aspirational future.
</summary>
    <author>
      <name>Bogdana Rakova</name>
    </author>
    <author>
      <name>Jingying Yang</name>
    </author>
    <author>
      <name>Henriette Cramer</name>
    </author>
    <author>
      <name>Rumman Chowdhury</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3449081</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3449081" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 24th ACM Conference on Computer-Supported
  Cooperative Work and Social Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.12358v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.12358v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.13555v1</id>
    <updated>2020-06-24T08:26:49Z</updated>
    <published>2020-06-24T08:26:49Z</published>
    <title>Defending against adversarial attacks on medical imaging AI system,
  classification or detection?</title>
    <summary>  Medical imaging AI systems such as disease classification and segmentation
are increasingly inspired and transformed from computer vision based AI
systems. Although an array of adversarial training and/or loss function based
defense techniques have been developed and proved to be effective in computer
vision, defending against adversarial attacks on medical images remains largely
an uncharted territory due to the following unique challenges: 1) label
scarcity in medical images significantly limits adversarial generalizability of
the AI system; 2) vastly similar and dominant fore- and background in medical
images make it hard samples for learning the discriminating features between
different disease classes; and 3) crafted adversarial noises added to the
entire medical image as opposed to the focused organ target can make clean and
adversarial examples more discriminate than that between different disease
classes. In this paper, we propose a novel robust medical imaging AI framework
based on Semi-Supervised Adversarial Training (SSAT) and Unsupervised
Adversarial Detection (UAD), followed by designing a new measure for assessing
systems adversarial risk. We systematically demonstrate the advantages of our
robust medical imaging AI system over the existing adversarial defense
techniques under diverse real-world settings of adversarial attacks using a
benchmark OCT imaging data set.
</summary>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Deng Pan</name>
    </author>
    <author>
      <name>Dongxiao Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/2006.13555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.13555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.13796v2</id>
    <updated>2020-06-28T01:47:46Z</updated>
    <published>2020-06-24T15:08:59Z</published>
    <title>A Methodology for Creating AI FactSheets</title>
    <summary>  As AI models and services are used in a growing number of highstakes areas, a
consensus is forming around the need for a clearer record of how these models
and services are developed to increase trust. Several proposals for higher
quality and more consistent AI documentation have emerged to address ethical
and legal concerns and general social impacts of such systems. However, there
is little published work on how to create this documentation. This is the first
work to describe a methodology for creating the form of AI documentation we
call FactSheets. We have used this methodology to create useful FactSheets for
nearly two dozen models. This paper describes this methodology and shares the
insights we have gathered. Within each step of the methodology, we describe the
issues to consider and the questions to explore with the relevant people in an
organization who will be creating and consuming the AI facts in a FactSheet.
This methodology will accelerate the broader adoption of transparent AI
documentation.
</summary>
    <author>
      <name>John Richards</name>
    </author>
    <author>
      <name>David Piorkowski</name>
    </author>
    <author>
      <name>Michael Hind</name>
    </author>
    <author>
      <name>Stephanie Houde</name>
    </author>
    <author>
      <name>Aleksandra Mojsilović</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.13796v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.13796v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.07734v1</id>
    <updated>2020-08-18T04:17:58Z</updated>
    <published>2020-08-18T04:17:58Z</published>
    <title>Trust and Medical AI: The challenges we face and the expertise needed to
  overcome them</title>
    <summary>  Artificial intelligence (AI) is increasingly of tremendous interest in the
medical field. However, failures of medical AI could have serious consequences
for both clinical outcomes and the patient experience. These consequences could
erode public trust in AI, which could in turn undermine trust in our healthcare
institutions. This article makes two contributions. First, it describes the
major conceptual, technical, and humanistic challenges in medical AI. Second,
it proposes a solution that hinges on the education and accreditation of new
expert groups who specialize in the development, verification, and operation of
medical AI technologies. These groups will be required to maintain trust in our
healthcare institutions.
</summary>
    <author>
      <name>Thomas P. Quinn</name>
    </author>
    <author>
      <name>Manisha Senadeera</name>
    </author>
    <author>
      <name>Stephan Jacobs</name>
    </author>
    <author>
      <name>Simon Coghlan</name>
    </author>
    <author>
      <name>Vuong Le</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.07734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.07734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.11101v1</id>
    <updated>2020-09-21T22:03:02Z</updated>
    <published>2020-09-21T22:03:02Z</published>
    <title>AI assisted Malware Analysis: A Course for Next Generation Cybersecurity
  Workforce</title>
    <summary>  The use of Artificial Intelligence (AI) and Machine Learning (ML) to solve
cybersecurity problems has been gaining traction within industry and academia,
in part as a response to widespread malware attacks on critical systems, such
as cloud infrastructures, government offices or hospitals, and the vast amounts
of data they generate. AI- and ML-assisted cybersecurity offers data-driven
automation that could enable security systems to identify and respond to cyber
threats in real time. However, there is currently a shortfall of professionals
trained in AI and ML for cybersecurity. Here we address the shortfall by
developing lab-intensive modules that enable undergraduate and graduate
students to gain fundamental and advanced knowledge in applying AI and ML
techniques to real-world datasets to learn about Cyber Threat Intelligence
(CTI), malware analysis, and classification, among other important topics in
cybersecurity.
  Here we describe six self-contained and adaptive modules in "AI-assisted
Malware Analysis." Topics include: (1) CTI and malware attack stages, (2)
malware knowledge representation and CTI sharing, (3) malware data collection
and feature identification, (4) AI-assisted malware detection, (5) malware
classification and attribution, and (6) advanced malware research topics and
case studies such as adversarial learning and Advanced Persistent Threat (APT)
detection.
</summary>
    <author>
      <name>Maanak Gupta</name>
    </author>
    <author>
      <name>Sudip Mittal</name>
    </author>
    <author>
      <name>Mahmoud Abdelsalam</name>
    </author>
    <link href="http://arxiv.org/abs/2009.11101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01985v1</id>
    <updated>2020-09-18T21:53:07Z</updated>
    <published>2020-09-18T21:53:07Z</published>
    <title>Measuring the Complexity of Domains Used to Evaluate AI Systems</title>
    <summary>  There is currently a rapid increase in the number of challenge problem,
benchmarking datasets and algorithmic optimization tests for evaluating AI
systems. However, there does not currently exist an objective measure to
determine the complexity between these newly created domains. This lack of
cross-domain examination creates an obstacle to effectively research more
general AI systems. We propose a theory for measuring the complexity between
varied domains. This theory is then evaluated using approximations by a
population of neural network based AI systems. The approximations are compared
to other well known standards and show it meets intuitions of complexity. An
application of this measure is then demonstrated to show its effectiveness as a
tool in varied situations. The experimental results show this measure has
promise as an effective tool for aiding in the evaluation of AI systems. We
propose the future use of such a complexity metric for use in computing an AI
system's intelligence.
</summary>
    <author>
      <name>Christopher Pereyda</name>
    </author>
    <author>
      <name>Lawrence Holder</name>
    </author>
    <link href="http://arxiv.org/abs/2010.01985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.07038v6</id>
    <updated>2021-07-26T11:51:05Z</updated>
    <published>2020-10-09T14:02:30Z</published>
    <title>OnRAMP for Regulating AI in Medical Products</title>
    <summary>  Medical Artificial Intelligence (AI) involves the application of machine
learning algorithms to biomedical datasets in order to improve medical
practices. Products incorporating medical AI require certification before
deployment in most jurisdictions. To date, clear pathways for regulating
medical AI are still under development. Below the level of formal pathways lies
the actual practice of developing a medical AI solution. This Perspective
proposes best practice guidelines for development compatible with the
production of a regulatory package which, regardless of the formal regulatory
path, will form a core component of a certification process. The approach is
predicated on a statistical risk perspective, typical of medical device
regulators, and a deep understanding of machine learning methodologies. These
guidelines will allow all parties to communicate more clearly in the
development of a common Good Machine Learning Practice (GMLP), and thus lead to
the enhanced development of both medical AI products and regulations.
</summary>
    <author>
      <name>David Higgins</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/aisy.202100042</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/aisy.202100042" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages, 3 tables, 1 figure. Published in Advanced Intelligent
  Systems, July 2021. (See DOI link)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advanced Intelligent Systems, 2021, 42</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.07038v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.07038v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.12015v1</id>
    <updated>2020-10-20T23:29:45Z</updated>
    <published>2020-10-20T23:29:45Z</published>
    <title>Artificial Tikkun Olam: AI Can Be Our Best Friend in Building an Open
  Human-Computer Society</title>
    <summary>  Technological advances of virtually every kind pose risks to society
including fairness and bias. We review a long-standing wisdom that a widespread
practical deployment of any technology may produce adverse side effects
misusing the knowhow. This includes AI but AI systems are not solely
responsible for societal risks. We describe some of the common and AI specific
risks in health industries and other sectors and propose both broad and
specific solutions. Each technology requires very specialized and informed
tracking, monitoring and creative solutions. We postulate that AI systems are
uniquely poised to produce conceptual and methodological solutions to both
fairness and bias in automated decision-making systems. We propose a simple
intelligent system quotient that may correspond to their adverse societal
impact and outline a multi-tier architecture for producing solutions of
increasing complexity to these risks. We also propose that universities may
consider forming interdisciplinary Study of Future Technology Centers to
investigate and predict the fuller range of risks posed by technology and seek
both common and AI specific solutions using computational, technical,
conceptual and ethical analysis
</summary>
    <author>
      <name>Simon Kasif</name>
    </author>
    <link href="http://arxiv.org/abs/2010.12015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.12015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.06338v2</id>
    <updated>2021-03-30T14:39:25Z</updated>
    <published>2020-12-08T02:36:04Z</published>
    <title>The Why, What and How of Artificial General Intelligence Chip
  Development</title>
    <summary>  The AI chips increasingly focus on implementing neural computing at low power
and cost. The intelligent sensing, automation, and edge computing applications
have been the market drivers for AI chips. Increasingly, the generalisation,
performance, robustness, and scalability of the AI chip solutions are compared
with human-like intelligence abilities. Such a requirement to transit from
application-specific to general intelligence AI chip must consider several
factors. This paper provides an overview of this cross-disciplinary field of
study, elaborating on the generalisation of intelligence as understood in
building artificial general intelligence (AGI) systems. This work presents a
listing of emerging AI chip technologies, classification of edge AI
implementations, and the funnel design flow for AGI chip development. Finally,
the design consideration required for building an AGI chip is listed along with
the methods for testing and validating it.
</summary>
    <author>
      <name>Alex James</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Cognitive and Developmental Systems, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.06338v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.06338v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.06060v2</id>
    <updated>2021-01-18T11:36:10Z</updated>
    <published>2021-01-15T11:03:15Z</published>
    <title>The Challenge of Value Alignment: from Fairer Algorithms to AI Safety</title>
    <summary>  This paper addresses the question of how to align AI systems with human
values and situates it within a wider body of thought regarding technology and
value. Far from existing in a vacuum, there has long been an interest in the
ability of technology to 'lock-in' different value systems. There has also been
considerable thought about how to align technologies with specific social
values, including through participatory design-processes. In this paper we look
more closely at the question of AI value alignment and suggest that the power
and autonomy of AI systems gives rise to opportunities and challenges in the
domain of value that have not been encountered before. Drawing important
continuities between the work of the fairness, accountability, transparency and
ethics community, and work being done by technical AI safety researchers, we
suggest that more attention needs to be paid to the question of 'social value
alignment' - that is, how to align AI systems with the plurality of values
endorsed by groups of people, especially on the global level.
</summary>
    <author>
      <name>Iason Gabriel</name>
    </author>
    <author>
      <name>Vafa Ghazavi</name>
    </author>
    <link href="http://arxiv.org/abs/2101.06060v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.06060v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.06704v1</id>
    <updated>2021-01-17T16:23:20Z</updated>
    <published>2021-01-17T16:23:20Z</published>
    <title>Adversarial Interaction Attack: Fooling AI to Misinterpret Human
  Intentions</title>
    <summary>  Understanding the actions of both humans and artificial intelligence (AI)
agents is important before modern AI systems can be fully integrated into our
daily life. In this paper, we show that, despite their current huge success,
deep learning based AI systems can be easily fooled by subtle adversarial noise
to misinterpret the intention of an action in interaction scenarios. Based on a
case study of skeleton-based human interactions, we propose a novel adversarial
attack on interactions, and demonstrate how DNN-based interaction models can be
tricked to predict the participants' reactions in unexpected ways. From a
broader perspective, the scope of our proposed attack method is not confined to
problems related to skeleton data but can also be extended to any type of
problems involving sequential regressions. Our study highlights potential risks
in the interaction loop with AI and humans, which need to be carefully
addressed when deploying AI systems in safety-critical applications.
</summary>
    <author>
      <name>Nodens Koren</name>
    </author>
    <author>
      <name>Qiuhong Ke</name>
    </author>
    <author>
      <name>Yisen Wang</name>
    </author>
    <author>
      <name>James Bailey</name>
    </author>
    <author>
      <name>Xingjun Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.06704v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.06704v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.03061v1</id>
    <updated>2021-02-05T08:54:38Z</updated>
    <published>2021-02-05T08:54:38Z</published>
    <title>Applications of Artificial Intelligence in Particle Radiotherapy</title>
    <summary>  Radiotherapy, due to its technology-intensive nature and reliance on digital
data and human-machine interactions, is particularly suited to benefit from
artificial intelligence (AI) to improve the accuracy and efficiency of its
clinical workflow. Recently, various artificial intelligence (AI) methods have
been successfully developed to exploit the benefit of the inherent physical
properties of particle therapy. Many reviews about AI applications in
radiotherapy have already been published, but none were specifically dedicated
to particle therapy. In this article, we present a comprehensive review of the
recent published works on AI applications in particle therapy, which can be
classified into particle therapy treatment planning, adaptive particle therapy,
range and dose verification and other applications in particle therapy.
Although promising results reported in these works demonstrate how AI-based
methods can help exploit the intrinsic physic advantages of particle therapy,
challenges remained to be address before AI applications in particle therapy
enjoy widespread implementation in clinical practice.
</summary>
    <author>
      <name>Chao Wu</name>
    </author>
    <author>
      <name>Dan Nguyen</name>
    </author>
    <author>
      <name>Jan Schuemann</name>
    </author>
    <author>
      <name>Andrea Mairani</name>
    </author>
    <author>
      <name>Yuehu Pu</name>
    </author>
    <author>
      <name>Steve Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/2102.03061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.03061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.03702v1</id>
    <updated>2021-02-07T02:37:05Z</updated>
    <published>2021-02-07T02:37:05Z</published>
    <title>Supporting Serendipity: Opportunities and Challenges for Human-AI
  Collaboration in Qualitative Analysis</title>
    <summary>  Qualitative inductive methods are widely used in CSCW and HCI research for
their ability to generatively discover deep and contextualized insights, but
these inherently manual and human-resource-intensive processes are often
infeasible for analyzing large corpora. Researchers have been increasingly
interested in ways to apply qualitative methods to "big" data problems, hoping
to achieve more generalizable results from larger amounts of data while
preserving the depth and richness of qualitative methods. In this paper, we
describe a study of qualitative researchers' work practices and their
challenges, with an eye towards whether this is an appropriate domain for
human-AI collaboration and what successful collaborations might entail. Our
findings characterize participants' diverse methodological practices and
nuanced collaboration dynamics, and identify areas where they might benefit
from AI-based tools. While participants highlight the messiness and uncertainty
of qualitative inductive analysis, they still want full agency over the process
and believe that AI should not interfere. Our study provides a deep
investigation of task delegability in human-AI collaboration in the context of
qualitative analysis, and offers directions for the design of AI assistance
that honor serendipity, human agency, and ambiguity.
</summary>
    <author>
      <name>Jialun Aaron Jiang</name>
    </author>
    <author>
      <name>Kandrea Wade</name>
    </author>
    <author>
      <name>Casey Fiesler</name>
    </author>
    <author>
      <name>Jed R. Brubaker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3449168</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3449168" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages. Accepted to ACM CSCW 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.03702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.03702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.03985v1</id>
    <updated>2021-02-08T03:37:40Z</updated>
    <published>2021-02-08T03:37:40Z</published>
    <title>Multisource AI Scorecard Table for System Evaluation</title>
    <summary>  The paper describes a Multisource AI Scorecard Table (MAST) that provides the
developer and user of an artificial intelligence (AI)/machine learning (ML)
system with a standard checklist focused on the principles of good analysis
adopted by the intelligence community (IC) to help promote the development of
more understandable systems and engender trust in AI outputs. Such a scorecard
enables a transparent, consistent, and meaningful understanding of AI tools
applied for commercial and government use. A standard is built on compliance
and agreement through policy, which requires buy-in from the stakeholders.
While consistency for testing might only exist across a standard data set, the
community requires discussion on verification and validation approaches which
can lead to interpretability, explainability, and proper use. The paper
explores how the analytic tradecraft standards outlined in Intelligence
Community Directive (ICD) 203 can provide a framework for assessing the
performance of an AI system supporting various operational needs. These include
sourcing, uncertainty, consistency, accuracy, and visualization. Three use
cases are presented as notional examples that support security for comparative
analysis.
</summary>
    <author>
      <name>Erik Blasch</name>
    </author>
    <author>
      <name>James Sung</name>
    </author>
    <author>
      <name>Tao Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at AAAI FSS-20: Artificial Intelligence in Government and
  Public Sector, Washington, DC, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.03985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.03985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.04805v1</id>
    <updated>2021-02-09T12:52:24Z</updated>
    <published>2021-02-09T12:52:24Z</published>
    <title>AI-based Blackbox Code Deobfuscation: Understand, Improve and Mitigate</title>
    <summary>  Code obfuscation aims at protecting Intellectual Property and other secrets
embedded into software from being retrieved. Recent works leverage advances in
artificial intelligence with the hope of getting blackbox deobfuscators
completely immune to standard (whitebox) protection mechanisms. While
promising, this new field of AI-based blackbox deobfuscation is still in its
infancy. In this article we deepen the state of AI-based blackbox deobfuscation
in three key directions: understand the current state-of-the-art, improve over
it and design dedicated protection mechanisms. In particular, we define a novel
generic framework for AI-based blackbox deobfuscation encompassing prior work
and highlighting key components; we are the first to point out that the search
space underlying code deobfuscation is too unstable for simulation-based
methods (e.g., Monte Carlo Tres Search used in prior work) and advocate the use
of robust methods such as S-metaheuritics; we propose the new optimized
AI-based blackbox deobfuscator Xyntia which significantly outperforms prior
work in terms of success rate (especially with small time budget) while being
completely immune to the most recent anti-analysis code obfuscation methods;
and finally we propose two novel protections against AI-based blackbox
deobfuscation, allowing to counter Xyntia's powerful attacks.
</summary>
    <author>
      <name>Grégoire Menguy</name>
    </author>
    <author>
      <name>Sébastien Bardin</name>
    </author>
    <author>
      <name>Richard Bonichon</name>
    </author>
    <author>
      <name>Cauim de Souza Lima</name>
    </author>
    <link href="http://arxiv.org/abs/2102.04805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.04805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.07638v1</id>
    <updated>2021-02-12T04:09:35Z</updated>
    <published>2021-02-12T04:09:35Z</published>
    <title>AI Uncertainty Based on Rademacher Complexity and Shannon Entropy</title>
    <summary>  In this paper from communication channel coding perspective we are able to
present both a theoretical and practical discussion of AI's uncertainty,
capacity and evolution for pattern classification based on the classical
Rademacher complexity and Shannon entropy. First AI capacity is defined as in
communication channels. It is shown qualitatively that the classical Rademacher
complexity and Shannon entropy used in communication theory is closely related
by their definitions, given a pattern classification problem with a complexity
measured by Rademacher complexity. Secondly based on the Shannon mathematical
theory on communication coding, we derive several sufficient and necessary
conditions for an AI's error rate approaching zero in classifications problems.
A 1/2 criteria on Shannon entropy is derived in this paper so that error rate
can approach zero or is zero for AI pattern classification problems. Last but
not least, we show our analysis and theory by providing examples of AI pattern
classifications with error rate approaching zero or being zero.
</summary>
    <author>
      <name>Mingyong Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2102.07638v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.07638v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.09343v1</id>
    <updated>2021-02-05T06:55:59Z</updated>
    <published>2021-02-05T06:55:59Z</published>
    <title>AI Can Stop Mass Shootings, and More</title>
    <summary>  We propose to build directly upon our longstanding, prior r&amp;d in AI/machine
ethics in order to attempt to make real the blue-sky idea of AI that can thwart
mass shootings, by bringing to bear its ethical reasoning. The r&amp;d in question
is overtly and avowedly logicist in form, and since we are hardly the only ones
who have established a firm foundation in the attempt to imbue AI's with their
own ethical sensibility, the pursuit of our proposal by those in different
methodological camps should, we believe, be considered as well. We seek herein
to make our vision at least somewhat concrete by anchoring our exposition to
two simulations, one in which the AI saves the lives of innocents by locking
out a malevolent human's gun, and a second in which this malevolent agent is
allowed by the AI to be neutralized by law enforcement. Along the way, some
objections are anticipated, and rebutted.
</summary>
    <author>
      <name>Selmer Bringsjord</name>
    </author>
    <author>
      <name>Naveen Sundar Govindarajulu</name>
    </author>
    <author>
      <name>Michael Giancola</name>
    </author>
    <link href="http://arxiv.org/abs/2102.09343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.09343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.10548v1</id>
    <updated>2021-03-18T22:25:44Z</updated>
    <published>2021-03-18T22:25:44Z</published>
    <title>Towards Productizing AI/ML Models: An Industry Perspective from Data
  Scientists</title>
    <summary>  The transition from AI/ML models to production-ready AI-based systems is a
challenge for both data scientists and software engineers. In this paper, we
report the results of a workshop conducted in a consulting company to
understand how this transition is perceived by practitioners. Starting from the
need for making AI experiments reproducible, the main themes that emerged are
related to the use of the Jupyter Notebook as the primary prototyping tool, and
the lack of support for software engineering best practices as well as data
science specific functionalities.
</summary>
    <author>
      <name>Filippo Lanubile</name>
    </author>
    <author>
      <name>Fabio Calefato</name>
    </author>
    <author>
      <name>Luigi Quaranta</name>
    </author>
    <author>
      <name>Maddalena Amoruso</name>
    </author>
    <author>
      <name>Fabio Fumarola</name>
    </author>
    <author>
      <name>Michele Filannino</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/WAIN52551.2021.00027</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/WAIN52551.2021.00027" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of 2021 IEEE/ACM 1st Workshop on AI Engineering - Software
  Engineering for AI (WAIN), pp. 129-132</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.10548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.10548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.12166v2</id>
    <updated>2021-03-30T01:12:02Z</updated>
    <published>2021-03-22T20:24:45Z</published>
    <title>Special Session: Reliability Analysis for ML/AI Hardware</title>
    <summary>  Artificial intelligence (AI) and Machine Learning (ML) are becoming pervasive
in today's applications, such as autonomous vehicles, healthcare, aerospace,
cybersecurity, and many critical applications. Ensuring the reliability and
robustness of the underlying AI/ML hardware becomes our paramount importance.
In this paper, we explore and evaluate the reliability of different AI/ML
hardware. The first section outlines the reliability issues in a commercial
systolic array-based ML accelerator in the presence of faults engendering from
device-level non-idealities in the DRAM. Next, we quantified the impact of
circuit-level faults in the MSB and LSB logic cones of the Multiply and
Accumulate (MAC) block of the AI accelerator on the AI/ML accuracy. Finally, we
present two key reliability issues -- circuit aging and endurance in emerging
neuromorphic hardware platforms and present our system-level approach to
mitigate them.
</summary>
    <author>
      <name>Shamik Kundu</name>
    </author>
    <author>
      <name>Kanad Basu</name>
    </author>
    <author>
      <name>Mehdi Sadi</name>
    </author>
    <author>
      <name>Twisha Titirsha</name>
    </author>
    <author>
      <name>Shihao Song</name>
    </author>
    <author>
      <name>Anup Das</name>
    </author>
    <author>
      <name>Ujjwal Guin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at VLSI Test Symposium</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.12166v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.12166v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.03483v3</id>
    <updated>2021-09-03T20:28:38Z</updated>
    <published>2021-04-08T02:51:36Z</published>
    <title>Question-Driven Design Process for Explainable AI User Experiences</title>
    <summary>  A pervasive design issue of AI systems is their explainability--how to
provide appropriate information to help users understand the AI. The technical
field of explainable AI (XAI) has produced a rich toolbox of techniques.
Designers are now tasked with the challenges of how to select the most suitable
XAI techniques and translate them into UX solutions. Informed by our previous
work studying design challenges around XAI UX, this work proposes a design
process to tackle these challenges. We review our and related prior work to
identify requirements that the process should fulfill, and accordingly, propose
a Question-Driven Design Process that grounds the user needs, choices of XAI
techniques, design, and evaluation of XAI UX all in the user questions. We
provide a mapping guide between prototypical user questions and exemplars of
XAI techniques to reframe the technical space of XAI, also serving as boundary
objects to support collaboration between designers and AI engineers. We
demonstrate it with a use case of designing XAI for healthcare adverse events
prediction, and discuss lessons learned for tackling design challenges of AI
systems.
</summary>
    <author>
      <name>Q. Vera Liao</name>
    </author>
    <author>
      <name>Milena Pribić</name>
    </author>
    <author>
      <name>Jaesik Han</name>
    </author>
    <author>
      <name>Sarah Miller</name>
    </author>
    <author>
      <name>Daby Sow</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">working paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.03483v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.03483v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.07595v2</id>
    <updated>2021-05-06T14:27:03Z</updated>
    <published>2021-04-15T16:53:34Z</published>
    <title>Towards A Process Model for Co-Creating AI Experiences</title>
    <summary>  Thinking of technology as a design material is appealing. It encourages
designers to explore the material's properties to understand its capabilities
and limitations, a prerequisite to generative design thinking. However, as a
material, AI resists this approach because its properties emerge as part of the
design process itself. Therefore, designers and AI engineers must collaborate
in new ways to create both the material and its application experience. We
investigate the co-creation process through a design study with 10 pairs of
designers and engineers. We find that design 'probes' with user data are a
useful tool in defining AI materials. Through data probes, designers construct
designerly representations of the envisioned AI experience (AIX) to identify
desirable AI characteristics. Data probes facilitate divergent thinking,
material testing, and design validation. Based on our findings, we propose a
process model for co-creating AIX and offer design considerations for
incorporating data probes in design tools.
</summary>
    <author>
      <name>Hariharan Subramonyam</name>
    </author>
    <author>
      <name>Colleen Seifert</name>
    </author>
    <author>
      <name>Eytan Adar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM DIS'21 pre-print</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.07595v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.07595v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12547v1</id>
    <updated>2021-04-09T23:44:37Z</updated>
    <published>2021-04-09T23:44:37Z</published>
    <title>A Framework for Ethical AI at the United Nations</title>
    <summary>  This paper aims to provide an overview of the ethical concerns in artificial
intelligence (AI) and the framework that is needed to mitigate those risks, and
to suggest a practical path to ensure the development and use of AI at the
United Nations (UN) aligns with our ethical values. The overview discusses how
AI is an increasingly powerful tool with potential for good, albeit one with a
high risk of negative side-effects that go against fundamental human rights and
UN values. It explains the need for ethical principles for AI aligned with
principles for data governance, as data and AI are tightly interwoven. It
explores different ethical frameworks that exist and tools such as assessment
lists. It recommends that the UN develop a framework consisting of ethical
principles, architectural standards, assessment methods, tools and
methodologies, and a policy to govern the implementation and adherence to this
framework, accompanied by an education program for staff.
</summary>
    <author>
      <name>Lambert Hogenhout</name>
    </author>
    <link href="http://arxiv.org/abs/2104.12547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.14088v1</id>
    <updated>2021-04-29T03:19:52Z</updated>
    <published>2021-04-29T03:19:52Z</published>
    <title>Connecting AI Learning and Blockchain Mining in 6G Systems</title>
    <summary>  The sixth generation (6G) systems are generally recognized to be established
on ubiquitous Artificial Intelligence (AI) and distributed ledger such as
blockchain. However, the AI training demands tremendous computing resource,
which is limited in most 6G devices. Meanwhile, miners in Proof-of-Work (PoW)
based blockchains devote massive computing power to block mining, and are
widely criticized for the waste of computation. To address this dilemma, we
propose an Evolved-Proof-of-Work (E-PoW) consensus that can integrate the
matrix computations, which are widely existed in AI training, into the process
of brute-force searches in the block mining. Consequently, E-PoW can connect AI
learning and block mining via the multiply used common computing resource.
Experimental results show that E-PoW can salvage by up to 80 percent computing
power from pure block mining for parallel AI training in 6G systems.
</summary>
    <author>
      <name>Yunkai Wei</name>
    </author>
    <author>
      <name>Zixian An</name>
    </author>
    <author>
      <name>Supeng Leng</name>
    </author>
    <author>
      <name>Kun Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures, submitted to IEEE Communications Magazine</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.14088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.14088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.00375v1</id>
    <updated>2021-05-02T01:52:59Z</updated>
    <published>2021-05-02T01:52:59Z</published>
    <title>Vehicle Emissions Prediction with Physics-Aware AI Models: Preliminary
  Results</title>
    <summary>  Given an on-board diagnostics (OBD) dataset and a physics-based emissions
prediction model, this paper aims to develop an accurate and
computational-efficient AI (Artificial Intelligence) method that predicts
vehicle emissions. The problem is of societal importance because vehicular
emissions lead to climate change and impact human health. This problem is
challenging because the OBD data does not contain enough parameters needed by
high-order physics models. Conversely, related work has shown that low-order
physics models have poor predictive accuracy when using available OBD data.
This paper uses a divergent window co-occurrence pattern detection method to
develop a spatiotemporal variability-aware AI model for predicting emission
values from the OBD datasets. We conducted a case study using real-world OBD
data from a local public transportation agency. Results show that the proposed
AI method has approximately 65% improved predictive accuracy than a non-AI
low-order physics model and is approximately 35% more accurate than a baseline
model.
</summary>
    <author>
      <name>Harish Panneer Selvam</name>
    </author>
    <author>
      <name>Yan Li</name>
    </author>
    <author>
      <name>Pengyue Wang</name>
    </author>
    <author>
      <name>William F. Northrop</name>
    </author>
    <author>
      <name>Shashi Shekhar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by Association for Advancement of Artificial Intelligence
  (AAAI) Fall Symposium Series 2020: Physics-Guided AI to Accelerate Scientific
  Discovery (https://sites.google.com/vt.edu/pgai-aaai-20)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PGAI-AAAI-20(2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2105.00375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.00375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.00667v1</id>
    <updated>2021-05-03T07:52:56Z</updated>
    <published>2021-05-03T07:52:56Z</published>
    <title>Explaining how your AI system is fair</title>
    <summary>  To implement fair machine learning in a sustainable way, choosing the right
fairness objective is key. Since fairness is a concept of justice which comes
in various, sometimes conflicting definitions, this is not a trivial task
though. The most appropriate fairness definition for an artificial intelligence
(AI) system is a matter of ethical standards and legal requirements, and the
right choice depends on the particular use case and its context. In this
position paper, we propose to use a decision tree as means to explain and
justify the implemented kind of fairness to the end users. Such a structure
would first of all support AI practitioners in mapping ethical principles to
fairness definitions for a concrete application and therefore make the
selection a straightforward and transparent process. However, this approach
would also help document the reasoning behind the decision making. Due to the
general complexity of the topic of fairness in AI, we argue that specifying
"fairness" for a given use case is the best way forward to maintain confidence
in AI systems. In this case, this could be achieved by sharing the reasons and
principles expressed during the decision making process with the broader
audience.
</summary>
    <author>
      <name>Boris Ruf</name>
    </author>
    <author>
      <name>Marcin Detyniecki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the ACM CHI 2021 Workshop on Operationalizing
  Human-Centered Perspectives in Explainable AI</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.00667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.00667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.04534v1</id>
    <updated>2021-05-10T17:38:38Z</updated>
    <published>2021-05-10T17:38:38Z</published>
    <title>Improving Fairness of AI Systems with Lossless De-biasing</title>
    <summary>  In today's society, AI systems are increasingly used to make critical
decisions such as credit scoring and patient triage. However, great convenience
brought by AI systems comes with troubling prevalence of bias against
underrepresented groups. Mitigating bias in AI systems to increase overall
fairness has emerged as an important challenge. Existing studies on mitigating
bias in AI systems focus on eliminating sensitive demographic information
embedded in data. Given the temporal and contextual complexity of
conceptualizing fairness, lossy treatment of demographic information may
contribute to an unnecessary trade-off between accuracy and fairness,
especially when demographic attributes and class labels are correlated. In this
paper, we present an information-lossless de-biasing technique that targets the
scarcity of data in the disadvantaged group. Unlike the existing work, we
demonstrate, both theoretically and empirically, that oversampling
underrepresented groups can not only mitigate algorithmic bias in AI systems
that consistently predict a favorable outcome for a certain group, but improve
overall accuracy by mitigating class imbalance within data that leads to a bias
towards the majority class. We demonstrate the effectiveness of our technique
on real datasets using a variety of fairness metrics.
</summary>
    <author>
      <name>Yan Zhou</name>
    </author>
    <author>
      <name>Murat Kantarcioglu</name>
    </author>
    <author>
      <name>Chris Clifton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.04534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.04534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.07879v2</id>
    <updated>2022-05-20T21:27:08Z</updated>
    <published>2021-05-12T15:53:44Z</published>
    <title>Conscious AI</title>
    <summary>  Recent advances in artificial intelligence (AI) have achieved human-scale
speed and accuracy for classification tasks. In turn, these capabilities have
made AI a viable replacement for many human activities that at their core
involve classification, such as basic mechanical and analytical tasks in
low-level service jobs. Current systems do not need to be conscious to
recognize patterns and classify them. However, for AI to progress to more
complicated tasks requiring intuition and empathy, it must develop capabilities
such as metathinking, creativity, and empathy akin to human self-awareness or
consciousness. We contend that such a paradigm shift is possible only through a
fundamental shift in the state of artificial intelligence toward consciousness,
a shift similar to what took place for humans through the process of natural
selection and evolution. As such, this paper aims to theoretically explore the
requirements for the emergence of consciousness in AI. It also provides a
principled understanding of how conscious AI can be detected and how it might
be manifested in contrast to the dominant paradigm that seeks to ultimately
create machines that are linguistically indistinguishable from humans.
</summary>
    <author>
      <name>Hadi Esmaeilzadeh</name>
    </author>
    <author>
      <name>Reza Vaezi</name>
    </author>
    <link href="http://arxiv.org/abs/2105.07879v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.07879v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.07112v2</id>
    <updated>2021-07-28T14:16:22Z</updated>
    <published>2021-06-13T23:27:45Z</published>
    <title>User Acceptance of Gender Stereotypes in Automated Career
  Recommendations</title>
    <summary>  Currently, there is a surge of interest in fair Artificial Intelligence (AI)
and Machine Learning (ML) research which aims to mitigate discriminatory bias
in AI algorithms, e.g. along lines of gender, age, and race. While most
research in this domain focuses on developing fair AI algorithms, in this work,
we show that a fair AI algorithm on its own may be insufficient to achieve its
intended results in the real world. Using career recommendation as a case
study, we build a fair AI career recommender by employing gender debiasing
machine learning techniques. Our offline evaluation showed that the debiased
recommender makes fairer career recommendations without sacrificing its
accuracy. Nevertheless, an online user study of more than 200 college students
revealed that participants on average prefer the original biased system over
the debiased system. Specifically, we found that perceived gender disparity is
a determining factor for the acceptance of a recommendation. In other words,
our results demonstrate we cannot fully address the gender bias issue in AI
recommendations without addressing the gender bias in humans.
</summary>
    <author>
      <name>Clarice Wang</name>
    </author>
    <author>
      <name>Kathryn Wang</name>
    </author>
    <author>
      <name>Andrew Bian</name>
    </author>
    <author>
      <name>Rashidul Islam</name>
    </author>
    <author>
      <name>Kamrun Naher Keya</name>
    </author>
    <author>
      <name>James Foulds</name>
    </author>
    <author>
      <name>Shimei Pan</name>
    </author>
    <link href="http://arxiv.org/abs/2106.07112v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.07112v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.09106v1</id>
    <updated>2021-06-16T20:19:04Z</updated>
    <published>2021-06-16T20:19:04Z</published>
    <title>Explainable AI for Natural Adversarial Images</title>
    <summary>  Adversarial images highlight how vulnerable modern image classifiers are to
perturbations outside of their training set. Human oversight might mitigate
this weakness, but depends on humans understanding the AI well enough to
predict when it is likely to make a mistake. In previous work we have found
that humans tend to assume that the AI's decision process mirrors their own.
Here we evaluate if methods from explainable AI can disrupt this assumption to
help participants predict AI classifications for adversarial and standard
images. We find that both saliency maps and examples facilitate catching AI
errors, but their effects are not additive, and saliency maps are more
effective than examples.
</summary>
    <author>
      <name>Tomas Folke</name>
    </author>
    <author>
      <name>ZhaoBin Li</name>
    </author>
    <author>
      <name>Ravi B. Sojitra</name>
    </author>
    <author>
      <name>Scott Cheng-Hsin Yang</name>
    </author>
    <author>
      <name>Patrick Shafto</name>
    </author>
    <link href="http://arxiv.org/abs/2106.09106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.09106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.13322v1</id>
    <updated>2021-06-23T03:59:39Z</updated>
    <published>2021-06-23T03:59:39Z</published>
    <title>Dr. Watson type Artificial Intellect (AI) Systems</title>
    <summary>  The article proposes a new type of AI system that does not give solutions
directly but rather points toward it, friendly prompting the user with
questions and adjusting messages. Models of AI human collaboration can be
deduced from the classic literary example of interaction between Mr. Holmes and
Dr. Watson from the stories by Conan Doyle, where the highly qualified expert
Mr. Holmes answers questions posed by Dr. Watson. Here Mr. Holmes, with his
rule-based calculations, logic, and memory management, apparently plays the
role of an AI system, and Dr. Watson is the user. Looking into the same
Holmes-Watson interaction, we find and promote another model in which the AI
behaves like Dr. Watson, who, by asking questions and acting in a particular
way, helps Holmes (the AI user) make the right decisions. We call the systems
based on this principle "Dr. Watson-type systems." The article describes the
properties of such systems and introduces two particular: Patient Management
System for intensive care physicians and Data Error Prevention System.
</summary>
    <author>
      <name>Saveli Goldberg</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MGH Radiation Oncology Department</arxiv:affiliation>
    </author>
    <author>
      <name>Stanislav Belyaev</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Eastern New Mexico Medical Center</arxiv:affiliation>
    </author>
    <author>
      <name>Vladimir Sluchak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages,13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.13322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.13322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.16050v1</id>
    <updated>2021-06-30T13:24:51Z</updated>
    <published>2021-06-30T13:24:51Z</published>
    <title>Ethical AI-Powered Regression Test Selection</title>
    <summary>  Test automation is common in software development; often one tests repeatedly
to identify regressions. If the amount of test cases is large, one may select a
subset and only use the most important test cases. The regression test
selection (RTS) could be automated and enhanced with Artificial Intelligence
(AI-RTS). This however could introduce ethical challenges. While such
challenges in AI are in general well studied, there is a gap with respect to
ethical AI-RTS. By exploring the literature and learning from our experiences
of developing an industry AI-RTS tool, we contribute to the literature by
identifying three challenges (assigning responsibility, bias in decision-making
and lack of participation) and three approaches (explicability, supervision and
diversity). Additionally, we provide a checklist for ethical AI-RTS to help
guide the decision-making of the stakeholders involved in the process.
</summary>
    <author>
      <name>Per Erik Strandberg</name>
    </author>
    <author>
      <name>Mirgita Frasheri</name>
    </author>
    <author>
      <name>Eduard Paul Enoiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure, accepted to AITest'21</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.16050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.16050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.03721v4</id>
    <updated>2022-06-13T10:33:35Z</updated>
    <published>2021-07-08T10:04:07Z</published>
    <title>Demystifying the Draft EU Artificial Intelligence Act</title>
    <summary>  In April 2021, the European Commission proposed a Regulation on Artificial
Intelligence, known as the AI Act. We present an overview of the Act and
analyse its implications, drawing on scholarship ranging from the study of
contemporary AI practices to the structure of EU product safety regimes over
the last four decades. Aspects of the AI Act, such as different rules for
different risk-levels of AI, make sense. But we also find that some provisions
of the Draft AI Act have surprising legal implications, whilst others may be
largely ineffective at achieving their stated goals. Several overarching
aspects, including the enforcement regime and the risks of maximum
harmonisation pre-empting legitimate national AI policy, engender significant
concern. These issues should be addressed as a priority in the legislative
process.
</summary>
    <author>
      <name>Michael Veale</name>
    </author>
    <author>
      <name>Frederik Zuiderveen Borgesius</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.9785/cri-2021-220402</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.9785/cri-2021-220402" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Law Review International (2021), 22(4) 97-112</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2107.03721v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.03721v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.5.0; K.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.06015v2</id>
    <updated>2021-10-20T08:12:23Z</updated>
    <published>2021-07-13T12:09:10Z</published>
    <title>A Classification of Artificial Intelligence Systems for Mathematics
  Education</title>
    <summary>  This chapter provides an overview of the different Artificial Intelligence
(AI) systems that are being used in contemporary digital tools for Mathematics
Education (ME). It is aimed at researchers in AI and Machine Learning (ML), for
whom we shed some light on the specific technologies that are being used in
educational applications; and at researchers in ME, for whom we clarify: i)
what the possibilities of the current AI technologies are, ii) what is still
out of reach and iii) what is to be expected in the near future. We start our
analysis by establishing a high-level taxonomy of AI tools that are found as
components in digital ME applications. Then, we describe in detail how these AI
tools, and in particular ML, are being used in two key applications,
specifically AI-based calculators and intelligent tutoring systems. We finish
the chapter with a discussion about student modeling systems and their
relationship to artificial general intelligence.
</summary>
    <author>
      <name>Steven Van Vaerenbergh</name>
    </author>
    <author>
      <name>Adrián Pérez-Suay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Chapter in the upcoming book "Mathematics Education in the Age of
  Artificial Intelligence: How Artificial Intelligence can serve Mathematical
  Human Learning", Springer Nature, edited by P. R. Richard, P. V\'elez, and S.
  Van Vaerenbergh</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.06015v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.06015v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.09258v1</id>
    <updated>2021-07-20T04:35:25Z</updated>
    <published>2021-07-20T04:35:25Z</published>
    <title>A Markov Game Model for AI-based Cyber Security Attack Mitigation</title>
    <summary>  The new generation of cyber threats leverages advanced AI-aided methods,
which make them capable to launch multi-stage, dynamic, and effective attacks.
Current cyber-defense systems encounter various challenges to defend against
such new and emerging threats. Modeling AI-aided threats through game theory
models can help the defender to select optimal strategies against the attacks
and make wise decisions to mitigate the attack's impact. This paper first
explores the current state-of-the-art in the new generation of threats in which
AI techniques such as deep neural network is used for the attacker and
discusses further challenges. We propose a Markovian dynamic game that can
evaluate the efficiency of defensive methods against the AI-aided attacker
under a cloud-based system in which the attacker utilizes an AI technique to
launch an advanced attack by finding the shortest attack path. We use the CVSS
metrics to quantify the values of this zero-sum game model for decision-making.
</summary>
    <author>
      <name>Hooman Alavizadeh</name>
    </author>
    <author>
      <name>Julian Jang-Jaccard</name>
    </author>
    <author>
      <name>Tansu Alpcan</name>
    </author>
    <author>
      <name>Seyit A. Camtepe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.09258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.09258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.13738v1</id>
    <updated>2021-07-29T04:14:53Z</updated>
    <published>2021-07-29T04:14:53Z</published>
    <title>Design-Driven Requirements for Computationally Co-Creative Game AI
  Design Tools</title>
    <summary>  Game AI designers must manage complex interactions between the AI character,
the game world, and the player, while achieving their design visions.
Computational co-creativity tools can aid them, but first, AI and HCI
researchers must gather requirements and determine design heuristics to build
effective co-creative tools. In this work, we present a participatory design
study that categorizes and analyzes game AI designers' workflows, goals, and
expectations for such tools. We evince deep connections between game AI design
and the design of co-creative tools, and present implications for future
co-creativity tool research and development.
</summary>
    <author>
      <name>Nathan Partlan</name>
    </author>
    <author>
      <name>Erica Kleinman</name>
    </author>
    <author>
      <name>Jim Howe</name>
    </author>
    <author>
      <name>Sabbir Ahmad</name>
    </author>
    <author>
      <name>Stacy Marsella</name>
    </author>
    <author>
      <name>Magy Seif El-Nasr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 1 figure. Accepted for publication in Foundations of
  Digital Games (FDG) 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.13738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.13738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.00588v4</id>
    <updated>2023-02-03T18:55:48Z</updated>
    <published>2021-08-02T01:39:12Z</published>
    <title>Human-AI Interaction for Diverse Humans: What Cognitive Style
  Disaggregation Reveals</title>
    <summary>  Although guidelines for human-AI interaction (HAI) provide important advice
on how to help improve user experiences with AI products, little is known about
HAI for diverse users' experiences with AI. Without understanding factors that
lie behind differences among diverse users' experiences with AI products,
designers lack information they need to make AI products more equitable and
inclusive. To investigate whether and how diverse users' different cognitive
styles might help account for their differences, we used data from 16
experiments on Amershi et al.'s HAI Guidelines, and disaggregated by the
participants' cognitive styles. The results of disaggregating revealed 112
phenomena that were not apparent without taking cognitive style diversity into
account. We also show how the cognitive style differences can explain
demographic differences among genders and among gender-age intersectional
groupings, and can point the way toward making HAI experiences more equitable
and inclusive.
</summary>
    <author>
      <name>Andrew Anderson</name>
    </author>
    <author>
      <name>Tianyi Li</name>
    </author>
    <author>
      <name>Mihaela Vorvoreanu</name>
    </author>
    <author>
      <name>Margaret Burnett</name>
    </author>
    <link href="http://arxiv.org/abs/2108.00588v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.00588v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.11844v1</id>
    <updated>2021-08-26T15:13:42Z</updated>
    <published>2021-08-26T15:13:42Z</published>
    <title>AI at work -- Mitigating safety and discriminatory risk with technical
  standards</title>
    <summary>  The use of artificial intelligence (AI) and AI methods in the workplace holds
both great opportunities as well as risks to occupational safety and
discrimination. In addition to legal regulation, technical standards will play
a key role in mitigating such risk by defining technical requirements for
development and testing of AI systems. This paper provides an overview and
assessment of existing international, European and German standards as well as
those currently under development. The paper is part of the research project
"ExamAI - Testing and Auditing of AI systems" and focusses on the use of AI in
an industrial production environment as well as in the realm of human resource
management (HR).
</summary>
    <author>
      <name>Nikolas Becker</name>
    </author>
    <author>
      <name>Pauline Junginger</name>
    </author>
    <author>
      <name>Lukas Martinez</name>
    </author>
    <author>
      <name>Daniel Krupka</name>
    </author>
    <author>
      <name>Leonie Beining</name>
    </author>
    <link href="http://arxiv.org/abs/2108.11844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.11844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.1; K.6.1; K.6.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.02944v1</id>
    <updated>2021-09-07T08:57:24Z</updated>
    <published>2021-09-07T08:57:24Z</published>
    <title>Dutch Comfort: The limits of AI governance through municipal registers</title>
    <summary>  In this commentary, we respond to a recent editorial letter by Professor
Luciano Floridi entitled 'AI as a public service: Learning from Amsterdam and
Helsinki'. Here, Floridi considers the positive impact of these municipal AI
registers, which collect a limited number of algorithmic systems used by the
city of Amsterdam and Helsinki. There are a number of assumptions about AI
registers as a governance model for automated systems that we seek to question.
Starting with recent attempts to normalize AI by decontextualizing and
depoliticizing it, which is a fraught political project that encourages what we
call 'ethics theater' given the proven dangers of using these systems in the
context of the digital welfare state. We agree with Floridi that much can be
learned from these registers about the role of AI systems in municipal city
management. Yet, the lessons we draw, on the basis of our extensive
ethnographic engagement with digital well-fare states are distinctly less
optimistic.
</summary>
    <author>
      <name>Corinne Cath</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Oxford Internet Institute University of Oxford</arxiv:affiliation>
    </author>
    <author>
      <name>Fieke Jansen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Data Justice Lab Cardiff University</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2109.02944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.02944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.12912v2</id>
    <updated>2021-11-05T08:55:07Z</updated>
    <published>2021-09-27T09:56:23Z</published>
    <title>A User-Centred Framework for Explainable Artificial Intelligence in
  Human-Robot Interaction</title>
    <summary>  State of the art Artificial Intelligence (AI) techniques have reached an
impressive complexity. Consequently, researchers are discovering more and more
methods to use them in real-world applications. However, the complexity of such
systems requires the introduction of methods that make those transparent to the
human user. The AI community is trying to overcome the problem by introducing
the Explainable AI (XAI) field, which is tentative to make AI algorithms less
opaque. However, in recent years, it became clearer that XAI is much more than
a computer science problem: since it is about communication, XAI is also a
Human-Agent Interaction problem. Moreover, AI came out of the laboratories to
be used in real life. This implies the need for XAI solutions tailored to
non-expert users. Hence, we propose a user-centred framework for XAI that
focuses on its social-interactive aspect taking inspiration from cognitive and
social sciences' theories and findings. The framework aims to provide a
structure for interactive XAI solutions thought for non-expert users.
</summary>
    <author>
      <name>Marco Matarese</name>
    </author>
    <author>
      <name>Francesco Rea</name>
    </author>
    <author>
      <name>Alessandra Sciutti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at AI-HRI symposium as part of AAAI-FSS 2021
  (arXiv:2109.10836)</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.12912v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.12912v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.15193v2</id>
    <updated>2021-10-22T01:39:43Z</updated>
    <published>2021-09-30T15:07:02Z</published>
    <title>AIive: Interactive Visualization and Sonification of Neural Networks in
  Virtual Reality</title>
    <summary>  Artificial Intelligence (AI), especially Neural Networks (NNs), has become
increasingly popular. However, people usually treat AI as a tool, focusing on
improving outcome, accuracy, and performance while paying less attention to the
representation of AI itself. We present AIive, an interactive visualization of
AI in Virtual Reality (VR) that brings AI "alive". AIive enables users to
manipulate the parameters of NNs with virtual hands and provides auditory
feedback for the real-time values of loss, accuracy, and hyperparameters. Thus,
AIive contributes an artistic and intuitive way to represent AI by integrating
visualization, sonification, and direct manipulation in VR, potentially
targeting a wide range of audiences.
</summary>
    <author>
      <name>Zhuoyue Lyu</name>
    </author>
    <author>
      <name>Jiannan Li</name>
    </author>
    <author>
      <name>Bryan Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/AIVR52153.2021.00057</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/AIVR52153.2021.00057" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 3 figures, 2021 IEEE International Conference on Artificial
  Intelligence and Virtual Reality (AIVR)</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.15193v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.15193v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.01108v1</id>
    <updated>2021-10-03T21:47:13Z</updated>
    <published>2021-10-03T21:47:13Z</published>
    <title>Human-Centered AI for Data Science: A Systematic Approach</title>
    <summary>  Human-Centered AI (HCAI) refers to the research effort that aims to design
and implement AI techniques to support various human tasks, while taking human
needs into consideration and preserving human control. In this short position
paper, we illustrate how we approach HCAI using a series of research projects
around Data Science (DS) works as a case study. The AI techniques built for
supporting DS works are collectively referred to as AutoML systems, and their
goals are to automate some parts of the DS workflow. We illustrate a three-step
systematical research approach(i.e., explore, build, and integrate) and four
practical ways of implementation for HCAI systems. We argue that our work is a
cornerstone towards the ultimate future of Human-AI Collaboration for DS and
beyond, where AI and humans can take complementary and indispensable roles to
achieve a better outcome and experience.
</summary>
    <author>
      <name>Dakuo Wang</name>
    </author>
    <author>
      <name>Xiaojuan Ma</name>
    </author>
    <author>
      <name>April Yi Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2110.01108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.01108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.02007v3</id>
    <updated>2022-04-26T11:03:03Z</updated>
    <published>2021-10-05T12:51:11Z</published>
    <title>Empowering Local Communities Using Artificial Intelligence</title>
    <summary>  Artificial Intelligence (AI) is increasingly used to analyze large amounts of
data in various practices, such as object recognition. We are specifically
interested in using AI-powered systems to engage local communities in
developing plans or solutions for pressing societal and environmental concerns.
Such local contexts often involve multiple stakeholders with different and even
contradictory agendas, resulting in mismatched expectations of these systems'
behaviors and desired outcomes. There is a need to investigate if AI models and
pipelines can work as expected in different contexts through co-creation and
field deployment. Based on case studies in co-creating AI-powered systems with
local people, we explain challenges that require more attention and provide
viable paths to bridge AI research with citizen needs. We advocate for
developing new collaboration approaches and mindsets that are needed to
co-create AI-powered systems in multi-stakeholder contexts to address local
concerns.
</summary>
    <author>
      <name>Yen-Chia Hsu</name>
    </author>
    <author>
      <name>Ting-Hao 'Kenneth' Huang</name>
    </author>
    <author>
      <name>Himanshu Verma</name>
    </author>
    <author>
      <name>Andrea Mauri</name>
    </author>
    <author>
      <name>Illah Nourbakhsh</name>
    </author>
    <author>
      <name>Alessandro Bozzon</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.patter.2022.100449</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.patter.2022.100449" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This manuscript is peer-reviewed and accepted by the Patterns journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.02007v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.02007v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.03026v1</id>
    <updated>2021-10-06T19:20:57Z</updated>
    <published>2021-10-06T19:20:57Z</published>
    <title>Human Capabilities as Guiding Lights for the Field of AI-HRI: Insights
  from Engineering Education</title>
    <summary>  Social Justice oriented Engineering Education frameworks have been developed
to help guide engineering students' decisions about which projects will
genuinely address human needs to create a better and more equitable society. In
this paper, we explore the role such theories might play in the field of
AI-HRI, consider the extent to which our community is (or is not) aligned with
these recommendations, and envision a future in which our research community
takes guidance from these theories. In particular, we analyze recent AI-HRI
(through analysis of 2020 AI-HRI papers) and consider possible futures of
AI-HRI (through a speculative ethics exercise). Both activities are guided
through the lens of the Engineering for Social Justice (E4SJ) framework, which
centers contextual listening and enhancement of human capabilities. Our
analysis suggests that current AI-HRI research is not well aligned with the
guiding principles of Engineering for Social Justice, and as such, does not
obviously meet the needs of the communities we could be helping most. As such,
we suggest that motivating future work through the E4SJ framework could help to
ensure that we as researchers are developing technologies that will actually
lead to a more equitable world.
</summary>
    <author>
      <name>Tom Williams</name>
    </author>
    <author>
      <name>Ruchen Wen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at AI-HRI symposium as part of AAAI-FSS 2021
  (arXiv:2109.10836)</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.03026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.03026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.03569v1</id>
    <updated>2021-10-07T15:42:18Z</updated>
    <published>2021-10-07T15:42:18Z</published>
    <title>Human in the Loop for Machine Creativity</title>
    <summary>  Artificial intelligence (AI) is increasingly utilized in synthesizing
visuals, texts, and audio. These AI-based works, often derived from neural
networks, are entering the mainstream market, as digital paintings, songs,
books, and others. We conceptualize both existing and future human-in-the-loop
(HITL) approaches for creative applications and to develop more expressive,
nuanced, and multimodal models. Particularly, how can our expertise as curators
and collaborators be encoded in AI models in an interactive manner? We examine
and speculate on long term implications for models, interfaces, and machine
creativity. Our selection, creation, and interpretation of AI art inherently
contain our emotional responses, cultures, and contexts. Therefore, the
proposed HITL may help algorithms to learn creative processes that are much
harder to codify or quantify. We envision multimodal HITL processes, where
texts, visuals, sounds, and other information are coupled together, with
automated analysis of humans and environments. Overall, these HITL approaches
will increase interaction between human and AI, and thus help the future AI
systems to better understand our own creative and emotional processes.
</summary>
    <author>
      <name>Neo Christopher Chung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9th AAAI Conference on Human Computation and Crowdsourcing (HCOMP
  2021), Blue Sky Ideas track</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.03569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.03569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.09232v1</id>
    <updated>2021-10-08T16:37:11Z</updated>
    <published>2021-10-08T16:37:11Z</published>
    <title>Accountability in AI: From Principles to Industry-specific Accreditation</title>
    <summary>  Recent AI-related scandals have shed a spotlight on accountability in AI,
with increasing public interest and concern. This paper draws on literature
from public policy and governance to make two contributions. First, we propose
an AI accountability ecosystem as a useful lens on the system, with different
stakeholders requiring and contributing to specific accountability mechanisms.
We argue that the present ecosystem is unbalanced, with a need for improved
transparency via AI explainability and adequate documentation and process
formalisation to support internal audit, leading up eventually to external
accreditation processes. Second, we use a case study in the gambling sector to
illustrate in a subset of the overall ecosystem the need for industry-specific
accountability principles and processes. We define and evaluate critically the
implementation of key accountability principles in the gambling industry,
namely addressing algorithmic bias and model explainability, before concluding
and discussing directions for future work based on our findings. Keywords:
Accountability, Explainable AI, Algorithmic Bias, Regulation.
</summary>
    <author>
      <name>Chris Percy</name>
    </author>
    <author>
      <name>Simo Dragicevic</name>
    </author>
    <author>
      <name>Sanjoy Sarkar</name>
    </author>
    <author>
      <name>Artur S. d'Avila Garcez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 2 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.09232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.09232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.01306v1</id>
    <updated>2021-11-02T00:15:04Z</updated>
    <published>2021-11-02T00:15:04Z</published>
    <title>On the Current and Emerging Challenges of Developing Fair and Ethical AI
  Solutions in Financial Services</title>
    <summary>  Artificial intelligence (AI) continues to find more numerous and more
critical applications in the financial services industry, giving rise to fair
and ethical AI as an industry-wide objective. While many ethical principles and
guidelines have been published in recent years, they fall short of addressing
the serious challenges that model developers face when building ethical AI
solutions. We survey the practical and overarching issues surrounding model
development, from design and implementation complexities, to the shortage of
tools, and the lack of organizational constructs. We show how practical
considerations reveal the gaps between high-level principles and concrete,
deployed AI applications, with the aim of starting industry-wide conversations
toward solution approaches.
</summary>
    <author>
      <name>Eren Kurshan</name>
    </author>
    <author>
      <name>Jiahao Chen</name>
    </author>
    <author>
      <name>Victor Storchan</name>
    </author>
    <author>
      <name>Hongda Shen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3490354.3494408</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3490354.3494408" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages; expanded from conference version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICAIF '21 - Proceedings of the 2nd ACM International Conference on
  AI in Finance (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2111.01306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.01306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4; K.5.2; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.04455v1</id>
    <updated>2021-10-29T11:54:51Z</updated>
    <published>2021-10-29T11:54:51Z</published>
    <title>Systematic Review for AI-based Language Learning Tools</title>
    <summary>  The Second Language Acquisition field has been significantly impacted by a
greater emphasis on individualized learning and rapid developments in
artificial intelligence (AI). Although increasingly adaptive language learning
tools are being developed with the application of AI to the Computer Assisted
Language Learning field, there have been concerns regarding insufficient
information and teacher preparation. To effectively utilize these tools,
teachers need an in-depth overview on recently developed AI-based language
learning tools. Therefore, this review synthesized information on AI tools that
were developed between 2017 and 2020. A majority of these tools utilized
machine learning and natural language processing, and were used to identify
errors, provide feedback, and assess language abilities. After using these
tools, learners demonstrated gains in their language abilities and knowledge.
This review concludes by presenting pedagogical implications and emerging
themes in the future research of AI-based language learning tools.
</summary>
    <author>
      <name>Jin Ha Woo</name>
    </author>
    <author>
      <name>Heeyoul Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.04455v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.04455v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.04456v1</id>
    <updated>2021-10-29T16:13:15Z</updated>
    <published>2021-10-29T16:13:15Z</published>
    <title>ACIPS: A Framework for Evaluating Patient Perception in the Introduction
  of AI-Enabled Healthcare</title>
    <summary>  In healthcare, the role of AI is continually evolving and understanding the
challenges its introduction poses on relationships between healthcare providers
and patients will require a regulatory and behavioural approach that can
provide a guiding base for all users involved. In this paper, we present ACIPS
(Acceptability, Comfortability, Informed Consent, Privacy, and Security), a
framework for evaluating patient response to the introduction of AI-enabled
digital technologies in healthcare settings. We justify the need for ACIPS with
a general introduction of the challenges with and perceived relevance of AI in
human-welfare centered fields, with an emphasis on the provision of healthcare.
The framework is composed of five principles that measure the perceptions of
acceptability, comfortability, informed consent, privacy, and security patients
hold when learning how AI is used in their healthcare. We propose that the
tenets composing this framework can be translated into guidelines outlining the
proper use of AI in healthcare while broadening the limited understanding of
this topic.
</summary>
    <author>
      <name>Chinasa T. Okolo</name>
    </author>
    <author>
      <name>Michelle González Amador</name>
    </author>
    <link href="http://arxiv.org/abs/2111.04456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.04456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.06420v1</id>
    <updated>2021-11-11T19:06:13Z</updated>
    <published>2021-11-11T19:06:13Z</published>
    <title>Explainable AI (XAI): A Systematic Meta-Survey of Current Challenges and
  Future Opportunities</title>
    <summary>  The past decade has seen significant progress in artificial intelligence
(AI), which has resulted in algorithms being adopted for resolving a variety of
problems. However, this success has been met by increasing model complexity and
employing black-box AI models that lack transparency. In response to this need,
Explainable AI (XAI) has been proposed to make AI more transparent and thus
advance the adoption of AI in critical domains. Although there are several
reviews of XAI topics in the literature that identified challenges and
potential research directions in XAI, these challenges and research directions
are scattered. This study, hence, presents a systematic meta-survey for
challenges and future research directions in XAI organized in two themes: (1)
general challenges and research directions in XAI and (2) challenges and
research directions in XAI based on machine learning life cycle's phases:
design, development, and deployment. We believe that our meta-survey
contributes to XAI literature by providing a guide for future exploration in
the XAI area.
</summary>
    <author>
      <name>Waddah Saeed</name>
    </author>
    <author>
      <name>Christian Omlin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 2 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.06420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.06420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.13365v4</id>
    <updated>2022-02-19T13:50:35Z</updated>
    <published>2021-11-26T08:58:09Z</published>
    <title>Machines &amp; Influence: An Information Systems Lens</title>
    <summary>  Policymakers face a broader challenge of how to view AI capabilities today
and where does society stand in terms of those capabilities. This paper surveys
AI capabilities and tackles this very issue, exploring it in context of
political security in digitally networked societies. We extend the ideas of
Information Management to better understand contemporary AI systems as part of
a larger and more complex information system. Comprehensively reviewing AI
capabilities and contemporary man-machine interactions, we undertake conceptual
development to suggest that better information management could allow states to
more optimally offset the risks of AI enabled influence and better utilise the
emerging capabilities which these systems have to offer to policymakers and
political institutions across the world. Hopefully this long essay will actuate
further debates and discussions over these ideas, and prove to be a useful
contribution towards governing the future of AI.
</summary>
    <author>
      <name>Shashank Yadav</name>
    </author>
    <link href="http://arxiv.org/abs/2111.13365v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.13365v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01242v1</id>
    <updated>2021-11-16T16:01:01Z</updated>
    <published>2021-11-16T16:01:01Z</published>
    <title>An AI-based Learning Companion Promoting Lifelong Learning Opportunities
  for All</title>
    <summary>  Artifical Intelligence (AI) in Education has great potential for building
more personalised curricula, as well as democratising education worldwide and
creating a Renaissance of new ways of teaching and learning. We believe this is
a crucial moment for setting the foundations of AI in education in the
beginning of this Fourth Industrial Revolution. This report aims to synthesize
how AI might change (and is already changing) how we learn, as well as what
technological features are crucial for these AI systems in education, with the
end goal of starting this pressing dialogue of how the future of AI in
education should unfold, engaging policy makers, engineers, researchers and
obviously, teachers and learners. This report also presents the advances within
the X5GON project, a European H2020 project aimed at building and deploying a
cross-modal, cross-lingual, cross-cultural, cross-domain and cross-site
personalised learning platform for Open Educational Resources (OER).
</summary>
    <author>
      <name>Maria Perez-Ortiz</name>
    </author>
    <author>
      <name>Erik Novak</name>
    </author>
    <author>
      <name>Sahan Bulathwela</name>
    </author>
    <author>
      <name>John Shawe-Taylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as an Opinion Report from the International Research Centre
  on Artificial Intelligence under the auspices of UNESCO</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.01242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01282v1</id>
    <updated>2021-11-29T12:55:33Z</updated>
    <published>2021-11-29T12:55:33Z</published>
    <title>Achieving a Data-driven Risk Assessment Methodology for Ethical AI</title>
    <summary>  The AI landscape demands a broad set of legal, ethical, and societal
considerations to be accounted for in order to develop ethical AI (eAI)
solutions which sustain human values and rights. Currently, a variety of
guidelines and a handful of niche tools exist to account for and tackle
individual challenges. However, it is also well established that many
organizations face practical challenges in navigating these considerations from
a risk management perspective. Therefore, new methodologies are needed to
provide a well-vetted and real-world applicable structure and path through the
checks and balances needed for ethically assessing and guiding the development
of AI. In this paper we show that a multidisciplinary research approach,
spanning cross-sectional viewpoints, is the foundation of a pragmatic
definition of ethical and societal risks faced by organizations using AI.
Equally important is the findings of cross-structural governance for
implementing eAI successfully. Based on evidence acquired from our
multidisciplinary research investigation, we propose a novel data-driven risk
assessment methodology, entitled DRESS-eAI. In addition, through the evaluation
of our methodological implementation, we demonstrate its state-of-the-art
relevance as a tool for sustaining human values in the data-driven AI era.
</summary>
    <author>
      <name>Anna Felländer</name>
    </author>
    <author>
      <name>Jonathan Rebane</name>
    </author>
    <author>
      <name>Stefan Larsson</name>
    </author>
    <author>
      <name>Mattias Wiggberg</name>
    </author>
    <author>
      <name>Fredrik Heintz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.01282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.10551v2</id>
    <updated>2021-12-22T14:18:33Z</updated>
    <published>2021-12-20T14:25:05Z</published>
    <title>Scope and Sense of Explainability for AI-Systems</title>
    <summary>  Certain aspects of the explainability of AI systems will be critically
discussed. This especially with focus on the feasibility of the task of making
every AI system explainable. Emphasis will be given to difficulties related to
the explainability of highly complex and efficient AI systems which deliver
decisions whose explanation defies classical logical schemes of cause and
effect. AI systems have provably delivered unintelligible solutions which in
retrospect were characterized as ingenious (for example move 37 of the game 2
of AlphaGo). It will be elaborated on arguments supporting the notion that if
AI-solutions were to be discarded in advance because of their not being
thoroughly comprehensible, a great deal of the potentiality of intelligent
systems would be wasted.
</summary>
    <author>
      <name>A. -M. Leventi-Peetz</name>
    </author>
    <author>
      <name>T. Östreich</name>
    </author>
    <author>
      <name>W. Lennartz</name>
    </author>
    <author>
      <name>K. Weber</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-82193-7_19</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-82193-7_19" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 2 : Improved hyphenations in references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Arai K. (eds) Intelligent Systems and Applications. IntelliSys
  2021. Lecture Notes in Networks and Systems, vol 294. Springer, Cham</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2112.10551v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.10551v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.05576v1</id>
    <updated>2022-01-07T10:54:06Z</updated>
    <published>2022-01-07T10:54:06Z</published>
    <title>AI and the Sense of Self</title>
    <summary>  After several winters, AI is center-stage once again, with current advances
enabling a vast array of AI applications. This renewed wave of AI has brought
back to the fore several questions from the past, about philosophical
foundations of intelligence and common sense -- predominantly motivated by
ethical concerns of AI decision-making. In this paper, we address some of the
arguments that led to research interest in intelligent agents, and argue for
their relevance even in today's context. Specifically we focus on the cognitive
sense of "self" and its role in autonomous decision-making leading to
responsible behaviour. The authors hope to make a case for greater research
interest in building richer computational models of AI agents with a sense of
self.
</summary>
    <author>
      <name>Srinath Srinivasa</name>
    </author>
    <author>
      <name>Jayati Deshmukh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Previous version of this paper was published in Jijnasa 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.05576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.05576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.11441v1</id>
    <updated>2022-01-27T10:56:33Z</updated>
    <published>2022-01-27T10:56:33Z</published>
    <title>Human-centered mechanism design with Democratic AI</title>
    <summary>  Building artificial intelligence (AI) that aligns with human values is an
unsolved problem. Here, we developed a human-in-the-loop research pipeline
called Democratic AI, in which reinforcement learning is used to design a
social mechanism that humans prefer by majority. A large group of humans played
an online investment game that involved deciding whether to keep a monetary
endowment or to share it with others for collective benefit. Shared revenue was
returned to players under two different redistribution mechanisms, one designed
by the AI and the other by humans. The AI discovered a mechanism that redressed
initial wealth imbalance, sanctioned free riders, and successfully won the
majority vote. By optimizing for human preferences, Democratic AI may be a
promising method for value-aligned policy innovation.
</summary>
    <author>
      <name>Raphael Koster</name>
    </author>
    <author>
      <name>Jan Balaguer</name>
    </author>
    <author>
      <name>Andrea Tacchetti</name>
    </author>
    <author>
      <name>Ari Weinstein</name>
    </author>
    <author>
      <name>Tina Zhu</name>
    </author>
    <author>
      <name>Oliver Hauser</name>
    </author>
    <author>
      <name>Duncan Williams</name>
    </author>
    <author>
      <name>Lucy Campbell-Gillingham</name>
    </author>
    <author>
      <name>Phoebe Thacker</name>
    </author>
    <author>
      <name>Matthew Botvinick</name>
    </author>
    <author>
      <name>Christopher Summerfield</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 4 figures, 54 pages including supplemental materials</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.11441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.11441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.09465v1</id>
    <updated>2022-02-18T22:54:04Z</updated>
    <published>2022-02-18T22:54:04Z</published>
    <title>Attacks, Defenses, And Tools: A Framework To Facilitate Robust AI/ML
  Systems</title>
    <summary>  Software systems are increasingly relying on Artificial Intelligence (AI) and
Machine Learning (ML) components. The emerging popularity of AI techniques in
various application domains attracts malicious actors and adversaries.
Therefore, the developers of AI-enabled software systems need to take into
account various novel cyber-attacks and vulnerabilities that these systems may
be susceptible to. This paper presents a framework to characterize attacks and
weaknesses associated with AI-enabled systems and provide mitigation techniques
and defense strategies. This framework aims to support software designers in
taking proactive measures in developing AI-enabled software, understanding the
attack surface of such systems, and developing products that are resilient to
various emerging attacks associated with ML. The developed framework covers a
broad spectrum of attacks, mitigation techniques, and defensive and offensive
tools. In this paper, we demonstrate the framework architecture and its major
components, describe their attributes, and discuss the long-term goals of this
research.
</summary>
    <author>
      <name>Mohamad Fazelnia</name>
    </author>
    <author>
      <name>Igor Khokhlov</name>
    </author>
    <author>
      <name>Mehdi Mirakhorli</name>
    </author>
    <link href="http://arxiv.org/abs/2202.09465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.09465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.13985v2</id>
    <updated>2022-03-01T11:23:04Z</updated>
    <published>2022-02-28T17:41:39Z</published>
    <title>The dangers in algorithms learning humans' values and irrationalities</title>
    <summary>  For an artificial intelligence (AI) to be aligned with human values (or human
preferences), it must first learn those values. AI systems that are trained on
human behavior, risk miscategorising human irrationalities as human values --
and then optimising for these irrationalities. Simply learning human values
still carries risks: AI learning them will inevitably also gain information on
human irrationalities and human behaviour/policy. Both of these can be
dangerous: knowing human policy allows an AI to become generically more
powerful (whether it is partially aligned or not aligned at all), while
learning human irrationalities allows it to exploit humans without needing to
provide value in return. This paper analyses the danger in developing
artificial intelligence that learns about human irrationalities and human
policy, and constructs a model recommendation system with various levels of
information about human biases, human policy, and human values. It concludes
that, whatever the power and knowledge of the AI, it is more dangerous for it
to know human irrationalities than human values. Thus it is better for the AI
to learn human values directly, rather than learning human biases and then
deducing values from behaviour.
</summary>
    <author>
      <name>Rebecca Gorman</name>
    </author>
    <author>
      <name>Stuart Armstrong</name>
    </author>
    <link href="http://arxiv.org/abs/2202.13985v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.13985v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.14010v2</id>
    <updated>2022-03-01T14:57:13Z</updated>
    <published>2022-02-28T18:27:41Z</published>
    <title>Proceedings of the Artificial Intelligence for Cyber Security (AICS)
  Workshop at AAAI 2022</title>
    <summary>  The workshop will focus on the application of AI to problems in cyber
security. Cyber systems generate large volumes of data, utilizing this
effectively is beyond human capabilities. Additionally, adversaries continue to
develop new attacks. Hence, AI methods are required to understand and protect
the cyber domain. These challenges are widely studied in enterprise networks,
but there are many gaps in research and practice as well as novel problems in
other domains.
  In general, AI techniques are still not widely adopted in the real world.
Reasons include: (1) a lack of certification of AI for security, (2) a lack of
formal study of the implications of practical constraints (e.g., power, memory,
storage) for AI systems in the cyber domain, (3) known vulnerabilities such as
evasion, poisoning attacks, (4) lack of meaningful explanations for security
analysts, and (5) lack of analyst trust in AI solutions. There is a need for
the research community to develop novel solutions for these practical issues.
</summary>
    <author>
      <name>James Holt</name>
    </author>
    <author>
      <name>Edward Raff</name>
    </author>
    <author>
      <name>Ahmad Ridley</name>
    </author>
    <author>
      <name>Dennis Ross</name>
    </author>
    <author>
      <name>Arunesh Sinha</name>
    </author>
    <author>
      <name>Diane Staheli</name>
    </author>
    <author>
      <name>William Streilen</name>
    </author>
    <author>
      <name>Milind Tambe</name>
    </author>
    <author>
      <name>Yevgeniy Vorobeychik</name>
    </author>
    <author>
      <name>Allan Wollaber</name>
    </author>
    <link href="http://arxiv.org/abs/2202.14010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.14010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.10996v1</id>
    <updated>2022-03-11T01:46:40Z</updated>
    <published>2022-03-11T01:46:40Z</published>
    <title>Technologies for AI-Driven Fashion Social Networking Service with
  E-Commerce</title>
    <summary>  The rapid growth of the online fashion market brought demands for innovative
fashion services and commerce platforms. With the recent success of deep
learning, many applications employ AI technologies such as visual search and
recommender systems to provide novel and beneficial services. In this paper, we
describe applied technologies for AI-driven fashion social networking service
that incorporate fashion e-commerce. In the application, people can share and
browse their outfit-of-the-day (OOTD) photos, while AI analyzes them and
suggests similar style OOTDs and related products. To this end, we trained deep
learning based AI models for fashion and integrated them to build a fashion
visual search system and a recommender system for OOTD. With aforementioned
technologies, the AI-driven fashion SNS platform, iTOO, has been successfully
launched.
</summary>
    <author>
      <name>Jinseok Seol</name>
    </author>
    <author>
      <name>Seongjae Kim</name>
    </author>
    <author>
      <name>Sungchan Park</name>
    </author>
    <author>
      <name>Holim Lim</name>
    </author>
    <author>
      <name>Hyunsoo Na</name>
    </author>
    <author>
      <name>Eunyoung Park</name>
    </author>
    <author>
      <name>Dohee Jung</name>
    </author>
    <author>
      <name>Soyoung Park</name>
    </author>
    <author>
      <name>Kangwoo Lee</name>
    </author>
    <author>
      <name>Sang-goo Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, accepted in International Semantic Intelligence Conference
  (ISIC) 2022, The Applications and Deployment Track</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.10996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.10996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.11157v1</id>
    <updated>2022-03-10T18:38:52Z</updated>
    <published>2022-03-10T18:38:52Z</published>
    <title>AI Annotated Recommendations in an Efficient Visual Learning Environment
  with Emphasis on YouTube (AI-EVL)</title>
    <summary>  In this article, we create a system called AI-EVL. This is an annotated-based
learning system. We extend AI to learning experience. If a user from the main
YouTube page browses YouTube videos and a user from the AI-EVL system does the
same, the amount of traffic used will be much less. It is due to ignoring
unwanted contents which indicates a reduction in bandwidth usage too. This
system is designed to be embedded with online learning tools and platforms to
enrich their curriculum. In evaluating the system using Google 2020 trend data,
we were able to extract rich ontological information for each data. Of the data
collected, 34.86% belong to wolfram, 30.41% to DBpedia, and 34.73% to
Wikipedia. The video subtitle information is displayed interactively and
functionally to the user over time as the video is played. This effective
visual learning system, due to the unique features, prevents the user's
distraction and makes learning more focused. The information about the subtitle
text is displayed in multiple layers including AI-annotated topics,
Wikipedia/DBpedia, and Wolfram enriched texts via interactive and visual
widgets.
</summary>
    <author>
      <name>Faeze Gholamrezaie</name>
    </author>
    <author>
      <name>Melika Bahman-Abadi</name>
    </author>
    <author>
      <name>M. B. Ghaznavi-Ghoushchi</name>
    </author>
    <link href="http://arxiv.org/abs/2203.11157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.11157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.12736v1</id>
    <updated>2022-03-23T21:40:01Z</updated>
    <published>2022-03-23T21:40:01Z</published>
    <title>An interactive music infilling interface for pop music composition</title>
    <summary>  Artificial intelligence (AI) has been widely applied to music generation
topics such as continuation, melody/harmony generation, genre transfer and
music infilling application. Although with the burst interest to apply AI to
music, there are still few interfaces for the musicians to take advantage of
the latest progress of the AI technology. This makes those tools less valuable
in practice and harder to find its advantage/drawbacks without utilizing them
in the real scenario. This work builds a max patch for interactive music
infilling application with different levels of control, including track
density/polyphony/occupation rate and bar tonal tension control. The user can
select the melody/bass/harmony track as the infilling content up to 16 bars.
The infilling algorithm is based on the author's previous work, and the
interface sends/receives messages to the AI system hosted in the cloud. This
interface lowers the barrier of AI technology and can generate different
variations of the selected content. Those results can give several alternatives
to the musicians' composition, and the interactive process realizes the value
of the AI infilling system.
</summary>
    <author>
      <name>Rui Guo</name>
    </author>
    <link href="http://arxiv.org/abs/2203.12736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.12736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.02362v2</id>
    <updated>2022-04-13T12:02:18Z</updated>
    <published>2022-04-04T12:47:07Z</published>
    <title>Challenges and Opportunities of Edge AI for Next-Generation Implantable
  BMIs</title>
    <summary>  Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.
</summary>
    <author>
      <name>MohammadAli Shaeri</name>
    </author>
    <author>
      <name>Arshia Afzal</name>
    </author>
    <author>
      <name>Mahsa Shoaran</name>
    </author>
    <link href="http://arxiv.org/abs/2204.02362v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.02362v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.07888v1</id>
    <updated>2022-04-17T00:12:45Z</updated>
    <published>2022-04-17T00:12:45Z</published>
    <title>AI, Ageing and Brain-Work Productivity: Technological Change in
  Professional Japanese Chess</title>
    <summary>  Using Japanese professional chess (Shogi) players records in the novel
setting, this paper examines how and the extent to which the emergence of
technological changes influences the ageing and innate ability of players
winning probability. We gathered games of professional Shogi players from 1968
to 2019.
  The major findings are: (1) diffusion of artificial intelligence (AI) reduces
innate ability, which reduces the performance gap among same-age players; (2)
players winning rates declined consistently from 20 years and as they get
older; (3) AI accelerated the ageing declination of the probability of winning,
which increased the performance gap among different aged players; (4) the
effects of AI on the ageing declination and the probability of winning are
observed for high innate skill players but not for low innate skill ones. This
implies that the diffusion of AI hastens players retirement from active play,
especially for those with high innate abilities. Thus, AI is a substitute for
innate ability in brain-work productivity.
</summary>
    <author>
      <name>Eiji Yamamura</name>
    </author>
    <author>
      <name>Ryohei Hayashi</name>
    </author>
    <link href="http://arxiv.org/abs/2204.07888v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.07888v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.08859v1</id>
    <updated>2022-04-19T12:54:23Z</updated>
    <published>2022-04-19T12:54:23Z</published>
    <title>On the Influence of Explainable AI on Automation Bias</title>
    <summary>  Artificial intelligence (AI) is gaining momentum, and its importance for the
future of work in many areas, such as medicine and banking, is continuously
rising. However, insights on the effective collaboration of humans and AI are
still rare. Typically, AI supports humans in decision-making by addressing
human limitations. However, it may also evoke human bias, especially in the
form of automation bias as an over-reliance on AI advice. We aim to shed light
on the potential to influence automation bias by explainable AI (XAI). In this
pre-test, we derive a research model and describe our study design.
Subsequentially, we conduct an online experiment with regard to hotel review
classifications and discuss first results. We expect our research to contribute
to the design and development of safe hybrid intelligence systems.
</summary>
    <author>
      <name>Max Schemmer</name>
    </author>
    <author>
      <name>Niklas Kühl</name>
    </author>
    <author>
      <name>Carina Benz</name>
    </author>
    <author>
      <name>Gerhard Satzger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Thirtieth European Conference on Information Systems (ECIS 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.08859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.08859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.01296v2</id>
    <updated>2022-05-04T15:04:47Z</updated>
    <published>2022-05-03T04:17:21Z</published>
    <title>Visual Knowledge Discovery with Artificial Intelligence: Challenges and
  Future Directions</title>
    <summary>  This volume is devoted to the emerging field of Integrated Visual Knowledge
Discovery that combines advances in Artificial Intelligence/Machine Learning
(AI/ML) and Visualization/Visual Analytics. Chapters included are extended
versions of the selected AI and Visual Analytics papers and related symposia at
the recent International Information Visualization Conferences (IV2019 and
IV2020). AI/ML face a long-standing challenge of explaining models to humans.
Models explanation is fundamentally human activity, not only an algorithmic
one. In this chapter we aim to present challenges and future directions within
the field of Visual Analytics, Visual Knowledge Discovery and AI/ML, and to
discuss the role of visualization in visual AI/ML. In addition, we describe
progress in emerging Full 2D ML, natural language processing, and AI/ML in
multidimensional data aided by visual means.
</summary>
    <author>
      <name>Boris Kovalerchuk</name>
    </author>
    <author>
      <name>Răzvan Andonie</name>
    </author>
    <author>
      <name>Nuno Datia</name>
    </author>
    <author>
      <name>Kawa Nazemi</name>
    </author>
    <author>
      <name>Ebad Banissi</name>
    </author>
    <link href="http://arxiv.org/abs/2205.01296v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.01296v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.03946v1</id>
    <updated>2022-05-08T20:25:13Z</updated>
    <published>2022-05-08T20:25:13Z</published>
    <title>What does it mean to be an AI Ethicist: An ontology of existing roles</title>
    <summary>  With the increasing adoption of Artificial Intelligence systems (AIS) in
various application and the growing efforts to regulate such systems, a new set
of occupations has emerged in the industry. This new set of roles take
different titles and hold varying responsibilities. However, the individuals in
these roles are tasked with interpreting and operationalizing best practices
for developing ethical and safe AI systems. We will broadly refer to this new
set of occupations as AI ethicists and recognize that they often hold a
specific role in the intersection of technology development, business needs,
and societal implications. In this work, we examine what it means to be an AI
ethicist in the industry and propose an ontology of existing roles under this
broad title along with their required competencies. We create this ontology by
examining the job postings for such roles over the past two years and conduct
expert interviews with fourteen individuals who currently hold such a role in
the industry. The proposed ontology will inform executives and leaders who are
looking to build responsible AI teams and provide educators the necessary
information for creating new learning objectives and curriculum.
</summary>
    <author>
      <name>Shalaleh Rismani</name>
    </author>
    <author>
      <name>AJung Moon</name>
    </author>
    <link href="http://arxiv.org/abs/2205.03946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.03946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.09376v1</id>
    <updated>2022-05-19T08:15:50Z</updated>
    <published>2022-05-19T08:15:50Z</published>
    <title>Multi-DNN Accelerators for Next-Generation AI Systems</title>
    <summary>  As the use of AI-powered applications widens across multiple domains, so do
increase the computational demands. Primary driver of AI technology are the
deep neural networks (DNNs). When focusing either on cloud-based systems that
serve multiple AI queries from different users each with their own DNN model,
or on mobile robots and smartphones employing pipelines of various models or
parallel DNNs for the concurrent processing of multi-modal data, the next
generation of AI systems will have multi-DNN workloads at their core.
Large-scale deployment of AI services and integration across mobile and
embedded systems require additional breakthroughs in the computer architecture
front, with processors that can maintain high performance as the number of DNNs
increases while meeting the quality-of-service requirements, giving rise to the
topic of multi-DNN accelerator design.
</summary>
    <author>
      <name>Stylianos I. Venieris</name>
    </author>
    <author>
      <name>Christos-Savvas Bouganis</name>
    </author>
    <author>
      <name>Nicholas D. Lane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at the IEEE Computer journal, 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.09376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.09376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.14121v1</id>
    <updated>2022-05-25T03:48:36Z</updated>
    <published>2022-05-25T03:48:36Z</published>
    <title>AI-aided multiscale modeling of physiologically-significant blood clots</title>
    <summary>  We have developed an AI-aided multiple time stepping (AI-MTS) algorithm and
multiscale modeling framework (AI-MSM) and implemented them on the Summit-like
supercomputer, AIMOS. AI-MSM is the first of its kind to integrate
multi-physics, including intra-platelet, inter-platelet, and fluid-platelet
interactions, into one system. It has simulated a record-setting multiscale
blood clotting model of 102 million particles, of which 70 flowing and 180
aggregating platelets, under dissipative particle dynamics to coarse-grained
molecular dynamics. By adaptively adjusting timestep sizes to match the
characteristic time scales of the underlying dynamics, AI-MTS optimally
balances speeds and accuracies of the simulations.
</summary>
    <author>
      <name>Yicong Zhu</name>
    </author>
    <author>
      <name>Changnian Han</name>
    </author>
    <author>
      <name>Peng Zhang</name>
    </author>
    <author>
      <name>Guojing Cong</name>
    </author>
    <author>
      <name>James R. Kozloski</name>
    </author>
    <author>
      <name>Chih-Chieh Yang</name>
    </author>
    <author>
      <name>Leili Zhang</name>
    </author>
    <author>
      <name>Yuefan Deng</name>
    </author>
    <link href="http://arxiv.org/abs/2205.14121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.14121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.15686v2</id>
    <updated>2022-11-09T10:30:24Z</updated>
    <published>2022-05-31T11:04:55Z</published>
    <title>The NOMAD Artificial-Intelligence Toolkit: Turning materials-science
  data into knowledge and understanding</title>
    <summary>  We present the Novel-Materials-Discovery (NOMAD) Artificial-Intelligence (AI)
Toolkit, a web-browser-based infrastructure for the interactive AI-based
analysis of materials-science findable, accessible, interoperable, and reusable
(FAIR) data. The AI Toolkit readily operates on the FAIR data stored in the
central server of the NOMAD Archive, the largest database of materials-science
data worldwide, as well as locally stored, users' owned data. The NOMAD Oasis,
a local, stand alone server can be also used to run the AI Toolkit. By using
Jupyter notebooks that run in a web-browser, the NOMAD data can be queried and
accessed; data mining, machine learning, and other AI techniques can be then
applied to analyse them. This infrastructure brings the concept of
reproducibility in materials science to the next level, by allowing researchers
to share not only the data contributing to their scientific publications, but
also all the developed methods and analytics tools. Besides reproducing
published results, users of the NOMAD AI toolkit can modify the Jupyter
notebooks towards their own research work.
</summary>
    <author>
      <name>Luigi Sbailò</name>
    </author>
    <author>
      <name>Ádám Fekete</name>
    </author>
    <author>
      <name>Luca M. Ghiringhelli</name>
    </author>
    <author>
      <name>Matthias Scheffler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication on npj Computational Materials on November
  9, 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.15686v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.15686v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.08401v1</id>
    <updated>2022-07-18T06:22:25Z</updated>
    <published>2022-07-18T06:22:25Z</published>
    <title>Marvista: A Human-AI Collaborative Reading Tool</title>
    <summary>  We present Marvista -- a human-AI collaborative tool that employs a suite of
natural language processing models to provide end-to-end support for reading
online articles. Before reading an article, Marvista helps a user plan what to
read by filtering text based on how much time one can spend and what questions
one is interested to find out from the article. During reading, Marvista helps
the user focus on one paragraph at a time and reflect on their understanding of
each paragraph with AI-generated questions. After reading, Marvista generates
an explainable human-AI summary that combines both AI's processing of the text,
the user's reading behavior, and user-generated data in the reading process. In
contrast to prior work that offered (content-independent) interaction
techniques or devices for reading, Marvista is the first human-AI collaborative
tool that contributes text-specific guidance (content-aware) to support the
entire reading process.
</summary>
    <author>
      <name>Xiang 'Anthony' Chen</name>
    </author>
    <author>
      <name>Chien-Sheng Wu</name>
    </author>
    <author>
      <name>Tong Niu</name>
    </author>
    <author>
      <name>Wenhao Liu</name>
    </author>
    <author>
      <name>Caiming Xiong</name>
    </author>
    <link href="http://arxiv.org/abs/2207.08401v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.08401v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.00406v2</id>
    <updated>2022-08-03T22:48:23Z</updated>
    <published>2022-07-31T09:34:53Z</published>
    <title>Eco2AI: carbon emissions tracking of machine learning models as the
  first step towards sustainable AI</title>
    <summary>  The size and complexity of deep neural networks continue to grow
exponentially, significantly increasing energy consumption for training and
inference by these models. We introduce an open-source package eco2AI to help
data scientists and researchers to track energy consumption and equivalent CO2
emissions of their models in a straightforward way. In eco2AI we put emphasis
on accuracy of energy consumption tracking and correct regional CO2 emissions
accounting. We encourage research community to search for new optimal
Artificial Intelligence (AI) architectures with a lower computational cost. The
motivation also comes from the concept of AI-based green house gases
sequestrating cycle with both Sustainable AI and Green AI pathways.
</summary>
    <author>
      <name>Semen Budennyy</name>
    </author>
    <author>
      <name>Vladimir Lazarev</name>
    </author>
    <author>
      <name>Nikita Zakharenko</name>
    </author>
    <author>
      <name>Alexey Korovin</name>
    </author>
    <author>
      <name>Olga Plosskaya</name>
    </author>
    <author>
      <name>Denis Dimitrov</name>
    </author>
    <author>
      <name>Vladimir Arkhipkin</name>
    </author>
    <author>
      <name>Ivan Oseledets</name>
    </author>
    <author>
      <name>Ivan Barsola</name>
    </author>
    <author>
      <name>Ilya Egorov</name>
    </author>
    <author>
      <name>Aleksandra Kosterina</name>
    </author>
    <author>
      <name>Leonid Zhukov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Source code for eco2AI package (energy consumption and carbon
  emission tracker of code in python) is available at:
  https://github.com/sb-ai-lab/Eco2AI , the package is also available at PyPi:
  https://pypi.org/project/eco2ai/</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.00406v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.00406v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.00870v1</id>
    <updated>2022-08-01T13:57:11Z</updated>
    <published>2022-08-01T13:57:11Z</published>
    <title>Suggestion Lists vs. Continuous Generation: Interaction Design for
  Writing with Generative Models on Mobile Devices Affect Text Length, Wording
  and Perceived Authorship</title>
    <summary>  Neural language models have the potential to support human writing. However,
questions remain on their integration and influence on writing and output. To
address this, we designed and compared two user interfaces for writing with AI
on mobile devices, which manipulate levels of initiative and control: 1)
Writing with continuously generated text, the AI adds text word-by-word and
user steers. 2) Writing with suggestions, the AI suggests phrases and user
selects from a list. In a supervised online study (N=18), participants used
these prototypes and a baseline without AI. We collected touch interactions,
ratings on inspiration and authorship, and interview data. With AI suggestions,
people wrote less actively, yet felt they were the author. Continuously
generated text reduced this perceived authorship, yet increased editing
behavior. In both designs, AI increased text length and was perceived to
influence wording. Our findings add new empirical evidence on the impact of UI
design decisions on user experience and output with co-creative systems.
</summary>
    <author>
      <name>Florian Lehmann</name>
    </author>
    <author>
      <name>Niklas Markert</name>
    </author>
    <author>
      <name>Hai Dang</name>
    </author>
    <author>
      <name>Daniel Buschek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3543758.3543947</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3543758.3543947" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-Print, to appear in MuC '22: Mensch und Computer 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.00870v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.00870v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.01305v1</id>
    <updated>2022-08-02T08:24:29Z</updated>
    <published>2022-08-02T08:24:29Z</published>
    <title>Humble Machines: Attending to the Underappreciated Costs of Misplaced
  Distrust</title>
    <summary>  It is curious that AI increasingly outperforms human decision makers, yet
much of the public distrusts AI to make decisions affecting their lives. In
this paper we explore a novel theory that may explain one reason for this. We
propose that public distrust of AI is a moral consequence of designing systems
that prioritize reduction of costs of false positives over less tangible costs
of false negatives. We show that such systems, which we characterize as
'distrustful', are more likely to miscategorize trustworthy individuals, with
cascading consequences to both those individuals and the overall human-AI trust
relationship. Ultimately, we argue that public distrust of AI stems from
well-founded concern about the potential of being miscategorized. We propose
that restoring public trust in AI will require that systems are designed to
embody a stance of 'humble trust', whereby the moral costs of the misplaced
distrust associated with false negatives is weighted appropriately during
development and use.
</summary>
    <author>
      <name>Bran Knowles</name>
    </author>
    <author>
      <name>Jason D'Cruz</name>
    </author>
    <author>
      <name>John T. Richards</name>
    </author>
    <author>
      <name>Kush R. Varshney</name>
    </author>
    <link href="http://arxiv.org/abs/2208.01305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.01305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.07698v3</id>
    <updated>2022-10-24T12:51:27Z</updated>
    <published>2022-08-16T12:13:29Z</published>
    <title>Score-Based Diffusion meets Annealed Importance Sampling</title>
    <summary>  More than twenty years after its introduction, Annealed Importance Sampling
(AIS) remains one of the most effective methods for marginal likelihood
estimation. It relies on a sequence of distributions interpolating between a
tractable initial distribution and the target distribution of interest which we
simulate from approximately using a non-homogeneous Markov chain. To obtain an
importance sampling estimate of the marginal likelihood, AIS introduces an
extended target distribution to reweight the Markov chain proposal. While much
effort has been devoted to improving the proposal distribution used by AIS, an
underappreciated issue is that AIS uses a convenient but suboptimal extended
target distribution. We here leverage recent progress in score-based generative
modeling (SGM) to approximate the optimal extended target distribution
minimizing the variance of the marginal likelihood estimate for AIS proposals
corresponding to the discretization of Langevin and Hamiltonian dynamics. We
demonstrate these novel, differentiable, AIS procedures on a number of
synthetic benchmark distributions and variational auto-encoders.
</summary>
    <author>
      <name>Arnaud Doucet</name>
    </author>
    <author>
      <name>Will Grathwohl</name>
    </author>
    <author>
      <name>Alexander G. D. G. Matthews</name>
    </author>
    <author>
      <name>Heiko Strathmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at NeurIPS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.07698v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.07698v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.03499v2</id>
    <updated>2022-09-12T17:51:07Z</updated>
    <published>2022-09-07T23:36:11Z</published>
    <title>Sell Me the Blackbox! Regulating eXplainable Artificial Intelligence
  (XAI) May Harm Consumers</title>
    <summary>  Recent AI algorithms are blackbox models whose decisions are difficult to
interpret. eXplainable AI (XAI) seeks to address lack of AI interpretability
and trust by explaining to customers their AI decision, e.g., decision to
reject a loan application. The common wisdom is that regulating AI by mandating
fully transparent XAI leads to greater social welfare. This paper challenges
this notion through a game theoretic model for a policy-maker who maximizes
social welfare, firms in a duopoly competition that maximize profits, and
heterogenous consumers. The results show that XAI regulation may be redundant.
In fact, mandating fully transparent XAI may make firms and customers worse
off. This reveals a trade-off between maximizing welfare and receiving
explainable AI outputs. We also discuss managerial implications for
policy-maker and firms.
</summary>
    <author>
      <name>Behnam Mohammadi</name>
    </author>
    <author>
      <name>Nikhil Malik</name>
    </author>
    <author>
      <name>Tim Derdenger</name>
    </author>
    <author>
      <name>Kannan Srinivasan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected the title</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.03499v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.03499v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.09491v2</id>
    <updated>2022-09-21T05:26:15Z</updated>
    <published>2022-09-20T06:04:26Z</published>
    <title>Deep Q-Network for AI Soccer</title>
    <summary>  Reinforcement learning has shown an outstanding performance in the
applications of games, particularly in Atari games as well as Go. Based on
these successful examples, we attempt to apply one of the well-known
reinforcement learning algorithms, Deep Q-Network, to the AI Soccer game. AI
Soccer is a 5:5 robot soccer game where each participant develops an algorithm
that controls five robots in a team to defeat the opponent participant. Deep
Q-Network is designed to implement our original rewards, the state space, and
the action space to train each agent so that it can take proper actions in
different situations during the game. Our algorithm was able to successfully
train the agents, and its performance was preliminarily proven through the
mini-competition against 10 teams wishing to take part in the AI Soccer
international competition. The competition was organized by the AI World Cup
committee, in conjunction with the WCG 2019 Xi'an AI Masters. With our
algorithm, we got the achievement of advancing to the round of 16 in this
international competition with 130 teams from 39 countries.
</summary>
    <author>
      <name>Curie Kim</name>
    </author>
    <author>
      <name>Yewon Hwang</name>
    </author>
    <author>
      <name>Jong-Hwan Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2209.09491v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.09491v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.00829v1</id>
    <updated>2022-10-03T11:26:39Z</updated>
    <published>2022-10-03T11:26:39Z</published>
    <title>Dancing with the Unexpected and Beyond: The Use of AI Assistance in
  Design Fiction Creation</title>
    <summary>  The creation process of design fiction is going participatory and inclusive
with non experts. Recognizing the potential of artificial intelligence in
creativity support, we explore the use of AI assistance in creating design
fiction. This investigation is based on a workshop on future work in 2040 with
Chinese youth. We look into fiction quality, participants experiences with the
AI agent, and their ways of incorporating those texts into writing. Our
findings show that human writers while responding to messy and unexpected
AI-generated texts, can elevate the richness and creativity in writing and
initiate joyful and inspirational interactions. Furthermore, for the design of
AI assistance in creativity support, we suggest two implications of enhancing
interactional quality between human and AI and prompt programming. Our study
indicates the potential of applying design fiction outside the design context
using a more inclusive approach for future speculation with critical reflection
on technology.
</summary>
    <author>
      <name>Yiying Wu</name>
    </author>
    <author>
      <name>Yunye Yu</name>
    </author>
    <author>
      <name>Pengcheng An</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Chinese CHI conference 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.00829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.00829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.03555v1</id>
    <updated>2022-10-07T13:41:15Z</updated>
    <published>2022-10-07T13:41:15Z</published>
    <title>In-situ Model Downloading to Realize Versatile Edge AI in 6G Mobile
  Networks</title>
    <summary>  The sixth-generation (6G) mobile networks are expected to feature the
ubiquitous deployment of machine learning and AI algorithms at the network
edge. With rapid advancements in edge AI, the time has come to realize
intelligence downloading onto edge devices (e.g., smartphones and sensors). To
materialize this version, we propose a novel technology in this article, called
in-situ model downloading, that aims to achieve transparent and real-time
replacement of on-device AI models by downloading from an AI library in the
network. Its distinctive feature is the adaptation of downloading to
time-varying situations (e.g., application, location, and time), devices'
heterogeneous storage-and-computing capacities, and channel states. A key
component of the presented framework is a set of techniques that dynamically
compress a downloaded model at the depth-level, parameter-level, or bit-level
to support adaptive model downloading. We further propose a virtualized 6G
network architecture customized for deploying in-situ model downloading with
the key feature of a three-tier (edge, local, and central) AI library.
Furthermore, experiments are conducted to quantify 6G connectivity requirements
and research opportunities pertaining to the proposed technology are discussed.
</summary>
    <author>
      <name>Kaibin Huang</name>
    </author>
    <author>
      <name>Hai Wu</name>
    </author>
    <author>
      <name>Zhiyan Liu</name>
    </author>
    <author>
      <name>Xiaojuan Qi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper has been submitted to IEEE for possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.03555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.03555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.04456v1</id>
    <updated>2022-10-10T06:53:13Z</updated>
    <published>2022-10-10T06:53:13Z</published>
    <title>The Guilty (Silicon) Mind: Blameworthiness and Liability in
  Human-Machine Teaming</title>
    <summary>  As human science pushes the boundaries towards the development of artificial
intelligence (AI), the sweep of progress has caused scholars and policymakers
alike to question the legality of applying or utilising AI in various human
endeavours. For example, debate has raged in international scholarship about
the legitimacy of applying AI to weapon systems to form lethal autonomous
weapon systems (LAWS). Yet the argument holds true even when AI is applied to a
military autonomous system that is not weaponised: how does one hold a machine
accountable for a crime? What about a tort? Can an artificial agent understand
the moral and ethical content of its instructions? These are thorny questions,
and in many cases these questions have been answered in the negative, as
artificial entities lack any contingent moral agency. So what if the AI is not
alone, but linked with or overseen by a human being, with their own moral and
ethical understandings and obligations? Who is responsible for any malfeasance
that may be committed? Does the human bear the legal risks of unethical or
immoral decisions by an AI? These are some of the questions this manuscript
seeks to engage with.
</summary>
    <author>
      <name>Dr Brendan Walker-Munro</name>
    </author>
    <author>
      <name>Dr Zena Assaad</name>
    </author>
    <link href="http://arxiv.org/abs/2210.04456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.04456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.10659v1</id>
    <updated>2022-10-17T09:31:51Z</updated>
    <published>2022-10-17T09:31:51Z</published>
    <title>Review of the state of the art in autonomous artificial intelligence</title>
    <summary>  This article presents a new design for autonomous artificial intelligence
(AI), based on the state-of-the-art algorithms, and describes a new autonomous
AI system called AutoAI. The methodology is used to assemble the design founded
on self-improved algorithms that use new and emerging sources of data (NEFD).
The objective of the article is to conceptualise the design of a novel AutoAI
algorithm. The conceptual approach is used to advance into building new and
improved algorithms. The article integrates and consolidates the findings from
existing literature and advances the AutoAI design into (1) using new and
emerging sources of data for teaching and training AI algorithms and (2)
enabling AI algorithms to use automated tools for training new and improved
algorithms. This approach is going beyond the state-of-the-art in AI algorithms
and suggests a design that enables autonomous algorithms to self-optimise and
self-adapt, and on a higher level, be capable to self-procreate.
</summary>
    <author>
      <name>Petar Radanliev</name>
    </author>
    <author>
      <name>David De Roure</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s43681-022-00176-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s43681-022-00176-2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AI Ethics (2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.10659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.10659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.12324v1</id>
    <updated>2022-10-22T01:30:50Z</updated>
    <published>2022-10-22T01:30:50Z</published>
    <title>Trustworthy Human Computation: A Survey</title>
    <summary>  Human computation is an approach to solving problems that prove difficult
using AI only, and involves the cooperation of many humans. Because human
computation requires close engagement with both "human populations as users"
and "human populations as driving forces," establishing mutual trust between AI
and humans is an important issue to further the development of human
computation. This survey lays the groundwork for the realization of trustworthy
human computation. First, the trustworthiness of human computation as computing
systems, that is, trust offered by humans to AI, is examined using the RAS
(Reliability, Availability, and Serviceability) analogy, which define measures
of trustworthiness in conventional computer systems. Next, the social
trustworthiness provided by human computation systems to users or participants
is discussed from the perspective of AI ethics, including fairness, privacy,
and transparency. Then, we consider human--AI collaboration based on two-way
trust, in which humans and AI build mutual trust and accomplish difficult tasks
through reciprocal collaboration. Finally, future challenges and research
directions for realizing trustworthy human computation are discussed.
</summary>
    <author>
      <name>Hisashi Kashima</name>
    </author>
    <author>
      <name>Satoshi Oyama</name>
    </author>
    <author>
      <name>Hiromi Arai</name>
    </author>
    <author>
      <name>Junichiro Mori</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 2 figures, 9 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.12324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.12324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.14306v2</id>
    <updated>2022-11-27T03:55:02Z</updated>
    <published>2022-10-25T20:01:15Z</published>
    <title>Reading Between the Lines: Modeling User Behavior and Costs in
  AI-Assisted Programming</title>
    <summary>  AI code-recommendation systems (CodeRec), such as Copilot, can assist
programmers inside an IDE by suggesting and autocompleting arbitrary code;
potentially improving their productivity. To understand how these AI improve
programmers in a coding session, we need to understand how they affect
programmers' behavior. To make progress, we studied GitHub Copilot, and
developed CUPS -- a taxonomy of 12 programmer activities common to AI code
completion systems. We then conducted a study with 21 programmers who completed
coding tasks and used our labeling tool to retrospectively label their sessions
with CUPS. We analyze over 3000 label instances, and visualize the results with
timelines and state machines to profile programmer-CodeRec interaction. This
reveals novel insights into the distribution and patterns of programmer
behavior, as well as inefficiencies and time costs. Finally, we use these
insights to inform future interventions to improve AI-assisted programming and
human-AI interaction.
</summary>
    <author>
      <name>Hussein Mozannar</name>
    </author>
    <author>
      <name>Gagan Bansal</name>
    </author>
    <author>
      <name>Adam Fourney</name>
    </author>
    <author>
      <name>Eric Horvitz</name>
    </author>
    <link href="http://arxiv.org/abs/2210.14306v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.14306v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.16651v1</id>
    <updated>2022-10-29T16:52:53Z</updated>
    <published>2022-10-29T16:52:53Z</published>
    <title>Libraries, Integrations and Hubs for Decentralized AI using IPFS</title>
    <summary>  AI requires heavy amounts of storage and compute. As a result, AI developers
are regular users of centralised cloud services such as AWS, GCP and Azure,
compute environments such as Jupyter and Colab notebooks, and AI Hubs such as
HuggingFace and ActiveLoop. There services are associated with certain benefits
and limitations that stem from the underlying infrastructure and governance
systems with which they are built. These limitations include high costs, lack
of monetization and reward, lack of control and difficulty of reproducibility.
At the same time, there are few libraries that allow data scientists to
interact with decentralised storage in the language that data scientists are
used to, and few hubs where they can discover and interact with AI assets. In
this report, we explore the potential of decentralized technologies - such as
Web3 wallets, peer-to-peer marketplaces, decentralized storage (IPFS and
Filecoin) and compute, and DAOs - to address some of the above limitations. We
showcase some of the libraries and integrations that we have built to tackle
these issues, as well as a proof of concept of a decentralized AI Hub app, that
all use IPFS as a core infrastructural component.
</summary>
    <author>
      <name>Richard Blythman</name>
    </author>
    <author>
      <name>Mohamed Arshath</name>
    </author>
    <author>
      <name>Jakub Smékal</name>
    </author>
    <author>
      <name>Hithesh Shaji</name>
    </author>
    <author>
      <name>Salvatore Vivona</name>
    </author>
    <author>
      <name>Tyrone Dunmore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.16651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.16651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.03979v1</id>
    <updated>2022-11-08T03:16:46Z</updated>
    <published>2022-11-08T03:16:46Z</published>
    <title>AI Testing Framework for Next-G O-RAN Networks: Requirements, Design,
  and Research Opportunities</title>
    <summary>  Openness and intelligence are two enabling features to be introduced in next
generation wireless networks, e.g. Beyond 5G and 6G, to support service
heterogeneity, open hardware, optimal resource utilization, and on-demand
service deployment. The open radio access network (O-RAN) is a promising RAN
architecture to achieve both openness and intelligence through virtualized
network elements and well-defined interfaces. While deploying artificial
intelligence (AI) models is becoming easier in O-RAN, one significant challenge
that has been long neglected is the comprehensive testing of their performance
in realistic environments. This article presents a general automated,
distributed and AI-enabled testing framework to test AI models deployed in
O-RAN in terms of their decision-making performance, vulnerability and
security. This framework adopts a master-actor architecture to manage a number
of end devices for distributed testing. More importantly, it leverages AI to
automatically and intelligently explore the decision space of AI models in
O-RAN. Both software simulation testing and software-defined radio hardware
testing are supported, enabling rapid proof of concept research and
experimental research on wireless research platforms.
</summary>
    <author>
      <name>Bo Tang</name>
    </author>
    <author>
      <name>Vijay K. Shah</name>
    </author>
    <author>
      <name>Vuk Marojevic</name>
    </author>
    <author>
      <name>Jeffrey H. Reed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in IEEE Wireless Communications Magazine</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.03979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.03979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.09038v2</id>
    <updated>2022-11-18T17:33:57Z</updated>
    <published>2022-11-16T16:51:54Z</published>
    <title>Recent Advancements of Artificial Intelligence in Particle Therapy</title>
    <summary>  We are in a golden age of progress in artificial intelligence (AI).
Radiotherapy, due to its technology-intensive nature as well as direct
human-machine interactions, is perfectly suited for benefitting from AI to
enhance accuracy and efficiency. Over the past few years, a vast majority of AI
research have already been published in the field of photon therapy, while the
applications of AI specifically targeted for particle therapy remain scarcely
investigated. There are two distinct differences between photon therapy and
particle therapy: beam interaction physics (photons vs. charged particles) and
beam delivery mode (e.g. IMRT/VMAT vs. pencil beam scanning). As a result,
different strategies of AI deployment are required between these two
radiotherapy modalities. In this article, we aim to present a comprehensive
survey of recent literatures exclusively focusing on AI-powered particle
therapy. Six major aspects are included: treatment planning, dose calculation,
range and dose verification, image guidance, quality assurance and adaptive
replanning. A number of perspectives as well as potential challenges and common
pitfalls, are also discussed.
</summary>
    <author>
      <name>Hao Peng</name>
    </author>
    <author>
      <name>Chao Wu</name>
    </author>
    <author>
      <name>Dan Nguyen</name>
    </author>
    <author>
      <name>Jan Schuemann</name>
    </author>
    <author>
      <name>Andrea Mairani</name>
    </author>
    <author>
      <name>Yuehu Pu</name>
    </author>
    <author>
      <name>Steve Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/2211.09038v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.09038v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.10384v1</id>
    <updated>2022-11-18T17:32:19Z</updated>
    <published>2022-11-18T17:32:19Z</published>
    <title>Indexing AI Risks with Incidents, Issues, and Variants</title>
    <summary>  Two years after publicly launching the AI Incident Database (AIID) as a
collection of harms or near harms produced by AI in the world, a backlog of
"issues" that do not meet its incident ingestion criteria have accumulated in
its review queue. Despite not passing the database's current criteria for
incidents, these issues advance human understanding of where AI presents the
potential for harm. Similar to databases in aviation and computer security, the
AIID proposes to adopt a two-tiered system for indexing AI incidents (i.e., a
harm or near harm event) and issues (i.e., a risk of a harm event). Further, as
some machine learning-based systems will sometimes produce a large number of
incidents, the notion of an incident "variant" is introduced. These proposed
changes mark the transition of the AIID to a new version in response to lessons
learned from editing 2,000+ incident reports and additional reports that fall
under the new category of "issue."
</summary>
    <author>
      <name>Sean McGregor</name>
    </author>
    <author>
      <name>Kevin Paeth</name>
    </author>
    <author>
      <name>Khoa Lam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in Human-Centered AI Workshop at NeurIPS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.10384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.10384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.13069v1</id>
    <updated>2022-11-19T18:45:02Z</updated>
    <published>2022-11-19T18:45:02Z</published>
    <title>Cultural Incongruencies in Artificial Intelligence</title>
    <summary>  Artificial intelligence (AI) systems attempt to imitate human behavior. How
well they do this imitation is often used to assess their utility and to
attribute human-like (or artificial) intelligence to them. However, most work
on AI refers to and relies on human intelligence without accounting for the
fact that human behavior is inherently shaped by the cultural contexts they are
embedded in, the values and beliefs they hold, and the social practices they
follow. Additionally, since AI technologies are mostly conceived and developed
in just a handful of countries, they embed the cultural values and practices of
these countries. Similarly, the data that is used to train the models also
fails to equitably represent global cultural diversity. Problems therefore
arise when these technologies interact with globally diverse societies and
cultures, with different values and interpretive practices. In this position
paper, we describe a set of cultural dependencies and incongruencies in the
context of AI-based language and vision technologies, and reflect on the
possibilities of and potential strategies towards addressing these
incongruencies.
</summary>
    <author>
      <name>Vinodkumar Prabhakaran</name>
    </author>
    <author>
      <name>Rida Qadri</name>
    </author>
    <author>
      <name>Ben Hutchinson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 page position paper, presented at the NeurIPS 2022 Workshop on
  Cultures in AI/AI in Culture</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.13069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.13069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.01233v2</id>
    <updated>2022-12-06T18:24:56Z</updated>
    <published>2022-12-02T15:23:15Z</published>
    <title>Safe machine learning model release from Trusted Research Environments:
  The AI-SDC package</title>
    <summary>  We present AI-SDC, an integrated suite of open source Python tools to
facilitate Statistical Disclosure Control (SDC) of Machine Learning (ML) models
trained on confidential data prior to public release. AI-SDC combines (i) a
SafeModel package that extends commonly used ML models to provide ante-hoc SDC
by assessing the vulnerability of disclosure posed by the training regime; and
(ii) an Attacks package that provides post-hoc SDC by rigorously assessing the
empirical disclosure risk of a model through a variety of simulated attacks
after training. The AI-SDC code and documentation are available under an MIT
license at https://github.com/AI-SDC/AI-SDC.
</summary>
    <author>
      <name>Jim Smith</name>
    </author>
    <author>
      <name>Richard J. Preen</name>
    </author>
    <author>
      <name>Andrew McCarthy</name>
    </author>
    <author>
      <name>Alba Crespi-Boixader</name>
    </author>
    <author>
      <name>James Liley</name>
    </author>
    <author>
      <name>Simon Rogers</name>
    </author>
    <link href="http://arxiv.org/abs/2212.01233v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.01233v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.03695v1</id>
    <updated>2022-12-07T15:12:09Z</updated>
    <published>2022-12-07T15:12:09Z</published>
    <title>Dark and bright autoionizing states in resonant high harmonic
  generation: simulation via 1D helium model</title>
    <summary>  We study the role of dark and bright autoionizing states (AIS) in
photoionization and high harmonic generation (HHG) using a 1D helium model.
This model allows numerical integration of the time-dependent Schr\"odinger
equation beyond the singe-electron approximation completely taking into account
electronic correlation. We find the level structure of the system and the
spatial distribution of the electronic density for several states including
AIS. Studying the HHG efficiency as a function of the detuning from the
resonances with AIS we find the HHG enhancement lines. The shapes of these
lines are different from the corresponding Fano lines in the photoelectronic
spectra, in agreement with the experimental studies in helium. Moreover, we
simulate HHG under the conditions when the fundamental frequency is close to
the even-order multiphoton resonance with the dark AIS. We find the enhanced
generation of the neighbouring odd harmonics. The details of the enhancement
lines for these harmonics can be understood taking into account the temporal
delay between the emission of the non-resonant and resonant XUV; this delay is
defined by the AIS lifetime. Finally, our simulations show that the HHG
enhancement due to the dark and the bright AIS is comparable in the studied
system.
</summary>
    <author>
      <name>V. V. Strelkov</name>
    </author>
    <link href="http://arxiv.org/abs/2212.03695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.03695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.atom-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.atom-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.optics" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.08364v1</id>
    <updated>2022-12-16T09:33:00Z</updated>
    <published>2022-12-16T09:33:00Z</published>
    <title>Three lines of defense against risks from AI</title>
    <summary>  Organizations that develop and deploy artificial intelligence (AI) systems
need to manage the associated risks - for economic, legal, and ethical reasons.
However, it is not always clear who is responsible for AI risk management. The
Three Lines of Defense (3LoD) model, which is considered best practice in many
industries, might offer a solution. It is a risk management framework that
helps organizations to assign and coordinate risk management roles and
responsibilities. In this article, I suggest ways in which AI companies could
implement the model. I also discuss how the model could help reduce risks from
AI: it could identify and close gaps in risk coverage, increase the
effectiveness of risk management practices, and enable the board of directors
to oversee management more effectively. The article is intended to inform
decision-makers at leading AI companies, regulators, and standard-setting
bodies.
</summary>
    <author>
      <name>Jonas Schuett</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.08364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.08364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.12050v1</id>
    <updated>2022-12-22T22:00:58Z</updated>
    <published>2022-12-22T22:00:58Z</published>
    <title>A Semantic Framework for Neural-Symbolic Computing</title>
    <summary>  Two approaches to AI, neural networks and symbolic systems, have been proven
very successful for an array of AI problems. However, neither has been able to
achieve the general reasoning ability required for human-like intelligence. It
has been argued that this is due to inherent weaknesses in each approach.
Luckily, these weaknesses appear to be complementary, with symbolic systems
being adept at the kinds of things neural networks have trouble with and
vice-versa. The field of neural-symbolic AI attempts to exploit this asymmetry
by combining neural networks and symbolic AI into integrated systems. Often
this has been done by encoding symbolic knowledge into neural networks.
Unfortunately, although many different methods for this have been proposed,
there is no common definition of an encoding to compare them. We seek to
rectify this problem by introducing a semantic framework for neural-symbolic
AI, which is then shown to be general enough to account for a large family of
neural-symbolic systems. We provide a number of examples and proofs of the
application of the framework to the neural encoding of various forms of
knowledge representation and neural network. These, at first sight disparate
approaches, are all shown to fall within the framework's formal definition of
what we call semantic encoding for neural-symbolic AI.
</summary>
    <author>
      <name>Simon Odense</name>
    </author>
    <author>
      <name>Artur d'Avila Garcez</name>
    </author>
    <link href="http://arxiv.org/abs/2212.12050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.12050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.01954v1</id>
    <updated>2023-01-05T08:29:15Z</updated>
    <published>2023-01-05T08:29:15Z</published>
    <title>Corrupted by Algorithms? How AI-generated and Human-written Advice Shape
  (Dis)honesty</title>
    <summary>  Artificial Intelligence (AI) increasingly becomes an indispensable advisor.
New ethical concerns arise if AI persuades people to behave dishonestly. In an
experiment, we study how AI advice (generated by a Natural-Language-Processing
algorithm) affects (dis)honesty, compare it to equivalent human advice, and
test whether transparency about advice source matters. We find that
dishonesty-promoting advice increases dishonesty, whereas honesty-promoting
advice does not increase honesty. This is the case for both AI- and human
advice. Algorithmic transparency, a commonly proposed policy to mitigate AI
risks, does not affect behaviour. The findings mark the first steps towards
managing AI advice responsibly.
</summary>
    <author>
      <name>Margarita Leib</name>
    </author>
    <author>
      <name>Nils Köbis</name>
    </author>
    <author>
      <name>Rainer Michael Rilke</name>
    </author>
    <author>
      <name>Marloes Hagens</name>
    </author>
    <author>
      <name>Bernd Irlenbusch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">* shared first-authorship This is an updated version of the pre-print
  arXiv:2102.07536 with a new data set</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.01954v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.01954v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.2; J.4; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.03740v1</id>
    <updated>2023-01-10T01:09:07Z</updated>
    <published>2023-01-10T01:09:07Z</published>
    <title>A Multi-Level Framework for the AI Alignment Problem</title>
    <summary>  AI alignment considers how we can encode AI systems in a way that is
compatible with human values. The normative side of this problem asks what
moral values or principles, if any, we should encode in AI. To this end, we
present a framework to consider the question at four levels: Individual,
Organizational, National, and Global. We aim to illustrate how AI alignment is
made up of value alignment problems at each of these levels, where values at
each level affect the others and effects can flow in either direction. We
outline key questions and considerations of each level and demonstrate an
application of this framework to the topic of AI content moderation.
</summary>
    <author>
      <name>Betty Li Hou</name>
    </author>
    <author>
      <name>Brian Patrick Green</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ML Safety Workshop, 36th Conference on Neural Information Processing
  Systems (NeurIPS 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.03740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.03740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.05763v1</id>
    <updated>2023-01-13T21:24:23Z</updated>
    <published>2023-01-13T21:24:23Z</published>
    <title>A Rigorous Uncertainty-Aware Quantification Framework Is Essential for
  Reproducible and Replicable Machine Learning Workflows</title>
    <summary>  The ability to replicate predictions by machine learning (ML) or artificial
intelligence (AI) models and results in scientific workflows that incorporate
such ML/AI predictions is driven by numerous factors. An uncertainty-aware
metric that can quantitatively assess the reproducibility of quantities of
interest (QoI) would contribute to the trustworthiness of results obtained from
scientific workflows involving ML/AI models. In this article, we discuss how
uncertainty quantification (UQ) in a Bayesian paradigm can provide a general
and rigorous framework for quantifying reproducibility for complex scientific
workflows. Such as framework has the potential to fill a critical gap that
currently exists in ML/AI for scientific workflows, as it will enable
researchers to determine the impact of ML/AI model prediction variability on
the predictive outcomes of ML/AI-powered workflows. We expect that the
envisioned framework will contribute to the design of more reproducible and
trustworthy workflows for diverse scientific applications, and ultimately,
accelerate scientific discoveries.
</summary>
    <author>
      <name>Line Pouchard</name>
    </author>
    <author>
      <name>Kristofer G. Reyes</name>
    </author>
    <author>
      <name>Francis J. Alexander Byung-Jun Yoon</name>
    </author>
    <link href="http://arxiv.org/abs/2301.05763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.05763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.07520v1</id>
    <updated>2023-01-17T08:49:54Z</updated>
    <published>2023-01-17T08:49:54Z</published>
    <title>Adversarial AI in Insurance: Pervasiveness and Resilience</title>
    <summary>  The rapid and dynamic pace of Artificial Intelligence (AI) and Machine
Learning (ML) is revolutionizing the insurance sector. AI offers significant,
very much welcome advantages to insurance companies, and is fundamental to
their customer-centricity strategy. It also poses challenges, in the project
and implementation phase. Among those, we study Adversarial Attacks, which
consist of the creation of modified input data to deceive an AI system and
produce false outputs. We provide examples of attacks on insurance AI
applications, categorize them, and argue on defence methods and precautionary
systems, considering that they can involve few-shot and zero-shot
multilabelling. A related topic, with growing interest, is the validation and
verification of systems incorporating AI and ML components. These topics are
discussed in various sections of this paper.
</summary>
    <author>
      <name>Elisa Luciano</name>
    </author>
    <author>
      <name>Matteo Cattaneo</name>
    </author>
    <author>
      <name>Ron Kenett</name>
    </author>
    <link href="http://arxiv.org/abs/2301.07520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.07520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.09001v1</id>
    <updated>2023-01-21T20:04:07Z</updated>
    <published>2023-01-21T20:04:07Z</published>
    <title>The Pipeline for the Continuous Development of Artificial Intelligence
  Models -- Current State of Research and Practice</title>
    <summary>  Companies struggle to continuously develop and deploy AI models to complex
production systems due to AI characteristics while assuring quality. To ease
the development process, continuous pipelines for AI have become an active
research area where consolidated and in-depth analysis regarding the
terminology, triggers, tasks, and challenges is required. This paper includes a
Multivocal Literature Review where we consolidated 151 relevant formal and
informal sources. In addition, nine-semi structured interviews with
participants from academia and industry verified and extended the obtained
information. Based on these sources, this paper provides and compares
terminologies for DevOps and CI/CD for AI, MLOps, (end-to-end) lifecycle
management, and CD4ML. Furthermore, the paper provides an aggregated list of
potential triggers for reiterating the pipeline, such as alert systems or
schedules. In addition, this work uses a taxonomy creation strategy to present
a consolidated pipeline comprising tasks regarding the continuous development
of AI. This pipeline consists of four stages: Data Handling, Model Learning,
Software Development and System Operations. Moreover, we map challenges
regarding pipeline implementation, adaption, and usage for the continuous
development of AI to these four stages.
</summary>
    <author>
      <name>Monika Steidl</name>
    </author>
    <author>
      <name>Michael Felderer</name>
    </author>
    <author>
      <name>Rudolf Ramler</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jss.2023.111615</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jss.2023.111615" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted in the Journal Systems and Software</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.09001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.09001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.09545v1</id>
    <updated>2023-01-23T17:03:54Z</updated>
    <published>2023-01-23T17:03:54Z</published>
    <title>The Entoptic Field Camera as Metaphor-Driven Research-through-Design
  with AI Technologies</title>
    <summary>  Artificial intelligence (AI) technologies are widely deployed in smartphone
photography; and prompt-based image synthesis models have rapidly become
commonplace. In this paper, we describe a Research-through-Design (RtD) project
which explores this shift in the means and modes of image production via the
creation and use of the Entoptic Field Camera. Entoptic phenomena usually refer
to perceptions of floaters or bright blue dots stemming from the physiological
interplay of the eye and brain. We use the term entoptic as a metaphor to
investigate how the material interplay of data and models in AI technologies
shapes human experiences of reality. Through our case study using first-person
design and a field study, we offer implications for critical, reflective,
more-than-human and ludic design to engage AI technologies; the
conceptualisation of an RtD research space which contributes to AI literacy
discourses; and outline a research trajectory concerning materiality and design
affordances of AI technologies.
</summary>
    <author>
      <name>Jesse Josua Benjamin</name>
    </author>
    <author>
      <name>Heidi Biggs</name>
    </author>
    <author>
      <name>Arne Berger</name>
    </author>
    <author>
      <name>Julija Rukanskaitė</name>
    </author>
    <author>
      <name>Michael Heidt</name>
    </author>
    <author>
      <name>Nick Merrill</name>
    </author>
    <author>
      <name>James Pierce</name>
    </author>
    <author>
      <name>Joseph Lindley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3544548.3581175</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3544548.3581175" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in Proceedings of the 2023 CHI Conference on Human
  Factors in Computing Systems (CHI '23), April 23--28, 2023, Hamburg, Germany</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.09545v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.09545v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.10026v1</id>
    <updated>2023-01-11T07:00:55Z</updated>
    <published>2023-01-11T07:00:55Z</published>
    <title>From Robots to Books: An Introduction to Smart Applications of AI in
  Education (AIEd)</title>
    <summary>  The world around us has undergone a radical transformation due to rapid
technological advancement in recent decades. The industry of the future
generation is evolving, and artificial intelligence is the following change in
the making popularly known as Industry 4.0. Indeed, experts predict that
artificial intelligence(AI) will be the main force behind the following
significant virtual shift in the way we stay, converse, study, live,
communicate and conduct business. All facets of our social connection are being
transformed by this growing technology. One of the newest areas of educational
technology is Artificial Intelligence in the field of Education(AIEd).This
study emphasizes the different applications of artificial intelligence in
education from both an industrial and academic standpoint. It highlights the
most recent contextualized learning novel transformative evaluations and
advancements in sophisticated tutoring systems. It analyses the AIEd's ethical
component and the influence of the transition on people, particularly students
and instructors as well. Finally, this article touches on AIEd's potential
future research and practices. The goal of this study is to introduce the
present-day applications to its intended audience.
</summary>
    <author>
      <name>Shubham Ojha</name>
    </author>
    <author>
      <name>Aditya Narendra</name>
    </author>
    <author>
      <name>Siddharth Mohapatra</name>
    </author>
    <author>
      <name>Ipsit Misra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Preparation for Conference Submission, 9 Pages, 5 Tables, 1 Figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.10026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.10026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.10233v1</id>
    <updated>2022-12-21T15:16:22Z</updated>
    <published>2022-12-21T15:16:22Z</published>
    <title>Introducing Political Ecology of Creative-Ai</title>
    <summary>  This chapter introduces the perspective of political ecology to the
application of artificial intelligence to artistic processes (Creative-Ai).
Hence, the environmental and social impact of the development and employment of
Creative-Ai are the focus of this text, when we consider them as part of an
economic system that transforms artistic creation to a commodity. I first
analyse specific Creative-Ai cases, and then conduct a speculation that takes
Jacques Attali's writing on the role of music in society as a vantage point,
and investigates the environmental and social consequences of an automatic
composition network controlled by a large music streaming platform. Whereas the
possibilities that emerge from Creative-Ai may be promising from an artistic
perspective, its entanglement with corporate interest raises severe concerns.
These concerns can only be addressed by a wide cross-sectoral alliance between
research and arts that develops a critical perspective on the future directions
of Creative-Ai.
</summary>
    <author>
      <name>Andre Holzapfel</name>
    </author>
    <link href="http://arxiv.org/abs/2301.10233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.10233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.10404v1</id>
    <updated>2023-01-25T04:45:06Z</updated>
    <published>2023-01-25T04:45:06Z</published>
    <title>Requirements Practices and Gaps When Engineering Human-Centered
  Artificial Intelligence Systems</title>
    <summary>  [Context] Engineering Artificial Intelligence (AI) software is a relatively
new area with many challenges, unknowns, and limited proven best practices. Big
companies such as Google, Microsoft, and Apple have provided a suite of recent
guidelines to assist engineering teams in building human-centered AI systems.
[Objective] The practices currently adopted by practitioners for developing
such systems, especially during Requirements Engineering (RE), are little
studied and reported to date. [Method] This paper presents the results of a
survey conducted to understand current industry practices in RE for AI (RE4AI)
and to determine which key human-centered AI guidelines should be followed. Our
survey is based on mapping existing industrial guidelines, best practices, and
efforts in the literature. [Results] We surveyed 29 professionals and found
most participants agreed that all the human-centered aspects we mapped should
be addressed in RE. Further, we found that most participants were using UML or
Microsoft Office to present requirements. [Conclusion] We identify that most of
the tools currently used are not equipped to manage AI-based software, and the
use of UML and Office may pose issues to the quality of requirements captured
for AI. Also, all human-centered practices mapped from the guidelines should be
included in RE.
</summary>
    <author>
      <name>Khlood Ahmad</name>
    </author>
    <author>
      <name>Mohamed Abdelrazek</name>
    </author>
    <author>
      <name>Chetan Arora</name>
    </author>
    <author>
      <name>Muneera Bano</name>
    </author>
    <author>
      <name>John Grundy</name>
    </author>
    <link href="http://arxiv.org/abs/2301.10404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.10404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.00096v1</id>
    <updated>2023-01-31T20:54:02Z</updated>
    <published>2023-01-31T20:54:02Z</published>
    <title>Ignore, Trust, or Negotiate: Understanding Clinician Acceptance of
  AI-Based Treatment Recommendations in Health Care</title>
    <summary>  Artificial intelligence (AI) in healthcare has the potential to improve
patient outcomes, but clinician acceptance remains a critical barrier. We
developed a novel decision support interface that provides interpretable
treatment recommendations for sepsis, a life-threatening condition in which
decisional uncertainty is common, treatment practices vary widely, and poor
outcomes can occur even with optimal decisions. This system formed the basis of
a mixed-methods study in which 24 intensive care clinicians made AI-assisted
decisions on real patient cases. We found that explanations generally increased
confidence in the AI, but concordance with specific recommendations varied
beyond the binary acceptance or rejection described in prior work. Although
clinicians sometimes ignored or trusted the AI, they also often prioritized
aspects of the recommendations to follow, reject, or delay in a process we term
"negotiation." These results reveal novel barriers to adoption of
treatment-focused AI tools and suggest ways to better support differing
clinician perspectives.
</summary>
    <author>
      <name>Venkatesh Sivaraman</name>
    </author>
    <author>
      <name>Leigh A. Bukowski</name>
    </author>
    <author>
      <name>Joel Levin</name>
    </author>
    <author>
      <name>Jeremy M. Kahn</name>
    </author>
    <author>
      <name>Adam Perer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CHI 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.00096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.00096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.01854v1</id>
    <updated>2023-02-03T16:56:25Z</updated>
    <published>2023-02-03T16:56:25Z</published>
    <title>Comparing Psychometric and Behavioral Predictors of Compliance During
  Human-AI Interactions</title>
    <summary>  Optimization of human-AI teams hinges on the AI's ability to tailor its
interaction to individual human teammates. A common hypothesis in adaptive AI
research is that minor differences in people's predisposition to trust can
significantly impact their likelihood of complying with recommendations from
the AI. Predisposition to trust is often measured with self-report inventories
that are administered before interactions. We benchmark a popular measure of
this kind against behavioral predictors of compliance. We find that the
inventory is a less effective predictor of compliance than the behavioral
measures in datasets taken from three previous research projects. This suggests
a general property that individual differences in initial behavior are more
predictive than differences in self-reported trust attitudes. This result also
shows a potential for easily accessible behavioral measures to provide an AI
with more accurate models without the use of (often costly) survey instruments.
</summary>
    <author>
      <name>Nikolos Gurney</name>
    </author>
    <author>
      <name>David V. Pynadath</name>
    </author>
    <author>
      <name>Ning Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Persuasive Technologies 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.01854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.01854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.07926v1</id>
    <updated>2023-02-15T19:55:57Z</updated>
    <published>2023-02-15T19:55:57Z</published>
    <title>Commonsense Reasoning for Conversational AI: A Survey of the State of
  the Art</title>
    <summary>  Large, transformer-based pretrained language models like BERT, GPT, and T5
have demonstrated a deep understanding of contextual semantics and language
syntax. Their success has enabled significant advances in conversational AI,
including the development of open-dialogue systems capable of coherent, salient
conversations which can answer questions, chat casually, and complete tasks.
However, state-of-the-art models still struggle with tasks that involve higher
levels of reasoning - including commonsense reasoning that humans find trivial.
This paper presents a survey of recent conversational AI research focused on
commonsense reasoning. The paper lists relevant training datasets and describes
the primary approaches to include commonsense in conversational AI. The paper
also discusses benchmarks used for evaluating commonsense in conversational AI
problems. Finally, the paper presents preliminary observations of the limited
commonsense capabilities of two state-of-the-art open dialogue models,
BlenderBot3 and LaMDA, and its negative effect on natural interactions. These
observations further motivate research on commonsense reasoning in
conversational AI.
</summary>
    <author>
      <name>Christopher Richardson</name>
    </author>
    <author>
      <name>Larry Heck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Workshop on Knowledge Augmented Methods for Natural
  Language Processing, in conjunction with AAAI 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.07926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.07926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.08067v1</id>
    <updated>2023-02-16T04:06:16Z</updated>
    <published>2023-02-16T04:06:16Z</published>
    <title>Modeling Reliance on XAI Indicating Its Purpose and Attention</title>
    <summary>  This study used XAI, which shows its purposes and attention as explanations
of its process, and investigated how these explanations affect human trust in
and use of AI. In this study, we generated heat maps indicating AI attention,
conducted Experiment 1 to confirm the validity of the interpretability of the
heat maps, and conducted Experiment 2 to investigate the effects of the purpose
and heat maps in terms of reliance (depending on AI) and compliance (accepting
answers of AI). The results of structural equation modeling (SEM) analyses
showed that (1) displaying the purpose of AI positively and negatively
influenced trust depending on the types of AI usage, reliance or compliance,
and task difficulty, (2) just displaying the heat maps negatively influenced
trust in a more difficult task, and (3) the heat maps positively influenced
trust according to their interpretability in a more difficult task.
</summary>
    <author>
      <name>Akihiro Maehigashi</name>
    </author>
    <author>
      <name>Yosuke Fukuchi</name>
    </author>
    <author>
      <name>Seiji Yamada</name>
    </author>
    <link href="http://arxiv.org/abs/2302.08067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.08067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.0654v4</id>
    <updated>2009-05-10T13:34:01Z</updated>
    <published>2008-09-03T15:55:25Z</published>
    <title>Self-sustained asynchronous irregular states and Up/Down states in
  thalamic, cortical and thalamocortical networks of nonlinear
  integrate-and-fire neurons</title>
    <summary>  Randomly-connected networks of integrate-and-fire (IF) neurons are known to
display asynchronous irregular (AI) activity states, which resemble the
discharge activity recorded in the cerebral cortex of awake animals. However,
it is not clear whether such activity states are specific to simple IF models,
or if they also exist in networks where neurons are endowed with complex
intrinsic properties similar to electrophysiological measurements. Here, we
investigate the occurrence of AI states in networks of nonlinear IF neurons,
such as the adaptive exponential IF (Brette-Gerstner-Izhikevich) model. This
model can display intrinsic properties such as low-threshold spike (LTS),
regular spiking (RS) or fast-spiking (FS). We successively investigate the
oscillatory and AI dynamics of thalamic, cortical and thalamocortical networks
using such models. AI states can be found in each case, sometimes with
surprisingly small network size of the order of a few tens of neurons. We show
that the presence of LTS neurons in cortex or in thalamus, explains the robust
emergence of AI states for relatively small network sizes. Finally, we
investigate the role of spike-frequency adaptation (SFA). In cortical networks
with strong SFA in RS cells, the AI state is transient, but when SFA is
reduced, AI states can be self-sustained for long times. In thalamocortical
networks, AI states are found when the cortex is itself in an AI state, but
with strong SFA, the thalamocortical network displays Up and Down state
transitions, similar to intracellular recordings during slow-wave sleep or
anesthesia. Self-sustained Up and Down states could also be generated by
two-layer cortical networks with LTS cells. These models suggest that intrinsic
properties such as LTS are crucial for AI states in thalamocortical networks.
</summary>
    <author>
      <name>Alain Destexhe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10827-009-0164-4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10827-009-0164-4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 figures, Journal of Computational Neuroscience (in press, 2009)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computational Neuroscience 27: 493-506, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.0654v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.0654v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.09301v2</id>
    <updated>2020-10-14T10:14:41Z</updated>
    <published>2020-03-18T08:45:10Z</published>
    <title>Distributed and Democratized Learning: Philosophy and Research
  Challenges</title>
    <summary>  Due to the availability of huge amounts of data and processing abilities,
current artificial intelligence (AI) systems are effective in solving complex
tasks. However, despite the success of AI in different areas, the problem of
designing AI systems that can truly mimic human cognitive capabilities such as
artificial general intelligence, remains largely open. Consequently, many
emerging cross-device AI applications will require a transition from
traditional centralized learning systems towards large-scale distributed AI
systems that can collaboratively perform multiple complex learning tasks. In
this paper, we propose a novel design philosophy called democratized learning
(Dem-AI) whose goal is to build large-scale distributed learning systems that
rely on the self-organization of distributed learning agents that are
well-connected, but limited in learning capabilities. Correspondingly, inspired
by the societal groups of humans, the specialized groups of learning agents in
the proposed Dem-AI system are self-organized in a hierarchical structure to
collectively perform learning tasks more efficiently. As such, the Dem-AI
learning system can evolve and regulate itself based on the underlying duality
of two processes which we call specialized and generalized processes. In this
regard, we present a reference design as a guideline to realize future Dem-AI
systems, inspired by various interdisciplinary fields. Accordingly, we
introduce four underlying mechanisms in the design such as plasticity-stability
transition mechanism, self-organizing hierarchical structuring, specialized
learning, and generalization. Finally, we establish possible extensions and new
challenges for the existing learning approaches to provide better scalable,
flexible, and more powerful learning systems with the new setting of Dem-AI.
</summary>
    <author>
      <name>Minh N. H. Nguyen</name>
    </author>
    <author>
      <name>Shashi Raj Pandey</name>
    </author>
    <author>
      <name>Kyi Thar</name>
    </author>
    <author>
      <name>Nguyen H. Tran</name>
    </author>
    <author>
      <name>Mingzhe Chen</name>
    </author>
    <author>
      <name>Walid Saad</name>
    </author>
    <author>
      <name>Choong Seon Hong</name>
    </author>
    <link href="http://arxiv.org/abs/2003.09301v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.09301v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.07193v3</id>
    <updated>2018-12-27T09:29:31Z</updated>
    <published>2018-09-19T13:45:47Z</published>
    <title>TStarBots: Defeating the Cheating Level Builtin AI in StarCraft II in
  the Full Game</title>
    <summary>  Starcraft II (SC2) is widely considered as the most challenging Real Time
Strategy (RTS) game. The underlying challenges include a large observation
space, a huge (continuous and infinite) action space, partial observations,
simultaneous move for all players, and long horizon delayed rewards for local
decisions. To push the frontier of AI research, Deepmind and Blizzard jointly
developed the StarCraft II Learning Environment (SC2LE) as a testbench of
complex decision making systems. SC2LE provides a few mini games such as
MoveToBeacon, CollectMineralShards, and DefeatRoaches, where some AI agents
have achieved the performance level of human professional players. However, for
full games, the current AI agents are still far from achieving human
professional level performance. To bridge this gap, we present two full game AI
agents in this paper - the AI agent TStarBot1 is based on deep reinforcement
learning over a flat action structure, and the AI agent TStarBot2 is based on
hard-coded rules over a hierarchical action structure. Both TStarBot1 and
TStarBot2 are able to defeat the built-in AI agents from level 1 to level 10 in
a full game (1v1 Zerg-vs-Zerg game on the AbyssalReef map), noting that level
8, level 9, and level 10 are cheating agents with unfair advantages such as
full vision on the whole map and resource harvest boosting. To the best of our
knowledge, this is the first public work to investigate AI agents that can
defeat the built-in AI in the StarCraft II full game.
</summary>
    <author>
      <name>Peng Sun</name>
    </author>
    <author>
      <name>Xinghai Sun</name>
    </author>
    <author>
      <name>Lei Han</name>
    </author>
    <author>
      <name>Jiechao Xiong</name>
    </author>
    <author>
      <name>Qing Wang</name>
    </author>
    <author>
      <name>Bo Li</name>
    </author>
    <author>
      <name>Yang Zheng</name>
    </author>
    <author>
      <name>Ji Liu</name>
    </author>
    <author>
      <name>Yongsheng Liu</name>
    </author>
    <author>
      <name>Han Liu</name>
    </author>
    <author>
      <name>Tong Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">add link for source code</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.07193v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.07193v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.07343v4</id>
    <updated>2022-03-17T03:53:20Z</updated>
    <published>2020-07-30T11:11:55Z</published>
    <title>Artificial Intelligence in the Battle against Coronavirus (COVID-19): A
  Survey and Future Research Directions</title>
    <summary>  Artificial intelligence (AI) has been applied widely in our daily lives in a
variety of ways with numerous success stories. AI has also contributed to
dealing with the coronavirus disease (COVID-19) pandemic, which has been
happening around the globe. This paper presents a survey of AI methods being
used in various applications in the fight against the COVID-19 outbreak and
outlines the crucial role of AI research in this unprecedented battle. We touch
on areas where AI plays as an essential component, from medical image
processing, data analytics, text mining and natural language processing, the
Internet of Things, to computational biology and medicine. A summary of
COVID-19 related data sources that are available for research purposes is also
presented. Research directions on exploring the potential of AI and enhancing
its capability and power in the pandemic battle are thoroughly discussed. We
identify 13 groups of problems related to the COVID-19 pandemic and highlight
promising AI methods and tools that can be used to address these problems. It
is envisaged that this study will provide AI researchers and the wider
community with an overview of the current status of AI applications, and
motivate researchers to harness AI's potential in the fight against COVID-19.
</summary>
    <author>
      <name>Thanh Thi Nguyen</name>
    </author>
    <author>
      <name>Quoc Viet Hung Nguyen</name>
    </author>
    <author>
      <name>Dung Tien Nguyen</name>
    </author>
    <author>
      <name>Samuel Yang</name>
    </author>
    <author>
      <name>Peter W. Eklund</name>
    </author>
    <author>
      <name>Thien Huynh-The</name>
    </author>
    <author>
      <name>Thanh Tam Nguyen</name>
    </author>
    <author>
      <name>Quoc-Viet Pham</name>
    </author>
    <author>
      <name>Imran Razzak</name>
    </author>
    <author>
      <name>Edbert B. Hsu</name>
    </author>
    <link href="http://arxiv.org/abs/2008.07343v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.07343v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.03906v1</id>
    <updated>2021-03-20T17:03:50Z</updated>
    <published>2021-03-20T17:03:50Z</published>
    <title>Explaining decisions made with AI: A workbook (Use case 1: AI-assisted
  recruitment tool)</title>
    <summary>  Over the last two years, The Alan Turing Institute and the Information
Commissioner's Office (ICO) have been working together to discover ways to
tackle the difficult issues surrounding explainable AI. The ultimate product of
this joint endeavour, Explaining decisions made with AI, published in May 2020,
is the most comprehensive practical guidance on AI explanation produced
anywhere to date. We have put together this workbook to help support the uptake
of that guidance. The goal of the workbook is to summarise some of main themes
from Explaining decisions made with AI and then to provide the materials for a
workshop exercise that has been built around a use case created to help you
gain a flavour of how to put the guidance into practice. In the first three
sections, we run through the basics of Explaining decisions made with AI. We
provide a precis of the four principles of AI explainability, the typology of
AI explanations, and the tasks involved in the explanation-aware design,
development, and use of AI/ML systems. We then provide some reflection
questions, which are intended to be a launching pad for group discussion, and a
starting point for the case-study-based exercise that we have included as
Appendix B. In Appendix A, we go into more detailed suggestions about how to
organise the workshop. These recommendations are based on two workshops we had
the privilege of co-hosting with our colleagues from the ICO and Manchester
Metropolitan University in January 2021. The participants of these workshops
came from both the private and public sectors, and we are extremely grateful to
them for their energy, enthusiasm, and tremendous insight. This workbook would
simply not exist without the commitment and keenness of all our collaborators
and workshop participants.
</summary>
    <author>
      <name>David Leslie</name>
    </author>
    <author>
      <name>Morgan Briggs</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.4624711</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.4624711" rel="related"/>
    <link href="http://arxiv.org/abs/2104.03906v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.03906v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.04022v2</id>
    <updated>2022-05-09T11:21:37Z</updated>
    <published>2021-06-17T18:44:55Z</published>
    <title>Immune Moral Models? Pro-Social Rule Breaking as a Moral Enhancement
  Approach for Ethical AI</title>
    <summary>  We are moving towards a future where Artificial Intelligence (AI) based
agents make many decisions on behalf of humans. From healthcare decision making
to social media censoring, these agents face problems, and make decisions with
ethical and societal implications. Ethical behaviour is a critical
characteristic that we would like in a human-centric AI. A common observation
in human-centric industries, like the service industry and healthcare, is that
their professionals tend to break rules, if necessary, for pro-social reasons.
This behaviour among humans is defined as pro-social rule breaking. To make AI
agents more human centric, we argue that there is a need for a mechanism that
helps AI agents identify when to break rules set by their designers. To
understand when AI agents need to break rules, we examine the conditions under
which humans break rules for pro-social reasons. In this paper, we present a
study that introduces a 'vaccination strategy dilemma' to human participants
and analyses their responses. In this dilemma, one needs to decide whether they
would distribute Covid-19 vaccines only to members of a high-risk group (follow
the enforced rule) or, in selected cases, administer the vaccine to a few
social influencers (break the rule), which might yield an overall greater
benefit to society. The results of the empirical study suggest a relationship
between stakeholder utilities and pro-social rule breaking (PSRB), which
neither deontological nor utilitarian ethics completely explain. Finally, the
paper discusses the design characteristics of an ethical agent capable of PSRB
and the future research directions on PSRB in the AI realm. We hope that this
will inform the design of future AI agents, and their decision-making
behaviour.
</summary>
    <author>
      <name>Rajitha Ramanayake</name>
    </author>
    <author>
      <name>Philipp Wicke</name>
    </author>
    <author>
      <name>Vivek Nallur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures, Accepted version for AI &amp; SOCIETY - Special
  Issue on AI for People</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.04022v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.04022v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.06266v4</id>
    <updated>2022-09-20T04:25:24Z</updated>
    <published>2021-11-11T15:15:52Z</published>
    <title>AlphaDDA: Strategies for Adjusting the Playing Strength of a Fully
  Trained AlphaZero System to a Suitable Human Training Partner</title>
    <summary>  Artificial intelligence (AI) has achieved superhuman performance in board
games such as Go, chess, and Othello (Reversi). In other words, the AI system
surpasses the level of a strong human expert player in such games. In this
context, it is difficult for a human player to enjoy playing the games with the
AI. To keep human players entertained and immersed in a game, the AI is
required to dynamically balance its skill with that of the human player. To
address this issue, we propose AlphaDDA, an AlphaZero-based AI with dynamic
difficulty adjustment (DDA). AlphaDDA consists of a deep neural network (DNN)
and a Monte Carlo tree search, as in AlphaZero. AlphaDDA learns and plays a
game the same way as AlphaZero, but can change its skills. AlphaDDA estimates
the value of the game state from only the board state using the DNN. AlphaDDA
changes a parameter dominantly controlling its skills according to the
estimated value. Consequently, AlphaDDA adjusts its skills according to a game
state. AlphaDDA can adjust its skill using only the state of a game without any
prior knowledge regarding an opponent. In this study, AlphaDDA plays Connect4,
Othello, and 6x6 Othello with other AI agents. Other AI agents are AlphaZero,
Monte Carlo tree search, the minimax algorithm, and a random player. This study
shows that AlphaDDA can balance its skill with that of the other AI agents,
except for a random player. The DDA ability of AlphaDDA is based on an accurate
estimation of the value from the state of a game. We believe that the AlphaDDA
approach for DDA can be used for any game AI system if the DNN can accurately
estimate the value of the game state and we know a parameter controlling the
skills of the AI system.
</summary>
    <author>
      <name>Kazuhisa Fujita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.06266v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.06266v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.06751v2</id>
    <updated>2022-05-16T21:24:42Z</updated>
    <published>2021-12-13T16:03:13Z</published>
    <title>Role of Human-AI Interaction in Selective Prediction</title>
    <summary>  Recent work has shown the potential benefit of selective prediction systems
that can learn to defer to a human when the predictions of the AI are
unreliable, particularly to improve the reliability of AI systems in
high-stakes applications like healthcare or conservation. However, most prior
work assumes that human behavior remains unchanged when they solve a prediction
task as part of a human-AI team as opposed to by themselves. We show that this
is not the case by performing experiments to quantify human-AI interaction in
the context of selective prediction. In particular, we study the impact of
communicating different types of information to humans about the AI system's
decision to defer. Using real-world conservation data and a selective
prediction system that improves expected accuracy over that of the human or AI
system working individually, we show that this messaging has a significant
impact on the accuracy of human judgements. Our results study two components of
the messaging strategy: 1) Whether humans are informed about the prediction of
the AI system and 2) Whether they are informed about the decision of the
selective prediction system to defer. By manipulating these messaging
components, we show that it is possible to significantly boost human
performance by informing the human of the decision to defer, but not revealing
the prediction of the AI. We therefore show that it is vital to consider how
the decision to defer is communicated to a human when designing selective
prediction systems, and that the composite accuracy of a human-AI team must be
carefully evaluated using a human-in-the-loop framework.
</summary>
    <author>
      <name>Elizabeth Bondi</name>
    </author>
    <author>
      <name>Raphael Koster</name>
    </author>
    <author>
      <name>Hannah Sheahan</name>
    </author>
    <author>
      <name>Martin Chadwick</name>
    </author>
    <author>
      <name>Yoram Bachrach</name>
    </author>
    <author>
      <name>Taylan Cemgil</name>
    </author>
    <author>
      <name>Ulrich Paquet</name>
    </author>
    <author>
      <name>Krishnamurthy Dvijotham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in AAAI 2022; added link to data, small formatting
  corrections for camera-ready, including small changes to Fig 6-7 that do not
  change conclusions</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.06751v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.06751v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.00966v1</id>
    <updated>2022-01-04T04:04:50Z</updated>
    <published>2022-01-04T04:04:50Z</published>
    <title>AI visualization in Nanoscale Microscopy</title>
    <summary>  Artificial Intelligence &amp; Nanotechnology are promising areas for the future
of humanity. While Deep Learning based Computer Vision has found applications
in many fields from medicine to automotive, its application in nanotechnology
can open doors for new scientific discoveries. Can we apply AI to explore
objects that our eyes can't see such as nano scale sized objects? An AI
platform to visualize nanoscale patterns learnt by a Deep Learning neural
network can open new frontiers for nanotechnology. The objective of this paper
is to develop a Deep Learning based visualization system on images of
nanomaterials obtained by scanning electron microscope. This paper contributes
an AI platform to enable any nanoscience researcher to use AI in visual
exploration of nanoscale morphologies of nanomaterials. This AI is developed by
a technique of visualizing intermediate activations of a Convolutional
AutoEncoder. In this method, a nano scale specimen image is transformed into
its feature representations by a Convolution Neural Network. The Convolutional
AutoEncoder is trained on 100% SEM dataset, and then CNN visualization is
applied. This AI generates various conceptual feature representations of the
nanomaterial.
  While Deep Learning based image classification of SEM images are widely
published in literature, there are not much publications that have visualized
Deep neural networks of nanomaterials. There is a significant opportunity to
gain insights from the learnings extracted by machine learning. This paper
unlocks the potential of applying Deep Learning based Visualization on electron
microscopy to offer AI extracted features and architectural patterns of various
nanomaterials. This is a contribution in Explainable AI in nano scale objects.
This paper contributes an open source AI with reproducible results at URL
(https://sites.google.com/view/aifornanotechnology)
</summary>
    <author>
      <name>Rajagopal A</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Indian Institute of Technology Madras</arxiv:affiliation>
    </author>
    <author>
      <name>Nirmala V</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Queen Marys College</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew J</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Karunya Institute of Technology and Sciences. India</arxiv:affiliation>
    </author>
    <author>
      <name>Arun Muthuraj Vedamanickam.</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Best paper award at International Conference On Big Data, Machine
  Learning and Applications 2021. http://bigdml.nits.ac.in/ In Springer
  Proceedings 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.00966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.00966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.11133v2</id>
    <updated>2022-02-17T21:44:53Z</updated>
    <published>2022-01-26T19:00:01Z</published>
    <title>Inference-optimized AI and high performance computing for gravitational
  wave detection at scale</title>
    <summary>  We introduce an ensemble of artificial intelligence models for gravitational
wave detection that we trained in the Summit supercomputer using 32 nodes,
equivalent to 192 NVIDIA V100 GPUs, within 2 hours. Once fully trained, we
optimized these models for accelerated inference using NVIDIA TensorRT. We
deployed our inference-optimized AI ensemble in the ThetaGPU supercomputer at
Argonne Leadership Computer Facility to conduct distributed inference. Using
the entire ThetaGPU supercomputer, consisting of 20 nodes each of which has 8
NVIDIA A100 Tensor Core GPUs and 2 AMD Rome CPUs, our NVIDIA TensorRT-optimized
AI ensemble processed an entire month of advanced LIGO data (including Hanford
and Livingston data streams) within 50 seconds. Our inference-optimized AI
ensemble retains the same sensitivity of traditional AI models, namely, it
identifies all known binary black hole mergers previously identified in this
advanced LIGO dataset and reports no misclassifications, while also providing a
3X inference speedup compared to traditional artificial intelligence models. We
used time slides to quantify the performance of our AI ensemble to process up
to 5 years worth of advanced LIGO data. In this synthetically enhanced dataset,
our AI ensemble reports an average of one misclassification for every month of
searched advanced LIGO data. We also present the receiver operating
characteristic curve of our AI ensemble using this 5 year long advanced LIGO
dataset. This approach provides the required tools to conduct accelerated,
AI-driven gravitational wave detection at scale.
</summary>
    <author>
      <name>Pranshu Chaturvedi</name>
    </author>
    <author>
      <name>Asad Khan</name>
    </author>
    <author>
      <name>Minyang Tian</name>
    </author>
    <author>
      <name>E. A. Huerta</name>
    </author>
    <author>
      <name>Huihuo Zheng</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/frai.2022.828672</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/frai.2022.828672" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 8 figures; v2. Accepted to Frontiers in Artificial
  Intelligence, Special Issue: Efficient AI in Particle Physics and
  Astrophysics</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Front. Artif. Intell. 5:828672 (2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2201.11133v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.11133v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10, 85-08, 83C35, 83C57" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.03188v1</id>
    <updated>2022-02-04T11:51:44Z</updated>
    <published>2022-02-04T11:51:44Z</published>
    <title>Knowledge-Integrated Informed AI for National Security</title>
    <summary>  The state of artificial intelligence technology has a rich history that dates
back decades and includes two fall-outs before the explosive resurgence of
today, which is credited largely to data-driven techniques. While AI technology
has and continues to become increasingly mainstream with impact across domains
and industries, it's not without several drawbacks, weaknesses, and potential
to cause undesired effects. AI techniques are numerous with many approaches and
variants, but they can be classified simply based on the degree of knowledge
they capture and how much data they require; two broad categories emerge as
prominent across AI to date: (1) techniques that are primarily, and often
solely, data-driven while leveraging little to no knowledge and (2) techniques
that primarily leverage knowledge and depend less on data. Now, a third
category is starting to emerge that leverages both data and knowledge, that
some refer to as "informed AI." This third category can be a game changer
within the national security domain where there is ample scientific and
domain-specific knowledge that stands ready to be leveraged, and where purely
data-driven AI can lead to serious unwanted consequences.
  This report shares findings from a thorough exploration of AI approaches that
exploit data as well as principled and/or practical knowledge, which we refer
to as "knowledge-integrated informed AI." Specifically, we review illuminating
examples of knowledge integrated in deep learning and reinforcement learning
pipelines, taking note of the performance gains they provide. We also discuss
an apparent trade space across variants of knowledge-integrated informed AI,
along with observed and prominent issues that suggest worthwhile future
research directions. Most importantly, this report suggests how the advantages
of knowledge-integrated informed AI stand to benefit the national security
domain.
</summary>
    <author>
      <name>Anu K. Myne</name>
    </author>
    <author>
      <name>Kevin J. Leahy</name>
    </author>
    <author>
      <name>Ryan J. Soklaski</name>
    </author>
    <link href="http://arxiv.org/abs/2202.03188v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.03188v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.12423v1</id>
    <updated>2022-09-26T05:12:58Z</updated>
    <published>2022-09-26T05:12:58Z</published>
    <title>The Interplay of AI and Digital Twin: Bridging the Gap between
  Data-Driven and Model-Driven Approaches</title>
    <summary>  The advancements of mixed reality services, with the evolution of network
virtualization and native artificial intelligence (AI) paradigms, have
conceptualized the vision of future wireless networks as a comprehensive entity
operating in whole over a digital platform, with smart interaction with the
physical domain, paving the way for the blooming of the Digital Twin (DT)
concept. The recent interest in the DT networks is fueled by the emergence of
novel wireless technologies and use-cases, that exacerbate the level of
complexity to orchestrate the network and to manage its resources. Driven by
the internet-of-sensing and AI, the key principle of the DT is to create a
virtual twin for the physical entities and network dynamics, where the virtual
twin will be leveraged to generate synthetic data, in addition to the received
sensed data from the physical twin in an on-demand manner. The available data
at the twin will be the foundation for AI models training and intelligent
inference process. Despite the common understanding that AI is the seed for DT,
we anticipate the DT and AI will be enablers for each other, in a way that
overcome their limitations and complement each other benefits. In this article,
we dig into the fundamentals of DT, where we reveal the role of DT in unifying
model-driven and data-driven approaches, and explore the opportunities offered
by DT in order to achieve the optimistic vision of 6G networks. We further
unfold the essential role of the theoretical underpinnings in unlocking further
opportunities by AI, and hence, we unveil their pivotal impact on the
realization of reliable, efficient, and low-latency DT. Finally, we identify
the limitations of AI-DT and overview potential future research directions, to
open the floor for further exploration in AI for DT and DT for AI.
</summary>
    <author>
      <name>Lina Bariah</name>
    </author>
    <author>
      <name>Merouane Debbah</name>
    </author>
    <link href="http://arxiv.org/abs/2209.12423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.12423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.05081v2</id>
    <updated>2022-12-21T17:46:25Z</updated>
    <published>2022-12-09T19:00:18Z</published>
    <title>FAIR AI Models in High Energy Physics</title>
    <summary>  The findable, accessible, interoperable, and reusable (FAIR) data principles
have provided a framework for examining, evaluating, and improving how we share
data with the aim of facilitating scientific discovery. Efforts have been made
to generalize these principles to research software and other digital products.
Artificial intelligence (AI) models -- algorithms that have been trained on
data rather than explicitly programmed -- are an important target for this
because of the ever-increasing pace with which AI is transforming scientific
and engineering domains. In this paper, we propose a practical definition of
FAIR principles for AI models and create a FAIR AI project template that
promotes adherence to these principles. We demonstrate how to implement these
principles using a concrete example from experimental high energy physics: a
graph neural network for identifying Higgs bosons decaying to bottom quarks. We
study the robustness of these FAIR AI models and their portability across
hardware architectures and software frameworks, and report new insights on the
interpretability of AI predictions by studying the interplay between FAIR
datasets and AI models. Enabled by publishing FAIR AI models, these studies
pave the way toward reliable and automated AI-driven scientific discovery.
</summary>
    <author>
      <name>Javier Duarte</name>
    </author>
    <author>
      <name>Haoyang Li</name>
    </author>
    <author>
      <name>Avik Roy</name>
    </author>
    <author>
      <name>Ruike Zhu</name>
    </author>
    <author>
      <name>E. A. Huerta</name>
    </author>
    <author>
      <name>Daniel Diaz</name>
    </author>
    <author>
      <name>Philip Harris</name>
    </author>
    <author>
      <name>Raghav Kansal</name>
    </author>
    <author>
      <name>Daniel S. Katz</name>
    </author>
    <author>
      <name>Ishaan H. Kavoori</name>
    </author>
    <author>
      <name>Volodymyr V. Kindratenko</name>
    </author>
    <author>
      <name>Farouk Mokhtar</name>
    </author>
    <author>
      <name>Mark S. Neubauer</name>
    </author>
    <author>
      <name>Sang Eon Park</name>
    </author>
    <author>
      <name>Melissa Quinnan</name>
    </author>
    <author>
      <name>Roger Rusack</name>
    </author>
    <author>
      <name>Zhizhen Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 8 figures, 9 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.05081v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.05081v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.13089v1</id>
    <updated>2023-01-30T17:28:33Z</updated>
    <published>2023-01-30T17:28:33Z</published>
    <title>Can an AI Win Ghana's National Science and Maths Quiz? An AI Grand
  Challenge for Education</title>
    <summary>  There is a lack of enough qualified teachers across Africa which hampers
efforts to provide adequate learning support such as educational question
answering (EQA) to students. An AI system that can enable students to ask
questions via text or voice and get instant answers will make high-quality
education accessible. Despite advances in the field of AI, there exists no
robust benchmark or challenge to enable building such an (EQA) AI within the
African context. Ghana's National Science and Maths Quiz competition (NSMQ) is
the perfect competition to evaluate the potential of such an AI due to its wide
coverage of scientific fields, variety of question types, highly competitive
nature, and live, real-world format. The NSMQ is a Jeopardy-style annual live
quiz competition in which 3 teams of 2 students compete by answering questions
across biology, chemistry, physics, and math in 5 rounds over 5 progressive
stages until a winning team is crowned for that year. In this position paper,
we propose the NSMQ AI Grand Challenge, an AI Grand Challenge for Education
using Ghana's National Science and Maths Quiz competition (NSMQ) as a case
study. Our proposed grand challenge is to "Build an AI to compete live in
Ghana's National Science and Maths Quiz (NSMQ) competition and win - performing
better than the best contestants in all rounds and stages of the competition."
We describe the competition, and key technical challenges to address along with
ideas from recent advances in machine learning that could be leveraged to solve
this challenge. This position paper is a first step towards conquering such a
challenge and importantly, making advances in AI for education in the African
context towards democratizing high-quality education across Africa.
</summary>
    <author>
      <name>George Boateng</name>
    </author>
    <author>
      <name>Victor Kumbol</name>
    </author>
    <author>
      <name>Elsie Effah Kaufmann</name>
    </author>
    <link href="http://arxiv.org/abs/2301.13089v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.13089v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.0187v1</id>
    <updated>2010-10-31T18:14:44Z</updated>
    <published>2010-10-31T18:14:44Z</published>
    <title>A Distributed AI Aided 3D Domino Game</title>
    <summary>  In the article a turn-based game played on four computers connected via
network is investigated. There are three computers with natural intelligence
and one with artificial intelligence. Game table is seen by each player's own
view point in all players' monitors. Domino pieces are three dimensional. For
distributed systems TCP/IP protocol is used. In order to get 3D image,
Microsoft XNA technology is applied. Domino 101 game is nondeterministic game
that is result of the game depends on the initial random distribution of the
pieces. Number of the distributions is equal to the multiplication of following
combinations: . Moreover, in this game that is played by four people, players
are divided into 2 pairs. Accordingly, we cannot predict how the player uses
the dominoes that is according to the dominoes of his/her partner or according
to his/her own dominoes. The fact that the natural intelligence can be a player
in any level affects the outcome. These reasons make it difficult to develop an
AI. In the article four levels of AI are developed. The AI in the first level
is equivalent to the intelligence of a child who knows the rules of the game
and recognizes the numbers. The AI in this level plays if it has any domino,
suitable to play or says pass. In most of the games which can be played on the
internet, the AI does the same. But the AI in the last level is a master
player, and it can develop itself according to its competitors' levels.
</summary>
    <author>
      <name>Şahin Emrah Amrahov</name>
    </author>
    <author>
      <name>Orhan A. Nooraden</name>
    </author>
    <link href="http://arxiv.org/abs/1011.0187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.0187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.5072v1</id>
    <updated>2011-09-23T13:36:10Z</updated>
    <published>2011-09-23T13:36:10Z</published>
    <title>Analysis of first prototype universal intelligence tests: evaluating and
  comparing AI algorithms and humans</title>
    <summary>  Today, available methods that assess AI systems are focused on using
empirical techniques to measure the performance of algorithms in some specific
tasks (e.g., playing chess, solving mazes or land a helicopter). However, these
methods are not appropriate if we want to evaluate the general intelligence of
AI and, even less, if we compare it with human intelligence. The ANYNT project
has designed a new method of evaluation that tries to assess AI systems using
well known computational notions and problems which are as general as possible.
This new method serves to assess general intelligence (which allows us to learn
how to solve any new kind of problem we face) and not only to evaluate
performance on a set of specific tasks. This method not only focuses on
measuring the intelligence of algorithms, but also to assess any intelligent
system (human beings, animals, AI, aliens?,...), and letting us to place their
results on the same scale and, therefore, to be able to compare them. This new
approach will allow us (in the future) to evaluate and compare any kind of
intelligent system known or even to build/find, be it artificial or biological.
This master thesis aims at ensuring that this new method provides consistent
results when evaluating AI algorithms, this is done through the design and
implementation of prototypes of universal intelligence tests and their
application to different intelligent systems (AI algorithms and humans beings).
From the study we analyze whether the results obtained by two different
intelligent systems are properly located on the same scale and we propose
changes and refinements to these prototypes in order to, in the future, being
able to achieve a truly universal intelligence test.
</summary>
    <author>
      <name>Javier Insa-Cabrera</name>
    </author>
    <author>
      <name>Jose Hernandez-Orallo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">114 pages, master thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.5072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.5072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0051v1</id>
    <updated>2014-06-30T20:45:07Z</updated>
    <published>2014-06-30T20:45:07Z</published>
    <title>Hands-on experiments on intelligent behavior for mobile robots</title>
    <summary>  In recent years, Artificial Intelligence techniques have emerged as useful
tools for solving various engineering problems that were not possible or
convenient to handle by traditional methods. AI has directly influenced many
areas of computer science and becomes an important part of the engineering
curriculum. However, determining the important topics for a single semester AI
course is a nontrivial task, given the lack of a general methodology. AI
concepts commonly overlap with many other disciplines involving a wide range of
subjects, including applied approaches to more formal mathematical issues. This
paper presents the use of a simple robotic platform to assist the learning of
basic AI concepts. The study is guided through some simple experiments using
autonomous mobile robots. The central algorithm is the Learning Automata. Using
LA, each robot action is applied to an environment to be evaluated by means of
a fitness value. The response of the environment is used by the automata to
select its next action. This procedure holds until the goal task is reached.
The proposal addresses the AI study by offering in LA a unifying context to
draw together several of the topics of AI and motivating the students to learn
by building some hands on laboratory exercises. The presented material has been
successfully tested as AI teaching aide in the University of Guadalajara
robotics group as it motivates students and increases enrolment and retention
while educating better computer engineers.
</summary>
    <author>
      <name>Erik Cuevas</name>
    </author>
    <author>
      <name>Daniel Zaldivar</name>
    </author>
    <author>
      <name>Marco Perez-</name>
    </author>
    <author>
      <name>Marte Ramirez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Electrical Engineering Education 48 (1),
  (2011), pp. 66-78</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.0051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00377v2</id>
    <updated>2015-11-09T19:05:13Z</updated>
    <published>2015-08-03T11:29:21Z</published>
    <title>Using Behavior Objects to Manage Complexity in Virtual Worlds</title>
    <summary>  The quality of high-level AI of non-player characters (NPCs) in commercial
open-world games (OWGs) has been increasing during the past years. However, due
to constraints specific to the game industry, this increase has been slow and
it has been driven by larger budgets rather than adoption of new complex AI
techniques. Most of the contemporary AI is still expressed as hard-coded
scripts. The complexity and manageability of the script codebase is one of the
key limiting factors for further AI improvements. In this paper we address this
issue. We present behavior objects - a general approach to development of NPC
behaviors for large OWGs. Behavior objects are inspired by object-oriented
programming and extend the concept of smart objects. Our approach promotes
encapsulation of data and code for multiple related behaviors in one place,
hiding internal details and embedding intelligence in the environment. Behavior
objects are a natural abstraction of five different techniques that we have
implemented to manage AI complexity in an upcoming AAA OWG. We report the
details of the implementations in the context of behavior trees and the lessons
learned during development. Our work should serve as inspiration for AI
architecture designers from both the academia and the industry.
</summary>
    <author>
      <name>Martin Černý</name>
    </author>
    <author>
      <name>Tomáš Plch</name>
    </author>
    <author>
      <name>Matěj Marko</name>
    </author>
    <author>
      <name>Jakub Gemrot</name>
    </author>
    <author>
      <name>Petr Ondráček</name>
    </author>
    <author>
      <name>Cyril Brom</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCIAIG.2016.2528499</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCIAIG.2016.2528499" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Currently under review in IEEE Transactions on Computational
  Intelligence and AI in Games</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.00377v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00377v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00396v1</id>
    <updated>2017-09-01T17:25:28Z</updated>
    <published>2017-09-01T17:25:28Z</published>
    <title>Smile for the Camera: Privacy and Policy Implications of Emotion AI</title>
    <summary>  The introduction of artificial intelligence (AI) on visual images for
emotional analysis obliterates the natural subjectivity and contextual
dependence of our facial displays. Emotion AI places itself as an algorithmic
lens on our digital artifacts and real-time interactions, creating the illusion
of a new, objective class of data: our emotional and mental states. Building
upon a rich network of existing public photographs--as well as fresh feeds from
surveillance footage or smart phone cameras--these emotion algorithms require
no additional infrastructure or improvements on image quality. In order to
examine the potential policy and legal remedies for emotion AI as an emerging
technology, we first establish a framework of actors, collection motivations,
time scales, and space considerations that differentiates emotion AI from other
algorithmic lenses. Each of these elements influences available policy
remedies, and should shape continuing discussions on the antecedent conditions
that make emotional AI acceptable or not in particular contexts. Based on our
framework of unique elements, we examine potential available policy remedies to
prevent or remediate harm. Specifically, our paper looks toward the regulatory
role of the Federal Trade Commission in the US, gaps in the EU's General Data
Protection Regulation (GDPR) allowing for emotion data collection, and
precedent set by polygraph technologies in evidentiary and use restrictions set
by law. We also examine the way social norms and adaptations could grow to also
modulate broader use. Given the challenges in controlling the flow of these
data, we call for further research and attention as emotion AI technology
remains poised for adoption.
</summary>
    <author>
      <name>Elaine Sedenberg</name>
    </author>
    <author>
      <name>John Chuang</name>
    </author>
    <link href="http://arxiv.org/abs/1709.00396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00699v2</id>
    <updated>2018-05-04T07:38:18Z</updated>
    <published>2018-02-01T04:26:21Z</published>
    <title>Data Dwarfs: A Lens Towards Fully Understanding Big Data and AI
  Workloads</title>
    <summary>  The complexity and diversity of big data and AI workloads make understanding
them difficult and challenging. This paper proposes a new approach to
characterizing big data and AI workloads. We consider each big data and AI
workload as a pipeline of one or more classes of unit of computations performed
on different initial or intermediate data inputs. Each class of unit of
computation captures the common requirements while being reasonably divorced
from individual implementations, and hence we call it a data dwarf. For the
first time, among a wide variety of big data and AI workloads, we identify
eight data dwarfs that takes up most of run time, including Matrix, Sampling,
Logic, Transform, Set, Graph, Sort and Statistic. We implement the eight data
dwarfs on different software stacks as the micro benchmarks of an open-source
big data and AI benchmark suite, and perform comprehensive characterization of
those data dwarfs from perspective of data sizes, types, sources, and patterns
as a lens towards fully understanding big data and AI workloads.
</summary>
    <author>
      <name>Wanling Gao</name>
    </author>
    <author>
      <name>Jianfeng Zhan</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Chunjie Luo</name>
    </author>
    <author>
      <name>Daoyi Zheng</name>
    </author>
    <author>
      <name>Fei Tang</name>
    </author>
    <author>
      <name>Biwei Xie</name>
    </author>
    <author>
      <name>Chen Zheng</name>
    </author>
    <author>
      <name>Qiang Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 16 figures and 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.00699v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00699v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.06092v1</id>
    <updated>2020-03-13T03:16:26Z</updated>
    <published>2020-03-13T03:16:26Z</published>
    <title>Managing aquatic invasions: optimal locations and operating times for
  watercraft inspection stations</title>
    <summary>  Aquatic invasive species (AIS) cause significant ecological and economic
damages around the world. A major spread mechanism for AIS is traffic of
boaters transporting their watercraft from invaded to uninvaded waterbodies. To
inhibit the spread of AIS, several Canadian provinces and American states set
up watercraft inspection stations at roadsides, where potentially infested
boats are screened for AIS and, if necessary, decontaminated. However, since
budgets for AIS control are limited, watercraft inspection stations can only be
operated at specific locations and daytimes. Though theoretical studies provide
managers with general guidelines for AIS management, more specific results are
needed to determine when and where watercraft inspections would be most
effective. This is the subject of this paper. We show how linear integer
programming techniques can be used to optimize watercraft inspection policies
under budget constraints. We introduce our approach as a general framework and
apply it to the prevention of the spread of zebra and quagga mussels (Dreissena
spp.) to the Canadian province British Columbia. We consider a variety of
scenarios and show how variations in budget constraints, propagule sources, and
model uncertainty affect the optimal policy. Based on these results, we
identify simple, generally applicable principles for optimal AIS management.
</summary>
    <author>
      <name>Samuel M. Fischer</name>
    </author>
    <author>
      <name>Martina Beck</name>
    </author>
    <author>
      <name>Leif-Matthias Herborg</name>
    </author>
    <author>
      <name>Mark A. Lewis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jenvman.2020.111923</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jenvman.2020.111923" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: aquatic invasive species; linear integer programming;
  optimal management; spatially explicit; zebra mussel</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Environmental Management 283 (2021): 111923</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.06092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.06920v1</id>
    <updated>2020-03-15T20:53:50Z</updated>
    <published>2020-03-15T20:53:50Z</published>
    <title>Getting Fairness Right: Towards a Toolbox for Practitioners</title>
    <summary>  The potential risk of AI systems unintentionally embedding and reproducing
bias has attracted the attention of machine learning practitioners and society
at large. As policy makers are willing to set the standards of algorithms and
AI techniques, the issue on how to refine existing regulation, in order to
enforce that decisions made by automated systems are fair and
non-discriminatory, is again critical. Meanwhile, researchers have demonstrated
that the various existing metrics for fairness are statistically mutually
exclusive and the right choice mostly depends on the use case and the
definition of fairness.
  Recognizing that the solutions for implementing fair AI are not purely
mathematical but require the commitments of the stakeholders to define the
desired nature of fairness, this paper proposes to draft a toolbox which helps
practitioners to ensure fair AI practices. Based on the nature of the
application and the available training data, but also on legal requirements and
ethical, philosophical and cultural dimensions, the toolbox aims to identify
the most appropriate fairness objective. This approach attempts to structure
the complex landscape of fairness metrics and, therefore, makes the different
available options more accessible to non-technical people. In the proven
absence of a silver bullet solution for fair AI, this toolbox intends to
produce the fairest AI systems possible with respect to their local context.
</summary>
    <author>
      <name>Boris Ruf</name>
    </author>
    <author>
      <name>Chaouki Boutharouite</name>
    </author>
    <author>
      <name>Marcin Detyniecki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the Workshop on Fair and Responsible AI at CHI2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.06920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.06920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02553v2</id>
    <updated>2017-12-31T18:43:40Z</updated>
    <published>2017-08-08T16:35:40Z</published>
    <title>Robust Computer Algebra, Theorem Proving, and Oracle AI</title>
    <summary>  In the context of superintelligent AI systems, the term "oracle" has two
meanings. One refers to modular systems queried for domain-specific tasks.
Another usage, referring to a class of systems which may be useful for
addressing the value alignment and AI control problems, is a superintelligent
AI system that only answers questions. The aim of this manuscript is to survey
contemporary research problems related to oracles which align with long-term
research goals of AI safety. We examine existing question answering systems and
argue that their high degree of architectural heterogeneity makes them poor
candidates for rigorous analysis as oracles. On the other hand, we identify
computer algebra systems (CASs) as being primitive examples of domain-specific
oracles for mathematics and argue that efforts to integrate computer algebra
systems with theorem provers, systems which have largely been developed
independent of one another, provide a concrete set of problems related to the
notion of provable safety that has emerged in the AI safety community. We
review approaches to interfacing CASs with theorem provers, describe
well-defined architectural deficiencies that have been identified with CASs,
and suggest possible lines of research and practical software projects for
scientists interested in AI safety.
</summary>
    <author>
      <name>Gopal P. Sarma</name>
    </author>
    <author>
      <name>Nick J. Hay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Informatica Vol. 41 No. 3 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.02553v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02553v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.12386v1</id>
    <updated>2019-07-29T12:41:50Z</updated>
    <published>2019-07-29T12:41:50Z</published>
    <title>Artificial Intelligence and the Future of Psychiatry: Insights from a
  Global Physician Survey</title>
    <summary>  Futurists have predicted that new technologies, embedded with artificial
intelligence (AI) and machine learning (ML), will lead to substantial job loss
in many sectors disrupting many aspects of healthcare. Mental health appears
ripe for such disruption given the global illness burden, stigma, and shortage
of care providers. Using Sermo, a global networking platform open to verified
and licensed physicians, we measured the opinions of psychiatrists about the
likelihood that future autonomous technology (referred to as AI/ML) would be
able to fully replace the average psychiatrist in performing 10 key tasks (e.g.
mental status exam, suicidality assessment, treatment planning) carried out in
mental health care. Survey respondents were 791 psychiatrists from 22
countries. Only 3.8% of respondents felt that AI/ML was likely to replace a
human clinician for providing empathetic care. Documenting (e.g. updating
medical records) and synthesizing information to reach a diagnosis were the two
tasks where a majority predicted that future AI/ML would replace human doctors.
About 1 in 2 doctors believed their jobs could be changed substantially by
future AI/ML. However, female and US-based doctors were more uncertain that the
possible benefits of AI would outweigh potential risks, versus their male and
global counterparts. To our knowledge, this is the first global survey to seek
the opinions of physicians on the impact of autonomous AI/ML on the future of
psychiatry. Our findings provide compelling insights into how physicians think
about intelligent technologies which may better help us integrate such tools
and reskill doctors, as needed, to enhance mental health care.
</summary>
    <author>
      <name>P. Murali Doraiswamy</name>
    </author>
    <author>
      <name>Charlotte Blease</name>
    </author>
    <author>
      <name>Kaylee Bodner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 7 tables, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.12386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.12386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00547v2</id>
    <updated>2017-12-05T04:23:25Z</updated>
    <published>2017-12-02T04:21:14Z</published>
    <title>Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to
  Stop Worrying and Love the Social and Behavioural Sciences</title>
    <summary>  In his seminal book `The Inmates are Running the Asylum: Why High-Tech
Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams
Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is
often poorly designed (from a user perspective) is that programmers are in
charge of design decisions, rather than interaction designers. As a result,
programmers design software for themselves, rather than for their target
audience, a phenomenon he refers to as the `inmates running the asylum'. This
paper argues that explainable AI risks a similar fate. While the re-emergence
of explainable AI is positive, this paper argues most of us as AI researchers
are building explanatory agents for ourselves, rather than for the intended
users. But explainable AI is more likely to succeed if researchers and
practitioners understand, adopt, implement, and improve models from the vast
and valuable bodies of research in philosophy, psychology, and cognitive
science, and if evaluation of these models is focused more on people than on
technology. From a light scan of literature, we demonstrate that there is
considerable scope to infuse more results from the social and behavioural
sciences into explainable AI, and present some key results from these fields
that are relevant to explainable AI.
</summary>
    <author>
      <name>Tim Miller</name>
    </author>
    <author>
      <name>Piers Howe</name>
    </author>
    <author>
      <name>Liz Sonenberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2017 Workshop on Explainable Artificial Intelligence (XAI)</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.00547v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00547v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07752v3</id>
    <updated>2018-06-28T22:24:09Z</updated>
    <published>2017-12-21T00:29:48Z</published>
    <title>Towards an unanimous international regulatory body for responsible use
  of Artificial Intelligence [UIRB-AI]</title>
    <summary>  Artificial Intelligence (AI), is once again in the phase of drastic
advancements. Unarguably, the technology itself can revolutionize the way we
live our everyday life. But the exponential growth of technology poses a
daunting task for policy researchers and law makers in making amendments to the
existing norms. In addition, not everyone in the society is studying the
potential socio-economic intricacies and cultural drifts that AI can bring
about. It is prudence to reflect from our historical past to propel the
development of technology in the right direction. To benefit the society of the
present and future, I scientifically explore the societal impact of AI. While
there are many public and private partnerships working on similar aspects, here
I describe the necessity for an Unanimous International Regulatory Body for all
applications of AI (UIRB-AI). I also discuss the benefits and drawbacks of such
an organization. To combat any drawbacks in the formation of an UIRB-AI, both
idealistic and pragmatic perspectives are discussed alternatively. The paper
further advances the discussion by proposing novel policies on how such
organization should be structured and how it can bring about a win-win
situation for everyone in the society.
</summary>
    <author>
      <name>Rajesh Chidambaram</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper covers a diverse range of topics but doesn't get into the
  details of any and hence the proposals remain pragmatically irrelevant</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.07752v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07752v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01396v1</id>
    <updated>2018-04-01T23:12:30Z</updated>
    <published>2018-04-01T23:12:30Z</published>
    <title>Artificial Intelligence and its Role in Near Future</title>
    <summary>  AI technology has a long history which is actively and constantly changing
and growing. It focuses on intelligent agents, which contain devices that
perceive the environment and based on which takes actions in order to maximize
goal success chances. In this paper, we will explain the modern AI basics and
various representative applications of AI. In the context of the modern
digitalized world, AI is the property of machines, computer programs, and
systems to perform the intellectual and creative functions of a person,
independently find ways to solve problems, be able to draw conclusions and make
decisions. Most artificial intelligence systems have the ability to learn,
which allows people to improve their performance over time. The recent research
on AI tools, including machine learning, deep learning and predictive analysis
intended toward increasing the planning, learning, reasoning, thinking and
action taking ability. Based on which, the proposed research intends towards
exploring on how the human intelligence differs from the artificial
intelligence. Moreover, we critically analyze what AI of today is capable of
doing, why it still cannot reach human intelligence and what are the open
challenges existing in front of AI to reach and outperform human level of
intelligence. Furthermore, it will explore the future predictions for
artificial intelligence and based on which potential solution will be
recommended to solve it within next decades.
</summary>
    <author>
      <name>Jahanzaib Shabbir</name>
    </author>
    <author>
      <name>Tarique Anwer</name>
    </author>
    <link href="http://arxiv.org/abs/1804.01396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.06606v1</id>
    <updated>2018-09-18T09:16:54Z</updated>
    <published>2018-09-18T09:16:54Z</published>
    <title>Proceedings of the AI-HRI Symposium at AAAI-FSS 2018</title>
    <summary>  The goal of the Interactive Learning for Artificial Intelligence (AI) for
Human-Robot Interaction (HRI) symposium is to bring together the large
community of researchers working on interactive learning scenarios for
interactive robotics. While current HRI research involves investigating ways
for robots to effectively interact with people, HRI's overarching goal is to
develop robots that are autonomous while intelligently modeling and learning
from humans. These goals greatly overlap with some central goals of AI and
interactive machine learning, such that HRI is an extremely challenging problem
domain for interactive learning and will elicit fresh problem areas for
robotics research. Present-day AI research still does not widely consider
situations for interacting directly with humans and within human-populated
environments, which present inherent uncertainty in dynamics, structure, and
interaction. We believe that the HRI community already offers a rich set of
principles and observations that can be used to structure new models of
interaction. The human-aware AI initiative has primarily been approached
through human-in-the-loop methods that use people's data and feedback to
improve refinement and performance of the algorithms, learned functions, and
personalization. We thus believe that HRI is an important component to
furthering AI and robotics research.
</summary>
    <author>
      <name>Kalesha Bullard</name>
    </author>
    <author>
      <name>Nick DePalma</name>
    </author>
    <author>
      <name>Richard G. Freedman</name>
    </author>
    <author>
      <name>Bradley Hayes</name>
    </author>
    <author>
      <name>Luca Iocchi</name>
    </author>
    <author>
      <name>Katrin Lohan</name>
    </author>
    <author>
      <name>Ross Mead</name>
    </author>
    <author>
      <name>Emmanuel Senft</name>
    </author>
    <author>
      <name>Tom Williams</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">HTML file with clickable links to papers - All papers have been
  reviewed by two reviewers and a meta reviewer in a single blind fashion -
  Symposium website: https://ai-hri.github.io/2018/</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.06606v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.06606v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.00919v1</id>
    <updated>2019-04-01T15:49:30Z</updated>
    <published>2019-04-01T15:49:30Z</published>
    <title>A Deterministic Method to Calculate the AIS Trauma Score from a Finite
  Element Organ Trauma Model (OTM)</title>
    <summary>  Traumatic injuries are measured using the Abbreviated Injury Scale (AIS),
which is a risk to life scale. New human computer models use stresses and
strains to evaluate whether serious or fatal injuries are reached,
unfortunately these tensors bear no direct relation to AIS. This paper proposes
to overcome this deficiency and suggests a unique Organ Trauma Model (OTM) able
to calculate the risk to life based on the severity on any organ injury,
focussing on real-life pedestrian accidents. The OTM uses a power method, named
Peak Virtual Power (PVP), and calculates the risk to life of brain white and
grey matters as a function of impact direction and impact speed. The OTM
firstly calibrates PVP against the medical critical AIS threshold observed in
each part of the head as a function of speed. This base PVP critical trauma
function is then scaled and banded across all AIS levels using the confirmed
property that AIS and the probability of death is statistically and numerically
a cubic one. The OTM model has been tested against four real-life pedestrian
accidents and proven to be able to predict pedestrian head trauma severity. In
some cases, the method did however under-estimate the head trauma by 1 AIS
level, because of post-impact haemorrhage which cannot be captured with the
employed Lagrangian Finite Element (FE) solver. It is also shown that the
location of the injury predictions using PVP coincide with the post mortem
reports and are different to the predictions made using maximum principal
strain.
</summary>
    <author>
      <name>C. Bastien</name>
    </author>
    <author>
      <name>C. Neal-Sturgess</name>
    </author>
    <author>
      <name>J. Christensen</name>
    </author>
    <author>
      <name>L. Wen</name>
    </author>
    <link href="http://arxiv.org/abs/1904.00919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.00919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.TO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.TO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.03285v4</id>
    <updated>2019-09-21T17:13:50Z</updated>
    <published>2019-04-05T21:26:39Z</published>
    <title>Can You Explain That? Lucid Explanations Help Human-AI Collaborative
  Image Retrieval</title>
    <summary>  While there have been many proposals on making AI algorithms explainable, few
have attempted to evaluate the impact of AI-generated explanations on human
performance in conducting human-AI collaborative tasks. To bridge the gap, we
propose a Twenty-Questions style collaborative image retrieval game,
Explanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy
of explanations (visual evidence or textual justification) in the context of
Visual Question Answering (VQA). In our proposed ExAG, a human user needs to
guess a secret image picked by the VQA agent by asking natural language
questions to it. We show that overall, when AI explains its answers, users
succeed more often in guessing the secret image correctly. Notably, a few
correct explanations can readily improve human performance when VQA answers are
mostly incorrect as compared to no-explanation games. Furthermore, we also show
that while explanations rated as "helpful" significantly improve human
performance, "incorrect" and "unhelpful" explanations can degrade performance
as compared to no-explanation games. Our experiments, therefore, demonstrate
that ExAG is an effective means to evaluate the efficacy of AI-generated
explanations on a human-AI collaborative task.
</summary>
    <author>
      <name>Arijit Ray</name>
    </author>
    <author>
      <name>Yi Yao</name>
    </author>
    <author>
      <name>Rakesh Kumar</name>
    </author>
    <author>
      <name>Ajay Divakaran</name>
    </author>
    <author>
      <name>Giedrius Burachas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2019 AAAI Conference on Human Computation and Crowdsourcing</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2019 AAAI Conference on Human Computation and Crowdsourcing</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.03285v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.03285v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.08921v1</id>
    <updated>2019-07-25T13:53:49Z</updated>
    <published>2019-07-25T13:53:49Z</published>
    <title>Intelligence Stratum for IoT. Architecture Requirements and Functions</title>
    <summary>  The use of Artificial Intelligence (AI) is becoming increasingly pervasive
and relevant in many different application areas. Researchers are putting a
considerable effort to take full advantage of the power of AI, while trying to
overcome the technical challenges that are intrinsically linked to almost any
domain area of application, such as the Internet of Things (IoT). One of the
biggest problems related to the use of AI in IoT is related to the difficulty
of coping with the wide variety of protocols and software technologies used, as
well as with the heterogeneity of the hardware resources consuming the AI. The
scattered IoT landscape accentuates the limitations on interoperability,
especially visible in the deployment of AI, affecting the seamless AI
life-cycle management as well. In this paper, it is discussed how to enable AI
distribution in IoT by introducing a layered intelligence architecture that
aims to face the undertaken challenges taking into account the special
requirements of nowadays IoT networks. It describes the main characteristics of
the new paradigm architecture, highlighting what are the implications of its
adoption from use cases perspective and their requirements. Finally, a set of
open technical and research challenges are enumerated to reach the full
potential of the intelligence distribution's vision.
</summary>
    <author>
      <name>Edgar Ramos</name>
    </author>
    <author>
      <name>Roberto Morabito</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article has been accepted for publication in the 17th IEEE
  International Conference on Pervasive Intelligence and Computing (PICom 2019)
  Copyright 2019 IEEE</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.08921v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08921v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.06842v9</id>
    <updated>2020-10-08T18:29:34Z</updated>
    <published>2019-09-15T17:30:05Z</published>
    <title>Benchmarking the Performance and Energy Efficiency of AI Accelerators
  for AI Training</title>
    <summary>  Deep learning has become widely used in complex AI applications. Yet,
training a deep neural network (DNNs) model requires a considerable amount of
calculations, long running time, and much energy. Nowadays, many-core AI
accelerators (e.g., GPUs and TPUs) are designed to improve the performance of
AI training. However, processors from different vendors perform dissimilarly in
terms of performance and energy consumption. To investigate the differences
among several popular off-the-shelf processors (i.e., Intel CPU, NVIDIA GPU,
AMD GPU, and Google TPU) in training DNNs, we carry out a comprehensive
empirical study on the performance and energy efficiency of these processors by
benchmarking a representative set of deep learning workloads, including
computation-intensive operations, classical convolutional neural networks
(CNNs), recurrent neural networks (LSTM), Deep Speech 2, and Transformer.
Different from the existing end-to-end benchmarks which only present the
training time, We try to investigate the impact of hardware, vendor's software
library, and deep learning framework on the performance and energy consumption
of AI training. Our evaluation methods and results not only provide an
informative guide for end-users to select proper AI accelerators, but also
expose some opportunities for the hardware vendors to improve their software
library.
</summary>
    <author>
      <name>Yuxin Wang</name>
    </author>
    <author>
      <name>Qiang Wang</name>
    </author>
    <author>
      <name>Shaohuai Shi</name>
    </author>
    <author>
      <name>Xin He</name>
    </author>
    <author>
      <name>Zhenheng Tang</name>
    </author>
    <author>
      <name>Kaiyong Zhao</name>
    </author>
    <author>
      <name>Xiaowen Chu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revised some minor issues</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.06842v9" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.06842v9" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02136v2</id>
    <updated>2019-10-21T20:21:56Z</updated>
    <published>2019-10-04T20:27:20Z</published>
    <title>Risks of Using Non-verified Open Data: A case study on using Machine
  Learning techniques for predicting Pregnancy Outcomes in India</title>
    <summary>  Artificial intelligence (AI) has evolved considerably in the last few years.
While applications of AI is now becoming more common in fields like retail and
marketing, application of AI in solving problems related to developing
countries is still an emerging topic. Specially, AI applications in
resource-poor settings remains relatively nascent. There is a huge scope of AI
being used in such settings. For example, researchers have started exploring AI
applications to reduce poverty and deliver a broad range of critical public
services. However, despite many promising use cases, there are many dataset
related challenges that one has to overcome in such projects. These challenges
often take the form of missing data, incorrectly collected data and improperly
labeled variables, among other factors. As a result, we can often end up using
data that is not representative of the problem we are trying to solve. In this
case study, we explore the challenges of using such an open dataset from India,
to predict an important health outcome. We highlight how the use of AI without
proper understanding of reporting metrics can lead to erroneous conclusions.
</summary>
    <author>
      <name>Anusua Trivedi</name>
    </author>
    <author>
      <name>Sumit Mukherjee</name>
    </author>
    <author>
      <name>Edmund Tse</name>
    </author>
    <author>
      <name>Anne Ewing</name>
    </author>
    <author>
      <name>Juan Lavista Ferres</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at NeurIPS 2019 Workshop on Machine Learning for the
  Developing World</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.02136v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02136v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.06136v1</id>
    <updated>2019-10-14T13:35:24Z</updated>
    <published>2019-10-14T13:35:24Z</published>
    <title>Component Mismatches Are a Critical Bottleneck to Fielding AI-Enabled
  Systems in the Public Sector</title>
    <summary>  The use of machine learning or artificial intelligence (ML/AI) holds
substantial potential toward improving many functions and needs of the public
sector. In practice however, integrating ML/AI components into public sector
applications is severely limited not only by the fragility of these components
and their algorithms, but also because of mismatches between components of
ML-enabled systems. For example, if an ML model is trained on data that is
different from data in the operational environment, field performance of the ML
component will be dramatically reduced. Separate from software engineering
considerations, the expertise needed to field an ML/AI component within a
system frequently comes from outside software engineering. As a result,
assumptions and even descriptive language used by practitioners from these
different disciplines can exacerbate other challenges to integrating ML/AI
components into larger systems. We are investigating classes of mismatches in
ML/AI systems integration, to identify the implicit assumptions made by
practitioners in different fields (data scientists, software engineers,
operations staff) and find ways to communicate the appropriate information
explicitly. We will discuss a few categories of mismatch, and provide examples
from each class. To enable ML/AI components to be fielded in a meaningful way,
we will need to understand the mismatches that exist and develop practices to
mitigate the impacts of these mismatches.
</summary>
    <author>
      <name>Grace A. Lewis</name>
    </author>
    <author>
      <name>Stephany Bellomo</name>
    </author>
    <author>
      <name>April Galyardt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at AAAI FSS-19: Artificial Intelligence in Government and
  Public Sector, Arlington, Virginia, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.06136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.08005v2</id>
    <updated>2020-05-19T04:48:17Z</updated>
    <published>2019-11-18T23:45:13Z</published>
    <title>The AI Liability Puzzle and A Fund-Based Work-Around</title>
    <summary>  Certainty around the regulatory environment is crucial to enable responsible
AI innovation and foster the social acceptance of these powerful new
technologies. One notable source of uncertainty is, however, that the existing
legal liability system is inapt to assign responsibility where a potentially
harmful conduct and/or the harm itself are unforeseeable, yet some
instantiations of AI and/or the harms they may trigger are not foreseeable in
the legal sense. The unpredictability of how courts would handle such cases
makes the risks involved in the investment and use of AI incalculable, creating
an environment that is not conducive to innovation and may deprive society of
some of the benefits AI could provide. To tackle this problem, we propose to
draw insights from financial regulatory best-practices and establish a system
of AI guarantee schemes. We envisage the system to form part of the broader
market-structuring regulatory framework, with the primary function to provide a
readily available, clear, and transparent funding mechanism to compensate
claims that are either extremely hard or impossible to realize via conventional
litigation. We propose it to be at least partially industry-funded, with
funding arrangements depending on whether it would pursue other potential
policy goals. We aim to engage in a high-level, comparative conceptual debate
around the suitability of the foreseeability concept to limit legal liability
rather than confronting the intricacies of the case law of specific
jurisdictions. Recognizing the importance of the latter task, we leave this to
further research in support of the legal system's incremental adaptation to the
novel challenges of present and future AI technologies.
</summary>
    <author>
      <name>Olivia J. Erdélyi</name>
    </author>
    <author>
      <name>Gábor Erdélyi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.08005v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.08005v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.01592v3</id>
    <updated>2022-01-09T21:45:41Z</updated>
    <published>2020-01-02T09:16:40Z</published>
    <title>Modeling Historical AIS Data For Vessel Path Prediction: A Comprehensive
  Treatment</title>
    <summary>  The prosperity of artificial intelligence has aroused intensive interests in
intelligent/autonomous navigation, in which path prediction is a key
functionality for decision supports, e.g. route planning, collision warning,
and traffic regulation. For maritime intelligence, Automatic Identification
System (AIS) plays an important role because it recently has been made
compulsory for large international commercial vessels and is able to provide
nearly real-time information of the vessel. Therefore AIS data based vessel
path prediction is a promising way in future maritime intelligence. However,
real-world AIS data collected online are just highly irregular trajectory
segments (AIS message sequences) from different types of vessels and
geographical regions, with possibly very low data quality. So even there are
some works studying how to build a path prediction model using historical AIS
data, but still, it is a very challenging problem. In this paper, we propose a
comprehensive framework to model massive historical AIS trajectory segments for
accurate vessel path prediction. Experimental comparisons with existing popular
methods are made to validate the proposed approach and results show that our
approach could outperform the baseline methods by a wide margin.
</summary>
    <author>
      <name>Enmei Tu</name>
    </author>
    <author>
      <name>Guanghao Zhang</name>
    </author>
    <author>
      <name>Shangbo Mao</name>
    </author>
    <author>
      <name>Lily Rachmawati</name>
    </author>
    <author>
      <name>Guang-Bin Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.01592v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01592v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09011v1</id>
    <updated>2020-01-18T14:47:28Z</updated>
    <published>2020-01-18T14:47:28Z</published>
    <title>Ownership preserving AI Market Places using Blockchain</title>
    <summary>  We present a blockchain based system that allows data owners, cloud vendors,
and AI developers to collaboratively train machine learning models in a
trustless AI marketplace. Data is a highly valued digital asset and central to
deriving business insights. Our system enables data owners to retain ownership
and privacy of their data, while still allowing AI developers to leverage the
data for training. Similarly, AI developers can utilize compute resources from
cloud vendors without loosing ownership or privacy of their trained models. Our
system protocols are set up to incentivize all three entities - data owners,
cloud vendors, and AI developers to truthfully record their actions on the
distributed ledger, so that the blockchain system provides verifiable evidence
of wrongdoing and dispute resolution. Our system is implemented on the
Hyperledger Fabric and can provide a viable alternative to centralized AI
systems that do not guarantee data or model privacy. We present experimental
performance results that demonstrate the latency and throughput of its
transactions under different network configurations where peers on the
blockchain may be spread across different datacenters and geographies. Our
results indicate that the proposed solution scales well to large number of data
and model owners and can train up to 70 models per second on a 12-peer non
optimized blockchain network and roughly 30 models per second in a 24 peer
network.
</summary>
    <author>
      <name>Nishant Baranwal Somy</name>
    </author>
    <author>
      <name>Kalapriya Kannan</name>
    </author>
    <author>
      <name>Vijay Arya</name>
    </author>
    <author>
      <name>Sandeep Hans</name>
    </author>
    <author>
      <name>Abhishek Singh</name>
    </author>
    <author>
      <name>Pranay Lohia</name>
    </author>
    <author>
      <name>Sameep Mehta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/Blockchain.2019.00029</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/Blockchain.2019.00029" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Blockchain, Blockchain 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2001.09011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09762v1</id>
    <updated>2020-01-14T09:39:09Z</updated>
    <published>2020-01-14T09:39:09Z</published>
    <title>Bias in Data-driven AI Systems -- An Introductory Survey</title>
    <summary>  AI-based systems are widely employed nowadays to make decisions that have
far-reaching impacts on individuals and society. Their decisions might affect
everyone, everywhere and anytime, entailing concerns about potential human
rights issues. Therefore, it is necessary to move beyond traditional AI
algorithms optimized for predictive performance and embed ethical and legal
principles in their design, training and deployment to ensure social good while
still benefiting from the huge potential of the AI technology. The goal of this
survey is to provide a broad multi-disciplinary overview of the area of bias in
AI systems, focusing on technical challenges and solutions as well as to
suggest new research directions towards approaches well-grounded in a legal
frame. In this survey, we focus on data-driven AI, as a large part of AI is
powered nowadays by (big) data and powerful Machine Learning (ML) algorithms.
If otherwise not specified, we use the general term bias to describe problems
related to the gathering or processing of data that might result in prejudiced
decisions on the bases of demographic features like race, sex, etc.
</summary>
    <author>
      <name>Eirini Ntoutsi</name>
    </author>
    <author>
      <name>Pavlos Fafalios</name>
    </author>
    <author>
      <name>Ujwal Gadiraju</name>
    </author>
    <author>
      <name>Vasileios Iosifidis</name>
    </author>
    <author>
      <name>Wolfgang Nejdl</name>
    </author>
    <author>
      <name>Maria-Esther Vidal</name>
    </author>
    <author>
      <name>Salvatore Ruggieri</name>
    </author>
    <author>
      <name>Franco Turini</name>
    </author>
    <author>
      <name>Symeon Papadopoulos</name>
    </author>
    <author>
      <name>Emmanouil Krasanakis</name>
    </author>
    <author>
      <name>Ioannis Kompatsiaris</name>
    </author>
    <author>
      <name>Katharina Kinder-Kurlanda</name>
    </author>
    <author>
      <name>Claudia Wagner</name>
    </author>
    <author>
      <name>Fariba Karimi</name>
    </author>
    <author>
      <name>Miriam Fernandez</name>
    </author>
    <author>
      <name>Harith Alani</name>
    </author>
    <author>
      <name>Bettina Berendt</name>
    </author>
    <author>
      <name>Tina Kruegel</name>
    </author>
    <author>
      <name>Christian Heinze</name>
    </author>
    <author>
      <name>Klaus Broelemann</name>
    </author>
    <author>
      <name>Gjergji Kasneci</name>
    </author>
    <author>
      <name>Thanassis Tiropanis</name>
    </author>
    <author>
      <name>Steffen Staab</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.09762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.07495v1</id>
    <updated>2020-06-12T22:28:09Z</updated>
    <published>2020-06-12T22:28:09Z</published>
    <title>Open Questions in Creating Safe Open-ended AI: Tensions Between Control
  and Creativity</title>
    <summary>  Artificial life originated and has long studied the topic of open-ended
evolution, which seeks the principles underlying artificial systems that
innovate continually, inspired by biological evolution. Recently, interest has
grown within the broader field of AI in a generalization of open-ended
evolution, here called open-ended search, wherein such questions of
open-endedness are explored for advancing AI, whatever the nature of the
underlying search algorithm (e.g. evolutionary or gradient-based). For example,
open-ended search might design new architectures for neural networks, new
reinforcement learning algorithms, or most ambitiously, aim at designing
artificial general intelligence. This paper proposes that open-ended evolution
and artificial life have much to contribute towards the understanding of
open-ended AI, focusing here in particular on the safety of open-ended search.
The idea is that AI systems are increasingly applied in the real world, often
producing unintended harms in the process, which motivates the growing field of
AI safety. This paper argues that open-ended AI has its own safety challenges,
in particular, whether the creativity of open-ended systems can be productively
and predictably controlled. This paper explains how unique safety problems
manifest in open-ended search, and suggests concrete contributions and research
questions to explore them. The hope is to inspire progress towards creative,
useful, and safe open-ended search algorithms.
</summary>
    <author>
      <name>Adrien Ecoffet</name>
    </author>
    <author>
      <name>Jeff Clune</name>
    </author>
    <author>
      <name>Joel Lehman</name>
    </author>
    <link href="http://arxiv.org/abs/2006.07495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.07495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.00900v1</id>
    <updated>2020-07-02T06:11:28Z</updated>
    <published>2020-07-02T06:11:28Z</published>
    <title>The Impact of Explanations on AI Competency Prediction in VQA</title>
    <summary>  Explainability is one of the key elements for building trust in AI systems.
Among numerous attempts to make AI explainable, quantifying the effect of
explanations remains a challenge in conducting human-AI collaborative tasks.
Aside from the ability to predict the overall behavior of AI, in many
applications, users need to understand an AI agent's competency in different
aspects of the task domain. In this paper, we evaluate the impact of
explanations on the user's mental model of AI agent competency within the task
of visual question answering (VQA). We quantify users' understanding of
competency, based on the correlation between the actual system performance and
user rankings. We introduce an explainable VQA system that uses spatial and
object features and is powered by the BERT language model. Each group of users
sees only one kind of explanation to rank the competencies of the VQA model.
The proposed model is evaluated through between-subject experiments to probe
explanations' impact on the user's perception of competency. The comparison
between two VQA models shows BERT based explanations and the use of object
features improve the user's prediction of the model's competencies.
</summary>
    <author>
      <name>Kamran Alipour</name>
    </author>
    <author>
      <name>Arijit Ray</name>
    </author>
    <author>
      <name>Xiao Lin</name>
    </author>
    <author>
      <name>Jurgen P. Schulze</name>
    </author>
    <author>
      <name>Yi Yao</name>
    </author>
    <author>
      <name>Giedrius T. Burachas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to HCCAI 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.00900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.00900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.08666v1</id>
    <updated>2020-07-16T21:52:13Z</updated>
    <published>2020-07-16T21:52:13Z</published>
    <title>Conservative AI and social inequality: Conceptualizing alternatives to
  bias through social theory</title>
    <summary>  In response to calls for greater interdisciplinary involvement from the
social sciences and humanities in the development, governance, and study of
artificial intelligence systems, this paper presents one sociologist's view on
the problem of algorithmic bias and the reproduction of societal bias.
Discussions of bias in AI cover much of the same conceptual terrain that
sociologists studying inequality have long understood using more specific terms
and theories. Concerns over reproducing societal bias should be informed by an
understanding of the ways that inequality is continually reproduced in society
-- processes that AI systems are either complicit in, or can be designed to
disrupt and counter. The contrast presented here is between conservative and
radical approaches to AI, with conservatism referring to dominant tendencies
that reproduce and strengthen the status quo, while radical approaches work to
disrupt systemic forms of inequality. The limitations of conservative
approaches to class, gender, and racial bias are discussed as specific
examples, along with the social structures and processes that biases in these
areas are linked to. Societal issues can no longer be out of scope for AI and
machine learning, given the impact of these systems on human lives. This
requires engagement with a growing body of critical AI scholarship that goes
beyond biased data to analyze structured ways of perpetuating inequality,
opening up the possibility for radical alternatives.
</summary>
    <author>
      <name>Mike Zajko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00146-021-01153-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00146-021-01153-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AI &amp; Soc (2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.08666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.08666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.01848v1</id>
    <updated>2020-08-04T21:46:46Z</updated>
    <published>2020-08-04T21:46:46Z</published>
    <title>Forecasting AI Progress: A Research Agenda</title>
    <summary>  Forecasting AI progress is essential to reducing uncertainty in order to
appropriately plan for research efforts on AI safety and AI governance. While
this is generally considered to be an important topic, little work has been
conducted on it and there is no published document that gives and objective
overview of the field. Moreover, the field is very diverse and there is no
published consensus regarding its direction. This paper describes the
development of a research agenda for forecasting AI progress which utilized the
Delphi technique to elicit and aggregate experts' opinions on what questions
and methods to prioritize. The results of the Delphi are presented; the
remainder of the paper follow the structure of these results, briefly reviewing
relevant literature and suggesting future work for each topic. Experts
indicated that a wide variety of methods should be considered for forecasting
AI progress. Moreover, experts identified salient questions that were both
general and completely unique to the problem of forecasting AI progress. Some
of the highest priority topics include the validation of (partially unresolved)
forecasts, how to make forecasting action-guiding and the quality of different
performance metrics. While statistical methods seem more promising, there is
also recognition that supplementing judgmental techniques can be quite
beneficial.
</summary>
    <author>
      <name>Ross Gruetzemacher</name>
    </author>
    <author>
      <name>Florian Dorner</name>
    </author>
    <author>
      <name>Niko Bernaola-Alvarez</name>
    </author>
    <author>
      <name>Charlie Giattino</name>
    </author>
    <author>
      <name>David Manheim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">40 pages including Appendices, 1 figure, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.01848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.01848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.04165v2</id>
    <updated>2020-10-28T03:49:19Z</updated>
    <published>2020-08-10T14:45:52Z</published>
    <title>Proof-Carrying Plans: a Resource Logic for AI Planning</title>
    <summary>  Recent trends in AI verification and Explainable AI have raised the question
of whether AI planning techniques can be verified. In this paper, we present a
novel resource logic, the Proof Carrying Plans (PCP) logic that can be used to
verify plans produced by AI planners. The PCP logic takes inspiration from
existing resource logics (such as Linear logic and Separation logic) as well as
Hoare logic when it comes to modelling states and resource-aware plan
execution. It also capitalises on the Curry-Howard approach to logics, in its
treatment of plans as functions and plan pre- and post-conditions as types.
This paper presents two main results. From the theoretical perspective, we show
that the PCP logic is sound relative to the standard possible world semantics
used in AI planning. From the practical perspective, we present a complete Agda
formalisation of the PCP logic and of its soundness proof. Moreover, we
showcase the Curry-Howard, or functional, value of this implementation by
supplementing it with the library that parses AI plans into Agda's proofs
automatically. We provide evaluation of this library and the resulting Agda
functions.
</summary>
    <author>
      <name>Alasdair Hill</name>
    </author>
    <author>
      <name>Ekaterina Komendantskaya</name>
    </author>
    <author>
      <name>Ronald P. A. Petrick</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3414080.3414094</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3414080.3414094" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PPDP 2020, 13 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.04165v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.04165v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Q60, 03B38, 68T27, 03B70" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.0; D.3; F.3; F.4; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.07449v1</id>
    <updated>2020-08-03T16:49:04Z</updated>
    <published>2020-08-03T16:49:04Z</published>
    <title>A Survey on the Use of AI and ML for Fighting the COVID-19 Pandemic</title>
    <summary>  Artificial intelligence (AI) and machine learning (ML) have made a paradigm
shift in health care which, eventually can be used for decision support and
forecasting by exploring the medical data. Recent studies showed that AI and ML
can be used to fight against the COVID-19 pandemic. Therefore, the objective of
this review study is to summarize the recent AI and ML based studies that have
focused to fight against COVID-19 pandemic. From an initial set of 634
articles, a total of 35 articles were finally selected through an extensive
inclusion-exclusion process. In our review, we have explored the
objectives/aims of the existing studies (i.e., the role of AI/ML in fighting
COVID-19 pandemic); context of the study (i.e., study focused to a specific
country-context or with a global perspective); type and volume of dataset;
methodology, algorithms or techniques adopted in the prediction or diagnosis
processes; and mapping the algorithms/techniques with the data type
highlighting their prediction/classification accuracy. We particularly focused
on the uses of AI/ML in analyzing the pandemic data in order to depict the most
recent progress of AI for fighting against COVID-19 and pointed out the
potential scope of further research.
</summary>
    <author>
      <name>Muhammad Nazrul Islam</name>
    </author>
    <author>
      <name>Toki Tahmid Inan</name>
    </author>
    <author>
      <name>Suzzana Rafi</name>
    </author>
    <author>
      <name>Syeda Sabrina Akter</name>
    </author>
    <author>
      <name>Iqbal H. Sarker</name>
    </author>
    <author>
      <name>A. K. M. Najmul Islam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.07449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.07449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.09072v1</id>
    <updated>2020-08-20T16:52:58Z</updated>
    <published>2020-08-20T16:52:58Z</published>
    <title>Utilizing Explainable AI for Quantization and Pruning of Deep Neural
  Networks</title>
    <summary>  For many applications, utilizing DNNs (Deep Neural Networks) requires their
implementation on a target architecture in an optimized manner concerning
energy consumption, memory requirement, throughput, etc. DNN compression is
used to reduce the memory footprint and complexity of a DNN before its
deployment on hardware. Recent efforts to understand and explain AI (Artificial
Intelligence) methods have led to a new research area, termed as explainable
AI. Explainable AI methods allow us to understand better the inner working of
DNNs, such as the importance of different neurons and features. The concepts
from explainable AI provide an opportunity to improve DNN compression methods
such as quantization and pruning in several ways that have not been
sufficiently explored so far. In this paper, we utilize explainable AI methods:
mainly DeepLIFT method. We use these methods for (1) pruning of DNNs; this
includes structured and unstructured pruning of \ac{CNN} filters pruning as
well as pruning weights of fully connected layers, (2) non-uniform quantization
of DNN weights using clustering algorithm; this is also referred to as Weight
Sharing, and (3) integer-based mixed-precision quantization; this is where each
layer of a DNN may use a different number of integer bits. We use typical image
classification datasets with common deep learning image classification models
for evaluation. In all these three cases, we demonstrate significant
improvements as well as new insights and opportunities from the use of
explainable AI in DNN compression.
</summary>
    <author>
      <name>Muhammad Sabih</name>
    </author>
    <author>
      <name>Frank Hannig</name>
    </author>
    <author>
      <name>Juergen Teich</name>
    </author>
    <link href="http://arxiv.org/abs/2008.09072v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.09072v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11675v2</id>
    <updated>2020-09-17T09:23:06Z</updated>
    <published>2020-08-26T16:58:32Z</published>
    <title>Optimising AI Training Deployments using Graph Compilers and Containers</title>
    <summary>  Artificial Intelligence (AI) applications based on Deep Neural Networks (DNN)
or Deep Learning (DL) have become popular due to their success in solving
problems likeimage analysis and speech recognition. Training a DNN is
computationally intensive and High Performance Computing(HPC) has been a key
driver in AI growth. Virtualisation and container technology have led to the
convergence of cloud and HPC infrastructure. These infrastructures with diverse
hardware increase the complexity of deploying and optimising AI training
workloads. AI training deployments in HPC or cloud can be optimised with
target-specific libraries, graph compilers, andby improving data movement or
IO. Graph compilers aim to optimise the execution of a DNN graph by generating
an optimised code for a target hardware/backend. As part of SODALITE (a Horizon
2020 project), MODAK tool is developed to optimise application deployment in
software defined infrastructures. Using input from the data scientist and
performance modelling, MODAK maps optimal application parameters to a target
infrastructure and builds an optimised container. In this paper, we introduce
MODAK and review container technologies and graph compilers for AI. We
illustrate optimisation of AI training deployments using graph compilers and
Singularity containers. Evaluation using MNIST-CNN and ResNet50 training
workloads shows that custom built optimised containers outperform the official
images from DockerHub. We also found that the performance of graph compilers
depends on the target hardware and the complexity of the neural network.
</summary>
    <author>
      <name>Nina Mujkanovic</name>
    </author>
    <author>
      <name>Karthee Sivalingam</name>
    </author>
    <author>
      <name>Alfio Lazzaro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">HPEC IEEE, 6 pages, 5 figues, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.11675v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11675v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.11722v1</id>
    <updated>2020-09-23T09:23:29Z</updated>
    <published>2020-09-23T09:23:29Z</published>
    <title>Cloud2Edge Elastic AI Framework for Prototyping and Deployment of AI
  Inference Engines in Autonomous Vehicles</title>
    <summary>  Self-driving cars and autonomous vehicles are revolutionizing the automotive
sector, shaping the future of mobility altogether. Although the integration of
novel technologies such as Artificial Intelligence (AI) and Cloud/Edge
computing provides golden opportunities to improve autonomous driving
applications, there is the need to modernize accordingly the whole prototyping
and deployment cycle of AI components. This paper proposes a novel framework
for developing so-called AI Inference Engines for autonomous driving
applications based on deep learning modules, where training tasks are deployed
elastically over both Cloud and Edge resources, with the purpose of reducing
the required network bandwidth, as well as mitigating privacy issues. Based on
our proposed data driven V-Model, we introduce a simple yet elegant solution
for the AI components development cycle, where prototyping takes place in the
cloud according to the Software-in-the-Loop (SiL) paradigm, while deployment
and evaluation on the target ECUs (Electronic Control Units) is performed as
Hardware-in-the-Loop (HiL) testing. The effectiveness of the proposed framework
is demonstrated using two real-world use-cases of AI inference engines for
autonomous vehicles, that is environment perception and most probable path
prediction.
</summary>
    <author>
      <name>Sorin Grigorescu</name>
    </author>
    <author>
      <name>Tiberiu Cocias</name>
    </author>
    <author>
      <name>Bogdan Trasnea</name>
    </author>
    <author>
      <name>Andrea Margheri</name>
    </author>
    <author>
      <name>Federico Lombardi</name>
    </author>
    <author>
      <name>Leonardo Aniello</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/s20195450</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/s20195450" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages Published in Sensors:
  https://www.mdpi.com/1424-8220/20/19/5450</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.11722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.11832v2</id>
    <updated>2021-01-31T08:23:05Z</updated>
    <published>2021-01-28T06:39:01Z</published>
    <title>Making Responsible AI the Norm rather than the Exception</title>
    <summary>  This report prepared by the Montreal AI Ethics Institute provides
recommendations in response to the National Security Commission on Artificial
Intelligence (NSCAI) Key Considerations for Responsible Development and
Fielding of Artificial Intelligence document. The report centres on the idea
that Responsible AI should be made the Norm rather than an Exception. It does
so by utilizing the guiding principles of: (1) alleviating friction in existing
workflows, (2) empowering stakeholders to get buy-in, and (3) conducting an
effective translation of abstract standards into actionable engineering
practices. After providing some overarching comments on the document from the
NSCAI, the report dives into the primary contribution of an actionable
framework to help operationalize the ideas presented in the document from the
NSCAI. The framework consists of: (1) a learning, knowledge, and information
exchange (LKIE), (2) the Three Ways of Responsible AI, (3) an
empirically-driven risk-prioritization matrix, and (4) achieving the right
level of complexity. All components reinforce each other to move from
principles to practice in service of making Responsible AI the norm rather than
the exception.
</summary>
    <author>
      <name>Abhishek Gupta</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Montreal AI Ethics Institute and Microsoft</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A report prepared by the Montreal AI Ethics Institute for the
  National Security Commission on Artificial Intelligence in response to their
  Key Considerations for Responsible Development and Fielding of Artificial
  Intelligence document; 26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.11832v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.11832v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.02437v2</id>
    <updated>2022-03-01T14:13:19Z</updated>
    <published>2021-02-04T06:39:31Z</published>
    <title>EUCA: the End-User-Centered Explainable AI Framework</title>
    <summary>  The ability to explain decisions to end-users is a necessity to deploy AI as
critical decision support. Yet making AI explainable to non-technical end-users
is a relatively ignored and challenging problem. To bridge the gap, we first
identify twelve end-user-friendly explanatory forms that do not require
technical knowledge to comprehend, including feature-, example-, and rule-based
explanations. We then instantiate the explanatory forms as prototyping cards in
four AI-assisted critical decision-making tasks, and conduct a user study to
co-design low-fidelity prototypes with 32 layperson participants. The results
confirm the relevance of using explanatory forms as building blocks of
explanations, and identify their proprieties - pros, cons, applicable
explanation goals, and design implications. The explanatory forms, their
proprieties, and prototyping supports (including a suggested prototyping
process, design templates and exemplars, and associated algorithms to actualize
explanatory forms) constitute the End-User-Centered explainable AI framework
EUCA, and is available at http://weinajin.github.io/end-user-xai . It serves as
a practical prototyping toolkit for HCI/AI practitioners and researchers to
understand user requirements and build end-user-centered explainable AI.
</summary>
    <author>
      <name>Weina Jin</name>
    </author>
    <author>
      <name>Jianyu Fan</name>
    </author>
    <author>
      <name>Diane Gromala</name>
    </author>
    <author>
      <name>Philippe Pasquier</name>
    </author>
    <author>
      <name>Ghassan Hamarneh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EUCA Framework, EUCA dataset (and accompanying code), and
  Supplementary Materials are available at:
  https://github.com/weinajin/end-user-xai</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.02437v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.02437v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.14561v2</id>
    <updated>2021-09-06T07:27:33Z</updated>
    <published>2021-03-25T08:34:35Z</published>
    <title>User-Oriented Smart General AI System under Causal Inference</title>
    <summary>  General AI system solves a wide range of tasks with high performance in an
automated fashion. The best general AI algorithm designed by one individual is
different from that devised by another. The best performance records achieved
by different users are also different. An inevitable component of general AI is
tacit knowledge that depends upon user-specific comprehension of task
information and individual model design preferences that are related to user
technical experiences. Tacit knowledge affects model performance but cannot be
automatically optimized in general AI algorithms. In this paper, we propose
User-Oriented Smart General AI System under Causal Inference, abbreviated as
UOGASuCI, where UOGAS represents User-Oriented General AI System and uCI means
under the framework of causal inference. User characteristics that have a
significant influence upon tacit knowledge can be extracted from observed model
training experiences of many users in external memory modules. Under the
framework of causal inference, we manage to identify the optimal value of user
characteristics that are connected with the best model performance designed by
users. We make suggestions to users about how different user characteristics
can improve the best model performance achieved by users. By recommending
updating user characteristics associated with individualized tacit knowledge
comprehension and technical preferences, UOGAS helps users design models with
better performance.
</summary>
    <author>
      <name>Huimin Peng</name>
    </author>
    <link href="http://arxiv.org/abs/2103.14561v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.14561v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04122v1</id>
    <updated>2021-04-09T00:41:11Z</updated>
    <published>2021-04-09T00:41:11Z</published>
    <title>Increasing the Speed and Accuracy of Data LabelingThrough an AI Assisted
  Interface</title>
    <summary>  Labeling data is an important step in the supervised machine learning
lifecycle. It is a laborious human activity comprised of repeated decision
making: the human labeler decides which of several potential labels to apply to
each example. Prior work has shown that providing AI assistance can improve the
accuracy of binary decision tasks. However, the role of AI assistance in more
complex data-labeling scenarios with a larger set of labels has not yet been
explored. We designed an AI labeling assistant that uses a semi-supervised
learning algorithm to predict the most probable labels for each example. We
leverage these predictions to provide assistance in two ways: (i) providing a
label recommendation and (ii) reducing the labeler's decision space by focusing
their attention on only the most probable labels. We conducted a user study
(n=54) to evaluate an AI-assisted interface for data labeling in this context.
Our results highlight that the AI assistance improves both labeler accuracy and
speed, especially when the labeler finds the correct label in the reduced label
space. We discuss findings related to the presentation of AI assistance and
design implications for intelligent labeling interfaces.
</summary>
    <author>
      <name>Michael Desmond</name>
    </author>
    <author>
      <name>Zahra Ashktorab</name>
    </author>
    <author>
      <name>Michelle Brachman</name>
    </author>
    <author>
      <name>Kristina Brimijoin</name>
    </author>
    <author>
      <name>Evelyn Duesterwald</name>
    </author>
    <author>
      <name>Casey Dugan</name>
    </author>
    <author>
      <name>Catherine Finegan-Dollak</name>
    </author>
    <author>
      <name>Michael Muller</name>
    </author>
    <author>
      <name>Narendra Nath Joshi</name>
    </author>
    <author>
      <name>Qian Pan</name>
    </author>
    <author>
      <name>Aabhas Sharma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3397481.3450698</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3397481.3450698" rel="related"/>
    <link href="http://arxiv.org/abs/2104.04122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.02407v1</id>
    <updated>2021-05-06T02:55:49Z</updated>
    <published>2021-05-06T02:55:49Z</published>
    <title>Reconfiguring Diversity and Inclusion for AI Ethics</title>
    <summary>  Activists, journalists, and scholars have long raised critical questions
about the relationship between diversity, representation, and structural
exclusions in data-intensive tools and services. We build on work mapping the
emergent landscape of corporate AI ethics to center one outcome of these
conversations: the incorporation of diversity and inclusion in corporate AI
ethics activities. Using interpretive document analysis and analytic tools from
the values in design field, we examine how diversity and inclusion work is
articulated in public-facing AI ethics documentation produced by three
companies that create application and services layer AI infrastructure: Google,
Microsoft, and Salesforce.
  We find that as these documents make diversity and inclusion more tractable
to engineers and technical clients, they reveal a drift away from civil rights
justifications that resonates with the managerialization of diversity by
corporations in the mid-1980s. The focus on technical artifacts, such as
diverse and inclusive datasets, and the replacement of equity with fairness
make ethical work more actionable for everyday practitioners. Yet, they appear
divorced from broader DEI initiatives and other subject matter experts that
could provide needed context to nuanced decisions around how to operationalize
these values. Finally, diversity and inclusion, as configured by engineering
logic, positions firms not as ethics owners but as ethics allocators; while
these companies claim expertise on AI ethics, the responsibility of defining
who diversity and inclusion are meant to protect and where it is relevant is
pushed downstream to their customers.
</summary>
    <author>
      <name>Nicole Chi</name>
    </author>
    <author>
      <name>Emma Lurie</name>
    </author>
    <author>
      <name>Deirdre K. Mulligan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/11952.107</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/11952.107" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AIES 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.02407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.02407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.00326v1</id>
    <updated>2021-06-01T09:01:14Z</updated>
    <published>2021-06-01T09:01:14Z</published>
    <title>AI-Ethics by Design. Evaluating Public Perception on the Importance of
  Ethical Design Principles of AI</title>
    <summary>  Despite the immense societal importance of ethically designing artificial
intelligence (AI), little research on the public perceptions of ethical AI
principles exists. This becomes even more striking when considering that
ethical AI development has the aim to be human-centric and of benefit for the
whole society. In this study, we investigate how ethical principles
(explainability, fairness, security, accountability, accuracy, privacy, machine
autonomy) are weighted in comparison to each other. This is especially
important, since simultaneously considering ethical principles is not only
costly, but sometimes even impossible, as developers must make specific
trade-off decisions. In this paper, we give first answers on the relative
importance of ethical principles given a specific use case - the use of AI in
tax fraud detection. The results of a large conjoint survey (n=1099) suggest
that, by and large, German respondents found the ethical principles equally
important. However, subsequent cluster analysis shows that different preference
models for ethically designed systems exist among the German population. These
clusters substantially differ not only in the preferred attributes, but also in
the importance level of the attributes themselves. We further describe how
these groups are constituted in terms of sociodemographics as well as opinions
on AI. Societal implications as well as design challenges are discussed.
</summary>
    <author>
      <name>Kimon Kieslich</name>
    </author>
    <author>
      <name>Birte Keller</name>
    </author>
    <author>
      <name>Christopher Starke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1177/20539517221092956</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1177/20539517221092956" rel="related"/>
    <link href="http://arxiv.org/abs/2106.00326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.00326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.11022v1</id>
    <updated>2021-06-10T09:49:34Z</updated>
    <published>2021-06-10T09:49:34Z</published>
    <title>Hard Choices in Artificial Intelligence</title>
    <summary>  As AI systems are integrated into high stakes social domains, researchers now
examine how to design and operate them in a safe and ethical manner. However,
the criteria for identifying and diagnosing safety risks in complex social
contexts remain unclear and contested. In this paper, we examine the vagueness
in debates about the safety and ethical behavior of AI systems. We show how
this vagueness cannot be resolved through mathematical formalism alone, instead
requiring deliberation about the politics of development as well as the context
of deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness
in terms of distinct design challenges at key stages in AI system development.
The resulting framework of Hard Choices in Artificial Intelligence (HCAI)
empowers developers by 1) identifying points of overlap between design
decisions and major sociotechnical challenges; 2) motivating the creation of
stakeholder feedback channels so that safety issues can be exhaustively
addressed. As such, HCAI contributes to a timely debate about the status of AI
development in democratic societies, arguing that deliberation should be the
goal of AI Safety, not just the procedure by which it is ensured.
</summary>
    <author>
      <name>Roel Dobbe</name>
    </author>
    <author>
      <name>Thomas Krendl Gilbert</name>
    </author>
    <author>
      <name>Yonatan Mintz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-print. Shorter versions published at Neurips 2019 Workshop on AI
  for Social Good and Conference on AI, Ethics and Society 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.11022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; K.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.14631v1</id>
    <updated>2021-06-28T12:34:16Z</updated>
    <published>2021-06-28T12:34:16Z</published>
    <title>Blockchain and AI-based Solutions to Combat Coronavirus (COVID-19)-like
  Epidemics: A Survey</title>
    <summary>  The beginning of 2020 has seen the emergence of coronavirus outbreak caused
by a novel virus called SARS-CoV-2. The sudden explosion and uncontrolled
worldwide spread of COVID-19 show the limitations of existing healthcare
systems in timely handling public health emergencies. In such contexts,
innovative technologies such as blockchain and Artificial Intelligence (AI)
have emerged as promising solutions for fighting coronavirus epidemic. In
particular, blockchain can combat pandemics by enabling early detection of
outbreaks, ensuring the ordering of medical data, and ensuring reliable medical
supply chain during the outbreak tracing. Moreover, AI provides intelligent
solutions for identifying symptoms caused by coronavirus for treatments and
supporting drug manufacturing. Therefore, we present an extensive survey on the
use of blockchain and AI for combating COVID-19 epidemics. First, we introduce
a new conceptual architecture which integrates blockchain and AI for fighting
COVID-19. Then, we survey the latest research efforts on the use of blockchain
and AI for fighting COVID-19 in various applications. The newly emerging
projects and use cases enabled by these technologies to deal with coronavirus
pandemic are also presented. A case study is also provided using federated AI
for COVID-19 detection. Finally, we point out challenges and future directions
that motivate more research efforts to deal with future coronavirus-like
epidemics.
</summary>
    <author>
      <name>Dinh C. Nguyen</name>
    </author>
    <author>
      <name>Ming Ding</name>
    </author>
    <author>
      <name>Pubudu N. Pathirana</name>
    </author>
    <author>
      <name>Aruna Seneviratne</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2021.3093633</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2021.3093633" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at IEEE Access Journal, 24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.14631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.14631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.06071v2</id>
    <updated>2021-11-15T06:15:07Z</updated>
    <published>2021-06-25T08:40:15Z</published>
    <title>aiSTROM -- A roadmap for developing a successful AI strategy</title>
    <summary>  A total of 34% of AI research and development projects fails or are
abandoned, according to a recent survey by Rackspace Technology of 1,870
companies. We propose a new strategic framework, aiSTROM, that empowers
managers to create a successful AI strategy based on a thorough literature
review. This provides a unique and integrated approach that guides managers and
lead developers through the various challenges in the implementation process.
In the aiSTROM framework, we start by identifying the top n potential projects
(typically 3-5). For each of those, seven areas of focus are thoroughly
analysed. These areas include creating a data strategy that takes into account
unique cross-departmental machine learning data requirements, security, and
legal requirements. aiSTROM then guides managers to think about how to put
together an interdisciplinary artificial intelligence (AI) implementation team
given the scarcity of AI talent. Once an AI team strategy has been established,
it needs to be positioned within the organization, either cross-departmental or
as a separate division. Other considerations include AI as a service (AIaas),
or outsourcing development. Looking at new technologies, we have to consider
challenges such as bias, legality of black-box-models, and keeping humans in
the loop. Next, like any project, we need value-based key performance
indicators (KPIs) to track and validate the progress. Depending on the
company's risk-strategy, a SWOT analysis (strengths, weaknesses, opportunities,
and threats) can help further classify the shortlisted projects. Finally, we
should make sure that our strategy includes continuous education of employees
to enable a culture of adoption. This unique and comprehensive framework offers
a valuable, literature supported, tool for managers and lead developers.
</summary>
    <author>
      <name>Dorien Herremans</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2021.3127548</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2021.3127548" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Access, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2107.06071v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.06071v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx, 97Pxx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.5; K.6; C.5; D.m; H.2; K.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.09776v1</id>
    <updated>2021-07-20T21:32:52Z</updated>
    <published>2021-07-20T21:32:52Z</published>
    <title>Anti-Integrability for 3-Dimensional Quadratic Maps</title>
    <summary>  We study the dynamics of the three-dimensional quadratic diffeomorphism using
a concept first introduced thirty years ago for the Frenkel-Kontorova model of
condensed matter physics: the anti-integrable (AI) limit. At the traditional AI
limit, orbits of a map degenerate to sequences of symbols and the dynamics is
reduced to the shift operator, a pure form of chaos. Under nondegeneracy
conditions, a contraction mapping argument can show that infinitely many AI
states continue to orbits of the deterministic map. For the 3D quadratic map,
the AI limit that we study is a quadratic correspondence whose branches, a pair
of one-dimensional maps, introduce symbolic dynamics on two symbols. The AI
states, however, are nontrivial orbits of this correspondence. The character of
these orbits depends on whether the quadratic takes the form of an ellipse, a
hyperbola, or a pair of lines. Using contraction arguments, we find parameter
domains for each case such that each symbol sequence corresponds to a unique AI
state. In some parameter domains, sufficient conditions are then found for each
such AI state to continue away from the limit to become an orbit of the
original 3D map. Numerical continuation methods extend these results, allowing
computation of bifurcations and obtaining orbits with horseshoe-like structures
and intriguing self-similarity. We conjecture that pairs of periodic orbits in
saddle-node or period doubling bifurcations have symbol sequences that differ
in exactly one position.
</summary>
    <author>
      <name>Amanda E Hampton</name>
    </author>
    <author>
      <name>James D Meiss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 34 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.09776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.09776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="37C05, 37D45, 39A33" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.13734v1</id>
    <updated>2021-07-29T03:57:53Z</updated>
    <published>2021-07-29T03:57:53Z</published>
    <title>An Ethical Framework for Guiding the Development of Affectively-Aware
  Artificial Intelligence</title>
    <summary>  The recent rapid advancements in artificial intelligence research and
deployment have sparked more discussion about the potential ramifications of
socially- and emotionally-intelligent AI. The question is not if research can
produce such affectively-aware AI, but when it will. What will it mean for
society when machines -- and the corporations and governments they serve -- can
"read" people's minds and emotions? What should developers and operators of
such AI do, and what should they not do? The goal of this article is to
pre-empt some of the potential implications of these developments, and propose
a set of guidelines for evaluating the (moral and) ethical consequences of
affectively-aware AI, in order to guide researchers, industry professionals,
and policy-makers. We propose a multi-stakeholder analysis framework that
separates the ethical responsibilities of AI Developers vis-\`a-vis the
entities that deploy such AI -- which we term Operators. Our analysis produces
two pillars that clarify the responsibilities of each of these stakeholders:
Provable Beneficence, which rests on proving the effectiveness of the AI, and
Responsible Stewardship, which governs responsible collection, use, and storage
of data and the decisions made from such data. We end with recommendations for
researchers, developers, operators, as well as regulators and law-makers.
</summary>
    <author>
      <name>Desmond C. Ong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at IEEE Affective Computing and Intelligent Interaction 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.13734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.13734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.01517v3</id>
    <updated>2022-12-09T02:54:44Z</updated>
    <published>2021-09-03T13:41:46Z</published>
    <title>A brief history of AI: how to prevent another winter (a critical review)</title>
    <summary>  The field of artificial intelligence (AI), regarded as one of the most
enigmatic areas of science, has witnessed exponential growth in the past decade
including a remarkably wide array of applications, having already impacted our
everyday lives. Advances in computing power and the design of sophisticated AI
algorithms have enabled computers to outperform humans in a variety of tasks,
especially in the areas of computer vision and speech recognition. Yet, AI's
path has never been smooth, having essentially fallen apart twice in its
lifetime ('winters' of AI), both after periods of popular success ('summers' of
AI). We provide a brief rundown of AI's evolution over the course of decades,
highlighting its crucial moments and major turning points from inception to the
present. In doing so, we attempt to learn, anticipate the future, and discuss
what steps may be taken to prevent another 'winter'.
</summary>
    <author>
      <name>Amirhosein Toosi</name>
    </author>
    <author>
      <name>Andrea Bottino</name>
    </author>
    <author>
      <name>Babak Saboury</name>
    </author>
    <author>
      <name>Eliot Siegel</name>
    </author>
    <author>
      <name>Arman Rahmim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cpet.2021.07.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cpet.2021.07.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 12 figures, 106 references, a Glossary section comes at the
  end of the paper, right after References. The article is accepted and going
  to be published by Elsevier, journal of PET - Clinics. Typos in the main text
  and in figure 3 fixed</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">PET clinics 16, no. 4 (2021): 449-469</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2109.01517v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.01517v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx, 68T01, 97P80" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; I.4.0; I.5.0; J.3; K.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.08880v1</id>
    <updated>2021-09-18T08:47:08Z</updated>
    <published>2021-09-18T08:47:08Z</published>
    <title>Computational Imaging and Artificial Intelligence: The Next Revolution
  of Mobile Vision</title>
    <summary>  Signal capture stands in the forefront to perceive and understand the
environment and thus imaging plays the pivotal role in mobile vision. Recent
explosive progresses in Artificial Intelligence (AI) have shown great potential
to develop advanced mobile platforms with new imaging devices. Traditional
imaging systems based on the "capturing images first and processing afterwards"
mechanism cannot meet this unprecedented demand. Differently, Computational
Imaging (CI) systems are designed to capture high-dimensional data in an
encoded manner to provide more information for mobile vision systems.Thanks to
AI, CI can now be used in real systems by integrating deep learning algorithms
into the mobile vision platform to achieve the closed loop of intelligent
acquisition, processing and decision making, thus leading to the next
revolution of mobile vision.Starting from the history of mobile vision using
digital cameras, this work first introduces the advances of CI in diverse
applications and then conducts a comprehensive review of current research
topics combining CI and AI. Motivated by the fact that most existing studies
only loosely connect CI and AI (usually using AI to improve the performance of
CI and only limited works have deeply connected them), in this work, we propose
a framework to deeply integrate CI and AI by using the example of self-driving
vehicles with high-speed communication, edge computing and traffic planning.
Finally, we outlook the future of CI plus AI by investigating new materials,
brain science and new computing techniques to shed light on new directions of
mobile vision systems.
</summary>
    <author>
      <name>Jinli Suo</name>
    </author>
    <author>
      <name>Weihang Zhang</name>
    </author>
    <author>
      <name>Jin Gong</name>
    </author>
    <author>
      <name>Xin Yuan</name>
    </author>
    <author>
      <name>David J. Brady</name>
    </author>
    <author>
      <name>Qionghai Dai</name>
    </author>
    <link href="http://arxiv.org/abs/2109.08880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.08880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.14728v1</id>
    <updated>2021-09-29T21:21:35Z</updated>
    <published>2021-09-29T21:21:35Z</published>
    <title>Collaborative Storytelling with Human Actors and AI Narrators</title>
    <summary>  Large language models can be used for collaborative storytelling. In this
work we report on using GPT-3 \cite{brown2020language} to co-narrate stories.
The AI system must track plot progression and character arcs while the human
actors perform scenes. This event report details how a novel conversational
agent was employed as creative partner with a team of professional improvisers
to explore long-form spontaneous story narration in front of a live public
audience. We introduced novel constraints on our language model to produce
longer narrative text and tested the model in rehearsals with a team of
professional improvisers. We then field tested the model with two live
performances for public audiences as part of a live theatre festival in Europe.
We surveyed audience members after each performance as well as performers to
evaluate how well the AI performed in its role as narrator. Audiences and
performers responded positively to AI narration and indicated preference for AI
narration over AI characters within a scene. Performers also responded
positively to AI narration and expressed enthusiasm for the creative and
meaningful novel narrative directions introduced to the scenes. Our findings
support improvisational theatre as a useful test-bed to explore how different
language models can collaborate with humans in a variety of social contexts.
</summary>
    <author>
      <name>Boyd Branch</name>
    </author>
    <author>
      <name>Piotr Mirowski</name>
    </author>
    <author>
      <name>Kory W. Mathewson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, accepted to ICCC as Short Paper: Event Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.14728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.14728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.08322v2</id>
    <updated>2021-11-09T15:00:23Z</updated>
    <published>2021-10-15T19:12:42Z</published>
    <title>Robustness of different loss functions and their impact on networks
  learning capability</title>
    <summary>  Recent developments in AI have made it ubiquitous, every industry is trying
to adopt some form of intelligent processing of their data. Despite so many
advances in the field, AIs full capability is yet to be exploited by the
industry. Industries that involve some risk factors still remain cautious about
the usage of AI due to the lack of trust in such autonomous systems.
Present-day AI might be very good in a lot of things but it is very bad in
reasoning and this behavior of AI can lead to catastrophic results. Autonomous
cars crashing into a person or a drone getting stuck in a tree are a few
examples where AI decisions lead to catastrophic results. To develop insight
and generate an explanation about the learning capability of AI, we will try to
analyze the working of loss functions. For our case, we will use two sets of
loss functions, generalized loss functions like Binary cross-entropy or BCE and
specialized loss functions like Dice loss or focal loss. Through a series of
experiments, we will establish whether combining different loss functions is
better than using a single loss function and if yes, then what is the reason
behind it. In order to establish the difference between generalized loss and
specialized losses, we will train several models using the above-mentioned
losses and then compare their robustness on adversarial examples. In
particular, we will look at how fast the accuracy of different models decreases
when we change the pixels corresponding to the most salient gradients.
</summary>
    <author>
      <name>Vishal Rajput</name>
    </author>
    <link href="http://arxiv.org/abs/2110.08322v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.08322v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.08637v1</id>
    <updated>2021-10-16T18:42:09Z</updated>
    <published>2021-10-16T18:42:09Z</published>
    <title>Conceptual Modeling and Artificial Intelligence: Mutual Benefits from
  Complementary Worlds</title>
    <summary>  Conceptual modeling (CM) applies abstraction to reduce the complexity of a
system under study (e.g., an excerpt of reality). As a result of the conceptual
modeling process a human interpretable, formalized representation (i.e., a
conceptual model) is derived which enables understanding and communication
among humans, and processing by machines. Artificial Intelligence (AI)
algorithms are also applied to complex realities (regularly represented by vast
amounts of data) to identify patterns or to classify entities in the data.
Aside from the commonalities of both approaches, a significant difference can
be observed by looking at the results. While conceptual models are
comprehensible, reproducible, and explicit knowledge representations, AI
techniques are capable of efficiently deriving an output from a given input
while acting as a black box. AI solutions often lack comprehensiveness and
reproducibility. Even the developers of AI systems can't explain why a certain
output is derived. In the Conceptual Modeling meets Artificial Intelligence
(CMAI) workshop, we are interested in tackling the intersection of the two,
thus far, mostly isolated approached disciplines of CM and AI. The workshop
embraces the assumption, that manifold mutual benefits can be realized by i)
investigating what Conceptual Modeling (CM) can contribute to AI, and ii) the
other way around, what Artificial Intelligence (AI) can contribute to CM.
</summary>
    <author>
      <name>Dominik Bork</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Editorial preface to the 3rd Int. Workshop on Conceptual Modeling
  Meets Artificial Intelligence (CMAI'2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.08637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.08637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.10332v4</id>
    <updated>2022-01-13T20:52:08Z</updated>
    <published>2021-10-20T01:05:47Z</published>
    <title>AI-Based Detection, Classification and Prediction/Prognosis in Medical
  Imaging: Towards Radiophenomics</title>
    <summary>  Artificial intelligence (AI) techniques have significant potential to enable
effective, robust and automated image phenotyping including identification of
subtle patterns. AI-based detection searches the image space to find the
regions of interest based on patterns and features. There is a spectrum of
tumor histologies from benign to malignant that can be identified by AI-based
classification approaches using image features. The extraction of minable
information from images gives way to the field of radiomics and can be explored
via explicit (handcrafted/engineered) and deep radiomics frameworks. Radiomics
analysis has the potential to be utilized as a noninvasive technique for the
accurate characterization of tumors to improve diagnosis and treatment
monitoring. This work reviews AI-based techniques, with a special focus on
oncological PET and PET/CT imaging, for different detection, classification,
and prediction/prognosis tasks. We also discuss needed efforts to enable the
translation of AI techniques to routine clinical workflows, and potential
improvements and complementary techniques such as the use of natural language
processing on electronic health records and neuro-symbolic AI techniques.
</summary>
    <author>
      <name>Fereshteh Yousefirizi</name>
    </author>
    <author>
      <name>Pierre Decazes</name>
    </author>
    <author>
      <name>Amine Amyar</name>
    </author>
    <author>
      <name>Su Ruan</name>
    </author>
    <author>
      <name>Babak Saboury</name>
    </author>
    <author>
      <name>Arman Rahmim</name>
    </author>
    <link href="http://arxiv.org/abs/2110.10332v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.10332v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.00364v2</id>
    <updated>2022-01-09T19:03:39Z</updated>
    <published>2021-10-30T23:36:10Z</published>
    <title>Sustainable AI: Environmental Implications, Challenges and Opportunities</title>
    <summary>  This paper explores the environmental impact of the super-linear growth
trends for AI from a holistic perspective, spanning Data, Algorithms, and
System Hardware. We characterize the carbon footprint of AI computing by
examining the model development cycle across industry-scale machine learning
use cases and, at the same time, considering the life cycle of system hardware.
Taking a step further, we capture the operational and manufacturing carbon
footprint of AI computing and present an end-to-end analysis for what and how
hardware-software design and at-scale optimization can help reduce the overall
carbon footprint of AI. Based on the industry experience and lessons learned,
we share the key challenges and chart out important development directions
across the many dimensions of AI. We hope the key messages and insights
presented in this paper can inspire the community to advance the field of AI in
an environmentally-responsible manner.
</summary>
    <author>
      <name>Carole-Jean Wu</name>
    </author>
    <author>
      <name>Ramya Raghavendra</name>
    </author>
    <author>
      <name>Udit Gupta</name>
    </author>
    <author>
      <name>Bilge Acun</name>
    </author>
    <author>
      <name>Newsha Ardalani</name>
    </author>
    <author>
      <name>Kiwan Maeng</name>
    </author>
    <author>
      <name>Gloria Chang</name>
    </author>
    <author>
      <name>Fiona Aga Behram</name>
    </author>
    <author>
      <name>James Huang</name>
    </author>
    <author>
      <name>Charles Bai</name>
    </author>
    <author>
      <name>Michael Gschwind</name>
    </author>
    <author>
      <name>Anurag Gupta</name>
    </author>
    <author>
      <name>Myle Ott</name>
    </author>
    <author>
      <name>Anastasia Melnikov</name>
    </author>
    <author>
      <name>Salvatore Candido</name>
    </author>
    <author>
      <name>David Brooks</name>
    </author>
    <author>
      <name>Geeta Chauhan</name>
    </author>
    <author>
      <name>Benjamin Lee</name>
    </author>
    <author>
      <name>Hsien-Hsin S. Lee</name>
    </author>
    <author>
      <name>Bugra Akyildiz</name>
    </author>
    <author>
      <name>Maximilian Balandat</name>
    </author>
    <author>
      <name>Joe Spisak</name>
    </author>
    <author>
      <name>Ravi Jain</name>
    </author>
    <author>
      <name>Mike Rabbat</name>
    </author>
    <author>
      <name>Kim Hazelwood</name>
    </author>
    <link href="http://arxiv.org/abs/2111.00364v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.00364v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.00826v4</id>
    <updated>2021-12-17T13:42:51Z</updated>
    <published>2021-11-01T10:58:35Z</published>
    <title>Reproducibility as a Mechanism for Teaching Fairness, Accountability,
  Confidentiality, and Transparency in Artificial Intelligence</title>
    <summary>  In this work, we explain the setup for a technical, graduate-level course on
Fairness, Accountability, Confidentiality, and Transparency in Artificial
Intelligence (FACT-AI) at the University of Amsterdam, which teaches FACT-AI
concepts through the lens of reproducibility. The focal point of the course is
a group project based on reproducing existing FACT-AI algorithms from top AI
conferences and writing a corresponding report. In the first iteration of the
course, we created an open source repository with the code implementations from
the group projects. In the second iteration, we encouraged students to submit
their group projects to the Machine Learning Reproducibility Challenge,
resulting in 9 reports from our course being accepted for publication in the
ReScience journal. We reflect on our experience teaching the course over two
years, where one year coincided with a global pandemic, and propose guidelines
for teaching FACT-AI through reproducibility in graduate-level AI study
programs. We hope this can be a useful resource for instructors who want to set
up similar courses in the future.
</summary>
    <author>
      <name>Ana Lucic</name>
    </author>
    <author>
      <name>Maurits Bleeker</name>
    </author>
    <author>
      <name>Sami Jullien</name>
    </author>
    <author>
      <name>Samarth Bhargav</name>
    </author>
    <author>
      <name>Maarten de Rijke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the AAAI Symposium on Educational Advances in AI (EAAI
  2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.00826v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.00826v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.02026v1</id>
    <updated>2021-11-03T05:30:42Z</updated>
    <published>2021-11-03T05:30:42Z</published>
    <title>The Powerful Use of AI in the Energy Sector: Intelligent Forecasting</title>
    <summary>  Artificial Intelligence (AI) techniques continue to broaden across
governmental and public sectors, such as power and energy - which serve as
critical infrastructures for most societal operations. However, due to the
requirements of reliability, accountability, and explainability, it is risky to
directly apply AI-based methods to power systems because society cannot afford
cascading failures and large-scale blackouts, which easily cost billions of
dollars. To meet society requirements, this paper proposes a methodology to
develop, deploy, and evaluate AI systems in the energy sector by: (1)
understanding the power system measurements with physics, (2) designing AI
algorithms to forecast the need, (3) developing robust and accountable AI
methods, and (4) creating reliable measures to evaluate the performance of the
AI model. The goal is to provide a high level of confidence to energy utility
users. For illustration purposes, the paper uses power system event forecasting
(PEF) as an example, which carefully analyzes synchrophasor patterns measured
by the Phasor Measurement Units (PMUs). Such a physical understanding leads to
a data-driven framework that reduces the dimensionality with physics and
forecasts the event with high credibility. Specifically, for dimensionality
reduction, machine learning arranges physical information from different
dimensions, resulting inefficient information extraction. For event
forecasting, the supervised learning model fuses the results of different
models to increase the confidence. Finally, comprehensive experiments
demonstrate the high accuracy, efficiency, and reliability as compared to other
state-of-the-art machine learning methods.
</summary>
    <author>
      <name>Erik Blasch</name>
    </author>
    <author>
      <name>Haoran Li</name>
    </author>
    <author>
      <name>Zhihao Ma</name>
    </author>
    <author>
      <name>Yang Weng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at AAAI FSS-21: Artificial Intelligence in Government and
  Public Sector, Washington, DC, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.02026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.02026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.00591v1</id>
    <updated>2021-12-01T16:03:06Z</updated>
    <published>2021-12-01T16:03:06Z</published>
    <title>AI Assurance using Causal Inference: Application to Public Policy</title>
    <summary>  Developing and implementing AI-based solutions help state and federal
government agencies, research institutions, and commercial companies enhance
decision-making processes, automate chain operations, and reduce the
consumption of natural and human resources. At the same time, most AI
approaches used in practice can only be represented as "black boxes" and suffer
from the lack of transparency. This can eventually lead to unexpected outcomes
and undermine trust in such systems. Therefore, it is crucial not only to
develop effective and robust AI systems, but to make sure their internal
processes are explainable and fair. Our goal in this chapter is to introduce
the topic of designing assurance methods for AI systems with high-impact
decisions using the example of the technology sector of the US economy. We
explain how these fields would benefit from revealing cause-effect
relationships between key metrics in the dataset by providing the causal
experiment on technology economics dataset. Several causal inference approaches
and AI assurance techniques are reviewed and the transformation of the data
into a graph-structured dataset is demonstrated.
</summary>
    <author>
      <name>Andrei Svetovidov</name>
    </author>
    <author>
      <name>Abdul Rahman</name>
    </author>
    <author>
      <name>Feras A. Batarseh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Chapter 8 in book: AI Assurance, by Elsevier Academic Press. Edited
  by: Feras A. Batarseh and Laura Freeman Publication year: 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.00591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.00591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.07669v2</id>
    <updated>2022-10-27T02:35:15Z</updated>
    <published>2021-12-13T19:00:00Z</published>
    <title>AI and extreme scale computing to learn and infer the physics of higher
  order gravitational wave modes of quasi-circular, spinning, non-precessing
  binary black hole mergers</title>
    <summary>  We use artificial intelligence (AI) to learn and infer the physics of higher
order gravitational wave modes of quasi-circular, spinning, non precessing
binary black hole mergers. We trained AI models using 14 million waveforms,
produced with the surrogate model NRHybSur3dq8, that include modes up to $\ell
\leq 4$ and $(5,5)$, except for $(4,0)$ and $(4,1)$, that describe binaries
with mass-ratios $q\leq8$, individual spins $s^z_{\{1,2\}}\in[-0.8, 0.8]$, and
inclination angle $\theta\in[0,\pi]$.Our probabilistic AI surrogates can
accurately constrain the mass-ratio, individual spins, effective spin, and
inclination angle of numerical relativity waveforms that describe such signal
manifold. We compared the predictions of our AI models with Gaussian process
regression, random forest, k-nearest neighbors, and linear regression, and with
traditional Bayesian inference methods through the PyCBC Inference toolkit,
finding that AI outperforms all these approaches in terms of accuracy, and are
between three to four orders of magnitude faster than traditional Bayesian
inference methods. Our AI surrogates were trained within 3.4 hours using
distributed training on 1,536 NVIDIA V100 GPUs in the Summit supercomputer.
</summary>
    <author>
      <name>Asad Khan</name>
    </author>
    <author>
      <name>E. A. Huerta</name>
    </author>
    <author>
      <name>Prayush Kumar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physletb.2022.137505</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physletb.2022.137505" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 12 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Physics Letters B, Volume 835, 10 December 2022, 137505</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2112.07669v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.07669v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10, 85-08, 83C35, 83C57" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.11191v1</id>
    <updated>2021-12-07T11:58:51Z</updated>
    <published>2021-12-07T11:58:51Z</published>
    <title>Developing a Trusted Human-AI Network for Humanitarian Benefit</title>
    <summary>  Humans and artificial intelligences (AI) will increasingly participate
digitally and physically in conflicts, yet there is a lack of trusted
communications across agents and platforms. For example, humans in disasters
and conflict already use messaging and social media to share information,
however, international humanitarian relief organisations treat this information
as unverifiable and untrustworthy. AI may reduce the 'fog-of-war' and improve
outcomes, however AI implementations are often brittle, have a narrow scope of
application and wide ethical risks. Meanwhile, human error causes significant
civilian harms even by combatants committed to complying with international
humanitarian law. AI offers an opportunity to help reduce the tragedy of war
and deliver humanitarian aid to those who need it. In this paper we consider
the integration of a communications protocol (the 'Whiteflag protocol'),
distributed ledger technology, and information fusion with artificial
intelligence (AI), to improve conflict communications called 'Protected
Assurance Understanding Situation and Entities' (PAUSE). Such a trusted
human-AI communication network could provide accountable information exchange
regarding protected entities, critical infrastructure; humanitarian signals and
status updates for humans and machines in conflicts.
</summary>
    <author>
      <name>Susannah Kate Devitt</name>
    </author>
    <author>
      <name>Jason Scholz</name>
    </author>
    <author>
      <name>Timo Schless</name>
    </author>
    <author>
      <name>Larry Lewis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 7 figures, 2 boxes, submitted for peer review to the
  Journal of Digital War, My War Special Issue</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.11191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.11191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.05371v1</id>
    <updated>2022-01-14T10:21:51Z</updated>
    <published>2022-01-14T10:21:51Z</published>
    <title>Artificial Intelligence in Software Testing : Impact, Problems,
  Challenges and Prospect</title>
    <summary>  Artificial Intelligence (AI) is making a significant impact in multiple areas
like medical, military, industrial, domestic, law, arts as AI is capable to
perform several roles such as managing smart factories, driving autonomous
vehicles, creating accurate weather forecasts, detecting cancer and personal
assistants, etc. Software testing is the process of putting the software to
test for some abnormal behaviour of the software. Software testing is a
tedious, laborious and most time-consuming process. Automation tools have been
developed that help to automate some activities of the testing process to
enhance quality and timely delivery. Over time with the inclusion of continuous
integration and continuous delivery (CI/CD) pipeline, automation tools are
becoming less effective. The testing community is turning to AI to fill the gap
as AI is able to check the code for bugs and errors without any human
intervention and in a much faster way than humans. In this study, we aim to
recognize the impact of AI technologies on various software testing activities
or facets in the STLC. Further, the study aims to recognize and explain some of
the biggest challenges software testers face while applying AI to testing. The
paper also proposes some key contributions of AI in the future to the domain of
software testing.
</summary>
    <author>
      <name>Zubair Khaliq</name>
    </author>
    <author>
      <name>Sheikh Umar Farooq</name>
    </author>
    <author>
      <name>Dawood Ashraf Khan</name>
    </author>
    <link href="http://arxiv.org/abs/2201.05371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.05371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.04565v1</id>
    <updated>2022-02-09T16:56:22Z</updated>
    <published>2022-02-09T16:56:22Z</published>
    <title>Precision Radiotherapy via Information Integration of Expert Human
  Knowledge and AI Recommendation to Optimize Clinical Decision Making</title>
    <summary>  In the precision medicine era, there is a growing need for precision
radiotherapy where the planned radiation dose needs to be optimally determined
by considering a myriad of patient-specific information in order to ensure
treatment efficacy. Existing artificial-intelligence (AI) methods can recommend
radiation dose prescriptions within the scope of this available information.
However, treating physicians may not fully entrust the AI's recommended
prescriptions due to known limitations or when the AI recommendation may go
beyond physicians' current knowledge. This paper lays out a systematic method
to integrate expert human knowledge with AI recommendations for optimizing
clinical decision making. Towards this goal, Gaussian process (GP) models are
integrated with deep neural networks (DNNs) to quantify the uncertainty of the
treatment outcomes given by physicians and AI recommendations, respectively,
which are further used as a guideline to educate clinical physicians and
improve AI models performance. The proposed method is demonstrated in a
comprehensive dataset where patient-specific information and treatment outcomes
are prospectively collected during radiotherapy of $67$ non-small cell lung
cancer patients and retrospectively analyzed.
</summary>
    <author>
      <name>Wenbo Sun</name>
    </author>
    <author>
      <name>Dipesh Niraula</name>
    </author>
    <author>
      <name>Issam El Naqa</name>
    </author>
    <author>
      <name>Randall K Ten Haken</name>
    </author>
    <author>
      <name>Ivo D Dinov</name>
    </author>
    <author>
      <name>Kyle Cuneo</name>
    </author>
    <author>
      <name>Judy Jin</name>
    </author>
    <link href="http://arxiv.org/abs/2202.04565v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.04565v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.08510v3</id>
    <updated>2022-07-13T12:14:48Z</updated>
    <published>2022-02-17T08:33:52Z</published>
    <title>Multi-scale Hybrid Vision Transformer for Learning Gastric Cancer
  Histology</title>
    <summary>  Gastric endoscopic screening is an effective way to decide appropriate
gastric cancer (GC) treatment at an early stage, reducing GC-associated
mortality rate. Although artificial intelligence (AI) has brought a great
promise to assist pathologist to screen digitalized whole slide images,
existing AI systems are limited in fine-grained cancer subclassifications and
have little usability in planning cancer treatment. We propose a practical AI
system that enables five subclassifications of GC pathology, which can be
directly matched to general GC treatment guidance. The AI system is designed to
efficiently differentiate multi-classes of GC through multi-scale
self-attention mechanism using 2-stage hybrid Vision Transformer (ViT)
networks, by mimicking the way how human pathologists understand histology. The
AI system demonstrates reliable diagnostic performance by achieving
class-average sensitivity of above 0.85 on a total of 1,212 slides from
multicentric cohort. Furthermore, AI-assisted pathologists show significantly
improved diagnostic sensitivity by 12% in addition to 18% reduced screening
time compared to human pathologists. Our results demonstrate that AI-assisted
gastric endoscopic screening has a great potential for providing presumptive
pathologic opinion and appropriate cancer treatment of gastric cancer in
practical clinical settings.
</summary>
    <author>
      <name>Yujin Oh</name>
    </author>
    <author>
      <name>Go Eun Bae</name>
    </author>
    <author>
      <name>Kyung-Hee Kim</name>
    </author>
    <author>
      <name>Min-Kyung Yeo</name>
    </author>
    <author>
      <name>Jong Chul Ye</name>
    </author>
    <link href="http://arxiv.org/abs/2202.08510v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.08510v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.10336v1</id>
    <updated>2022-02-15T03:34:56Z</updated>
    <published>2022-02-15T03:34:56Z</published>
    <title>Artificial Intelligence for the Metaverse: A Survey</title>
    <summary>  Along with the massive growth of the Internet from the 1990s until now,
various innovative technologies have been created to bring users breathtaking
experiences with more virtual interactions in cyberspace. Many virtual
environments with thousands of services and applications, from social networks
to virtual gaming worlds, have been developed with immersive experience and
digital transformation, but most are incoherent instead of being integrated
into a platform. In this context, metaverse, a term formed by combining meta
and universe, has been introduced as a shared virtual world that is fueled by
many emerging technologies, such as fifth-generation networks and beyond,
virtual reality, and artificial intelligence (AI). Among such technologies, AI
has shown the great importance of processing big data to enhance immersive
experience and enable human-like intelligence of virtual agents. In this
survey, we make a beneficial effort to explore the role of AI in the foundation
and development of the metaverse. We first deliver a preliminary of AI,
including machine learning algorithms and deep learning architectures, and its
role in the metaverse. We then convey a comprehensive investigation of AI-based
methods concerning six technical aspects that have potentials for the
metaverse: natural language processing, machine vision, blockchain, networking,
digital twin, and neural interface, and being potential for the metaverse.
Subsequently, several AI-aided applications, such as healthcare, manufacturing,
smart cities, and gaming, are studied to be deployed in the virtual worlds.
Finally, we conclude the key contribution of this survey and open some future
research directions in AI for the metaverse.
</summary>
    <author>
      <name>Thien Huynh-The</name>
    </author>
    <author>
      <name>Quoc-Viet Pham</name>
    </author>
    <author>
      <name>Xuan-Qui Pham</name>
    </author>
    <author>
      <name>Thanh Thi Nguyen</name>
    </author>
    <author>
      <name>Zhu Han</name>
    </author>
    <author>
      <name>Dong-Seong Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2202.10336v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.10336v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.01556v2</id>
    <updated>2022-06-15T12:51:38Z</updated>
    <published>2022-03-03T08:12:15Z</published>
    <title>DareFightingICE Competition: A Fighting Game Sound Design and AI
  Competition</title>
    <summary>  This paper presents a new competition -- at the 2022 IEEE Conference on Games
(CoG) -- called DareFightingICE Competition. The competition has two tracks: a
sound design track and an AI track. The game platform for this competition is
also called DareFightingICE, a fighting game platform. DareFightingICE is a
sound-design-enhanced version of FightingICE, used earlier in a competition at
CoG until 2021 to promote artificial intelligence (AI) research in fighting
games. In the sound design track, participants compete for the best sound
design, given the default sound design of DareFightingICE as a sample, where we
define a sound design as a set of sound effects combined with the source code
that implements their timing-control algorithm. Participants of the AI track
are asked to develop their AI algorithm that controls a character given only
sound as the input (blind AI) to fight against their opponent; a sample
deep-learning blind AI will be provided by us. Our means to maximize the
synergy between the two tracks are also described. This competition serves to
come up with effective sound designs for visually impaired players, a group in
the gaming community which has been mostly ignored. To the best of our
knowledge, DareFightingICE Competition is the first of its kind within and
outside of CoG.
</summary>
    <author>
      <name>Ibrahim Khan</name>
    </author>
    <author>
      <name>Thai Van Nguyen</name>
    </author>
    <author>
      <name>Xincheng Dai</name>
    </author>
    <author>
      <name>Ruck Thawonmas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2022 IEEE Conference on Games</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.01556v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.01556v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; H.5.2; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.08465v1</id>
    <updated>2022-03-16T08:45:52Z</updated>
    <published>2022-03-16T08:45:52Z</published>
    <title>Building AI Innovation Labs together with Companies</title>
    <summary>  In the future, most companies will be confronted with the topic of Artificial
Intelligence (AI) and will have to decide on their strategy in this regards.
Currently, a lot of companies are thinking about whether and how AI and the
usage of data will impact their business model and what potential use cases
could look like. One of the biggest challenges lies in coming up with
innovative solution ideas with a clear business value. This requires business
competencies on the one hand and technical competencies in AI and data
analytics on the other hand. In this article, we present the concept of AI
innovation labs and demonstrate a comprehensive framework, from coming up with
the right ideas to incrementally implementing and evaluating them regarding
their business value and their feasibility based on a company's capabilities.
The concept is the result of nine years of working on data-driven innovations
with companies from various domains. Furthermore, we share some lessons learned
from its practical applications. Even though a lot of technical publications
can be found in the literature regarding the development of AI models and many
consultancy companies provide corresponding services for building AI
innovations, we found very few publications sharing details about what an
end-to-end framework could look like.
</summary>
    <author>
      <name>Jens Heidrich</name>
    </author>
    <author>
      <name>Andreas Jedlitschka</name>
    </author>
    <author>
      <name>Adam Trendowicz</name>
    </author>
    <author>
      <name>Anna Maria Vollmer</name>
    </author>
    <link href="http://arxiv.org/abs/2203.08465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.08465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68N30" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; D.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.01128v1</id>
    <updated>2022-05-02T18:00:10Z</updated>
    <published>2022-05-02T18:00:10Z</published>
    <title>Neurocompositional computing: From the Central Paradox of Cognition to a
  new generation of AI systems</title>
    <summary>  What explains the dramatic progress from 20th-century to 21st-century AI, and
how can the remaining limitations of current AI be overcome? The widely
accepted narrative attributes this progress to massive increases in the
quantity of computational and data resources available to support statistical
learning in deep artificial neural networks. We show that an additional crucial
factor is the development of a new type of computation. Neurocompositional
computing adopts two principles that must be simultaneously respected to enable
human-level cognition: the principles of Compositionality and Continuity. These
have seemed irreconcilable until the recent mathematical discovery that
compositionality can be realized not only through discrete methods of symbolic
computing, but also through novel forms of continuous neural computing. The
revolutionary recent progress in AI has resulted from the use of limited forms
of neurocompositional computing. New, deeper forms of neurocompositional
computing create AI systems that are more robust, accurate, and comprehensible.
</summary>
    <author>
      <name>Paul Smolensky</name>
    </author>
    <author>
      <name>R. Thomas McCoy</name>
    </author>
    <author>
      <name>Roland Fernandez</name>
    </author>
    <author>
      <name>Matthew Goldrick</name>
    </author>
    <author>
      <name>Jianfeng Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 6 figures. For a general AI audience: to appear in AI
  Magazine. A more extensive presentation of this work is "Neurocompositional
  computing in human and machine intelligence: A tutorial", Microsoft Technical
  Report MSR-TR-2022-5; see
  https://www.microsoft.com/en-us/research/publication/neurocompositional-computing-in-human-and-machine-intelligence-a-tutorial/</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.01128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.01128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.03215v1</id>
    <updated>2022-05-11T12:07:47Z</updated>
    <published>2022-05-11T12:07:47Z</published>
    <title>Proteção intelectual de obras produzidas por sistemas baseados em
  inteligência artificial: uma visão tecnicista sobre o tema</title>
    <summary>  The pervasiveness of Artificial Intelligence (AI) is unquestionable in our
society. Even in the arts, AI is present. A notorious case is the song "Hey
Ya!" of the OutKast group, successful in the 2000s. At this time, the music
industry began to make decisions based on data to strategize based on
predictions of listeners' habits. This case is just one of the countless
examples of AI applications in the arts. The advent of deep learning made it
possible to build systems capable of accurately recognizing artistic style in
paintings. Content generation is also possible; for example, Deepart customizes
images from two \textit{inputs}: 1) an image to be customized; 2) a style of
painting. The generation of songs according to specific styles from AI-based
systems is also possible. Such possibilities raise questions about the
intellectual property of such works. On this occasion, who owns the copyright
of a work produced from a system based on Artificial Intelligence? To the
creator of the AI? The company/corporation that subsidized the development of
this system? Or AI itself as a creator? This essay aims to contribute with a
technicist view on the discussion of copyright applicability from works
produced by AI.
</summary>
    <author>
      <name>Fábio Manoel França Lobato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Portuguese language. Texto publicado pelo Instituto Observat\'orio
  de Direito Autoral, dispon\'ivel em:
  https://ioda.org.br/protecao-intelectual-de-obras-produzidas-por-sistemas-baseados-em-inteligencia-artificial-uma-visao-tecnicista-sobre-o-tema/</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.03215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.03215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.03262v3</id>
    <updated>2022-11-28T11:34:36Z</updated>
    <published>2022-05-17T07:39:25Z</published>
    <title>Using sensitive data to prevent discrimination by artificial
  intelligence: Does the GDPR need a new exception?</title>
    <summary>  Organisations can use artificial intelligence to make decisions about people
for a variety of reasons, for instance, to select the best candidates from many
job applications. However, AI systems can have discriminatory effects when used
for decision-making. To illustrate, an AI system could reject applications of
people with a certain ethnicity, while the organisation did not plan such
ethnicity discrimination. But in Europe, an organisation runs into a problem
when it wants to assess whether its AI system accidentally discriminates based
on ethnicity: the organisation may not know the applicants' ethnicity. In
principle, the GDPR bans the use of certain 'special categories of data'
(sometimes called 'sensitive data'), which include data on ethnicity, religion,
and sexual preference. The proposal for an AI Act of the European Commission
includes a provision that would enable organisations to use special categories
of data for auditing their AI systems. This paper asks whether the GDPR's rules
on special categories of personal data hinder the prevention of AI-driven
discrimination. We argue that the GDPR does prohibit such use of special
category data in many circumstances. We also map out the arguments for and
against creating an exception to the GDPR's ban on using special categories of
personal data, to enable preventing discrimination by AI systems. The paper
discusses European law, but the paper can be relevant outside Europe too, as
many policymakers in the world grapple with the tension between privacy and
non-discrimination policy.
</summary>
    <author>
      <name>Marvin van Bekkum</name>
    </author>
    <author>
      <name>Frederik Zuiderveen Borgesius</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.clsr.2022.105770</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.clsr.2022.105770" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Law &amp; Security Review 48 (2023) 105770</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2206.03262v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.03262v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.00545v1</id>
    <updated>2022-08-01T00:01:17Z</updated>
    <published>2022-08-01T00:01:17Z</published>
    <title>Distributed Intelligence in Wireless Networks</title>
    <summary>  The cloud-based solutions are becoming inefficient due to considerably large
time delays, high power consumption, security and privacy concerns caused by
billions of connected wireless devices and typically zillions bytes of data
they produce at the network edge. A blend of edge computing and Artificial
Intelligence (AI) techniques could optimally shift the resourceful computation
servers closer to the network edge, which provides the support for advanced AI
applications (e.g., video/audio surveillance and personal recommendation
system) by enabling intelligent decision making on computing at the point of
data generation as and when it is needed, and distributed Machine Learning (ML)
with its potential to avoid the transmission of large dataset and possible
compromise of privacy that may exist in cloud-based centralized learning.
Therefore, AI is envisioned to become native and ubiquitous in future
communication and networking systems. In this paper, we conduct a comprehensive
overview of recent advances in distributed intelligence in wireless networks
under the umbrella of native-AI wireless networks, with a focus on the basic
concepts of native-AI wireless networks, on the AI-enabled edge computing, on
the design of distributed learning architectures for heterogeneous networks, on
the communication-efficient technologies to support distributed learning, and
on the AI-empowered end-to-end communications. We highlight the advantages of
hybrid distributed learning architectures compared to the state-of-art
distributed learning techniques. We summarize the challenges of existing
research contributions in distributed intelligence in wireless networks and
identify the potential future opportunities.
</summary>
    <author>
      <name>Xiaolan Liu</name>
    </author>
    <author>
      <name>Jiadong Yu</name>
    </author>
    <author>
      <name>Yuanwei Liu</name>
    </author>
    <author>
      <name>Yue Gao</name>
    </author>
    <author>
      <name>Toktam Mahmoodi</name>
    </author>
    <author>
      <name>Sangarapillai Lambotharan</name>
    </author>
    <author>
      <name>Danny H. K. Tsang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE Communications Surveys &amp; Tutorials</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.00545v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.00545v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.09966v1</id>
    <updated>2022-08-21T21:24:42Z</updated>
    <published>2022-08-21T21:24:42Z</published>
    <title>Performance, Opaqueness, Consequences, and Assumptions: Simple questions
  for responsible planning of machine learning solutions</title>
    <summary>  The data revolution has generated a huge demand for data-driven solutions.
This demand propels a growing number of easy-to-use tools and training for
aspiring data scientists that enable the rapid building of predictive models.
Today, weapons of math destruction can be easily built and deployed without
detailed planning and validation. This rapidly extends the list of AI failures,
i.e. deployments that lead to financial losses or even violate democratic
values such as equality, freedom and justice. The lack of planning, rules and
standards around the model development leads to the ,,anarchisation of AI".
This problem is reported under different names such as validation debt,
reproducibility crisis, and lack of explainability. Post-mortem analysis of AI
failures often reveals mistakes made in the early phase of model development or
data acquisition. Thus, instead of curing the consequences of deploying harmful
models, we shall prevent them as early as possible by putting more attention to
the initial planning stage.
  In this paper, we propose a quick and simple framework to support planning of
AI solutions. The POCA framework is based on four pillars: Performance,
Opaqueness, Consequences, and Assumptions. It helps to set the expectations and
plan the constraints for the AI solution before any model is built and any data
is collected. With the help of the POCA method, preliminary requirements can be
defined for the model-building process, so that costly model misspecification
errors can be identified as soon as possible or even avoided. AI researchers,
product owners and business analysts can use this framework in the initial
stages of building AI solutions.
</summary>
    <author>
      <name>Przemyslaw Biecek</name>
    </author>
    <link href="http://arxiv.org/abs/2208.09966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.00881v1</id>
    <updated>2022-09-23T14:04:37Z</updated>
    <published>2022-09-23T14:04:37Z</published>
    <title>Predicting the Future of AI with AI: High-quality link prediction in an
  exponentially growing knowledge network</title>
    <summary>  A tool that could suggest new personalized research directions and ideas by
taking insights from the scientific literature could significantly accelerate
the progress of science. A field that might benefit from such an approach is
artificial intelligence (AI) research, where the number of scientific
publications has been growing exponentially over the last years, making it
challenging for human researchers to keep track of the progress. Here, we use
AI techniques to predict the future research directions of AI itself. We
develop a new graph-based benchmark based on real-world data -- the
Science4Cast benchmark, which aims to predict the future state of an evolving
semantic network of AI. For that, we use more than 100,000 research papers and
build up a knowledge network with more than 64,000 concept nodes. We then
present ten diverse methods to tackle this task, ranging from pure statistical
to pure learning methods. Surprisingly, the most powerful methods use a
carefully curated set of network features, rather than an end-to-end AI
approach. It indicates a great potential that can be unleashed for purely ML
approaches without human knowledge. Ultimately, better predictions of new
future research directions will be a crucial component of more advanced
research suggestion tools.
</summary>
    <author>
      <name>Mario Krenn</name>
    </author>
    <author>
      <name>Lorenzo Buffoni</name>
    </author>
    <author>
      <name>Bruno Coutinho</name>
    </author>
    <author>
      <name>Sagi Eppel</name>
    </author>
    <author>
      <name>Jacob Gates Foster</name>
    </author>
    <author>
      <name>Andrew Gritsevskiy</name>
    </author>
    <author>
      <name>Harlin Lee</name>
    </author>
    <author>
      <name>Yichao Lu</name>
    </author>
    <author>
      <name>Joao P. Moutinho</name>
    </author>
    <author>
      <name>Nima Sanjabi</name>
    </author>
    <author>
      <name>Rishi Sonthalia</name>
    </author>
    <author>
      <name>Ngoc Mai Tran</name>
    </author>
    <author>
      <name>Francisco Valente</name>
    </author>
    <author>
      <name>Yangxinyu Xie</name>
    </author>
    <author>
      <name>Rose Yu</name>
    </author>
    <author>
      <name>Michael Kopp</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures. Comments welcome!</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.00881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.00881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.01797v1</id>
    <updated>2022-10-01T01:41:17Z</updated>
    <published>2022-10-01T01:41:17Z</published>
    <title>Ten Years after ImageNet: A 360° Perspective on AI</title>
    <summary>  It is ten years since neural networks made their spectacular comeback.
Prompted by this anniversary, we take a holistic perspective on Artificial
Intelligence (AI). Supervised Learning for cognitive tasks is effectively
solved - provided we have enough high-quality labeled data. However, deep
neural network models are not easily interpretable, and thus the debate between
blackbox and whitebox modeling has come to the fore. The rise of attention
networks, self-supervised learning, generative modeling, and graph neural
networks has widened the application space of AI. Deep Learning has also
propelled the return of reinforcement learning as a core building block of
autonomous decision making systems. The possible harms made possible by new AI
technologies have raised socio-technical issues such as transparency, fairness,
and accountability. The dominance of AI by Big-Tech who control talent,
computing resources, and most importantly, data may lead to an extreme AI
divide. Failure to meet high expectations in high profile, and much heralded
flagship projects like self-driving vehicles could trigger another AI winter.
</summary>
    <author>
      <name>Sanjay Chawla</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <author>
      <name>Ahmed Ali</name>
    </author>
    <author>
      <name>Wendy Hall</name>
    </author>
    <author>
      <name>Issa Khalil</name>
    </author>
    <author>
      <name>Xiaosong Ma</name>
    </author>
    <author>
      <name>Husrev Taha Sencar</name>
    </author>
    <author>
      <name>Ingmar Weber</name>
    </author>
    <author>
      <name>Michael Wooldridge</name>
    </author>
    <author>
      <name>Ting Yu</name>
    </author>
    <link href="http://arxiv.org/abs/2210.01797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.01797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.15236v1</id>
    <updated>2022-10-27T07:47:50Z</updated>
    <published>2022-10-27T07:47:50Z</published>
    <title>Painting the black box white: experimental findings from applying XAI to
  an ECG reading setting</title>
    <summary>  The shift from symbolic AI systems to black-box, sub-symbolic, and
statistical ones has motivated a rapid increase in the interest toward
explainable AI (XAI), i.e. approaches to make black-box AI systems explainable
to human decision makers with the aim of making these systems more acceptable
and more usable tools and supports. However, we make the point that, rather
than always making black boxes transparent, these approaches are at risk of
\emph{painting the black boxes white}, thus failing to provide a level of
transparency that would increase the system's usability and comprehensibility;
or, even, at risk of generating new errors, in what we termed the
\emph{white-box paradox}. To address these usability-related issues, in this
work we focus on the cognitive dimension of users' perception of explanations
and XAI systems. To this aim, we designed and conducted a questionnaire-based
experiment by which we involved 44 cardiology residents and specialists in an
AI-supported ECG reading task. In doing so, we investigated different research
questions concerning the relationship between users' characteristics (e.g.
expertise) and their perception of AI and XAI systems, including their trust,
the perceived explanations' quality and their tendency to defer the decision
process to automation (i.e. technology dominance), as well as the mutual
relationships among these different dimensions. Our findings provide a
contribution to the evaluation of AI-based support systems from a Human-AI
interaction-oriented perspective and lay the ground for further investigation
of XAI and its effects on decision making and user experience.
</summary>
    <author>
      <name>Federico Cabitza</name>
    </author>
    <author>
      <name>Matteo Cameli</name>
    </author>
    <author>
      <name>Andrea Campagner</name>
    </author>
    <author>
      <name>Chiara Natali</name>
    </author>
    <author>
      <name>Luca Ronzio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.15236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.15236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1; H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.17087v1</id>
    <updated>2022-10-31T06:29:08Z</updated>
    <published>2022-10-31T06:29:08Z</published>
    <title>DanZero: Mastering GuanDan Game with Reinforcement Learning</title>
    <summary>  Card game AI has always been a hot topic in the research of artificial
intelligence. In recent years, complex card games such as Mahjong, DouDizhu and
Texas Hold'em have been solved and the corresponding AI programs have reached
the level of human experts. In this paper, we are devoted to developing an AI
program for a more complex card game, GuanDan, whose rules are similar to
DouDizhu but much more complicated. To be specific, the characteristics of
large state and action space, long length of one episode and the unsure number
of players in the GuanDan pose great challenges for the development of the AI
program. To address these issues, we propose the first AI program DanZero for
GuanDan using reinforcement learning technique. Specifically, we utilize a
distributed framework to train our AI system. In the actor processes, we
carefully design the state features and agents generate samples by self-play.
In the learner process, the model is updated by Deep Monte-Carlo Method. After
training for 30 days using 160 CPUs and 1 GPU, we get our DanZero bot. We
compare it with 8 baseline AI programs which are based on heuristic rules and
the results reveal the outstanding performance of DanZero. We also test DanZero
with human players and demonstrate its human-level performance.
</summary>
    <author>
      <name>Yudong Lu</name>
    </author>
    <author>
      <name>Jian Zhao</name>
    </author>
    <author>
      <name>Youpeng Zhao</name>
    </author>
    <author>
      <name>Wengang Zhou</name>
    </author>
    <author>
      <name>Houqiang Li</name>
    </author>
    <link href="http://arxiv.org/abs/2210.17087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.17087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.00192v1</id>
    <updated>2022-10-31T23:22:54Z</updated>
    <published>2022-10-31T23:22:54Z</published>
    <title>AI Assistants: A Framework for Semi-Automated Data Wrangling</title>
    <summary>  Data wrangling tasks such as obtaining and linking data from various sources,
transforming data formats, and correcting erroneous records, can constitute up
to 80% of typical data engineering work. Despite the rise of machine learning
and artificial intelligence, data wrangling remains a tedious and manual task.
We introduce AI assistants, a class of semi-automatic interactive tools to
streamline data wrangling. An AI assistant guides the analyst through a
specific data wrangling task by recommending a suitable data transformation
that respects the constraints obtained through interaction with the analyst.
  We formally define the structure of AI assistants and describe how existing
tools that treat data cleaning as an optimization problem fit the definition.
We implement AI assistants for four common data wrangling tasks and make AI
assistants easily accessible to data analysts in an open-source notebook
environment for data science, by leveraging the common structure they follow.
We evaluate our AI assistants both quantitatively and qualitatively through
three example scenarios. We show that the unified and interactive design makes
it easy to perform tasks that would be difficult to do manually or with a fully
automatic tool.
</summary>
    <author>
      <name>Tomas Petricek</name>
    </author>
    <author>
      <name>Gerrit J. J. van den Burg</name>
    </author>
    <author>
      <name>Alfredo Nazábal</name>
    </author>
    <author>
      <name>Taha Ceritli</name>
    </author>
    <author>
      <name>Ernesto Jiménez-Ruiz</name>
    </author>
    <author>
      <name>Christopher K. I. Williams</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in IEEE Transactions on Knowledge and Data
  Engineering</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.00192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.00192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.06326v1</id>
    <updated>2022-10-31T10:18:20Z</updated>
    <published>2022-10-31T10:18:20Z</published>
    <title>Bad, mad and cooked apples: Responsibility for unlawful targeting in
  human-AI military teams</title>
    <summary>  A Nation's responsibility is to predict in advance and protect human
wellbeing in conflict including protection from moral injury and unjust
attribution of responsibility for their actions. This position paper considers
responsibility for unlawful killings by human AI teams drawing on a metaphor
from Neta Crawford's chapter, When Soldiers Snap: Bad Apples and Mad Apples, in
Accountability for Killing: Moral responsibility for collateral damage in
America's post 911 wars. This paper contends that although militaries may have
some bad apples responsible for war crimes and some mad apples unable to be
responsible for their actions during a conflict, increasingly militaries may
cook their good apples by putting them in untenable decision making
environments with AI. A cooked apple may be pushed beyond reasonable limits
leading to a loss of situational awareness, cognitive overload, loss of agency
and autonomy leading to automation bias. In these cases, moral responsibility
and perhaps even legal responsibility for unlawful deaths may be contested for
cooked apples, risking operators becoming moral crumple zones and or suffering
moral injury from being part of larger human AI systems authorised by the
state. Nations are responsible for minimising risks to humans within reasonable
bounds and compliance with legal obligations in human AI military teams, and
the military systems used to make or implement decisions. The paper suggests
that best practise WHS frameworks might be drawn on in development, acquisition
and training ahead of deployment of systems in conflicts to predict and
mitigate risks of human AI military teams.
</summary>
    <author>
      <name>Susannah Kate Devitt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, under review for a special edition Responsible AI in
  Military Applications of Ethics and Information Technology, Springer</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.06326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.06326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4; K.4.0; K.4.1; K.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.13087v1</id>
    <updated>2022-11-23T16:16:52Z</updated>
    <published>2022-11-23T16:16:52Z</published>
    <title>Human or Machine? Turing Tests for Vision and Language</title>
    <summary>  As AI algorithms increasingly participate in daily activities that used to be
the sole province of humans, we are inevitably called upon to consider how much
machines are really like us. To address this question, we turn to the Turing
test and systematically benchmark current AIs in their abilities to imitate
humans. We establish a methodology to evaluate humans versus machines in
Turing-like tests and systematically evaluate a representative set of selected
domains, parameters, and variables. The experiments involved testing 769 human
agents, 24 state-of-the-art AI agents, 896 human judges, and 8 AI judges, in
21,570 Turing tests across 6 tasks encompassing vision and language modalities.
Surprisingly, the results reveal that current AIs are not far from being able
to impersonate human judges across different ages, genders, and educational
levels in complex visual and language challenges. In contrast, simple AI judges
outperform human judges in distinguishing human answers versus machine answers.
The curated large-scale Turing test datasets introduced here and their
evaluation metrics provide valuable insights to assess whether an agent is
human or not. The proposed formulation to benchmark human imitation ability in
current AIs paves a way for the research community to expand Turing tests to
other research areas and conditions. All of source code and data are publicly
available at https://tinyurl.com/8x8nha7p
</summary>
    <author>
      <name>Mengmi Zhang</name>
    </author>
    <author>
      <name>Giorgia Dellaferrera</name>
    </author>
    <author>
      <name>Ankur Sikarwar</name>
    </author>
    <author>
      <name>Marcelo Armendariz</name>
    </author>
    <author>
      <name>Noga Mudrik</name>
    </author>
    <author>
      <name>Prachi Agrawal</name>
    </author>
    <author>
      <name>Spandan Madan</name>
    </author>
    <author>
      <name>Andrei Barbu</name>
    </author>
    <author>
      <name>Haochen Yang</name>
    </author>
    <author>
      <name>Tanishq Kumar</name>
    </author>
    <author>
      <name>Meghna Sadwani</name>
    </author>
    <author>
      <name>Stella Dellaferrera</name>
    </author>
    <author>
      <name>Michele Pizzochero</name>
    </author>
    <author>
      <name>Hanspeter Pfister</name>
    </author>
    <author>
      <name>Gabriel Kreiman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">134 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.13087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.13087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.11133v1</id>
    <updated>2022-12-21T15:58:42Z</updated>
    <published>2022-12-21T15:58:42Z</published>
    <title>Device-Bind Key-Storageless Hardware AI Model IP Protection: A PUF and
  Permute-Diffusion Encryption-Enabled Approach</title>
    <summary>  Machine learning as a service (MLaaS) framework provides intelligent services
or well-trained artificial intelligence (AI) models for local devices. However,
in the process of model transmission and deployment, there are security issues,
i.e. AI model leakage due to the unreliable transmission environments and
illegal abuse at local devices without permission. Although existing works
study the intellectual property (IP) protection of AI models, they mainly focus
on the watermark-based and encryption-based methods and have the following
problems: (i) The watermark-based methods only provide passive verification
afterward rather than active protection. (ii) Encryption-based methods are low
efficiency in computation and low security in key storage. (iii) The existing
methods are not device-bind without the ability to avoid illegal abuse of AI
models. To deal with these problems, we propose a device-bind and
key-storageless hardware AI model IP protection mechanism. First, a physical
unclonable function (PUF) and permute-diffusion encryption-based AI model
protection framework is proposed, including the PUF-based secret key generation
and the geometric-value transformation-based weights encryption. Second, we
design a PUF-based key generation protocol, where delay-based Anderson PUF is
adopted to generate the derive-bind secret key. Besides, convolutional coding
and convolutional interleaving technologies are combined to improve the
stability of PUF-based key generation and reconstruction. Third, a permute and
diffusion-based intelligent model weights encryption/decryption method is
proposed to achieve effective IP protection, where chaos theory is utilized to
convert the PUF-based secret key to encryption/decryption keys. Finally,
experimental evaluation demonstrates the effectiveness of the proposed
intelligent model IP protection mechanism.
</summary>
    <author>
      <name>Qianqian Pan</name>
    </author>
    <author>
      <name>Mianxiong Dong</name>
    </author>
    <author>
      <name>Kaoru Ota</name>
    </author>
    <author>
      <name>Jun Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 9 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.11133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.11133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68M25" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.11279v2</id>
    <updated>2022-12-29T11:38:07Z</updated>
    <published>2022-12-21T16:46:24Z</published>
    <title>Annotated History of Modern AI and Deep Learning</title>
    <summary>  Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.
</summary>
    <author>
      <name>Juergen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">75 pages, over 500 references. arXiv admin note: substantial text
  overlap with arXiv:2005.05744</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.11279v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.11279v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.12305v1</id>
    <updated>2022-12-15T21:23:11Z</updated>
    <published>2022-12-15T21:23:11Z</published>
    <title>Influence of AI in human lives</title>
    <summary>  Artificial Intelligence is one of the most significant and prominent
technological innovations which has reshaped all aspects of human life on the
lines of ease from magnitudes like shopping, data collection, driving, everyday
life, medical approach and many more. On the contrary, although recent
developments in both subjects that are backed by technology, progress on AI
alongside CE must have mostly been undertaken in isolation, providing little
understanding into how the two areas intersect. Artificial intelligence is now
widely used in services, from back-office tasks to front-line interactions with
customers. This trend has accelerated in recent years. Artificial intelligence
(AI)-based virtual assistants are changing successful engagement away from
being dominated by humans and toward being dominated by technologies. As a
result, people are expected to solve their own problems before calling customer
care representatives, eventually emerging as a crucial component of providing
services as value co-creators. AI-powered chats may potentially go awry, which
could enrage, perplex, and anger customers. Considering all these, the main
objectives of this study will engage the following
  1. To identify the alterations in the scope of human searches for information
offered by the application of AI?
  2. To analyse how AI helps in the way someone drives the car
  3. To evaluate how AI has changed the way customer interact with the
customers
</summary>
    <author>
      <name>Meenu Varghese</name>
    </author>
    <author>
      <name>Satheesh Raj</name>
    </author>
    <author>
      <name>Vigneshwaran Venkatesh</name>
    </author>
    <link href="http://arxiv.org/abs/2212.12305v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.12305v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.13338v2</id>
    <updated>2023-01-04T18:21:37Z</updated>
    <published>2022-12-27T01:52:52Z</published>
    <title>Teamwork under extreme uncertainty: AI for Pokemon ranks 33rd in the
  world</title>
    <summary>  The highest grossing media franchise of all times, with over \$90 billion in
total revenue, is Pokemon. The video games belong to the class of Japanese Role
Playing Games (J-RPG). Developing a powerful AI agent for these games is very
hard because they present big challenges to MinMax, Monte Carlo Tree Search and
statistical Machine Learning, as they are vastly different from the well
explored in AI literature games. An AI agent for one of these games means
significant progress in AI agents for the entire class. Further, the key
principles of such work can hopefully inspire approaches to several domains
that require excellent teamwork under conditions of extreme uncertainty,
including managing a team of doctors, robots or employees in an ever changing
environment, like a pandemic stricken region or a war-zone. In this paper we
first explain the mechanics of the game and we perform a game analysis. We
continue by proposing unique AI algorithms based on our understanding that the
two biggest challenges in the game are keeping a balanced team and dealing with
three sources of uncertainty. Later on, we describe why evaluating the
performance of such agents is challenging and we present the results of our
approach. Our AI agent performed significantly better than all previous
attempts and peaked at the 33rd place in the world, in one of the most popular
battle formats, while running on only 4 single socket servers.
</summary>
    <author>
      <name>Nicholas R. Sarantinos</name>
    </author>
    <link href="http://arxiv.org/abs/2212.13338v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.13338v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.04122v1</id>
    <updated>2022-12-16T18:46:37Z</updated>
    <published>2022-12-16T18:46:37Z</published>
    <title>Mystique: Accurate and Scalable Production AI Benchmarks Generation</title>
    <summary>  Building and maintaining large AI fleets to efficiently support the
fast-growing DL workloads is an active research topic for modern cloud
infrastructure providers. Generating accurate benchmarks plays an essential
role in the design and evaluation of rapidly evoloving software and hardware
solutions in this area. Two fundamental challenges to make this process
scalable are (i) workload representativeness and (ii) the ability to quickly
incorporate changes to the fleet into the benchmarks.
  To overcome these issues, we propose Mystique, an accurate and scalable
framework for production AI benchmark generation. It leverages the PyTorch
execution graph (EG), a new feature that captures the runtime information of AI
models at the granularity of operators, in a graph format, together with their
metadata. By sourcing EG traces from the fleet, we can build AI benchmarks that
are portable and representative. Mystique is scalable, with its lightweight
data collection, in terms of runtime overhead and user instrumentation efforts.
It is also adaptive, as the expressiveness and composability of EG format
allows flexible user control over benchmark creation.
  We evaluate our methodology on several production AI workloads, and show that
benchmarks generated with Mystique closely resemble original AI models, both in
execution time and system-level metrics. We also showcase the portability of
the generated benchmarks across platforms, and demonstrate several use cases
enabled by the fine-grained composability of the execution graph.
</summary>
    <author>
      <name>Mingyu Liang</name>
    </author>
    <author>
      <name>Wenyin Fu</name>
    </author>
    <author>
      <name>Louis Feng</name>
    </author>
    <author>
      <name>Zhongyi Lin</name>
    </author>
    <author>
      <name>Pavani Panakanti</name>
    </author>
    <author>
      <name>Srinivas Sridharan</name>
    </author>
    <author>
      <name>Christina Delimitrou</name>
    </author>
    <link href="http://arxiv.org/abs/2301.04122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.04122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.10007v1</id>
    <updated>2023-01-04T10:21:46Z</updated>
    <published>2023-01-04T10:21:46Z</published>
    <title>Information Governance as a Socio-Technical Process in the Development
  of Trustworthy Healthcare AI</title>
    <summary>  In order to develop trustworthy healthcare artificial intelligence (AI)
prospective and ergonomics studies that consider the complexity and reality of
real-world applications of AI systems are needed. To achieve this, technology
developers and deploying organisations need to form collaborative partnerships.
This entails access to healthcare data, which frequently might also include
potentially identifiable data such as audio recordings of calls made to an
ambulance service call centre. Information Governance (IG) processes have been
put in place to govern the use of personal confidential data. However,
navigating IG processes in the formative stages of AI development and
pre-deployment can be challenging, because the legal basis for data sharing is
explicit only for the purpose of delivering patient care, i.e., once a system
is put into service. In this paper we describe our experiences of managing IG
for the assurance of healthcare AI, using the example of an
out-of-hospital-cardiac-arrest recognition software within the context of the
Welsh Ambulance Service. We frame IG as a socio-technical process. IG processes
for the development of trustworthy healthcare AI rely on information governance
work, which entails dialogue, negotiation, and trade-offs around the legal
basis for data sharing, data requirements and data control. Information
governance work should start early in the design life cycle and will likely
continue throughout. This includes a focus on establishing and building
relationships, as well as a focus on organisational readiness deeper
understanding of both AI technologies as well as their safety assurance
requirements.
</summary>
    <author>
      <name>Nigel Rees</name>
    </author>
    <author>
      <name>Kelly Holding</name>
    </author>
    <author>
      <name>Mark Sujan</name>
    </author>
    <link href="http://arxiv.org/abs/2301.10007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.10007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.10009v1</id>
    <updated>2023-01-19T06:22:14Z</updated>
    <published>2023-01-19T06:22:14Z</published>
    <title>Remote patient monitoring using artificial intelligence: Current state,
  applications, and challenges</title>
    <summary>  The adoption of artificial intelligence (AI) in healthcare is growing
rapidly. Remote patient monitoring (RPM) is one of the common healthcare
applications that assist doctors to monitor patients with chronic or acute
illness at remote locations, elderly people in-home care, and even hospitalized
patients. The reliability of manual patient monitoring systems depends on staff
time management which is dependent on their workload. Conventional patient
monitoring involves invasive approaches which require skin contact to monitor
health status. This study aims to do a comprehensive review of RPM systems
including adopted advanced technologies, AI impact on RPM, challenges and
trends in AI-enabled RPM. This review explores the benefits and challenges of
patient-centric RPM architectures enabled with Internet of Things wearable
devices and sensors using the cloud, fog, edge, and blockchain technologies.
The role of AI in RPM ranges from physical activity classification to chronic
disease monitoring and vital signs monitoring in emergency settings. This
review results show that AI-enabled RPM architectures have transformed
healthcare monitoring applications because of their ability to detect early
deterioration in patients' health, personalize individual patient health
parameter monitoring using federated learning, and learn human behavior
patterns using techniques such as reinforcement learning. This review discusses
the challenges and trends to adopt AI to RPM systems and implementation issues.
The future directions of AI in RPM applications are analyzed based on the
challenges and trends
</summary>
    <author>
      <name>Thanveer Shaik</name>
    </author>
    <author>
      <name>Xiaohui Tao</name>
    </author>
    <author>
      <name>Niall Higgins</name>
    </author>
    <author>
      <name>Lin Li</name>
    </author>
    <author>
      <name>Raj Gururajan</name>
    </author>
    <author>
      <name>Xujuan Zhou</name>
    </author>
    <author>
      <name>U. Rajendra Acharya</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/widm.1485</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/widm.1485" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Wiley Interdisciplinary Reviews: Data Mining and Knowledge
  Discovery (2023): e1485</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2301.10009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.10009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.11891v1</id>
    <updated>2023-01-27T18:08:04Z</updated>
    <published>2023-01-27T18:08:04Z</published>
    <title>Polycraft World AI Lab (PAL): An Extensible Platform for Evaluating
  Artificial Intelligence Agents</title>
    <summary>  As artificial intelligence research advances, the platforms used to evaluate
AI agents need to adapt and grow to continue to challenge them. We present the
Polycraft World AI Lab (PAL), a task simulator with an API based on the
Minecraft mod Polycraft World. Our platform is built to allow AI agents with
different architectures to easily interact with the Minecraft world, train and
be evaluated in multiple tasks. PAL enables the creation of tasks in a flexible
manner as well as having the capability to manipulate any aspect of the task
during an evaluation. All actions taken by AI agents and external actors
(non-player-characters, NPCs) in the open-world environment are logged to
streamline evaluation. Here we present two custom tasks on the PAL platform,
one focused on multi-step planning and one focused on navigation, and
evaluations of agents solving them. In summary, we report a versatile and
extensible AI evaluation platform with a low barrier to entry for AI
researchers to utilize.
</summary>
    <author>
      <name>Stephen A. Goss</name>
    </author>
    <author>
      <name>Robert J. Steininger</name>
    </author>
    <author>
      <name>Dhruv Narayanan</name>
    </author>
    <author>
      <name>Daniel V. Olivença</name>
    </author>
    <author>
      <name>Yutong Sun</name>
    </author>
    <author>
      <name>Peng Qiu</name>
    </author>
    <author>
      <name>Jim Amato</name>
    </author>
    <author>
      <name>Eberhard O. Voit</name>
    </author>
    <author>
      <name>Walter E. Voit</name>
    </author>
    <author>
      <name>Eric J. Kildebeck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.11891v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.11891v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.13016v1</id>
    <updated>2022-12-28T17:28:24Z</updated>
    <published>2022-12-28T17:28:24Z</published>
    <title>Exploring the Versal AI engines for accelerating stencil-based
  atmospheric advection simulation</title>
    <summary>  AMD Xilinx's new Versal Adaptive Compute Acceleration Platform (ACAP) is an
FPGA architecture combining reconfigurable fabric with other on-chip hardened
compute resources. AI engines are one of these and, by operating in a highly
vectorized manner, they provide significant raw compute that is potentially
beneficial for a range of workloads including HPC simulation. However, this
technology is still early-on, and as yet unproven for accelerating HPC codes,
with a lack of benchmarking and best practice.
  This paper presents an experience report, exploring porting of the Piacsek
and Williams (PW) advection scheme onto the Versal ACAP, using the chip's AI
engines to accelerate the compute. A stencil-based algorithm, advection is
commonplace in atmospheric modelling, including several Met Office codes who
initially developed this scheme. Using this algorithm as a vehicle, we explore
optimal approaches for structuring AI engine compute kernels and how best to
interface the AI engines with programmable logic. Evaluating performance using
a VCK5000 against non-AI engine FPGA configurations on the VCK5000 and Alveo
U280, as well as a 24-core Xeon Platinum Cascade Lake CPU and Nvidia V100 GPU,
we found that whilst the number of channels between the fabric and AI engines
are a limitation, by leveraging the ACAP we can double performance compared to
an Alveo U280.
</summary>
    <author>
      <name>Nick Brown</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3543622.3573047</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3543622.3573047" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted version of paper appearing in the 2023 ACM/SIGDA
  International Symposium on Field Programmable Gate Arrays (FPGA '23),
  February 12--14, 2023, Monterey, CA, US</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.13016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.13016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.13637v1</id>
    <updated>2023-01-31T13:51:37Z</updated>
    <published>2023-01-31T13:51:37Z</published>
    <title>Tricking AI chips into Simulating the Human Brain: A Detailed
  Performance Analysis</title>
    <summary>  Challenging the Nvidia monopoly, dedicated AI-accelerator chips have begun
emerging for tackling the computational challenge that the inference and,
especially, the training of modern deep neural networks (DNNs) poses to modern
computers. The field has been ridden with studies assessing the performance of
these contestants across various DNN model types. However, AI-experts are aware
of the limitations of current DNNs and have been working towards the fourth AI
wave which will, arguably, rely on more biologically inspired models,
predominantly on spiking neural networks (SNNs). At the same time, GPUs have
been heavily used for simulating such models in the field of computational
neuroscience, yet AI-chips have not been tested on such workloads. The current
paper aims at filling this important gap by evaluating multiple, cutting-edge
AI-chips (Graphcore IPU, GroqChip, Nvidia GPU with Tensor Cores and Google TPU)
on simulating a highly biologically detailed model of a brain region, the
inferior olive (IO). This IO application stress-tests the different
AI-platforms for highlighting architectural tradeoffs by varying its compute
density, memory requirements and floating-point numerical accuracy. Our
performance analysis reveals that the simulation problem maps extremely well
onto the GPU and TPU architectures, which for networks of 125,000 cells leads
to a 28x respectively 1,208x speedup over CPU runtimes. At this speed, the TPU
sets a new record for largest real-time IO simulation. The GroqChip outperforms
both platforms for small networks but, due to implementing some floating-point
operations at reduced accuracy, is found not yet usable for brain simulation.
</summary>
    <author>
      <name>Lennart P. L. Landsmeer</name>
    </author>
    <author>
      <name>Max C. W. Engelen</name>
    </author>
    <author>
      <name>Rene Miedema</name>
    </author>
    <author>
      <name>Christos Strydis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.13637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.13637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.02337v4</id>
    <updated>2023-02-10T15:38:27Z</updated>
    <published>2023-02-05T08:56:45Z</published>
    <title>Regulating ChatGPT and other Large Generative AI Models</title>
    <summary>  Large generative AI models (LGAIMs), such as ChatGPT or Stable Diffusion, are
rapidly transforming the way we communicate, illustrate, and create. However,
AI regulation, in the EU and beyond, has primarily focused on conventional AI
models, not LGAIMs. This paper will situate these new generative models in the
current debate on trustworthy AI regulation, and ask how the law can be
tailored to their capabilities. After laying technical foundations, the legal
part of the paper proceeds in four steps, covering (1) direct regulation, (2)
data protection, (3) content moderation, and (4) policy proposals. It suggests
a novel terminology to capture the AI value chain in LGAIM settings by
differentiating between LGAIM developers, deployers, professional and
non-professional users, as well as recipients of LGAIM output. We tailor
regulatory duties to these different actors along the value chain and suggest
four strategies to ensure that LGAIMs are trustworthy and deployed for the
benefit of society at large. Rules in the AI Act and other direct regulation
must match the specificities of pre-trained models. In particular, regulation
should focus on concrete high-risk applications, and not the pre-trained model
itself, and should include (i) obligations regarding transparency and (ii) risk
management. Non-discrimination provisions (iii) may, however, apply to LGAIM
developers. Lastly, (iv) the core of the DSA content moderation rules should be
expanded to cover LGAIMs. This includes notice and action mechanisms, and
trusted flaggers. In all areas, regulators and lawmakers need to act fast to
keep track with the dynamics of ChatGPT et al.
</summary>
    <author>
      <name>Philipp Hacker</name>
    </author>
    <author>
      <name>Andreas Engel</name>
    </author>
    <author>
      <name>Marco Mauer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.02337v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.02337v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.00886v1</id>
    <updated>2018-11-27T19:20:35Z</updated>
    <published>2018-11-27T19:20:35Z</published>
    <title>AI Matrix - Synthetic Benchmarks for DNN</title>
    <summary>  Deep neural network (DNN) architectures, such as convolutional neural
networks (CNN), involve heavy computation and require hardware, such as CPU,
GPU, and AI accelerators, to provide the massive computing power. With the many
varieties of AI hardware prevailing on the market, it is often hard to decide
which one is the best to use. Thus, benchmarking AI hardware effectively
becomes important and is of great help to select and optimize AI hardware.
Unfortunately, there are few AI benchmarks available in both academia and
industry. Examples are BenchNN[1], DeepBench[2], and Dawn Bench[3], which are
usually a collection of typical real DNN applications. While these benchmarks
provide performance comparison across different AI hardware, they suffer from a
number of drawbacks. First, they cannot adapt to the emerging changes of DNN
algorithms and are fixed once selected. Second, they contain tens to hundreds
of applications and take very long time to finish running. Third, they are
mainly selected from open sources, which are restricted by copyright and are
not representable to proprietary applications. In this work, a synthetic
benchmarks framework is firstly proposed to address the above drawbacks of AI
benchmarks. Instead of pre-selecting a set of open-sourced benchmarks and
running all of them, the synthetic approach generates only a one or few
benchmarks that best represent a broad range of applications using profiled
workload characteristics data of these applications. Thus, it can adapt to
emerging changes of new DNN algorithms by re-profiling new applications and
updating itself, greatly reduce benchmark count and running time, and strongly
represent DNN applications of interests. The generated benchmarks are called AI
Matrix, serving as a performance benchmarks matching the statistical workload
characteristics of a combination of applications of interests.
</summary>
    <author>
      <name>Wei Wei</name>
    </author>
    <author>
      <name>Lingjie Xu</name>
    </author>
    <author>
      <name>Lingling Jin</name>
    </author>
    <author>
      <name>Wei Zhang</name>
    </author>
    <author>
      <name>Tianjun Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by SC' 18
  https://sc18.supercomputing.org/proceedings/tech_poster/tech_poster_pages/post153.html</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.00886v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.00886v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.14346v1</id>
    <updated>2020-11-29T11:55:59Z</updated>
    <published>2020-11-29T11:55:59Z</published>
    <title>Methods Matter: A Trading Agent with No Intelligence Routinely
  Outperforms AI-Based Traders</title>
    <summary>  There's a long tradition of research using computational intelligence
(methods from artificial intelligence (AI) and machine learning (ML)), to
automatically discover, implement, and fine-tune strategies for autonomous
adaptive automated trading in financial markets, with a sequence of research
papers on this topic published at AI conferences such as IJCAI and in journals
such as Artificial Intelligence: we show here that this strand of research has
taken a number of methodological mis-steps and that actually some of the
reportedly best-performing public-domain AI/ML trading strategies can routinely
be out-performed by extremely simple trading strategies that involve no AI or
ML at all. The results that we highlight here could easily have been revealed
at the time that the relevant key papers were published, more than a decade
ago, but the accepted methodology at the time of those publications involved a
somewhat minimal approach to experimental evaluation of trader-agents, making
claims on the basis of a few thousand test-sessions of the trader-agent in a
small number of market scenarios. In this paper we present results from
exhaustive testing over wide ranges of parameter values, using parallel
cloud-computing facilities, where we conduct millions of tests and thereby
create much richer data from which firmer conclusions can be drawn. We show
that the best public-domain AI/ML traders in the published literature can be
routinely outperformed by a "sub-zero-intelligence" trading strategy that at
face value appears to be so simple as to be financially ruinous, but which
interacts with the market in such a way that in practice it is more profitable
than the well-known AI/ML strategies from the research literature. That such a
simple strategy can outperform established AI/ML-based strategies is a sign
that perhaps the AI/ML trading strategies were good answers to the wrong
question.
</summary>
    <author>
      <name>Dave Cliff</name>
    </author>
    <author>
      <name>Michael Rollins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at the IEEE Symposium on Computational Intelligence
  in Financial Engineering (CIFEr2020) in Canberra, Australia, 1-4 December
  2020; 8 pages; 3 figures. arXiv admin note: text overlap with
  arXiv:2009.06905</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.14346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.14346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.02639v2</id>
    <updated>2021-07-11T22:11:13Z</updated>
    <published>2021-01-07T17:19:00Z</published>
    <title>More Reliable AI Solution: Breast Ultrasound Diagnosis Using Multi-AI
  Combination</title>
    <summary>  Objective: Breast cancer screening is of great significance in contemporary
women's health prevention. The existing machines embedded in the AI system do
not reach the accuracy that clinicians hope. How to make intelligent systems
more reliable is a common problem. Methods: 1) Ultrasound image
super-resolution: the SRGAN super-resolution network reduces the unclearness of
ultrasound images caused by the device itself and improves the accuracy and
generalization of the detection model. 2) In response to the needs of medical
images, we have improved the YOLOv4 and the CenterNet models. 3) Multi-AI
model: based on the respective advantages of different AI models, we employ two
AI models to determine clinical resuls cross validation. And we accept the same
results and refuses others. Results: 1) With the help of the super-resolution
model, the YOLOv4 model and the CenterNet model both increased the mAP score by
9.6% and 13.8%. 2) Two methods for transforming the target model into a
classification model are proposed. And the unified output is in a specified
format to facilitate the call of the molti-AI model. 3) In the classification
evaluation experiment, concatenated by the YOLOv4 model (sensitivity 57.73%,
specificity 90.08%) and the CenterNet model (sensitivity 62.64%, specificity
92.54%), the multi-AI model will refuse to make judgments on 23.55% of the
input data. Correspondingly, the performance has been greatly improved to
95.91% for the sensitivity and 96.02% for the specificity. Conclusion: Our work
makes the AI model more reliable in medical image diagnosis. Significance: 1)
The proposed method makes the target detection model more suitable for
diagnosing breast ultrasound images. 2) It provides a new idea for artificial
intelligence in medical diagnosis, which can more conveniently introduce target
detection models from other fields to serve medical lesion screening.
</summary>
    <author>
      <name>Jian Dai</name>
    </author>
    <author>
      <name>Shuge Lei</name>
    </author>
    <author>
      <name>Licong Dong</name>
    </author>
    <author>
      <name>Xiaona Lin</name>
    </author>
    <author>
      <name>Huabin Zhang</name>
    </author>
    <author>
      <name>Desheng Sun</name>
    </author>
    <author>
      <name>Kehong Yuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.02639v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.02639v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.05967v1</id>
    <updated>2021-01-15T04:55:03Z</updated>
    <published>2021-01-15T04:55:03Z</published>
    <title>Responsible AI Challenges in End-to-end Machine Learning</title>
    <summary>  Responsible AI is becoming critical as AI is widely used in our everyday
lives. Many companies that deploy AI publicly state that when training a model,
we not only need to improve its accuracy, but also need to guarantee that the
model does not discriminate against users (fairness), is resilient to noisy or
poisoned data (robustness), is explainable, and more. In addition, these
objectives are not only relevant to model training, but to all steps of
end-to-end machine learning, which include data collection, data cleaning and
validation, model training, model evaluation, and model management and serving.
Finally, responsible AI is conceptually challenging, and supporting all the
objectives must be as easy as possible. We thus propose three key research
directions towards this vision - depth, breadth, and usability - to measure
progress and introduce our ongoing research. First, responsible AI must be
deeply supported where multiple objectives like fairness and robust must be
handled together. To this end, we propose FR-Train, a holistic framework for
fair and robust model training in the presence of data bias and poisoning.
Second, responsible AI must be broadly supported, preferably in all steps of
machine learning. Currently we focus on the data pre-processing steps and
propose Slice Tuner, a selective data acquisition framework for training fair
and accurate models, and MLClean, a data cleaning framework that also improves
fairness and robustness. Finally, responsible AI must be usable where the
techniques must be easy to deploy and actionable. We propose FairBatch, a batch
selection approach for fairness that is effective and simple to use, and Slice
Finder, a model evaluation tool that automatically finds problematic slices. We
believe we scratched the surface of responsible AI for end-to-end machine
learning and suggest research challenges moving forward.
</summary>
    <author>
      <name>Steven Euijong Whang</name>
    </author>
    <author>
      <name>Ki Hyun Tae</name>
    </author>
    <author>
      <name>Yuji Roh</name>
    </author>
    <author>
      <name>Geon Heo</name>
    </author>
    <link href="http://arxiv.org/abs/2101.05967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.05967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.04213v1</id>
    <updated>2021-01-24T09:53:12Z</updated>
    <published>2021-01-24T09:53:12Z</published>
    <title>My Boss the Computer: A Bayesian analysis of socio-demographic and
  cross-cultural determinants of attitude toward the Non-Human Resource
  Management</title>
    <summary>  Human resource management technologies have moved from biometric surveillance
to emotional artificial intelligence (AI) that monitor employees' engagement
and productivity, analyze video interviews and CVs of job applicants. The rise
of the US$20 billion emotional AI industry will transform the future workplace.
Yet, besides no international consensus on the principles or standards for such
technologies, there is a lack of cross-cultural research on future job seekers'
attitude toward such use of AI technologies. This study collects a
cross-sectional dataset of 1,015 survey responses of international students
from 48 countries and 8 regions worldwide. A majority of the respondents (52%)
are concerned about being managed by AI. Following the hypothetico-deductivist
philosophy of science, we use the MCMC Hamiltonian approach and conduct a
detailed comparison of 10 Bayesian network models with the PSIS-LOO method. We
consistently find having a higher income, being male, majoring in business,
and/or self-rated familiarity with AI correlate with a more positive view of
emotional AI in the workplace. There is also a stark cross-cultural and
cross-regional difference. Our analysis shows people from economically less
developed regions (Africa, Oceania, Central Asia) tend to exhibit less concern
for AI managers. And for East Asian countries, 64% of the Japanese, 56% of the
South Korean, and 42% of the Chinese professed the trusting attitude. In
contrast, an overwhelming majority of 75% of the European and Northern American
possesses the worrying/neutral attitude toward being managed by AI. Regarding
religion, Muslim students correlate with the most concern toward emotional AI
in the workplace. When religiosity is higher, the correlation becomes stronger
for Muslim and Buddhist students.
</summary>
    <author>
      <name>Mantello Peter</name>
    </author>
    <author>
      <name>Manh-Tung Ho</name>
    </author>
    <author>
      <name>Minh-Hoang Nguyen</name>
    </author>
    <author>
      <name>Quan-Hoang Vuong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">58 pages, 9 tables, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.04213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.04213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.01591v1</id>
    <updated>2021-08-03T15:59:59Z</updated>
    <published>2021-08-03T15:59:59Z</published>
    <title>The application of artificial intelligence in software engineering: a
  review challenging conventional wisdom</title>
    <summary>  The field of artificial intelligence (AI) is witnessing a recent upsurge in
research, tools development, and deployment of applications. Multiple software
companies are shifting their focus to developing intelligent systems; and many
others are deploying AI paradigms to their existing processes. In parallel, the
academic research community is injecting AI paradigms to provide solutions to
traditional engineering problems. Similarly, AI has evidently been proved
useful to software engineering (SE). When one observes the SE phases
(requirements, design, development, testing, release, and maintenance), it
becomes clear that multiple AI paradigms (such as neural networks, machine
learning, knowledge-based systems, natural language processing) could be
applied to improve the process and eliminate many of the major challenges that
the SE field has been facing. This survey chapter is a review of the most
commonplace methods of AI applied to SE. The review covers methods between
years 1975-2017, for the requirements phase, 46 major AI-driven methods are
found, 19 for design, 15 for development, 68 for testing, and 15 for release
and maintenance. Furthermore, the purpose of this chapter is threefold;
firstly, to answer the following questions: is there sufficient intelligence in
the SE lifecycle? What does applying AI to SE entail? Secondly, to measure,
formulize, and evaluate the overlap of SE phases and AI disciplines. Lastly,
this chapter aims to provide serious questions to challenging the current
conventional wisdom (i.e., status quo) of the state-of-the-art, craft a call
for action, and to redefine the path forward.
</summary>
    <author>
      <name>Feras A. Batarseh</name>
    </author>
    <author>
      <name>Rasika Mohod</name>
    </author>
    <author>
      <name>Abhinav Kumar</name>
    </author>
    <author>
      <name>Justin Bui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Data Democracy: 1st Edition At the Nexus of Artificial
  Intelligence, Software Development, and Knowledge Engineering. Chapter 10.
  https://www.elsevier.com/books/data-democracy/batarseh/978-0-12-818366-3.
  arXiv admin note: text overlap with arXiv:1611.09934 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.01591v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.01591v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.05071v1</id>
    <updated>2021-11-09T11:59:47Z</updated>
    <published>2021-11-09T11:59:47Z</published>
    <title>Conformity Assessments and Post-market Monitoring: A Guide to the Role
  of Auditing in the Proposed European AI Regulation</title>
    <summary>  The proposed European Artificial Intelligence Act (AIA) is the first attempt
to elaborate a general legal framework for AI carried out by any major global
economy. As such, the AIA is likely to become a point of reference in the
larger discourse on how AI systems can (and should) be regulated. In this
article, we describe and discuss the two primary enforcement mechanisms
proposed in the AIA: the conformity assessments that providers of high-risk AI
systems are expected to conduct, and the post-market monitoring plans that
providers must establish to document the performance of high-risk AI systems
throughout their lifetimes. We argue that AIA can be interpreted as a proposal
to establish a Europe-wide ecosystem for conducting AI auditing, albeit in
other words. Our analysis offers two main contributions. First, by describing
the enforcement mechanisms included in the AIA in terminology borrowed from
existing literature on AI auditing, we help providers of AI systems understand
how they can prove adherence to the requirements set out in the AIA in
practice. Second, by examining the AIA from an auditing perspective, we seek to
provide transferable lessons from previous research about how to refine further
the regulatory approach outlined in the AIA. We conclude by highlighting seven
aspects of the AIA where amendments (or simply clarifications) would be
helpful. These include, above all, the need to translate vague concepts into
verifiable criteria and to strengthen the institutional safeguards concerning
conformity assessments based on internal checks.
</summary>
    <author>
      <name>Jakob Mokander</name>
    </author>
    <author>
      <name>Maria Axente</name>
    </author>
    <author>
      <name>Federico Casolari</name>
    </author>
    <author>
      <name>Luciano Floridi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11023-021-09577-4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11023-021-09577-4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial intelligence, Auditing, Certification, Conformity
  assessment, European Union, Governance, Regulation, Technology</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Mokander, J., Axente, M., Casolari, F. &amp; Floridi, L. (2021).
  Conformity Assessments and Post-market Monitoring: A Guide to the Role of
  Auditing in the Proposed European AI Regulation. Minds and Machines</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2111.05071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.05071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.5; K.4; K.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.12210v7</id>
    <updated>2023-01-23T17:58:01Z</updated>
    <published>2021-11-24T00:45:27Z</published>
    <title>From Kepler to Newton: Explainable AI for Science</title>
    <summary>  The Observation--Hypothesis--Prediction--Experimentation loop paradigm for
scientific research has been practiced by researchers for years towards
scientific discoveries. However, with data explosion in both mega-scale and
milli-scale scientific research, it has been sometimes very difficult to
manually analyze the data and propose new hypotheses to drive the cycle for
scientific discovery. In this paper, we discuss the role of Explainable AI in
scientific discovery process by demonstrating an Explainable AI-based paradigm
for science discovery. The key is to use Explainable AI to help derive data or
model interpretations, hypotheses, as well as scientific discoveries or
insights. We show how computational and data-intensive methodology -- together
with experimental and theoretical methodology -- can be seamlessly integrated
for scientific research. To demonstrate the AI-based science discovery process,
and to pay our respect to some of the greatest minds in human history, we show
how Kepler's laws of planetary motion and Newton's law of universal gravitation
can be rediscovered by (Explainable) AI based on Tycho Brahe's astronomical
observation data, whose works were leading the scientific revolution in the
16-17th century. This work also highlights the important role of Explainable AI
(as compared to Blackbox AI) in science discovery to help humans prevent or
better prepare for the possible technological singularity that may happen in
the future, since science is not only about the know how, but also the know
why. Presentation of the work is available at
https://slideslive.com/38986142/from-kepler-to-newton-explainable-ai-for-science-discovery.
</summary>
    <author>
      <name>Zelong Li</name>
    </author>
    <author>
      <name>Jianchao Ji</name>
    </author>
    <author>
      <name>Yongfeng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICML-AI4Science 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.12210v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.12210v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.05402v1</id>
    <updated>2022-02-11T01:28:59Z</updated>
    <published>2022-02-11T01:28:59Z</published>
    <title>Do People Engage Cognitively with AI? Impact of AI Assistance on
  Incidental Learning</title>
    <summary>  When people receive advice while making difficult decisions, they often make
better decisions in the moment and also increase their knowledge in the
process. However, such incidental learning can only occur when people
cognitively engage with the information they receive and process this
information thoughtfully. How do people process the information and advice they
receive from AI, and do they engage with it deeply enough to enable learning?
To answer these questions, we conducted three experiments in which individuals
were asked to make nutritional decisions and received simulated AI
recommendations and explanations. In the first experiment, we found that when
people were presented with both a recommendation and an explanation before
making their choice, they made better decisions than they did when they
received no such help, but they did not learn. In the second experiment,
participants first made their own choice, and only then saw a recommendation
and an explanation from AI; this condition also resulted in improved decisions,
but no learning. However, in our third experiment, participants were presented
with just an AI explanation but no recommendation and had to arrive at their
own decision. This condition led to both more accurate decisions and learning
gains. We hypothesize that learning gains in this condition were due to deeper
engagement with explanations needed to arrive at the decisions. This work
provides some of the most direct evidence to date that it may not be sufficient
to include explanations together with AI-generated recommendation to ensure
that people engage carefully with the AI-provided information. This work also
presents one technique that enables incidental learning and, by implication,
can help people process AI recommendations and explanations more carefully.
</summary>
    <author>
      <name>Krzysztof Z. Gajos</name>
    </author>
    <author>
      <name>Lena Mamykina</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3490099.3511138</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3490099.3511138" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures, to appear in ACM IUI'22</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.05402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.05402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.02779v3</id>
    <updated>2022-12-11T19:27:52Z</updated>
    <published>2022-04-05T15:02:18Z</published>
    <title>A Dempster-Shafer approach to trustworthy AI with application to fetal
  brain MRI segmentation</title>
    <summary>  Deep learning models for medical image segmentation can fail unexpectedly and
spectacularly for pathological cases and images acquired at different centers
than training images, with labeling errors that violate expert knowledge. Such
errors undermine the trustworthiness of deep learning models for medical image
segmentation. Mechanisms for detecting and correcting such failures are
essential for safely translating this technology into clinics and are likely to
be a requirement of future regulations on artificial intelligence (AI). In this
work, we propose a trustworthy AI theoretical framework and a practical system
that can augment any backbone AI system using a fallback method and a fail-safe
mechanism based on Dempster-Shafer theory. Our approach relies on an actionable
definition of trustworthy AI. Our method automatically discards the voxel-level
labeling predicted by the backbone AI that violate expert knowledge and relies
on a fallback for those voxels. We demonstrate the effectiveness of the
proposed trustworthy AI approach on the largest reported annotated dataset of
fetal MRI consisting of 540 manually annotated fetal brain 3D T2w MRIs from 13
centers. Our trustworthy AI method improves the robustness of a
state-of-the-art backbone AI for fetal brain MRIs acquired across various
centers and for fetuses with various brain abnormalities.
</summary>
    <author>
      <name>Lucas Fidon</name>
    </author>
    <author>
      <name>Michael Aertsen</name>
    </author>
    <author>
      <name>Florian Kofler</name>
    </author>
    <author>
      <name>Andrea Bink</name>
    </author>
    <author>
      <name>Anna L. David</name>
    </author>
    <author>
      <name>Thomas Deprest</name>
    </author>
    <author>
      <name>Doaa Emam</name>
    </author>
    <author>
      <name>Frédéric Guffens</name>
    </author>
    <author>
      <name>András Jakab</name>
    </author>
    <author>
      <name>Gregor Kasprian</name>
    </author>
    <author>
      <name>Patric Kienast</name>
    </author>
    <author>
      <name>Andrew Melbourne</name>
    </author>
    <author>
      <name>Bjoern Menze</name>
    </author>
    <author>
      <name>Nada Mufti</name>
    </author>
    <author>
      <name>Ivana Pogledic</name>
    </author>
    <author>
      <name>Daniela Prayer</name>
    </author>
    <author>
      <name>Marlene Stuempflen</name>
    </author>
    <author>
      <name>Esther Van Elslander</name>
    </author>
    <author>
      <name>Sébastien Ourselin</name>
    </author>
    <author>
      <name>Jan Deprest</name>
    </author>
    <author>
      <name>Tom Vercauteren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.02779v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.02779v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.10912v1</id>
    <updated>2022-06-22T08:35:04Z</updated>
    <published>2022-06-22T08:35:04Z</published>
    <title>AI-based software for lung nodule detection in chest X-rays -- Time for
  a second reader approach?</title>
    <summary>  Objectives: To compare artificial intelligence (AI) as a second reader in
detecting lung nodules on chest X-rays (CXR) versus radiologists of two
binational institutions, and to evaluate AI performance when using two
different modes: automated versus assisted (additional remote radiologist
review).
  Methods: The CXR public database (n = 247) of the Japanese Society of
Radiological Technology with various types and sizes of lung nodules was
analyzed. Eight radiologists evaluated the CXR images with regard to the
presence of lung nodules and nodule conspicuity. After radiologist review, the
AI software processed and flagged the CXR with the highest probability of
missed nodules. The calculated accuracy metrics were the area under the curve
(AUC), sensitivity, specificity, F1 score, false negative case number (FN), and
the effect of different AI modes (automated/assisted) on the accuracy of nodule
detection.
  Results: For radiologists, the average AUC value was 0.77 $\pm$ 0.07, while
the average FN was 52.63 $\pm$ 17.53 (all studies) and 32 $\pm$ 11.59 (studies
containing a nodule of malignant etiology = 32% rate of missed malignant
nodules). Both AI modes -- automated and assisted -- produced an average
increase in sensitivity (by 14% and 12%) and of F1-score (5% and 6%) and a
decrease in specificity (by 10% and 3%, respectively).
  Conclusions: Both AI modes flagged the pulmonary nodules missed by
radiologists in a significant number of cases. AI as a second reader has a high
potential to improve diagnostic accuracy and radiology workflow. AI might
detect certain pulmonary nodules earlier than radiologists, with a potentially
significant impact on patient outcomes.
</summary>
    <author>
      <name>Susanne Ohlmann-Knafo</name>
    </author>
    <author>
      <name>Naglis Ramanauskas</name>
    </author>
    <author>
      <name>Sebastian Huettinger</name>
    </author>
    <author>
      <name>Emil Johnson Jeyakumar</name>
    </author>
    <author>
      <name>Darius Barušauskas</name>
    </author>
    <author>
      <name>Neringa Bielskienė</name>
    </author>
    <author>
      <name>Vytautas Naujalis</name>
    </author>
    <author>
      <name>Jonas Bialopetravičius</name>
    </author>
    <author>
      <name>Jonas Ražanskas</name>
    </author>
    <author>
      <name>Artūras Samuilis</name>
    </author>
    <author>
      <name>Jūratė Dementavičienė</name>
    </author>
    <author>
      <name>Dirk Pickuth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is in submission process to the European Radiology journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.10912v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.10912v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.09204v1</id>
    <updated>2022-08-31T09:54:24Z</updated>
    <published>2022-08-31T09:54:24Z</published>
    <title>Robustness of an Artificial Intelligence Solution for Diagnosis of
  Normal Chest X-Rays</title>
    <summary>  Purpose: Artificial intelligence (AI) solutions for medical diagnosis require
thorough evaluation to demonstrate that performance is maintained for all
patient sub-groups and to ensure that proposed improvements in care will be
delivered equitably. This study evaluates the robustness of an AI solution for
the diagnosis of normal chest X-rays (CXRs) by comparing performance across
multiple patient and environmental subgroups, as well as comparing AI errors
with those made by human experts.
  Methods: A total of 4,060 CXRs were sampled to represent a diverse dataset of
NHS patients and care settings. Ground-truth labels were assigned by a
3-radiologist panel. AI performance was evaluated against assigned labels and
sub-groups analysis was conducted against patient age and sex, as well as CXR
view, modality, device manufacturer and hospital site.
  Results: The AI solution was able to remove 18.5% of the dataset by
classification as High Confidence Normal (HCN). This was associated with a
negative predictive value (NPV) of 96.0%, compared to 89.1% for diagnosis of
normal scans by radiologists. In all AI false negative (FN) cases, a
radiologist was found to have also made the same error when compared to final
ground-truth labels. Subgroup analysis showed no statistically significant
variations in AI performance, whilst reduced normal classification was observed
in data from some hospital sites.
  Conclusion: We show the AI solution could provide meaningful workload savings
by diagnosis of 18.5% of scans as HCN with a superior NPV to human readers. The
AI solution is shown to perform well across patient subgroups and error cases
were shown to be subjective or subtle in nature.
</summary>
    <author>
      <name>Tom Dyer</name>
    </author>
    <author>
      <name>Jordan Smith</name>
    </author>
    <author>
      <name>Gaetan Dissez</name>
    </author>
    <author>
      <name>Nicole Tay</name>
    </author>
    <author>
      <name>Qaiser Malik</name>
    </author>
    <author>
      <name>Tom Naunton Morgan</name>
    </author>
    <author>
      <name>Paul Williams</name>
    </author>
    <author>
      <name>Liliana Garcia-Mondragon</name>
    </author>
    <author>
      <name>George Pearse</name>
    </author>
    <author>
      <name>Simon Rasalingham</name>
    </author>
    <link href="http://arxiv.org/abs/2209.09204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.09204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.07542v1</id>
    <updated>2022-12-14T22:57:44Z</updated>
    <published>2022-12-14T22:57:44Z</published>
    <title>Build-a-Bot: Teaching Conversational AI Using a Transformer-Based Intent
  Recognition and Question Answering Architecture</title>
    <summary>  As artificial intelligence (AI) becomes a prominent part of modern life, AI
literacy is becoming important for all citizens, not just those in technology
careers. Previous research in AI education materials has largely focused on the
introduction of terminology as well as AI use cases and ethics, but few allow
students to learn by creating their own machine learning models. Therefore,
there is a need for enriching AI educational tools with more adaptable and
flexible platforms for interested educators with any level of technical
experience to utilize within their teaching material. As such, we propose the
development of an open-source tool (Build-a-Bot) for students and teachers to
not only create their own transformer-based chatbots based on their own course
material, but also learn the fundamentals of AI through the model creation
process. The primary concern of this paper is the creation of an interface for
students to learn the principles of artificial intelligence by using a natural
language pipeline to train a customized model to answer questions based on
their own school curriculums. The model uses contexts given by their
instructor, such as chapters of a textbook, to answer questions and is deployed
on an interactive chatbot/voice agent. The pipeline teaches students data
collection, data augmentation, intent recognition, and question answering by
having them work through each of these processes while creating their AI agent,
diverging from previous chatbot work where students and teachers use the bots
as black-boxes with no abilities for customization or the bots lack AI
capabilities, with the majority of dialogue scripts being rule-based. In
addition, our tool is designed to make each step of this pipeline intuitive for
students at a middle-school level. Further work primarily lies in providing our
tool to schools and seeking student and teacher evaluations.
</summary>
    <author>
      <name>Kate Pearce</name>
    </author>
    <author>
      <name>Sharifa Alghowinem</name>
    </author>
    <author>
      <name>Cynthia Breazeal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for presentation at EAAI-23</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.07542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.07542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.0625v2</id>
    <updated>2008-02-11T12:34:04Z</updated>
    <published>2007-09-05T11:36:01Z</published>
    <title>Efficient Implementation of the AI-REML Iteration for Variance Component
  QTL Analysis</title>
    <summary>  Regions in the genome that affect complex traits, quantitative trait loci
(QTL), can be identified using statistical analysis of genetic and phenotypic
data. When restricted maximum-likelihood (REML) models are used, the mapping
procedure is normally computationally demanding. We develop a new efficient
computational scheme for QTL mapping using variance component analysis and the
AI-REML algorithm. The algorithm uses an exact or approximative low-rank
representation of the identity-by-descent matrix, which combined with the
Woodbury formula for matrix inversion results in that the computations in the
AI-REML iteration body can be performed more efficiently. For cases where an
exact low-rank representation of the IBD matrix is available a-priori, the
improved AI-REML algorithm normally runs almost twice as fast compared to the
standard version. When an exact low-rank representation is not available, a
truncated spectral decomposition is used to determine a low-rank approximation.
We show that also in this case, the computational efficiency of the AI-REML
scheme can often be significantly improved.
</summary>
    <author>
      <name>Kateryna Mishchenko</name>
    </author>
    <author>
      <name>Sverker Holmgren</name>
    </author>
    <author>
      <name>Lars Ronnegard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0709.0625v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.0625v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.1251v2</id>
    <updated>2009-12-14T04:17:34Z</updated>
    <published>2009-09-07T13:55:59Z</published>
    <title>On the obstructed Lagrangian Floer theory</title>
    <summary>  Lagrangian Floer homology in a general case has been constructed by Fukaya,
Oh, Ohta and Ono, where they construct an $\AI$-algebra or an $\AI$-bimodule
from Lagrangian submanifolds, and studied the obstructions and deformation
theories. But for obstructed Lagrangian submanifolds, standard Lagrangian Floer
homology can not be defined.
  We explore several well-known cohomology theories on these $\AI$-objects and
explore their properties, which are well-defined and invariant even in the
obstructed cases. These are Hochschild and cyclic homology of an $\AI$-objects
and Chevalley-Eilenberg or cyclic Chevalley-Eilenberg homology of their
underlying $\LI$ objects. We explain how the existence of $m_0$ effects the
usual homological algebra of these homology theories. We also provide some
computations. We show that for an obstructed $\AI$-algebra with a non-trivial
primary obstruction, Chevalley-Eilenberg Floer homology vanishes, whose proof
is inspired by the comparison with cluster homology theory of Lagrangian
submanifolds by Cornea and Lalonde.
  In contrast, we also provide an example of an obstructed case whose cyclic
Floer homology is non-vanishing.
</summary>
    <author>
      <name>Cheol-Hyun Cho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Adv. Math. 229 (2012) 804-853</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.1251v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.1251v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.SG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.SG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.QA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="53D40, 53D12" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.4838v1</id>
    <updated>2012-09-21T14:58:33Z</updated>
    <published>2012-09-21T14:58:33Z</published>
    <title>Formal Definition of AI</title>
    <summary>  A definition of Artificial Intelligence was proposed in [1] but this
definition was not absolutely formal at least because the word "Human" was
used. In this paper we will formalize the definition from [1]. The biggest
problem in this definition was that the level of intelligence of AI is compared
to the intelligence of a human being. In order to change this we will introduce
some parameters to which AI will depend. One of this parameters will be the
level of intelligence and we will define one AI to each level of intelligence.
We assume that for some level of intelligence the respective AI will be more
intelligent than a human being. Nevertheless, we cannot say which is this level
because we cannot calculate its exact value.
</summary>
    <author>
      <name>Dimiter Dobrev</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal "Information Theories &amp; Applications",
  vol.12, Number 3, 2005, pp.277-285</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.4838v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.4838v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.2260v1</id>
    <updated>2013-01-10T16:22:53Z</updated>
    <published>2013-01-10T16:22:53Z</published>
    <title>Confidence Inference in Bayesian Networks</title>
    <summary>  We present two sampling algorithms for probabilistic confidence inference in
Bayesian networks. These two algorithms (we call them AIS-BN-mu and
AIS-BN-sigma algorithms) guarantee that estimates of posterior probabilities
are with a given probability within a desired precision bound. Our algorithms
are based on recent advances in sampling algorithms for (1) estimating the mean
of bounded random variables and (2) adaptive importance sampling in Bayesian
networks. In addition to a simple stopping rule for sampling that they provide,
the AIS-BN-mu and AIS-BN-sigma algorithms are capable of guiding the learning
process in the AIS-BN algorithm. An empirical evaluation of the proposed
algorithms shows excellent performance, even for very unlikely evidence.
</summary>
    <author>
      <name>Jian Cheng</name>
    </author>
    <author>
      <name>Marek J. Druzdzel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.2260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.2260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.06165v1</id>
    <updated>2015-08-25T14:23:04Z</updated>
    <published>2015-08-25T14:23:04Z</published>
    <title>Compositional dependence of anomalous thermal expansion in
  perovskite-like ABX3 formates</title>
    <summary>  The compositional dependence of thermal expansion behaviour in 19 different
perovskite-like metal-organic frameworks (MOFs) of composition [AI][MII(HCOO)3]
(A = alkylammonium cation; M = octahedrally-coordinated divalent metal) is
studied using variable-temperature X-ray powder diffraction measurements. While
all systems show essentially the same type of thermomechanical
response-irrespective of their particular structural details-the magnitude of
this response is shown to be a function of AI and MII cation radii, as well as
the molecular anisotropy of AI. Flexibility is maximised for large MII and
small AI, while the shape of AI has implications for the direction of framework
hingeing.
</summary>
    <author>
      <name>Ines E. Collings</name>
    </author>
    <author>
      <name>Joshua A. Hill</name>
    </author>
    <author>
      <name>Andrew B. Cairns</name>
    </author>
    <author>
      <name>Richard I. Cooper</name>
    </author>
    <author>
      <name>Amber L. Thompson</name>
    </author>
    <author>
      <name>Julia E. Parker</name>
    </author>
    <author>
      <name>Chiu C. Tang</name>
    </author>
    <author>
      <name>Andrew L. Goodwin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1039/C5DT03263F</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1039/C5DT03263F" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.06165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.06165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01608v1</id>
    <updated>2016-12-06T00:46:57Z</updated>
    <published>2016-12-06T00:46:57Z</published>
    <title>AI Researchers, Video Games Are Your Friends!</title>
    <summary>  If you are an artificial intelligence researcher, you should look to video
games as ideal testbeds for the work you do. If you are a video game developer,
you should look to AI for the technology that makes completely new types of
games possible. This chapter lays out the case for both of these propositions.
It asks the question "what can video games do for AI", and discusses how in
particular general video game playing is the ideal testbed for artificial
general intelligence research. It then asks the question "what can AI do for
video games", and lays out a vision for what video games might look like if we
had significantly more advanced AI at our disposal. The chapter is based on my
keynote at IJCCI 2015, and is written in an attempt to be accessible to a broad
audience.
</summary>
    <author>
      <name>Julian Togelius</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-48506-5_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-48506-5_1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Studies in Computational Intelligence Studies in Computational
  Intelligence, Volume 669 2017. Springer</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.01608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05703v1</id>
    <updated>2017-09-17T18:17:55Z</updated>
    <published>2017-09-17T18:17:55Z</published>
    <title>AI Programmer: Autonomously Creating Software Programs Using Genetic
  Algorithms</title>
    <summary>  In this paper, we present the first-of-its-kind machine learning (ML) system,
called AI Programmer, that can automatically generate full software programs
requiring only minimal human guidance. At its core, AI Programmer uses genetic
algorithms (GA) coupled with a tightly constrained programming language that
minimizes the overhead of its ML search space. Part of AI Programmer's novelty
stems from (i) its unique system design, including an embedded, hand-crafted
interpreter for efficiency and security and (ii) its augmentation of GAs to
include instruction-gene randomization bindings and programming
language-specific genome construction and elimination techniques. We provide a
detailed examination of AI Programmer's system design, several examples
detailing how the system works, and experimental data demonstrating its
software generation capabilities and performance using only mainstream CPUs.
</summary>
    <author>
      <name>Kory Becker</name>
    </author>
    <author>
      <name>Justin Gottschlich</name>
    </author>
    <link href="http://arxiv.org/abs/1709.05703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01704v2</id>
    <updated>2018-01-15T11:54:34Z</updated>
    <published>2018-01-05T10:51:55Z</published>
    <title>Artificial Intelligence (AI) Methods in Optical Networks: A
  Comprehensive Survey</title>
    <summary>  Artificial intelligence (AI) is an extensive scientific discipline which
enables computer systems to solve problems by emulating complex biological
processes such as learning, reasoning and self-correction. This paper presents
a comprehensive review of the application of AI techniques for improving
performance of optical communication systems and networks. The use of AI-based
techniques is first studied in applications related to optical transmission,
ranging from the characterization and operation of network components to
performance monitoring, mitigation of nonlinearities, and quality of
transmission estimation. Then, applications related to optical network control
and management are also reviewed, including topics like optical network
planning and operation in both transport and access networks. Finally, the
paper also presents a summary of opportunities and challenges in optical
networking where AI is expected to play a key role in the near future.
</summary>
    <author>
      <name>Javier Mata</name>
    </author>
    <author>
      <name>Ignacio de Miguel</name>
    </author>
    <author>
      <name>Ramó n J. Durá n</name>
    </author>
    <author>
      <name>Noemí Merayo</name>
    </author>
    <author>
      <name>Sandeep Kumar Singh</name>
    </author>
    <author>
      <name>Admela Jukan</name>
    </author>
    <author>
      <name>Mohit Chamania</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.osn.2017.12.006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.osn.2017.12.006" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Optical Switching and Networking, Jan 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1801.01704v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01704v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03351v2</id>
    <updated>2018-09-12T17:07:36Z</updated>
    <published>2018-01-10T12:42:54Z</published>
    <title>A triple comparison between anticipating stochastic integrals in
  financial modeling</title>
    <summary>  We consider a simplified version of the problem of insider trading in a
financial market. We approach it by means of anticipating stochastic calculus
and compare the use of the Hitsuda-Skorokhod, the Ayed-Kuo, and the
Russo-Vallois forward integrals within this context. Our results give some
indication that, while the forward integral yields results with a suitable
financial meaning, the Hitsuda-Skorokhod and the Ayed-Kuo integrals do not
provide an appropriate formulation of this problem. Further results regarding
the use of the Ayed-Kuo integral in this context are also provided, including
the proof of the fact that the expectation of a Russo-Vallois solution is
strictly greater than that of an Ayed-Kuo solution. Finally, we conjecture the
explicit solution of an Ayed-Kuo stochastic differential equation that
possesses discontinuous sample paths with finite probability.
</summary>
    <author>
      <name>Joan C. Bastons</name>
    </author>
    <author>
      <name>Carlos Escudero</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Communications on Stochastic Analysis: Vol. 12: No. 1, Article 6
  (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1801.03351v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03351v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02654v1</id>
    <updated>2018-05-07T17:56:04Z</updated>
    <published>2018-05-07T17:56:04Z</published>
    <title>Investigation of the Thermal Stability of the Carbon Framework of
  Graphene Oxide</title>
    <summary>  In this study, we use our recently prepared graphene oxide (GO) with an
almost intact {\sigma}-framework of C-atoms (ai-GO) to probe the thermal
stability of the carbon framework for the first time. Ai-GO exhibits few
defects only by preventing CO2 formation during synthesis. Ai-GO was thermally
treated before chemical reduction and subsequently the resulting defect density
in graphene was determined by statistical Raman microscopy. Surprisingly, the
carbon framework of ai-GO is stable in thin films up to 100 {\deg}C.
Furthermore, we find evidence for an increasing quality of ai-GO upon annealing
at 50 {\deg}C before reduction. The carbon framework of GO prepared according
to the popular Hummers method (GO-c)appears to be less stable and decomposition
starts at 50 {\deg}C what is qualitatively indicated by CO2-trapping
experiments in {\mu}m-thin films. Information about the stability of GO is
important for storing, processing and applying GO in applications.
</summary>
    <author>
      <name>Siegfried Eigler</name>
    </author>
    <author>
      <name>Stefan Grimm</name>
    </author>
    <author>
      <name>Andreas Hirsch</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/chem.201304048</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/chem.201304048" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Chem. Eur. J. 2014, 20, 984-989</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.02654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.8566v1</id>
    <updated>2014-12-30T06:13:10Z</updated>
    <published>2014-12-30T06:13:10Z</published>
    <title>Accurate and Conservative Estimates of MRF Log-likelihood using Reverse
  Annealing</title>
    <summary>  Markov random fields (MRFs) are difficult to evaluate as generative models
because computing the test log-probabilities requires the intractable partition
function. Annealed importance sampling (AIS) is widely used to estimate MRF
partition functions, and often yields quite accurate results. However, AIS is
prone to overestimate the log-likelihood with little indication that anything
is wrong. We present the Reverse AIS Estimator (RAISE), a stochastic lower
bound on the log-likelihood of an approximation to the original MRF model.
RAISE requires only the same MCMC transition operators as standard AIS.
Experimental results indicate that RAISE agrees closely with AIS
log-probability estimates for RBMs, DBMs, and DBNs, but typically errs on the
side of underestimating, rather than overestimating, the log-likelihood.
</summary>
    <author>
      <name>Yuri Burda</name>
    </author>
    <author>
      <name>Roger B. Grosse</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <link href="http://arxiv.org/abs/1412.8566v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.8566v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.00431v1</id>
    <updated>2020-03-01T07:54:01Z</updated>
    <published>2020-03-01T07:54:01Z</published>
    <title>A Study on Multimodal and Interactive Explanations for Visual Question
  Answering</title>
    <summary>  Explainability and interpretability of AI models is an essential factor
affecting the safety of AI. While various explainable AI (XAI) approaches aim
at mitigating the lack of transparency in deep networks, the evidence of the
effectiveness of these approaches in improving usability, trust, and
understanding of AI systems are still missing. We evaluate multimodal
explanations in the setting of a Visual Question Answering (VQA) task, by
asking users to predict the response accuracy of a VQA agent with and without
explanations. We use between-subjects and within-subjects experiments to probe
explanation effectiveness in terms of improving user prediction accuracy,
confidence, and reliance, among other factors. The results indicate that the
explanations help improve human prediction accuracy, especially in trials when
the VQA system's answer is inaccurate. Furthermore, we introduce active
attention, a novel method for evaluating causal attentional effects through
intervention by editing attention maps. User explanation ratings are strongly
correlated with human prediction accuracy and suggest the efficacy of these
explanations in human-machine AI collaboration tasks.
</summary>
    <author>
      <name>Kamran Alipour</name>
    </author>
    <author>
      <name>Jurgen P. Schulze</name>
    </author>
    <author>
      <name>Yi Yao</name>
    </author>
    <author>
      <name>Avi Ziskind</name>
    </author>
    <author>
      <name>Giedrius Burachas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://ceur-ws.org/Vol-2560/paper44.pdf</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Workshop on Artificial Intelligence Safety
  (SafeAI 2020) co-located with 34th AAAI Conference on Artificial Intelligence
  (AAAI 2020), New York, USA, Feb 7, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.00431v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.00431v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.01593v1</id>
    <updated>2020-03-03T15:27:30Z</updated>
    <published>2020-03-03T15:27:30Z</published>
    <title>Marketplace for AI Models</title>
    <summary>  Artificial intelligence shows promise for solving many practical societal
problems in areas such as healthcare and transportation. However, the current
mechanisms for AI model diffusion such as Github code repositories, academic
project webpages, and commercial AI marketplaces have some limitations; for
example, a lack of monetization methods, model traceability, and model
auditabilty. In this work, we sketch guidelines for a new AI diffusion method
based on a decentralized online marketplace. We consider the technical,
economic, and regulatory aspects of such a marketplace including a discussion
of solutions for problems in these areas. Finally, we include a comparative
analysis of several current AI marketplaces that are already available or in
development. We find that most of these marketplaces are centralized commercial
marketplaces with relatively few models.
</summary>
    <author>
      <name>Abhishek Kumar</name>
    </author>
    <author>
      <name>Benjamin Finley</name>
    </author>
    <author>
      <name>Tristan Braud</name>
    </author>
    <author>
      <name>Sasu Tarkoma</name>
    </author>
    <author>
      <name>Pan Hui</name>
    </author>
    <link href="http://arxiv.org/abs/2003.01593v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.01593v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.08471v1</id>
    <updated>2020-03-18T20:59:05Z</updated>
    <published>2020-03-18T20:59:05Z</published>
    <title>Scientific AI in materials science: a path to a sustainable and scalable
  paradigm</title>
    <summary>  Recently there has been an ever-increasing trend in the use of machine
learning (ML) and artificial intelligence (AI) methods by the materials
science, condensed matter physics, and chemistry communities. This perspective
article identifies key scientific, technical, and social opportunities that the
materials community must prioritize to consistently develop and leverage
Scientific AI to provide a credible path towards the advancement of current
materials-limited technologies. Here we highlight the intersections of these
opportunities with a series of proposed paths forward. The opportunities are
roughly sorted from scientific/technical (e.g., development of robust,
physically meaningful multiscale material representations) to social (e.g.,
promoting an AI-ready workforce). The proposed paths forward range from
developing new infrastructure and capabilities to deploying them in industry
and academia. We provide a brief introduction to AI in materials science and
engineering, followed by detailed discussions of each of the opportunities and
paths forward.
</summary>
    <author>
      <name>Brian DeCost</name>
    </author>
    <author>
      <name>Jason Hattrick-Simpers</name>
    </author>
    <author>
      <name>Zachary Trautt</name>
    </author>
    <author>
      <name>Aaron Kusne</name>
    </author>
    <author>
      <name>Eva Campo</name>
    </author>
    <author>
      <name>Martin Green</name>
    </author>
    <link href="http://arxiv.org/abs/2003.08471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.09744v1</id>
    <updated>2020-03-21T20:21:01Z</updated>
    <published>2020-03-21T20:21:01Z</published>
    <title>Towards an Enterprise-Ready Implementation of Artificial
  Intelligence-Enabled, Blockchain-Based Smart Contracts</title>
    <summary>  Blockchain technology and artificial intelligence (AI) are current hot topics
in research and practice. However, the potentials of their combination have
been studied just recently to a larger extend. While different use cases for
combining AI and blockchain have been discussed, the idea of enabling
blockchain-based smart contracts to perform "smarter" decisions by using AI or
machine learning (ML) models has only been considered on the conceptual level
so far. It remained open, how such AI-enabled smart contracts could be
implemented in a robust way for real-world applications. Therefore, in this
paper a new, enterprise-class implementation of AI-enabled smart contracts is
presented and first insights regarding its feasibility are discussed.
</summary>
    <author>
      <name>Philipp Brune</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Neu-Ulm University of Applied Sciences, Neu-Ulm, Germany</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.09744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.09744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.10688v1</id>
    <updated>2020-03-24T07:03:09Z</updated>
    <published>2020-03-24T07:03:09Z</published>
    <title>SOL: Effortless Device Support for AI Frameworks without Source Code
  Changes</title>
    <summary>  Modern high performance computing clusters heavily rely on accelerators to
overcome the limited compute power of CPUs. These supercomputers run various
applications from different domains such as simulations, numerical applications
or artificial intelligence (AI). As a result, vendors need to be able to
efficiently run a wide variety of workloads on their hardware. In the AI domain
this is in particular exacerbated by the existence of a number of popular
frameworks (e.g, PyTorch, TensorFlow, etc.) that have no common code base, and
can vary in functionality. The code of these frameworks evolves quickly, making
it expensive to keep up with all changes and potentially forcing developers to
go through constant rounds of upstreaming. In this paper we explore how to
provide hardware support in AI frameworks without changing the framework's
source code in order to minimize maintenance overhead. We introduce SOL, an AI
acceleration middleware that provides a hardware abstraction layer that allows
us to transparently support heterogeneous hardware. As a proof of concept, we
implemented SOL for PyTorch with three backends: CPUs, GPUs and vector
processors.
</summary>
    <author>
      <name>Nicolas Weber</name>
    </author>
    <author>
      <name>Felipe Huici</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">HPML Workshop 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.10688v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.10688v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00989v2</id>
    <updated>2018-10-03T08:02:54Z</updated>
    <published>2018-06-04T07:20:07Z</published>
    <title>Asymptotic optimality of adaptive importance sampling</title>
    <summary>  Adaptive importance sampling (AIS) uses past samples to update the
\textit{sampling policy} $q_t$ at each stage $t$. Each stage $t$ is formed with
two steps : (i) to explore the space with $n_t$ points according to $q_t$ and
(ii) to exploit the current amount of information to update the sampling
policy. The very fundamental question raised in this paper concerns the
behavior of empirical sums based on AIS. Without making any assumption on the
allocation policy $n_t$, the theory developed involves no restriction on the
split of computational resources between the explore (i) and the exploit (ii)
step. It is shown that AIS is asymptotically optimal : the asymptotic behavior
of AIS is the same as some "oracle" strategy that knows the targeted sampling
policy from the beginning. From a practical perspective, weighted AIS is
introduced, a new method that allows to forget poor samples from early stages.
</summary>
    <author>
      <name>Bernard Delyon</name>
    </author>
    <author>
      <name>François Portier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00989v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00989v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03972v3</id>
    <updated>2018-08-07T21:12:20Z</updated>
    <published>2018-06-06T19:21:09Z</published>
    <title>A Multi-task Deep Learning Architecture for Maritime Surveillance using
  AIS Data Streams</title>
    <summary>  In a world of global trading, maritime safety, security and efficiency are
crucial issues. We propose a multi-task deep learning framework for vessel
monitoring using Automatic Identification System (AIS) data streams. We combine
recurrent neural networks with latent variable modeling and an embedding of AIS
messages to a new representation space to jointly address key issues to be
dealt with when considering AIS data streams: massive amount of streaming data,
noisy data and irregular timesampling. We demonstrate the relevance of the
proposed deep learning framework on real AIS datasets for a three-task setting,
namely trajectory reconstruction, anomaly detection and vessel type
identification.
</summary>
    <author>
      <name>Duong Nguyen</name>
    </author>
    <author>
      <name>Rodolphe Vadaine</name>
    </author>
    <author>
      <name>Guillaume Hajduch</name>
    </author>
    <author>
      <name>René Garello</name>
    </author>
    <author>
      <name>Ronan Fablet</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DSAA.2018.00044</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DSAA.2018.00044" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE DSAA 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.03972v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03972v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03454v1</id>
    <updated>2018-08-10T08:40:32Z</updated>
    <published>2018-08-10T08:40:32Z</published>
    <title>AIQ: Measuring Intelligence of Business AI Software</title>
    <summary>  Focusing on Business AI, this article introduces the AIQ quadrant that
enables us to measure AI for business applications in a relative comparative
manner, i.e. to judge that software A has more or less intelligence than
software B. Recognizing that the goal of Business software is to maximize value
in terms of business results, the dimensions of the quadrant are the key
factors that determine the business value of AI software: Level of Output
Quality (Smartness) and Level of Automation. The use of the quadrant is
illustrated by several software solutions to support the real life business
challenge of field service scheduling. The role of machine learning and
conversational digital assistants in increasing the business value are also
discussed and illustrated with a recent integration of existing intelligent
digital assistants for factory floor decision making with the new version of
Google Glass. Such hands free AI solutions elevate the AIQ level to its
ultimate position.
</summary>
    <author>
      <name>Moshe BenBassat</name>
    </author>
    <link href="http://arxiv.org/abs/1808.03454v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03454v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.08597v1</id>
    <updated>2018-12-13T12:45:26Z</updated>
    <published>2018-12-13T12:45:26Z</published>
    <title>Interaction Design for Explainable AI: Workshop Proceedings</title>
    <summary>  As artificial intelligence (AI) systems become increasingly complex and
ubiquitous, these systems will be responsible for making decisions that
directly affect individuals and society as a whole. Such decisions will need to
be justified due to ethical concerns as well as trust, but achieving this has
become difficult due to the `black-box' nature many AI models have adopted.
Explainable AI (XAI) can potentially address this problem by explaining its
actions, decisions and behaviours of the system to users. However, much
research in XAI is done in a vacuum using only the researchers' intuition of
what constitutes a `good' explanation while ignoring the interaction and the
human aspect. This workshop invites researchers in the HCI community and
related fields to have a discourse about human-centred approaches to XAI rooted
in interaction and to shed light and spark discussion on interaction design
challenges in XAI.
</summary>
    <author>
      <name>Prashan Madumal</name>
    </author>
    <author>
      <name>Ronal Singh</name>
    </author>
    <author>
      <name>Joshua Newn</name>
    </author>
    <author>
      <name>Frank Vetere</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop proceedings of Interaction Design for Explainable AI
  workshop held at OzCHI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.08597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.08597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.10578v1</id>
    <updated>2018-12-27T00:00:49Z</updated>
    <published>2018-12-27T00:00:49Z</published>
    <title>Towards effective AI-powered agile project management</title>
    <summary>  The rise of Artificial intelligence (AI) has the potential to significantly
transform the practice of project management. Project management has a large
socio-technical element with many uncertainties arising from variability in
human aspects e.g., customers' needs, developers' performance and team
dynamics. AI can assist project managers and team members by automating
repetitive, high-volume tasks to enable project analytics for estimation and
risk prediction, providing actionable recommendations, and even making
decisions. AI is potentially a game changer for project management in helping
to accelerate productivity and increase project success rates. In this paper,
we propose a framework where AI technologies can be leveraged to offer support
for managing agile projects, which have become increasingly popular in the
industry.
</summary>
    <author>
      <name>Hoa Khanh Dam</name>
    </author>
    <author>
      <name>Truyen Tran</name>
    </author>
    <author>
      <name>John Grundy</name>
    </author>
    <author>
      <name>Aditya Ghose</name>
    </author>
    <author>
      <name>Yasutaka Kamei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of International Conference on Software Engineering
  (ICSE 2019), (To appear), NIER track, May 2019 (Montreal, Canada)</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.10578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.10578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.01876v1</id>
    <updated>2019-02-05T19:16:17Z</updated>
    <published>2019-02-05T19:16:17Z</published>
    <title>Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of
  Key Ideas and Publications, and Bibliography for Explainable AI</title>
    <summary>  This is an integrative review that address the question, "What makes for a
good explanation?" with reference to AI systems. Pertinent literatures are
vast. Thus, this review is necessarily selective. That said, most of the key
concepts and issues are expressed in this Report. The Report encapsulates the
history of computer science efforts to create systems that explain and instruct
(intelligent tutoring systems and expert systems). The Report expresses the
explainability issues and challenges in modern AI, and presents capsule views
of the leading psychological theories of explanation. Certain articles stand
out by virtue of their particular relevance to XAI, and their methods, results,
and key points are highlighted. It is recommended that AI/XAI researchers be
encouraged to include in their research reports fuller details on their
empirical or experimental methods, in the fashion of experimental psychology
research reports: details on Participants, Instructions, Procedures, Tasks,
Dependent Variables (operational definitions of the measures and metrics),
Independent Variables (conditions), and Control Conditions.
</summary>
    <author>
      <name>Shane T. Mueller</name>
    </author>
    <author>
      <name>Robert R. Hoffman</name>
    </author>
    <author>
      <name>William Clancey</name>
    </author>
    <author>
      <name>Abigail Emrey</name>
    </author>
    <author>
      <name>Gary Klein</name>
    </author>
    <link href="http://arxiv.org/abs/1902.01876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.01876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.03171v1</id>
    <updated>2019-02-25T19:30:56Z</updated>
    <published>2019-02-25T19:30:56Z</published>
    <title>Challenges for an Ontology of Artificial Intelligence</title>
    <summary>  Of primary importance in formulating a response to the increasing prevalence
and power of artificial intelligence (AI) applications in society are questions
of ontology. Questions such as: What "are" these systems? How are they to be
regarded? How does an algorithm come to be regarded as an agent? We discuss
three factors which hinder discussion and obscure attempts to form a clear
ontology of AI: (1) the various and evolving definitions of AI, (2) the
tendency for pre-existing technologies to be assimilated and regarded as
"normal," and (3) the tendency of human beings to anthropomorphize. This list
is not intended as exhaustive, nor is it seen to preclude entirely a clear
ontology, however, these challenges are a necessary set of topics for
consideration. Each of these factors is seen to present a 'moving target' for
discussion, which poses a challenge for both technical specialists and
non-practitioners of AI systems development (e.g., philosophers and
theologians) to speak meaningfully given that the corpus of AI structures and
capabilities evolves at a rapid pace. Finally, we present avenues for moving
forward, including opportunities for collaborative synthesis for scholars in
philosophy and science.
</summary>
    <author>
      <name>Scott H. Hawley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, accepted for publication in Journal of the American
  Scientific Affiliation. In press, expected publication March 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.03171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.03171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; K.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.09130v1</id>
    <updated>2019-05-22T13:34:45Z</updated>
    <published>2019-05-22T13:34:45Z</published>
    <title>AI-CARGO: A Data-Driven Air-Cargo Revenue Management System</title>
    <summary>  We propose AI-CARGO, a revenue management system for air-cargo that combines
machine learning prediction with decision-making using mathematical
optimization methods. AI-CARGO addresses a problem that is unique to the
air-cargo business, namely the wide discrepancy between the quantity (weight or
volume) that a shipper will book and the actual received amount at departure
time by the airline. The discrepancy results in sub-optimal and inefficient
behavior by both the shipper and the airline resulting in the overall loss of
potential revenue for the airline. AI-CARGO also includes a data cleaning
component to deal with the heterogeneous forms in which booking data is
transmitted to the airline cargo system. AI-CARGO is deployed in the production
environment of a large commercial airline company. We have validated the
benefits of AI-CARGO using real and synthetic datasets. Especially, we have
carried out simulations using dynamic programming techniques to elicit the
impact on offloading costs and revenue generation of our proposed system. Our
results suggest that combining prediction within a decision-making framework
can help dramatically to reduce offloading costs and optimize revenue
generation.
</summary>
    <author>
      <name>Stefano Giovanni Rizzo</name>
    </author>
    <author>
      <name>Ji Lucas</name>
    </author>
    <author>
      <name>Zoi Kaoudi</name>
    </author>
    <author>
      <name>Jorge-Arnulfo Quiane-Ruiz</name>
    </author>
    <author>
      <name>Sanjay Chawla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.09130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.09130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.00430v1</id>
    <updated>2019-06-30T18:55:31Z</updated>
    <published>2019-06-30T18:55:31Z</published>
    <title>Requisite Variety in Ethical Utility Functions for AI Value Alignment</title>
    <summary>  Being a complex subject of major importance in AI Safety research, value
alignment has been studied from various perspectives in the last years.
However, no final consensus on the design of ethical utility functions
facilitating AI value alignment has been achieved yet. Given the urgency to
identify systematic solutions, we postulate that it might be useful to start
with the simple fact that for the utility function of an AI not to violate
human ethical intuitions, it trivially has to be a model of these intuitions
and reflect their variety $ - $ whereby the most accurate models pertaining to
human entities being biological organisms equipped with a brain constructing
concepts like moral judgements, are scientific models. Thus, in order to better
assess the variety of human morality, we perform a transdisciplinary analysis
applying a security mindset to the issue and summarizing variety-relevant
background knowledge from neuroscience and psychology. We complement this
information by linking it to augmented utilitarianism as a suitable ethical
framework. Based on that, we propose first practical guidelines for the design
of approximate ethical goal functions that might better capture the variety of
human moral judgements. Finally, we conclude and address future possible
challenges.
</summary>
    <author>
      <name>Nadisha-Marie Aliman</name>
    </author>
    <author>
      <name>Leon Kester</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2019 AI Safety Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.00430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.00430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.10424v1</id>
    <updated>2019-07-22T15:57:00Z</updated>
    <published>2019-07-22T15:57:00Z</published>
    <title>Less (Data) Is More: Why Small Data Holds the Key to the Future of
  Artificial Intelligence</title>
    <summary>  The claims that big data holds the key to enterprise successes and that
Artificial Intelligence is going to replace humanity have become increasingly
more popular over the past few years, both in academia and in the industry.
However, while these claims may indeed capture some truth, they have also been
massively oversold, or so we contend here. The goal of this paper is two-fold.
First, we provide a qualified defence of the value of less data within the
context of AI. This is done by carefully reviewing two distinct problems for
big data driven AI, namely a) the limited track record of Deep Learning in key
areas such as Natural Language Processing, b) the regulatory and business
significance of being able to learn from few data points. Second, we briefly
sketch what we refer to as a case of AI with humans and for humans, namely an
AI paradigm whereby the systems we build are privacy-oriented and focused on
human-machine collaboration, not competition. Combining our claims above, we
conclude that when seen through the lens of cognitively inspired AI, the bright
future of the discipline is about less data, not more, and more humans, not
fewer.
</summary>
    <author>
      <name>Ciro Greco</name>
    </author>
    <author>
      <name>Andrea Polonioli</name>
    </author>
    <author>
      <name>Jacopo Tagliabue</name>
    </author>
    <link href="http://arxiv.org/abs/1907.10424v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.10424v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.02272v1</id>
    <updated>2020-11-02T20:04:18Z</updated>
    <published>2020-11-02T20:04:18Z</published>
    <title>Trustworthy AI</title>
    <summary>  Modern AI systems are reaping the advantage of novel learning methods. With
their increasing usage, we are realizing the limitations and shortfalls of
these systems. Brittleness to minor adversarial changes in the input data,
ability to explain the decisions, address the bias in their training data, high
opacity in terms of revealing the lineage of the system, how they were trained
and tested, and under which parameters and conditions they can reliably
guarantee a certain level of performance, are some of the most prominent
limitations. Ensuring the privacy and security of the data, assigning
appropriate credits to data sources, and delivering decent outputs are also
required features of an AI system. We propose the tutorial on Trustworthy AI to
address six critical issues in enhancing user and public trust in AI systems,
namely: (i) bias and fairness, (ii) explainability, (iii) robust mitigation of
adversarial attacks, (iv) improved privacy and security in model building, (v)
being decent, and (vi) model attribution, including the right level of credit
assignment to the data sources, model architectures, and transparency in
lineage.
</summary>
    <author>
      <name>Richa Singh</name>
    </author>
    <author>
      <name>Mayank Vatsa</name>
    </author>
    <author>
      <name>Nalini Ratha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM CODS-COMAD 2021 Tutorial</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.02272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.02272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.04105v1</id>
    <updated>2020-11-08T23:33:12Z</updated>
    <published>2020-11-08T23:33:12Z</published>
    <title>Evolution of Artificial Intelligent Plane</title>
    <summary>  With the growth of the internet, it is becoming hard to manage, configure and
monitor networks. Recent trends to control and operate them is artificial
intelligence based automation to minimize human intervention. Albeit this
concept has been introduced since a decade with several different names, but
the underlying goal remains the same, which is to make network intelligent
enough to assemble, reassemble if configuration changes, and detect a problem
on its own and fix it. As a result, in addition to Data Plane, Control Plane
and Management Plane, a new plane called Artificial Intelligence (AI) Plane is
introduced. Our main objective is to analyze all major AI plane techniques,
frameworks and algorithms proposed in various types of networks. We propose a
comprehensive and network independent framework to cover all aspects of AI
plane, in particular we provide a systematically means of comparison. In
conjunction to make AI plane understand simpler, this framework highlights
relevant challenges and design considerations for future research. To the best
of our knowledge this is the first survey report which represents a complete
comparison of AI planes with their investigation issues in several types of
networks.
</summary>
    <author>
      <name>Puneet Kumar</name>
    </author>
    <link href="http://arxiv.org/abs/2011.04105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.04105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.11484v1</id>
    <updated>2020-11-19T04:34:45Z</updated>
    <published>2020-11-19T04:34:45Z</published>
    <title>A Theory on AI Uncertainty Based on Rademacher Complexity and Shannon
  Entropy</title>
    <summary>  In this paper, we present a theoretical discussion on AI deep learning neural
network uncertainty investigation based on the classical Rademacher complexity
and Shannon entropy. First it is shown that the classical Rademacher complexity
and Shannon entropy is closely related by quantity by definitions. Secondly
based on the Shannon mathematical theory on communication [3], we derive a
criteria to ensure AI correctness and accuracy in classifications problems.
Last but not the least based on Peter Barlette's work, we show both a relaxing
condition and a stricter condition to guarantee the correctness and accuracy in
AI classification . By elucidating in this paper criteria condition in terms of
Shannon entropy based on Shannon theory, it becomes easier to explore other
criteria in terms of other complexity measurements such as Vapnik-Cheronenkis,
Gaussian complexity by taking advantage of the relations studies results in
other references. A close to 0.5 criteria on Shannon entropy is derived in this
paper for the theoretical investigation of AI accuracy and correctness for
classification problems.
</summary>
    <author>
      <name>Mingyong Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2011.11484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.11484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00047v1</id>
    <updated>2015-01-31T00:40:14Z</updated>
    <published>2015-01-31T00:40:14Z</published>
    <title>Laser-Ranging Long Baseline Differential Atom Interferometers for Space</title>
    <summary>  High sensitivity differential atom interferometers are promising for
precision measurements in science frontiers in space, including gravity field
mapping for Earth science studies and gravitational wave detection. We propose
a new configuration of twin atom interferometers connected by a laser ranging
interferometer (LRI-AI) to provide precise information of the displacements
between the two AI reference mirrors and a means to phase-lock the two
independent interferometer lasers over long distances, thereby further
enhancing the feasibility of long baseline differential atom interferometers.
We show that a properly implemented LRI-AI can achieve equivalent functionality
to the conventional differential atom interferometer measurement system. LRI-AI
isolates the laser requirements for atom interferometers and for optical phase
readout between distant locations, thus enabling optimized allocation of
available laser power within a limited physical size and resource budget. A
unique aspect of LRI-AI also enables extended dynamic range of differential
signals and the highest possible effective data rate.
</summary>
    <author>
      <name>Sheng-wey Chiow</name>
    </author>
    <author>
      <name>Jason Williams</name>
    </author>
    <author>
      <name>Nan Yu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1103/PhysRevA.92.063613</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1103/PhysRevA.92.063613" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Phys. Rev. A 92, 063613 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1502.00047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.atom-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.atom-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00494v4</id>
    <updated>2019-02-13T09:14:55Z</updated>
    <published>2016-10-03T11:15:12Z</published>
    <title>One-Trial Correction of Legacy AI Systems and Stochastic Separation
  Theorems</title>
    <summary>  We consider the problem of efficient "on the fly" tuning of existing, or {\it
legacy}, Artificial Intelligence (AI) systems. The legacy AI systems are
allowed to be of arbitrary class, albeit the data they are using for computing
interim or final decision responses should posses an underlying structure of a
high-dimensional topological real vector space. The tuning method that we
propose enables dealing with errors without the need to re-train the system.
Instead of re-training a simple cascade of perceptron nodes is added to the
legacy system. The added cascade modulates the AI legacy system's decisions. If
applied repeatedly, the process results in a network of modulating rules
"dressing up" and improving performance of existing AI systems. Mathematical
rationale behind the method is based on the fundamental property of measure
concentration in high dimensional spaces. The method is illustrated with an
example of fine-tuning a deep convolutional network that has been pre-trained
to detect pedestrians in images.
</summary>
    <author>
      <name>Alexander N. Gorban</name>
    </author>
    <author>
      <name>Ilya Romanenko</name>
    </author>
    <author>
      <name>Richard Burton</name>
    </author>
    <author>
      <name>Ivan Y. Tyukin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ins.2019.02.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ins.2019.02.001" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Sciences, 484, 237-254, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.00494v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00494v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10967v3</id>
    <updated>2018-03-01T20:52:33Z</updated>
    <published>2017-10-30T14:25:39Z</published>
    <title>Artificial Intelligence as Structural Estimation: Economic
  Interpretations of Deep Blue, Bonanza, and AlphaGo</title>
    <summary>  Artificial intelligence (AI) has achieved superhuman performance in a growing
number of tasks, but understanding and explaining AI remain challenging. This
paper clarifies the connections between machine-learning algorithms to develop
AIs and the econometrics of dynamic structural models through the case studies
of three famous game AIs. Chess-playing Deep Blue is a calibrated value
function, whereas shogi-playing Bonanza is an estimated value function via
Rust's (1987) nested fixed-point method. AlphaGo's "supervised-learning policy
network" is a deep neural network implementation of Hotz and Miller's (1993)
conditional choice probability estimation; its "reinforcement-learning value
network" is equivalent to Hotz, Miller, Sanders, and Smith's (1994) conditional
choice simulation method. Relaxing these AIs' implicit econometric assumptions
would improve their structural interpretability.
</summary>
    <author>
      <name>Mitsuru Igami</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1093/ectj/utaa005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1093/ectj/utaa005" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">the Econometrics Journal, 23:3 (September, 2020), S1-S24</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.10967v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10967v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.EM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03724v1</id>
    <updated>2017-12-11T11:31:28Z</updated>
    <published>2017-12-11T11:31:28Z</published>
    <title>Cogniculture: Towards a Better Human-Machine Co-evolution</title>
    <summary>  Research in Artificial Intelligence is breaking technology barriers every
day. New algorithms and high performance computing are making things possible
which we could only have imagined earlier. Though the enhancements in AI are
making life easier for human beings day by day, there is constant fear that AI
based systems will pose a threat to humanity. People in AI community have
diverse set of opinions regarding the pros and cons of AI mimicking human
behavior. Instead of worrying about AI advancements, we propose a novel idea of
cognitive agents, including both human and machines, living together in a
complex adaptive ecosystem, collaborating on human computation for producing
essential social goods while promoting sustenance, survival and evolution of
the agents' life cycle. We highlight several research challenges and technology
barriers in achieving this goal. We propose a governance mechanism around this
ecosystem to ensure ethical behaviors of all cognitive agents. Along with a
novel set of use-cases of Cogniculture, we discuss the road map ahead for this
journey.
</summary>
    <author>
      <name>Rakesh R Pimplikar</name>
    </author>
    <author>
      <name>Kushal Mukherjee</name>
    </author>
    <author>
      <name>Gyana Parija</name>
    </author>
    <author>
      <name>Harit Vishwakarma</name>
    </author>
    <author>
      <name>Ramasuri Narayanam</name>
    </author>
    <author>
      <name>Sarthak Ahuja</name>
    </author>
    <author>
      <name>Rohith D Vallam</name>
    </author>
    <author>
      <name>Ritwik Chaudhuri</name>
    </author>
    <author>
      <name>Joydeep Mondal</name>
    </author>
    <link href="http://arxiv.org/abs/1712.03724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.07819v1</id>
    <updated>2018-04-20T20:44:09Z</updated>
    <published>2018-04-20T20:44:09Z</published>
    <title>Understanding AI Data Repositories with Automatic Query Generation</title>
    <summary>  We describe a set of techniques to generate queries automatically based on
one or more ingested, input corpuses. These queries require no a priori domain
knowledge, and hence no human domain experts. Thus, these auto-generated
queries help address the epistemological question of how we know what we know,
or more precisely in this case, how an AI system with ingested data knows what
it knows. These auto-generated queries can also be used to identify and remedy
problem areas in ingested material -- areas for which the knowledge of the AI
system is incomplete or even erroneous. Similarly, the proposed techniques
facilitate tests of AI capability -- both in terms of coverage and accuracy. By
removing humans from the main learning loop, our approach also allows more
effective scaling of AI and cognitive capabilities to provide (1) broader
coverage in a single domain such as health or geology; and (2) more rapid
deployment to new domains. The proposed techniques also allow ingested
knowledge to be extended naturally. Our investigations are early, and this
paper provides a description of the techniques. Assessment of their efficacy is
our next step for future work.
</summary>
    <author>
      <name>Erik Altman</name>
    </author>
    <link href="http://arxiv.org/abs/1804.07819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.04399v1</id>
    <updated>2018-09-12T13:12:07Z</updated>
    <published>2018-09-12T13:12:07Z</published>
    <title>Artificial Intelligence for the Public Sector: Opportunities and
  challenges of cross-sector collaboration</title>
    <summary>  Public sector organisations are increasingly interested in using data science
and artificial intelligence capabilities to deliver policy and generate
efficiencies in high uncertainty environments. The long-term success of data
science and AI in the public sector relies on effectively embedding it into
delivery solutions for policy implementation. However, governments cannot do
this integration of AI into public service delivery on their own. The UK
Government Industrial Strategy is clear that delivering on the AI grand
challenge requires collaboration between universities and public and private
sectors. This cross-sectoral collaborative approach is the norm in applied AI
centres of excellence around the world. Despite their popularity, cross-sector
collaborations entail serious management challenges that hinder their success.
In this article we discuss the opportunities and challenges from AI for public
sector. Finally, we propose a series of strategies to successfully manage these
cross-sectoral collaborations.
</summary>
    <author>
      <name>Slava Jankin Mikhaylov</name>
    </author>
    <author>
      <name>Marc Esteve</name>
    </author>
    <author>
      <name>Averill Campion</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1098/rsta.2017.0357</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1098/rsta.2017.0357" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Philosophical Transactions of the Royal Society A, 2018, Volume
  376, Issue 2128</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.04399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.04399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.06338v1</id>
    <updated>2018-10-15T13:13:48Z</updated>
    <published>2018-10-15T13:13:48Z</published>
    <title>Towards Providing Explanations for AI Planner Decisions</title>
    <summary>  In order to engender trust in AI, humans must understand what an AI system is
trying to achieve, and why. To overcome this problem, the underlying AI process
must produce justifications and explanations that are both transparent and
comprehensible to the user. AI Planning is well placed to be able to address
this challenge. In this paper we present a methodology to provide initial
explanations for the decisions made by the planner. Explanations are created by
allowing the user to suggest alternative actions in plans and then compare the
resulting plans with the one found by the planner. The methodology is
implemented in the new XAI-Plan framework.
</summary>
    <author>
      <name>Rita Borgo</name>
    </author>
    <author>
      <name>Michael Cashmore</name>
    </author>
    <author>
      <name>Daniele Magazzeni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the IJCAI/ECAI 2018 Workshop on Explainable Artificial
  Intelligence (XAI)
  (http://home.earthlink.net/~dwaha/research/meetings/faim18-xai). Stockholm,
  July 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.06338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.06338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.06606v2</id>
    <updated>2018-11-19T01:48:49Z</updated>
    <published>2018-11-15T21:59:41Z</published>
    <title>Economics of Human-AI Ecosystem: Value Bias and Lost Utility in
  Multi-Dimensional Gaps</title>
    <summary>  In recent years, artificial intelligence (AI) decision-making and autonomous
systems became an integrated part of the economy, industry, and society. The
evolving economy of the human-AI ecosystem raising concerns regarding the risks
and values inherited in AI systems. This paper investigates the dynamics of
creation and exchange of values and points out gaps in perception of
cost-value, knowledge, space and time dimensions. It shows aspects of value
bias in human perception of achievements and costs that encoded in AI systems.
It also proposes rethinking hard goals definitions and cost-optimal
problem-solving principles in the lens of effectiveness and efficiency in the
development of trusted machines. The paper suggests a value-driven with cost
awareness strategy and principles for problem-solving and planning of effective
research progress to address real-world problems that involve diverse forms of
achievements, investments, and survival scenarios.
</summary>
    <author>
      <name>Daniel Muller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, typos corrected, examples added to Table 1</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.06606v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06606v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.02090v2</id>
    <updated>2019-06-26T08:37:53Z</updated>
    <published>2019-06-05T15:57:20Z</published>
    <title>Artificial Intelligence in Clinical Health Care Applications: Viewpoint</title>
    <summary>  The idea of Artificial Intelligence (AI) has a long history. It turned out,
however, that reaching intelligence at human levels is more complicated than
originally anticipated. Currently we are experiencing a renewed interest in AI,
fueled by an enormous increase in computing power and an even larger increase
in data, in combination with improved AI technologies like deep learning.
Healthcare is considered the next domain to be revolutionized by Artificial
Intelligence. While AI approaches are excellently suited to develop certain
algorithms, for biomedical applications there are specific challenges. We
propose recommendations to improve AI projects in the biomedical space and
especially clinical healthcare.
</summary>
    <author>
      <name>Michael van Hartskamp</name>
    </author>
    <author>
      <name>Sergio Consoli</name>
    </author>
    <author>
      <name>Wim Verhaegh</name>
    </author>
    <author>
      <name>Milan Petković</name>
    </author>
    <author>
      <name>Anja van de Stolpe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2196/12100</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2196/12100" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Medical Internet Research (2019), 21(4):e12100</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.02090v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02090v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.10244v1</id>
    <updated>2019-06-24T21:41:55Z</updated>
    <published>2019-06-24T21:41:55Z</published>
    <title>Generating User-friendly Explanations for Loan Denials using GANs</title>
    <summary>  Financial decisions impact our lives, and thus everyone from the regulator to
the consumer is interested in fair, sound, and explainable decisions. There is
increasing competitive desire and regulatory incentive to deploy AI mindfully
within financial services. An important mechanism towards that end is to
explain AI decisions to various stakeholders. State-of-the-art explainable AI
systems mostly serve AI engineers and offer little to no value to business
decision makers, customers, and other stakeholders. Towards addressing this
gap, in this work we consider the scenario of explaining loan denials. We build
the first-of-its-kind dataset that is representative of loan-applicant friendly
explanations. We design a novel Generative Adversarial Network (GAN) that can
accommodate smaller datasets, to generate user-friendly textual explanations.
We demonstrate how our system can also generate explanations serving different
purposes: those that help educate the loan applicants, or help them take
appropriate action towards a future approval.
</summary>
    <author>
      <name>Ramya Srinivasan</name>
    </author>
    <author>
      <name>Ajay Chander</name>
    </author>
    <author>
      <name>Pouya Pezeshkpour</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the NeurIPS 2018 Workshop on Challenges and
  Opportunities for AI in Financial Services: the Impact of Fairness,
  Explainability, Accuracy, and Privacy, Montreal, Canada</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.10244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01024v3</id>
    <updated>2019-08-09T23:17:52Z</updated>
    <published>2019-08-02T19:25:57Z</published>
    <title>What is the Point of Fairness? Disability, AI and The Complexity of
  Justice</title>
    <summary>  Work integrating conversations around AI and Disability is vital and valued,
particularly when done through a lens of fairness. Yet at the same time,
analyzing the ethical implications of AI for disabled people solely through the
lens of a singular idea of "fairness" risks reinforcing existing power
dynamics, either through reinforcing the position of existing medical
gatekeepers, or promoting tools and techniques that benefit
otherwise-privileged disabled people while harming those who are rendered
outliers in multiple ways. In this paper we present two case studies from
within computer vision - a subdiscipline of AI focused on training algorithms
that can "see" - of technologies putatively intended to help disabled people
but, through failures to consider structural injustices in their design, are
likely to result in harms not addressed by a "fairness" framing of ethics.
Drawing on disability studies and critical data science, we call on researchers
into AI ethics and disability to move beyond simplistic notions of fairness,
and towards notions of justice.
</summary>
    <author>
      <name>Cynthia L. Bennett</name>
    </author>
    <author>
      <name>Os Keyes</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01024v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01024v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
